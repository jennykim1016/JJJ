<id>
0705.0569v1
<category>
stat.AP
<abstract>
Longitudinal studies could be complicated by left-censored repeated measures.
For example, in Human Immunodeficiency Virus infection, there is a detection
limit of the assay used to quantify the plasma viral load. Simple imputation of
the limit of the detection or of half of this limit for left-censored measures
biases estimations and their standard errors. In this paper, we review two
likelihood-based methods proposed to handle left-censoring of the outcome in
linear mixed model. We show how to fit these models using SAS Proc NLMIXED and
we compare this tool with other programs. Indications and limitations of the
programs are discussed and an example in the field of HIV infection is shown.

<id>
0705.2515v1
<category>
stat.AP
<abstract>
This paper compares the Maximum-likelihood method and Bayesian method for
finite element model updating. The Maximum-likelihood method was implemented
using genetic algorithm while the Bayesian method was implemented using the
Markov Chain Monte Carlo. These methods were tested on a simple beam and an
unsymmetrical H-shaped structure. The results show that the Bayesian method
gave updated finite element models that predicted more accurate modal
properties than the updated finite element models obtained through the use of
the Maximum-likelihood method. Furthermore, both these methods were found to
require the same levels of computational loads.

<id>
0705.3257v2
<category>
stat.AP
<abstract>
We present a quantitative analysis of throwing ability for major league
outfielders and catchers. We use detailed game event data to tabulate success
and failure events in outfielder and catcher throwing opportunities. We
attribute a run contribution to each success or failure which are tabulated for
each player in each season. We use four seasons of data to estimate the overall
throwing ability of each player using a Bayesian hierarchical model. This model
allows us to shrink individual player estimates towards an overall population
mean depending on the number of opportunities for each player. We use the
posterior distribution of player abilities from this model to identify players
with significant positive and negative throwing contributions.

<id>
0707.0462v1
<category>
stat.AP
<abstract>
Probability models are proposed for passage time data collected in
experiments with a device designed to measure particle flow during aerial
application of fertilizer. Maximum likelihood estimation of flow intensity is
reviewed for the simple linear Boolean model, which arises with the assumption
that each particle requires the same known passage time. M-estimation is
developed for a generalization of the model in which passage times behave as a
random sample from a distribution with a known mean. The generalized model
improves fit in these experiments. An estimator of total particle flow is
constructed by conditioning on lengths of multi-particle clumps.

<id>
0707.0600v1
<category>
stat.AP
<abstract>
A two-sex Basic Reproduction Number (BRN) is used to investigate the
conditions under which the Human Immunodeficiency Virus (HIV) may spread
through heterosexual contacts in Sub-Saharan Africa. (The BRN is the expected
number of new infections generated by one infected individual; the disease
spreads if the BRN is larger than 1). A simple analytical expression for the
BRN is derived on the basis of recent data on survival rates, transmission
probabilities, and levels of sexual activity. Baseline results show that in the
population at large (characterized by equal numbers of men and women) the BRN
is larger than 1 if every year each person has 82 sexual contacts with
different partners. the BRN is also larger than 1 for commercial sex workers
(CSWs) and their clients (two populations of different sizes) if each CSW has
about 256 clients per year and each client visits one CSW every two weeks. A
sensitivity analysis explores the effect on the BRN of a doubling (or a
halving) of the transmission probabilities. Implications and extensions are
discussed.

<id>
0707.3013v1
<category>
stat.AP
<abstract>
This paper defines and implements a non-Bayesian fusion rule for combining
densities of probabilities estimated by local (non-linear) filters for tracking
a moving target by passive sensors. This rule is the restriction to a strict
probabilistic paradigm of the recent and efficient Proportional Conflict
Redistribution rule no 5 (PCR5) developed in the DSmT framework for fusing
basic belief assignments. A sampling method for probabilistic PCR5 (p-PCR5) is
defined. It is shown that p-PCR5 is more robust to an erroneous modeling and
allows to keep the modes of local densities and preserve as much as possible
the whole information inherent to each densities to combine. In particular,
p-PCR5 is able of maintaining multiple hypotheses/modes after fusion, when the
hypotheses are too distant in regards to their deviations. This new p-PCR5 rule
has been tested on a simple example of distributed non-linear filtering
application to show the interest of such approach for future developments. The
non-linear distributed filter is implemented through a basic particles
filtering technique. The results obtained in our simulations show the ability
of this p-PCR5-based filter to track the target even when the models are not
well consistent in regards to the initialization and real cinematic.

<id>
0709.0165v1
<category>
stat.AP
<abstract>
In high-throughput genomics, large-scale designed experiments are becoming
common, and analysis approaches based on highly multivariate regression and
anova concepts are key tools. Shrinkage models of one form or another can
provide comprehensive approaches to the problems of simultaneous inference that
involve implicit multiple comparisons over the many, many parameters
representing effects of design factors and covariates. We use such approaches
here in a study of cardiovascular genomics. The primary experimental context
concerns a carefully designed, and rich, gene expression study focused on
gene-environment interactions, with the goals of identifying genes implicated
in connection with disease states and known risk factors, and in generating
expression signatures as proxies for such risk factors. A coupled exploratory
analysis investigates cross-species extrapolation of gene expression
signatures--how these mouse-model signatures translate to humans. The latter
involves exploration of sparse latent factor analysis of human observational
data and of how it relates to projected risk signatures derived in the animal
models. The study also highlights a range of applied statistical and genomic
data analysis issues, including model specification, computational questions
and model-based correction of experimental artifacts in DNA microarray data.

<id>
0709.0366v1
<category>
stat.AP
<abstract>
The Bonferroni multiple testing procedure is commonly perceived as being
overly conservative in large-scale simultaneous testing situations such as
those that arise in microarray data analysis. The objective of the present
study is to show that this popular belief is due to overly stringent
requirements that are typically imposed on the procedure rather than to its
conservative nature. To get over its notorious conservatism, we advocate using
the Bonferroni selection rule as a procedure that controls the per family error
rate (PFER). The present paper reports the first study of stability properties
of the Bonferroni and Benjamini--Hochberg procedures. The Bonferroni procedure
shows a superior stability in terms of the variance of both the number of true
discoveries and the total number of discoveries, a property that is especially
important in the presence of correlations between individual $p$-values. Its
stability and the ability to provide strong control of the PFER make the
Bonferroni procedure an attractive choice in microarray studies.

<id>
0709.0394v1
<category>
stat.AP
<abstract>
The spatial dependence of total column ozone varies strongly with latitude,
so that homogeneous models (invariant to all rotations) are clearly unsuitable.
However, an assumption of axial symmetry, which means that the process model is
invariant to rotations about the Earth's axis, is much more plausible and
considerably simplifies the modeling. Using TOMS (Total Ozone Mapping
Spectrometer) measurements of total column ozone over a six-day period, this
work investigates the modeling of axially symmetric processes on the sphere
using expansions in spherical harmonics. It turns out that one can capture many
of the large scale features of the spatial covariance structure using a
relatively small number of terms in such an expansion, but the resulting fitted
model provides a horrible fit to the data when evaluated via its likelihood
because of its inability to describe accurately the process's local behavior.
Thus, there remains the challenge of developing computationally tractable
models that capture both the large and small scale structure of these data.

<id>
0709.0406v1
<category>
stat.AP
<abstract>
Early detection of person-to-person transmission of emerging infectious
diseases such as avian influenza is crucial for containing pandemics. We
developed a simple permutation test and its refined version for this purpose. A
simulation study shows that the refined permutation test is as powerful as or
outcompetes the conventional test built on asymptotic theory, especially when
the sample size is small. In addition, our resampling methods can be applied to
a broad range of problems where an asymptotic test is not available or fails.
We also found that decent statistical power could be attained with just a small
number of cases, if the disease is moderately transmissible between humans.

<id>
0709.0421v1
<category>
stat.AP
<abstract>
The Joint United Nations Programme on HIV/AIDS (UNAIDS) has developed the
Estimation and Projection Package (EPP) for making national estimates and
short-term projections of HIV prevalence based on observed prevalence trends at
antenatal clinics. Assessing the uncertainty about its estimates and
projections is important for informed policy decision making, and we propose
the use of Bayesian melding for this purpose. Prevalence data and other
information about the EPP model's input parameters are used to derive a
probabilistic HIV prevalence projection, namely a probability distribution over
a set of future prevalence trajectories. We relate antenatal clinic prevalence
to population prevalence and account for variability between clinics using a
random effects model. Predictive intervals for clinic prevalence are derived
for checking the model. We discuss predictions given by the EPP model and the
results of the Bayesian melding procedure for Uganda, where prevalence peaked
at around 28% in 1990; the 95% prediction interval for 2010 ranges from 2% to
7%.

<id>
0709.0427v1
<category>
stat.AP
<abstract>
Storm surge, the onshore rush of sea water caused by the high winds and low
pressure associated with a hurricane, can compound the effects of inland
flooding caused by rainfall, leading to loss of property and loss of life for
residents of coastal areas. Numerical ocean models are essential for creating
storm surge forecasts for coastal areas. These models are driven primarily by
the surface wind forcings. Currently, the gridded wind fields used by ocean
models are specified by deterministic formulas that are based on the central
pressure and location of the storm center. While these equations incorporate
important physical knowledge about the structure of hurricane surface wind
fields, they cannot always capture the asymmetric and dynamic nature of a
hurricane. A new Bayesian multivariate spatial statistical modeling framework
is introduced combining data with physical knowledge about the wind fields to
improve the estimation of the wind vectors. Many spatial models assume the data
follow a Gaussian distribution. However, this may be overly-restrictive for
wind fields data which often display erratic behavior, such as sudden changes
in time or space. In this paper we develop a semiparametric multivariate
spatial model for these data. Our model builds on the stick-breaking prior,
which is frequently used in Bayesian modeling to capture uncertainty in the
parametric form of an outcome. The stick-breaking prior is extended to the
spatial setting by assigning each location a different, unknown distribution,
and smoothing the distributions in space with a series of kernel functions.
This semiparametric spatial model is shown to improve prediction compared to
usual Bayesian Kriging methods for the wind field of Hurricane Ivan.

<id>
0709.1307v1
<category>
stat.AP
<abstract>
We propose a new statistics for the detection of differentially expressed
genes, when the genes are activated only in a subset of the samples. Statistics
designed for this unconventional circumstance has proved to be valuable for
most cancer studies, where oncogenes are activated for a small number of
disease samples. Previous efforts made in this direction include COPA, OS and
ORT. We propose a new statistics called maximum ordered subset t-statistics
(MOST) which seems to be natural when the number of activated samples is
unknown. We compare MOST to other statistics and find the proposed method often
has more power then its competitors.

<id>
0709.1640v1
<category>
stat.AP
<abstract>
This paper introduces a novel paradigm to impute missing data that combines a
decision tree with an auto-associative neural network (AANN) based model and a
principal component analysis-neural network (PCA-NN) based model. For each
model, the decision tree is used to predict search bounds for a genetic
algorithm that minimize an error function derived from the respective model.
The models' ability to impute missing data is tested and compared using HIV
sero-prevalance data. Results indicate an average increase in accuracy of 13%
with the AANN based model's average accuracy increasing from 75.8% to 86.3%
while that of the PCA-NN based model increasing from 66.1% to 81.6%.

<id>
0709.4166v2
<category>
stat.AP
<abstract>
A wealth of epidemiological data suggests an association between
mortality/morbidity from pulmonary and cardiovascular adverse events and air
pollution, but uncertainty remains as to the extent implied by those
associations although the abundance of the data. In this paper we describe an
SSA (Singular Spectrum Analysis) based approach in order to decompose the
time-series of particulate matter concentration into a set of exposure
variables, each one representing a different timescale. We implement our
methodology to investigate both acute and long-term effects of $PM_{10}$
exposure on morbidity from respiratory causes within the urban area of Bari,
Italy.

<id>
0710.0559v1
<category>
stat.AP
<abstract>
The problem addressed in this article is the bias to income and expenditure
elasticities estimated on pseudo-panel data caused by measurement error and
unobserved heterogeneity. We gauge empirically these biases by comparing
cross-sectional, pseudo-panel and true panel data from both Polish and American
expenditure surveys. Our results suggest that unobserved heterogeneity imparts
a downward bias to cross-section estimates of income elasticities of at-home
food expenditures and an upward bias to estimates of income elasticities of
away-from-home food expenditures. "Within" and first-difference estimators
suffer less bias, but only if the effects of measurement error are accounted
for with instrumental variables.

<id>
0710.0849v1
<category>
stat.AP
<abstract>
We test against two different sets of data an apparently new approach to the
analysis of the variance of a numerical variable which depends on qualitative
characters. We suggest that this approach be used to complement other existing
techniques to study the interdependence of the variables involved. According to
our method the variance is expressed as a sum of orthogonal components,
obtained as differences of conditional means, with respect to the qualitative
characters. The resulting expression for the variance depends on the ordering
in which the characters are considered. We suggest an algorithm which leads to
an ordering which is deemed natural. The first set of data concerns the score
achieved by a population of students, on an entrance examination, based on a
multiple choice test with 30 questions. In this case the qualitative characters
are dyadic and correspond to correct or incorrect answer to each question. The
second set of data concerns the delay in obtaining the degree for a population
of graduates of Italian universities. The variance in this case is analyzed
with respect to a set of seven specific qualitative characters of the
population studied (gender, previous education, working condition, parent's
educational level, field of study, etc.)

<id>
0710.2740v1
<category>
stat.AP
<abstract>
This paper consider the problem of determining the reliability of a software
system which can be decomposed in a number of modules. We have derived the
expression of the reliability of a system using the Markovian model for the
transfer of control between modules in order. We have given the expression of
reliability by considering both benign and catastrophic failure. The expression
of reliability presented in this work is applicable for some control software
which are designed to detect its own internal errors.

<id>
0710.3447v1
<category>
stat.AP
<abstract>
Some problems of testology are discussed.

<id>
0710.4171v1
<category>
stat.AP
<abstract>
Least mean square-partial parallel interference cancelation (LMS-PPIC) is a
partial interference cancelation using adaptive multistage structure in which
the normalized least mean square (NLMS) adaptive algorithm is engaged to obtain
the cancelation weights. The performance of the NLMS algorithm is mostly
dependent to its step-size. A fixed and non-optimized step-size causes the
propagation of error from one stage to the next one. When all user channels are
balanced, the unit magnitude is the principal property of the cancelation
weight elements. Based on this fact and using a set of NLMS algorithms with
different step-sizes, the parallel LMS-PPIC (PLMS-PPIC) method is proposed. In
each iteration of the algorithm, the parameter estimate of the NLMS algorithm
is chosen to match the elements' magnitudes of the cancelation weight estimate
with unity. Simulation results are given to compare the performance of our
method with the LMS-PPIC algorithm in three cases: balanced channel, unbalanced
channel and time varying channel.

<id>
0710.4172v1
<category>
stat.AP
<abstract>
Parallel least mean square-partial parallel interference cancelation
(PLMS-PPIC) is a partial interference cancelation which employs adaptive
multistage structure. In this algorithm the channel phases for all users are
assumed to be known. Having only their quarters in (0,2\pi), a modified version
of PLMS-PPIC is proposed in this paper to simultaneously estimate the channel
phases and the cancelation weights. Simulation examples are given in the cases
of balanced, unbalanced and time varying channels to show the performance of
the modified PLMS-PPIC method.

<id>
0710.4173v1
<category>
stat.AP
<abstract>
Partial feedback in multiple-input multiple-output (MIMO) communication
systems provides tremendous capacity gain and enables the transmitter to
exploit channel condition and to eliminate channel interference. In the case of
severely limited feedback, constructing a quantized partial feedback is an
important issue. To reduce the computational complexity of the feedback system,
in this paper we introduce an adaptive partial method in which at the
transmitter, an easy to implement least square adaptive algorithm is engaged to
compute the channel state information. In this scheme at the receiver, the time
varying step-size is replied to the transmitter via a reliable feedback
channel. The transmitter iteratively employs this feedback information to
estimate the channel weights. This method is independent of the employed
space-time coding schemes and gives all channel components. Simulation examples
are given to evaluate the performance of the proposed method.

<id>
0710.4404v1
<category>
stat.AP
<abstract>
This paper provides an analysis of the effects of attrition and non-response
on employment and wages using the Canadian Survey of Labour and Income
Dynamics. We consider a structural model composed of three freely correlated
equations for nonattrition/response, employment and wages. The model is
estimated using microdata from 22,990 individuals who provided sufficient
information in the first wave of the 1996-2001 panel. The main findings of the
paper are that attrition is not random. Attritors and non-respondents likely
are less attached to employment and come from low-income population. The
correlation between non-attrition and employment is positive and statistically
significant, though small. Also, wage estimates are biased upwards. Observed
wages are on average higher than wages that would be observed if all the
individuals initially selected in the panel remained in the sample.

<id>
0710.5268v1
<category>
stat.AP
<abstract>
To evaluate the calibration of a disease risk prediction tool, the quantity
$E/O$, i.e., the ratio of the expected number of events to the observed number
of events, is generally computed. However, because of censoring, or more
precisely because of individuals who drop out before the termination of the
study, this quantity is generally unavailable for the complete population study
and an alternative estimate has to be computed. In this paper, we present and
compare four methods to do this. We show that two of the most commonly used
methods generally lead to biased estimates. Our arguments are first based on
some theoretic considerations. Then, we perform a simulation study to highlight
the magnitude of the previously mentioned biases. As a concluding example, we
evaluate the calibration of an existing predictive model for breast cancer on
the E3N-EPIC cohort.

<id>
0710.5805v1
<category>
stat.AP
<abstract>
This paper presents an approach to estimating the health effects of an
environmental hazard. The approach is general in nature, but is applied here to
the case of air pollution. It uses a computer model involving ambient pollution
and temperature inputs, to simulate the exposures experienced by individuals in
an urban area, whilst incorporating the mechanisms that determine exposures.
The output from the model comprises a set of daily exposures for a sample of
individuals from the population of interest. These daily exposures are
approximated by parametric distributions, so that the predictive exposure
distribution of a randomly selected individual can be generated. These
distributions are then incorporated into a hierarchical Bayesian framework
(with inference using Markov Chain Monte Carlo simulation) in order to examine
the relationship between short-term changes in exposures and health outcomes,
whilst making allowance for long-term trends, seasonality, the effect of
potential confounders and the possibility of ecological bias.
  The paper applies this approach to particulate pollution (PM$_{10}$) and
respiratory mortality counts for seniors in greater London ($\geq$65 years)
during 1997. Within this substantive epidemiological study, the effects on
health of ambient concentrations and (estimated) personal exposures are
compared.

<id>
0712.0660v1
<category>
stat.AP
<abstract>
The effect of vigorous physical activity on mortality in the elderly is
difficult to estimate using conventional approaches to causal inference that
define this effect by comparing the mortality risks corresponding to
hypothetical scenarios in which all subjects in the target population engage in
a given level of vigorous physical activity. A causal effect defined on the
basis of such a static treatment intervention can only be identified from
observed data if all subjects in the target population have a positive
probability of selecting each of the candidate treatment options, an assumption
that is highly unrealistic in this case since subjects with serious health
problems will not be able to engage in higher levels of vigorous physical
activity. This problem can be addressed by focusing instead on causal effects
that are defined on the basis of realistic individualized treatment rules and
intention-to-treat rules that explicitly take into account the set of treatment
options that are available to each subject. We present a data analysis to
illustrate that estimators of static causal effects in fact tend to
overestimate the beneficial impact of high levels of vigorous physical activity
while corresponding estimators based on realistic individualized treatment
rules and intention-to-treat rules can yield unbiased estimates. We emphasize
that the problems encountered in estimating static causal effects are not
restricted to the IPTW estimator, but are also observed with the
$G$-computation estimator, the DR-IPTW estimator, and the targeted MLE. Our
analyses based on realistic individualized treatment rules and
intention-to-treat rules suggest that high levels of vigorous physical activity
may confer reductions in mortality risk on the order of 15-30%, although in
most cases the evidence for such an effect does not quite reach the 0.05 level
of significance.

<id>
0712.0974v1
<category>
stat.AP
<abstract>
Forensic science is usually taken to mean the application of a broad spectrum
of scientific tools to answer questions of interest to the legal system.
Despite such popular television series as CSI: Crime Scene Investigation and
its spinoffs--CSI: Miami and CSI: New York--on which the forensic scientists
use the latest high-tech scientific tools to identify the perpetrator of a
crime and always in under an hour, forensic science is under assault, in the
public media, popular magazines [Talbot (2007), Toobin (2007)] and in the
scientific literature [Kennedy (2003), Saks and Koehler (2005)]. Ironically,
this growing controversy over forensic science has occurred precisely at the
time that DNA evidence has become the ``gold standard'' in the courts, leading
to the overturning of hundreds of convictions many of which were based on
clearly less credible forensic evidence, including eyewitness testimony [Berger
(2006)].

<id>
0712.1099v1
<category>
stat.AP
<abstract>
It is now widely accepted that forensic DNA profiles are rare, so it was a
surprise to some people that different people represented in offender databases
are being found to have the same profile. In the first place this is just an
illustration of the birthday problem, but a deeper analysis must take into
account dependencies among profiles caused by family or population membership.

<id>
0712.1106v1
<category>
stat.AP
<abstract>
Several different types of statistical interaction are defined and
distinguished, primarily on the basis of the nature of the factors defining the
interaction. Illustrative examples, mostly epidemiological, are given. The
emphasis is primarily on interpretation rather than on methods for detecting
interactions.

<id>
0712.1111v1
<category>
stat.AP
<abstract>
Recently there has been much interest in data that, in statistical language,
may be described as having a large crossed and severely unbalanced random
effects structure. Such data sets arise for recommender engines and information
retrieval problems. Many large bipartite weighted graphs have this structure
too. We would like to assess the stability of algorithms fit to such data. Even
for linear statistics, a naive form of bootstrap sampling can be seriously
misleading and McCullagh [Bernoulli 6 (2000) 285--301] has shown that no
bootstrap method is exact. We show that an alternative bootstrap separately
resampling rows and columns of the data matrix satisfies a mean consistency
property even in heteroscedastic crossed unbalanced random effects models. This
alternative does not require the user to fit a crossed random effects model to
the data.

<id>
0707.0167v1
<category>
stat.CO
<abstract>
The computation of the Tukey depth, also called halfspace depth, is very
demanding, even in low dimensional spaces, because it requires the
consideration of all possible one-dimensional projections. In this paper we
propose a random depth which approximates the Tukey depth. It only takes into
account a finite number of one-dimensional projections which are chosen at
random. Thus, this random depth requires a very small computation time even in
high dimensional spaces. Moreover, it is easily extended to cover the
functional framework.
  We present some simulations indicating how many projections should be
considered depending on the sample size and on the dimension of the sample
space. We also compare this depth with some others proposed in the literature.
It is noteworthy that the random depth, based on a very low number of
projections, obtains results very similar to those obtained with other depths.

<id>
0709.1721v1
<category>
stat.CO
<abstract>
Monte Carlo sampling methods often suffer from long correlation times.
Consequently, these methods must be run for many steps to generate an
independent sample. In this paper a method is proposed to overcome this
difficulty. The method utilizes information from rapidly equilibrating coarse
Markov chains that sample marginal distributions of the full system. This is
accomplished through exchanges between the full chain and the auxiliary coarse
chains. Results of numerical tests on the bridge sampling and
filtering/smoothing problems for a stochastic differential equation are
presented.

<id>
0709.3560v7
<category>
stat.CO
<abstract>
The super-parametric density estimators and its related algorism were
suggested by Y. -S. Tsai et al [7]. The number of parameters is unlimited in
the super- parametric estimators and it is a general theory in sense of
unifying or connecting nonparametric and parametric estimators. Before applying
to numerical examples, we can not give any comment of the estimators. In this
paper, we will focus on the implementation, the computer programming, of the
algorism and strategies of choosing window functions. B-splines, Bezier splines
and covering windows are studied as well. According to the criterion of the
convergence conditions for Parzen window, the number of the window functions
shall be, roughly, proportional to the number of samples and so is the number
of the variables. Since the algorism is designed for solving the optimization
of likelihood function, there will be a set of nonlinear equations with a large
number of variables. The results show that algorism suggested by Y. -S. Tsai is
very powerful and effective in the sense of mathematics, that is, the iteration
procedures converge and the rates of convergence are very fast. Also, the
numerical results of different window functions show that the approach of
super-parametric density estimators has ushered a new era of statistics.

<id>
0710.4242v4
<category>
stat.CO
<abstract>
In this paper, we propose an adaptive algorithm that iteratively updates both
the weights and component parameters of a mixture importance sampling density
so as to optimise the importance sampling performances, as measured by an
entropy criterion. The method is shown to be applicable to a wide class of
importance sampling densities, which includes in particular mixtures of
multivariate Student t distributions. The performances of the proposed scheme
are studied on both artificial and real examples, highlighting in particular
the benefit of a novel Rao-Blackwellisation device which can be easily
incorporated in the updating scheme.

<id>
0710.5098v1
<category>
stat.CO
<abstract>
We consider multiscale stochastic systems that are partially observed at
discrete points of the slow time scale. We introduce a particle filter that
takes advantage of the multiscale structure of the system to efficiently
approximate the optimal filter.

<id>
0710.5670v2
<category>
stat.CO
<abstract>
Generating multivariate Poisson data is essential in many applications.
Current simulation methods suffer from limitations ranging from computational
complexity to restrictions on the structure of the correlation matrix. We
propose a computationally efficient and conceptually appealing method for
generating multivariate Poisson data. The method is based on simulating
multivariate Normal data and converting them to achieve a specific correlation
matrix and Poisson rate vector. This allows for generating data that have
positive or negative correlations as well as different rates.

<id>
0801.3552v3
<category>
stat.CO
<abstract>
This paper considers the problem of cardinality estimation in data stream
applications. We present a statistical analysis of probabilistic counting
algorithms, focusing on two techniques that use pseudo-random variates to form
low-dimensional data sketches. We apply conventional statistical methods to
compare probabilistic algorithms based on storing either selected order
statistics, or random projections. We derive estimators of the cardinality in
both cases, and show that the maximal-term estimator is recursively computable
and has exponentially decreasing error bounds. Furthermore, we show that the
estimators have comparable asymptotic efficiency, and explain this result by
demonstrating an unexpected connection between the two approaches.

<id>
0801.3559v1
<category>
stat.CO
<abstract>
In recent years, large high-dimensional data sets have become commonplace in
a wide range of applications in science and commerce. Techniques for dimension
reduction are of primary concern in statistical analysis. Projection methods
play an important role. We investigate the use of projection algorithms that
exploit properties of the alpha-stable distributions. We show that l_{alpha}
distances and quasi-distances can be recovered from random projections with
full statistical efficiency by L-estimation. The computational requirements of
our algorithm are modest; after a once-and-for-all calculation to determine an
array of length k, the algorithm runs in O(k) time for each distance, where k
is the reduced dimension of the projection.

<id>
0802.3690v1
<category>
stat.CO
<abstract>
Population Monte Carlo has been introduced as a sequential importance
sampling technique to overcome poor fit of the importance function. In this
paper, we compare the performances of the original Population Monte Carlo
algorithm with a modified version that eliminates the influence of the
transition particle via a double Rao-Blackwellisation. This modification is
shown to improve the exploration of the modes through an large simulation
experiment on posterior distributions of mean mixtures of distributions.

<id>
0804.0390v1
<category>
stat.CO
<abstract>
In the manuscript, we present a practical way to find the matching priors
proposed by Welch & Peers (1963) and Peers (1965). We investigate the use of
saddlepoint approximations combined with matching priors and obtain p-values of
the test of an interest parameter in the presence of nuisance parameter. The
advantage of our procedure is the flexibility of choosing different initial
conditions so that one can adjust the performance of the test. Two examples
have been studied, with coverage verified via Monte Carlo simulation. One
relates to the ratio of two exponential means, and the other relates the
logistic regression model. Particularly, we are interested in small sample
settings.

<id>
0807.0725v2
<category>
stat.CO
<abstract>
Case-deleted analysis is a popular method for evaluating the influence of a
subset of cases on inference. The use of Monte Carlo estimation strategies in
complicated Bayesian settings leads naturally to the use of importance sampling
techniques to assess the divergence between full-data and case-deleted
posteriors and to provide estimates under the case-deleted posteriors. However,
the dependability of the importance sampling estimators depends critically on
the variability of the case-deleted weights. We provide theoretical results
concerning the assessment of the dependability of case-deleted importance
sampling estimators in several Bayesian models. In particular, these results
allow us to establish whether or not the estimators satisfy a central limit
theorem. Because the conditions we derive are of a simple analytical nature,
the assessment of the dependability of the estimators can be verified routinely
before estimation is performed. We illustrate the use of the results in several
examples.

<id>
0809.2274v4
<category>
stat.CO
<abstract>
Principal component analysis (PCA) requires the computation of a low-rank
approximation to a matrix containing the data being analyzed. In many
applications of PCA, the best possible accuracy of any rank-deficient
approximation is at most a few digits (measured in the spectral norm, relative
to the spectral norm of the matrix being approximated). In such circumstances,
efficient algorithms have not come with guarantees of good accuracy, unless one
or both dimensions of the matrix being approximated are small. We describe an
efficient algorithm for the low-rank approximation of matrices that produces
accuracy very close to the best possible, for matrices of arbitrary sizes. We
illustrate our theoretical results via several numerical examples.

<id>
0809.4047v1
<category>
stat.CO
<abstract>
This paper presents an improved result on the negative-binomial Monte Carlo
technique analyzed in a previous paper for the estimation of an unknown
probability p. Specifically, the confidence level associated to a relative
interval [p/\mu_2, p\mu_1], with \mu_1, \mu_2 > 1, is proved to exceed its
asymptotic value for a broader range of intervals than that given in the
referred paper, and for any value of p. This extends the applicability of the
estimator, relaxing the conditions that guarantee a given confidence level.

<id>
0809.4654v1
<category>
stat.CO
<abstract>
In areas such as kernel smoothing and non-parametric regression there is
emphasis on smooth interpolation and smooth statistical models. Splines are
known to have optimal smoothness properties in one and higher dimensions. It is
shown, with special attention to polynomial models, that smooth interpolators
can be constructed by first extending the monomial basis and then minimising a
measure of smoothness with respect to the free parameters in the extended
basis. Algebraic methods are a help in choosing the extended basis which can
also be found as a saturated basis for an extended experimental design with
dummy design points. One can get arbitrarily close to optimal smoothing for any
dimension and over any region, giving a simple alternative models of spline
type. The relationship to splines is shown in one and two dimensions. A case
study is given which includes benchmarking against kriging methods.

<id>
0810.1163v1
<category>
stat.CO
<abstract>
We present a sequential Monte Carlo sampler algorithm for the Bayesian
analysis of generalised linear mixed models (GLMMs). These models support a
variety of interesting regression-type analyses, but performing inference is
often extremely difficult, even when using the Bayesian approach combined with
Markov chain Monte Carlo (MCMC). The Sequential Monte Carlo sampler (SMC) is a
new and general method for producing samples from posterior distributions. In
this article we demonstrate use of the SMC method for performing inference for
GLMMs. We demonstrate the effectiveness of the method on both simulated and
real data, and find that sequential Monte Carlo is a competitive alternative to
the available MCMC techniques.

<id>
0811.2843v1
<category>
stat.CO
<abstract>
A descent algorithm, "Quasi-Quadratic Minimization with Memory" (QQMM), is
proposed for unconstrained minimization of the sum, $F$, of a non-negative
convex function, $V$, and a quadratic form. Such problems come up in
regularized estimation in machine learning and statistics. In addition to
values of $F$, QQMM requires the (sub)gradient of $V$. Two features of QQMM
help keep low the number of evaluations of the objective function it needs.
First, QQMM provides good control over stopping the iterative search. This
feature makes QQMM well adapted to statistical problems because in such
problems the objective function is based on random data and therefore stopping
early is sensible. Secondly, QQMM uses a complex method for determining trial
minimizers of $F$. After a description of the problem and algorithm a
simulation study comparing QQMM to the popular BFGS optimization algorithm is
described. The simulation study and other experiments suggest that QQMM is
generally substantially faster than BFGS in the problem domain for which it was
designed. A QQMM-BFGS hybrid is also generally substantially faster than BFGS
but does better than QQMM when QQMM is very slow.

<id>
0811.4095v3
<category>
stat.CO
<abstract>
Recently developed adaptive Markov chain Monte Carlo (MCMC) methods have been
applied successfully to many problems in Bayesian statistics. Grapham is a new
open source implementation covering several such methods, with emphasis on
graphical models for directed acyclic graphs. The implemented algorithms
include the seminal Adaptive Metropolis algorithm adjusting the proposal
covariance according to the history of the chain and a Metropolis algorithm
adjusting the proposal scale based on the observed acceptance probability.
Different variants of the algorithms allow one, for example, to use these two
algorithms together, employ delayed rejection and adjust several parameters of
the algorithms. The implemented Metropolis-within-Gibbs update allows arbitrary
sampling blocks. The software is written in C and uses a simple extension
language Lua in configuration.

<id>
0901.0876v1
<category>
stat.CO
<abstract>
The presence of groups containing high leverage outliers makes linear
regression a difficult problem due to the masking effect. The available high
breakdown estimators based on Least Trimmed Squares often do not succeed in
detecting masked high leverage outliers in finite samples.
  An alternative to the LTS estimator, called Penalised Trimmed Squares (PTS)
estimator, was introduced by the contributors in \cite{ZiouAv:05,ZiAvPi:07} and it
appears to be less sensitive to the masking problem. This estimator is defined
by a Quadratic Mixed Integer Programming (QMIP) problem, where in the objective
function a penalty cost for each observation is included which serves as an
upper bound on the residual error for any feasible regression line. Since the
PTS does not require presetting the number of outliers to delete from the data
set, it has better efficiency with respect to other estimators. However, due to
the high computational complexity of the resulting QMIP problem, exact
solutions for moderately large regression problems is infeasible.
  In this paper we further establish the theoretical properties of the PTS
estimator, such as high breakdown and efficiency, and propose an approximate
algorithm called Fast-PTS to compute the PTS estimator for large data sets
efficiently. Extensive computational experiments on sets of benchmark instances
with varying degrees of outlier contamination, indicate that the proposed
algorithm performs well in identifying groups of high leverage outliers in
reasonable computational time.

<id>
0902.0442v1
<category>
stat.CO
<abstract>
One of the most popular class of tests for independence between two random
variables is the general class of rank statistics which are invariant under
permutations. This class contains Spearman's coefficient of rank correlation
statistic, Fisher-Yates statistic, weighted Mann statistic and others. Under
the null hypothesis of independence these test statistics have a permutation
distribution that usually the normal asymptotic theory used to approximate the
p-values for these tests. In this note we suggest using a saddlepoint approach
that almost exact and need no extensive simulation calculations to calculate
the p-value of such class of tests.

<id>
0902.4117v1
<category>
stat.CO
<abstract>
This note presents a simple and elegant sampler which could be used as an
alternative to the reversible jump MCMC methodology.

<id>
0905.2441v3
<category>
stat.CO
<abstract>
We present a case-study on the utility of graphics cards to perform massively
parallel simulation of advanced Monte Carlo methods. Graphics cards, containing
multiple Graphics Processing Units (GPUs), are self-contained parallel
computational devices that can be housed in conventional desktop and laptop
computers. For certain classes of Monte Carlo algorithms they offer massively
parallel simulation, with the added advantage over conventional distributed
multi-core processors that they are cheap, easily accessible, easy to maintain,
easy to code, dedicated local devices with low power consumption. On a
canonical set of stochastic simulation examples including population-based
Markov chain Monte Carlo methods and Sequential Monte Carlo methods, we find
speedups from 35 to 500 fold over conventional single-threaded computer code.
Our findings suggest that GPUs have the potential to facilitate the growth of
statistical modelling into complex data rich domains through the availability
of cheap and accessible many-core computation. We believe the speedup we
observe should motivate wider use of parallelizable simulation methods and
greater methodological attention to their design.

<id>
0907.1835v1
<category>
stat.CO
<abstract>
The information geometry of the 2-manifold of gamma probability density
functions provides a framework in which pseudorandom number generators may be
evaluated using a neighbourhood of the curve of exponential density functions.
The process is illustrated using the pseudorandom number generator in
Mathematica. This methodology may be useful to add to the current family of
test procedures in real applications to finite sampling data.

<id>
0907.1997v2
<category>
stat.CO
<abstract>
We investigate the existence of bounded-memory consistent estimators of
various statistical functionals. This question is resolved in the negative in a
rather strong sense. We propose various bounded-memory approximations, using
techniques from automata theory and stochastic processes. Some questions of
potential interest are raised for future work.

<id>
0907.3521v2
<category>
stat.CO
<abstract>
For the most popular sequential change detection rules such as CUSUM, EWMA,
and the Shiryaev-Roberts test, we develop integral equations and a concise
numerical method to compute a number of performance metrics, including average
detection delay and average time to false alarm. We pay special attention to
the Shiryaev-Roberts procedure and evaluate its performance for various
initialization strategies. Regarding the randomized initialization variant
proposed by Pollak, known to be asymptotically optimal of order-3, we offer a
means for numerically computing the quasi-stationary distribution of the
Shiryaev-Roberts statistic that is the distribution of the initializing random
variable, thus making this test applicable in practice. A significant
side-product of our computational technique is the observation that
deterministic initializations of the Shiryaev-Roberts procedure can also enjoy
the same order-3 optimality property as Pollak's randomized test and, after
careful selection, even uniformly outperform it.

<id>
0907.4018v2
<category>
stat.CO
<abstract>
Assume that one aims to simulate an event of unknown probability $s\in (0,1)$
which is uniquely determined, however only its approximations can be obtained
using a finite computational effort. Such settings are often encountered in
statistical simulations. We consider two specific examples. First, the exact
simulation of non-linear diffusions, second, the celebrated Bernoulli factory
problem of generating an $f(p)-$coin given a sequence $X_1,X_2,...$ of
independent tosses of a $p-$coin (with known $f$ and unknown $p$). We describe
a general framework and provide algorithms where this kind of problems can be
fitted and solved. The algorithms are straightforward to implement and thus
allow for effective simulation of desired events of probability $s.$ In the
case of diffusions, we obtain the algorithm of \cite{BeskosRobertsEA1} as a
specific instance of the generic framework developed here. In the case of the
Bernoulli factory, our work offers a statistical understanding of the
Nacu-Peres algorithm for $f(p) = \min\{2p, 1-2\varepsilon\}$ (which is central
to the general question) and allows for its immediate implementation that
avoids algorithmic difficulties of the original version.

<id>
0907.5123v1
<category>
stat.CO
<abstract>
In this note, we shortly survey some recent approaches on the approximation
of the Bayes factor used in Bayesian hypothesis testing and in Bayesian model
choice. In particular, we reassess importance sampling, harmonic mean sampling,
and nested sampling from a unified perspective.

<id>
0909.0389v1
<category>
stat.CO
<abstract>
Monte Carlo methods are now an essential part of the statistician's toolbox,
to the point of being more familiar to graduate students than the measure
theoretic notions upon which they are based! We recall in this note some of the
advances made in the design of Monte Carlo techniques towards their use in
Statistics, referring to Robert and Casella (2004,2010) for an in-depth
coverage.

<id>
0911.0412v2
<category>
stat.CO
<abstract>
In this paper we parameterize non-negative matrices of sum one and rank at
most two. More precisely, we give a family of parameterizations using the least
possible number of parameters. We also show how these parameterizations relate
to a class of statistical models, known in Probability and Statistics as
mixture models for contingency tables.

<id>
0912.4729v1
<category>
stat.CO
<abstract>
$\alpha$-stable distributions are utilised as models for heavy-tailed noise
in many areas of statistics, finance and signal processing engineering.
  However, in general, neither univariate nor multivariate $\alpha$-stable
models admit closed form densities which can be evaluated pointwise. This
complicates the inferential procedure.
  As a result, $\alpha$-stable models are practically limited to the univariate
setting under the Bayesian paradigm, and to bivariate models under the
classical framework.
  In this article we develop a novel Bayesian approach to modelling univariate
and multivariate $\alpha$-stable distributions based on recent advances in
"likelihood-free" inference.
  We present an evaluation of the performance of this procedure in 1, 2 and 3
dimensions, and provide an analysis of real daily currency exchange rate data.
The proposed approach provides a feasible inferential methodology at a moderate
computational cost.

<id>
1001.2797v1
<category>
stat.CO
<abstract>
We consider various versions of adaptive Gibbs and Metropolis within-Gibbs
samplers, which update their selection probabilities (and perhaps also their
proposal distributions) on the fly during a run, by learning as they go in an
attempt to optimise the algorithm. We present a cautionary example of how even
a simple-seeming adaptive Gibbs sampler may fail to converge. We then present
various positive results guaranteeing convergence of adaptive Gibbs samplers
under certain conditions.

<id>
0705.2363v1
<category>
stat.ML
<abstract>
We consider the problem of binary classification where one can, for a
particular cost, choose not to classify an observation. We present a simple
proof for the oracle inequality for the excess risk of structural risk
minimizers using a lasso type penalty.

<id>
0707.3536v1
<category>
stat.ML
<abstract>
Dendrograms used in data analysis are ultrametric spaces, hence objects of
nonarchimedean geometry. It is known that there exist $p$-adic representation
of dendrograms. Completed by a point at infinity, they can be viewed as
subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.
The implications are that certain moduli spaces known in algebraic geometry are
$p$-adic parameter spaces of (families of) dendrograms, and stochastic
classification can also be handled within this framework. At the end, we
calculate the topology of the hidden part of a dendrogram.

<id>
0707.4072v1
<category>
stat.ML
<abstract>
A conceptual framework for cluster analysis from the viewpoint of p-adic
geometry is introduced by describing the space of all dendrograms for n
datapoints and relating it to the moduli space of p-adic Riemannian spheres
with punctures using a method recently applied by Murtagh (2004b). This method
embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the
p-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.
After explaining the definitions, the concept of classifiers is discussed in
the context of moduli spaces, and upper bounds for the number of hidden
vertices in dendrograms are given.

<id>
0709.2760v3
<category>
stat.ML
<abstract>
In recent years, kernel density estimation has been exploited by computer
scientists to model machine learning problems. The kernel density estimation
based approaches are of interest due to the low time complexity of either O(n)
or O(n*log(n)) for constructing a classifier, where n is the number of sampling
instances. Concerning design of kernel density estimators, one essential issue
is how fast the pointwise mean square error (MSE) and/or the integrated mean
square error (IMSE) diminish as the number of sampling instances increases. In
this article, it is shown that with the proposed kernel function it is feasible
to make the pointwise MSE of the density estimator converge at O(n^-2/3)
regardless of the dimension of the vector space, provided that the probability
density function at the point of interest meets certain conditions.

<id>
0709.2936v1
<category>
stat.ML
<abstract>
This thesis responds to the challenges of using a large number, such as
thousands, of features in regression and classification problems.
  There are two situations where such high dimensional features arise. One is
when high dimensional measurements are available, for example, gene expression
data produced by microarray techniques. For computational or other reasons,
people may select only a small subset of features when modelling such data, by
looking at how relevant the features are to predicting the response, based on
some measure such as correlation with the response in the training data.
Although it is used very commonly, this procedure will make the response appear
more predictable than it actually is. In Chapter 2, we propose a Bayesian
method to avoid this selection bias, with application to naive Bayes models and
mixture models.
  High dimensional features also arise when we consider high-order
interactions. The number of parameters will increase exponentially with the
order considered. In Chapter 3, we propose a method for compressing a group of
parameters into a single one, by exploiting the fact that many predictor
variables derived from high-order interactions have the same values for all the
training cases. The number of compressed parameters may have converged before
considering the highest possible order. We apply this compression method to
logistic sequence prediction models and logistic classification models.
  We use both simulated data and real data to test our methods in both
chapters.

<id>
0709.2989v1
<category>
stat.ML
<abstract>
Simulated annealing is a popular method for approaching the solution of a
global optimization problem. Existing results on its performance apply to
discrete combinatorial optimization where the optimization variables can assume
only a finite set of possible values. We introduce a new general formulation of
simulated annealing which allows one to guarantee finite-time performance in
the optimization of functions of continuous variables. The results hold
universally for any optimization problem on a bounded domain and establish a
connection between simulated annealing and up-to-date theory of convergence of
Markov chain Monte Carlo methods on continuous domains. This work is inspired
by the concept of finite-time learning with known accuracy and confidence
developed in statistical learning theory.

<id>
0710.0845v3
<category>
stat.ML
<abstract>
We present the nested Chinese restaurant process (nCRP), a stochastic process
which assigns probability distributions to infinitely-deep,
infinitely-branching trees. We show how this stochastic process can be used as
a prior distribution in a Bayesian nonparametric model of document collections.
Specifically, we present an application to information retrieval in which
documents are modeled as paths down a random tree, and the preferential
attachment dynamics of the nCRP leads to clustering of documents according to
sharing of topics at multiple levels of abstraction. Given a corpus of
documents, a posterior inference algorithm finds an approximation to a
posterior distribution over trees, topics and allocations of words to levels of
the tree. We demonstrate this algorithm on collections of scientific abstracts
from several journals. This model exemplifies a recent trend in statistical
machine learning--the use of Bayesian nonparametric methods to infer
distributions on flexible data structures.

<id>
0710.3183v1
<category>
stat.ML
<abstract>
We provide self-contained proof of a theorem relating probabilistic coherence
of forecasts to their non-domination by rival forecasts with respect to any
proper scoring rule. The theorem appears to be new but is closely related to
results achieved by other investigators.

<id>
0710.3742v1
<category>
stat.ML
<abstract>
Changepoints are abrupt variations in the generative parameters of a data
sequence. Online detection of changepoints is useful in modelling and
prediction of time series in application areas such as finance, biometrics, and
robotics. While frequentist methods have yielded online filtering and
prediction techniques, most Bayesian papers have focused on the retrospective
segmentation problem. Here we examine the case where the model parameters
before and after the changepoint are independent and we derive an online
algorithm for exact inference of the most recent changepoint. We compute the
probability distribution of the length of the current ``run,'' or time since
the last changepoint, using a simple message-passing algorithm. Our
implementation is highly modular so that the algorithm may be applied to a
variety of types of data. We illustrate this modularity by demonstrating the
algorithm on three different real-world data sets.

<id>
0712.0248v1
<category>
stat.ML
<abstract>
This monograph deals with adaptive supervised classification, using tools
borrowed from statistical mechanics and information theory, stemming from the
PACBayesian approach pioneered by David McAllester and applied to a conception
of statistical learning theory forged by Vladimir Vapnik. Using convex analysis
on the set of posterior probability measures, we show how to get local measures
of the complexity of the classification model involving the relative entropy of
posterior distributions with respect to Gibbs posterior measures. We then
discuss relative bounds, comparing the generalization error of two
classification rules, showing how the margin assumption of Mammen and Tsybakov
can be replaced with some empirical measure of the covariance structure of the
classification model.We show how to associate to any posterior distribution an
effective temperature relating it to the Gibbs prior distribution with the same
level of expected error rate, and how to estimate this effective temperature
from data, resulting in an estimator whose expected error rate converges
according to the best possible power of the sample size adaptively under any
margin and parametric complexity assumptions. We describe and study an
alternative selection scheme based on relative bounds between estimators, and
present a two step localization technique which can handle the selection of a
parametric model from a family of those. We show how to extend systematically
all the results obtained in the inductive setting to transductive learning, and
use this to improve Vapnik's generalization bounds, extending them to the case
when the sample is made of independent non-identically distributed pairs of
patterns and labels. Finally we review briefly the construction of Support
Vector Machines and show how to derive generalization bounds for them,
measuring the complexity either through the number of support vectors or
through the value of the transductive or inductive margin.

<id>
0802.2906v2
<category>
stat.ML
<abstract>
Dimensionality reduction is a topic of recent interest. In this paper, we
present the classification constrained dimensionality reduction (CCDR)
algorithm to account for label information. The algorithm can account for
multiple classes as well as the semi-supervised setting. We present an
out-of-sample expressions for both labeled and unlabeled data. For unlabeled
data, we introduce a method of embedding a new point as preprocessing to a
classifier. For labeled data, we introduce a method that improves the embedding
during the training phase using the out-of-sample extension. We investigate
classification performance using the CCDR algorithm on hyper-spectral satellite
imagery data. We demonstrate the performance gain for both local and global
classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors
algorithm performance. We present a connection between intrinsic dimension
estimation and the optimal embedding dimension obtained using the CCDR
algorithm.

<id>
0804.1026v1
<category>
stat.ML
<abstract>
We propose to investigate test statistics for testing homogeneity in
reproducing kernel Hilbert spaces. Asymptotic null distributions under null
hypothesis are derived, and consistency against fixed and local alternatives is
assessed. Finally, experimental evidence of the performance of the proposed
approach on both artificial data and a speaker verification task is provided.

<id>
0804.1325v1
<category>
stat.ML
<abstract>
When using the K-nearest neighbors method, one often ignores uncertainty in
the choice of K. To account for such uncertainty, Holmes and Adams (2002)
proposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN
(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain
Monte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams
(2002) focused on the performance of BKNN in terms of misclassification error
but did not assess its ability to quantify uncertainty. We present some
evidence to show that BKNN still significantly underestimates model
uncertainty.

<id>
0804.2848v1
<category>
stat.ML
<abstract>
Flow cytometry is often used to characterize the malignant cells in leukemia
and lymphoma patients, traced to the level of the individual cell. Typically,
flow cytometric data analysis is performed through a series of 2-dimensional
projections onto the axes of the data set. Through the years, clinicians have
determined combinations of different fluorescent markers which generate
relatively known expression patterns for specific subtypes of leukemia and
lymphoma -- cancers of the hematopoietic system. By only viewing a series of
2-dimensional projections, the high-dimensional nature of the data is rarely
exploited. In this paper we present a means of determining a low-dimensional
projection which maintains the high-dimensional relationships (i.e.
information) between differing oncological data sets. By using machine learning
techniques, we allow clinicians to visualize data in a low dimension defined by
a linear combination of all of the available markers, rather than just 2 at a
time. This provides an aid in diagnosing similar forms of cancer, as well as a
means for variable selection in exploratory flow cytometric research. We refer
to our method as Information Preserving Component Analysis (IPCA).

<id>
0806.2646v1
<category>
stat.ML
<abstract>
We analyze the performance of a class of manifold-learning algorithms that
find their output by minimizing a quadratic form under some normalization
constraints. This class consists of Locally Linear Embedding (LLE), Laplacian
Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and
Diffusion maps. We present and prove conditions on the manifold that are
necessary for the success of the algorithms. Both the finite sample case and
the limit case are analyzed. We show that there are simple manifolds in which
the necessary conditions are violated, and hence the algorithms cannot recover
the underlying manifolds. Finally, we present numerical results that
demonstrate our claims.

<id>
0806.2669v1
<category>
stat.ML
<abstract>
We present the Procrustes measure, a novel measure based on Procrustes
rotation that enables quantitative comparison of the output of manifold-based
embedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum
et al, 2000)). The measure also serves as a natural tool when choosing
dimension-reduction parameters. We also present two novel dimension-reduction
techniques that attempt to minimize the suggested measure, and compare the
results of these techniques to the results of existing algorithms. Finally, we
suggest a simple iterative method that can be used to improve the output of
existing algorithms.

<id>
0806.2831v1
<category>
stat.ML
<abstract>
The problem of supervised classification (or discrimination) with functional
data is considered, with a special interest on the popular k-nearest neighbors
(k-NN) classifier. First, relying on a recent result by Cerou and Guyader
(2006), we prove the consistency of the k-NN classifier for functional data
whose distribution belongs to a broad family of Gaussian processes with
triangular covariance functions. Second, on a more practical side, we check the
behavior of the k-NN method when compared with a few other functional
classifiers. This is carried out through a small simulation study and the
analysis of several real functional data sets. While no global "uniform" winner
emerges from such comparisons, the overall performance of the k-NN method,
together with its sound intuitive motivation and relative simplicity, suggests
that it could represent a reasonable benchmark for the classification problem
with functional data.

<id>
0806.4115v4
<category>
stat.ML
<abstract>
We propose a new sparsity-smoothness penalty for high-dimensional generalized
additive models. The combination of sparsity and smoothness is crucial for
mathematical theory as well as performance for finite-sample data. We present a
computationally efficient algorithm, with provable numerical convergence
properties, for optimizing the penalized likelihood. Furthermore, we provide
oracle results which yield asymptotic optimality of our estimator for high
dimensional but sparse additive models. Finally, an adaptive version of our
sparsity-smoothness penalized approach yields large additional performance
gains.

<id>
0808.0780v1
<category>
stat.ML
<abstract>
The local linear embedding algorithm (LLE) is a non-linear dimension-reducing
technique, widely used due to its computational simplicity and intuitive
approach. LLE first linearly reconstructs each input point from its nearest
neighbors and then preserves these neighborhood relations in the
low-dimensional embedding. We show that the reconstruction weights computed by
LLE capture the high-dimensional structure of the neighborhoods, and not the
low-dimensional manifold structure. Consequently, the weight vectors are highly
sensitive to noise. Moreover, this causes LLE to converge to a linear
projection of the input, as opposed to its non-linear embedding goal. To
overcome both of these problems, we propose to compute the weight vectors using
a low-dimensional neighborhood representation. We prove theoretically that this
straightforward and computationally simple modification of LLE reduces LLE's
sensitivity to noise. This modification also removes the need for
regularization when the number of neighbors is larger than the dimension of the
input. We present numerical examples demonstrating both the perturbation and
linear projection problems, and the improved outputs using the low-dimensional
neighborhood representation.

<id>
0808.2241v1
<category>
stat.ML
<abstract>
We construct a framework for studying clustering algorithms, which includes
two key ideas: persistence and functoriality. The first encodes the idea that
the output of a clustering scheme should carry a multiresolution structure, the
second the idea that one should be able to compare the results of clustering
algorithms as one varies the data set, for example by adding points or by
applying functions to it. We show that within this framework, one can prove a
theorem analogous to one of J. Kleinberg, in which one obtains an existence and
uniqueness theorem instead of a non-existence result. We explore further
properties of this unique scheme, stability and convergence are established.

<id>
0808.2337v1
<category>
stat.ML
<abstract>
We consider principal component analysis (PCA) in decomposable Gaussian
graphical models. We exploit the prior information in these models in order to
distribute its computation. For this purpose, we reformulate the problem in the
sparse inverse covariance (concentration) domain and solve the global
eigenvalue problem using a sequence of local eigenvalue problems in each of the
cliques of the decomposable graph. We demonstrate the application of our
methodology in the context of decentralized anomaly detection in the Abilene
backbone network. Based on the topology of the network, we propose an
approximate statistical graphical model and distribute the computation of PCA.

<id>
0810.0901v2
<category>
stat.ML
<abstract>
Many problems of low-level computer vision and image processing, such as
denoising, deconvolution, tomographic reconstruction or super-resolution, can
be addressed by maximizing the posterior distribution of a sparse linear model
(SLM). We show how higher-order Bayesian decision-making problems, such as
optimizing image acquisition in magnetic resonance scanners, can be addressed
by querying the SLM posterior covariance, unrelated to the density's mode. We
propose a scalable algorithmic framework, with which SLM posteriors over full,
high-resolution images can be approximated for the first time, solving a
variational optimization problem which is convex iff posterior mode finding is
convex. These methods successfully drive the optimization of sampling
trajectories for real-world magnetic resonance imaging through Bayesian
experimental design, which has not been attempted before. Our methodology
provides new insight into similarities and differences between sparse
reconstruction and approximate Bayesian inference, and has important
implications for compressive sensing of real-world images.

<id>
0810.4553v1
<category>
stat.ML
<abstract>
We present a new online boosting algorithm for adapting the weights of a
boosted classifier, which yields a closer approximation to Freund and
Schapire's AdaBoost algorithm than previous online boosting algorithms. We also
contribute a new way of deriving the online algorithm that ties together
previous online boosting work. We assume that the weak hypotheses were selected
beforehand, and only their weights are updated during online boosting. The
update rule is derived by minimizing AdaBoost's loss when viewed in an
incremental form. The equations show that optimization is computationally
expensive. However, a fast online approximation is possible. We compare
approximation error to batch AdaBoost on synthetic datasets and generalization
error on face datasets and the MNIST dataset.

<id>
0810.5117v1
<category>
stat.ML
<abstract>
In this report, we derive a non-negative series expansion for the
Jensen-Shannon divergence (JSD) between two probability distributions. This
series expansion is shown to be useful for numerical calculations of the JSD,
when the probability distributions are nearly equal, and for which,
consequently, small numerical errors dominate evaluation.

<id>
0811.1239v1
<category>
stat.ML
<abstract>
We consider the problem of jointly estimating the parameters as well as the
structure of binary valued Markov Random Fields, in contrast to earlier work
that focus on one of the two problems. We formulate the problem as a
maximization of $\ell_1$-regularized surrogate likelihood that allows us to
find a sparse solution. Our optimization technique efficiently incorporates the
cutting-plane algorithm in order to obtain a tighter outer bound on the
marginal polytope, which results in improvement of both parameter estimates and
approximation to marginals. On synthetic data, we compare our algorithm on the
two estimation tasks to the other existing methods. We analyze the method in
the high-dimensional setting, where the number of dimensions $p$ is allowed to
grow with the number of observations $n$. The rate of convergence of the
estimate is demonstrated to depend explicitly on the sparsity of the underlying
graph.

<id>
0811.3499v1
<category>
stat.ML
<abstract>
The most direct way to express arbitrary dependencies in datasets is to
estimate the joint distribution and to apply afterwards the argmax-function to
obtain the mode of the corresponding conditional distribution. This method is
in practice difficult, because it requires a global optimization of a
complicated function, the joint distribution by fixed input variables. This
article proposes a method for finding global maxima if the joint distribution
is modeled by a kernel density estimation. Some experiments show advantages and
shortcomings of the resulting regression method in comparison to the standard
Nadaraya-Watson regression technique, which approximates the optimum by the
expectation value.

<id>
0811.3579v3
<category>
stat.ML
<abstract>
We present a procedure for effective estimation of entropy and mutual
information from small-sample data, and apply it to the problem of inferring
high-dimensional gene association networks. Specifically, we develop a
James-Stein-type shrinkage estimator, resulting in a procedure that is highly
efficient statistically as well as computationally. Despite its simplicity, we
show that it outperforms eight other entropy estimation procedures across a
diverse range of sampling scenarios and data-generating models, even in cases
of severe undersampling. We illustrate the approach by analyzing E. coli gene
expression data and computing an entropy-based gene-association network from
gene expression data. A computer program is available that implements the
proposed shrinkage estimator.

<id>
0811.3619v1
<category>
stat.ML
<abstract>
This paper examines from an experimental perspective random forests, the
increasingly used statistical method for classification and regression problems
introduced by Leo Breiman in 2001. It first aims at confirming, known but
sparse, advice for using random forests and at proposing some complementary
remarks for both standard problems as well as high dimensional ones for which
the number of variables hugely exceeds the sample size. But the main
contribution of this paper is twofold: to provide some insights about the
behavior of the variable importance index based on random forests and in
addition, to propose to investigate two classical issues of variable selection.
The first one is to find important variables for interpretation and the second
one is more restrictive and try to design a good prediction model. The strategy
involves a ranking of explanatory variables using the random forests score of
importance and a stepwise ascending variable introduction strategy.

<id>
0811.4208v1
<category>
stat.ML
<abstract>
Min-cut clustering, based on minimizing one of two heuristic cost-functions
proposed by Shi and Malik, has spawned tremendous research, both analytic and
algorithmic, in the graph partitioning and image segmentation communities over
the last decade. It is however unclear if these heuristics can be derived from
a more general principle facilitating generalization to new problem settings.
Motivated by an existing graph partitioning framework, we derive relationships
between optimizing relevance information, as defined in the Information
Bottleneck method, and the regularized cut in a K-partitioned graph. For fast
mixing graphs, we show that the cost functions introduced by Shi and Malik can
be well approximated as the rate of loss of predictive information about the
location of random walkers on the graph. For graphs generated from a stochastic
algorithm designed to model community structure, the optimal information
theoretic partition and the optimal min-cut partition are shown to be the same
with high probability.

<id>
0812.1615v1
<category>
stat.ML
<abstract>
Autoencoder neural network is implemented to estimate the missing data.
Genetic algorithm is implemented for network optimization and estimating the
missing data. Missing data is treated as Missing At Random mechanism by
implementing maximum likelihood algorithm. The network performance is
determined by calculating the mean square error of the network prediction. The
network is further optimized by implementing Decision Forest. The impact of
missing data is then investigated and decision forrests are found to improve
the results.

<id>
0705.0700v3
<category>
stat.ME
<abstract>
This paper considers the issue of modeling fractional data observed in the
interval [0,1), (0,1] or [0,1]. Mixed continuous-discrete distributions are
proposed. The beta distribution is used to describe the continuous component of
the model since its density can have quite diferent shapes depending on the
values of the two parameters that index the distribution. Properties of the
proposed distributions are examined. Also, maximum likelihood and method of
moments estimation is discussed. Finally, practical applications that employ
real data are presented.

<id>
0705.2938v1
<category>
stat.ME
<abstract>
Using predictive adaptive arithmetic coding and the Minimum Description
Length principle, we derive an efficient tool for model selection problems :
the RIC information criterion. We then present an extension of these coding
techniques to non-parametrical estimation of a distribution and illustrate it
on the gray scales histogram of an image.
  Key-words : Information criteria, MDL, model selection, non-parametrical
estimation, histograms.

<id>
0705.4588v1
<category>
stat.ME
<abstract>
We propose the variable selection procedure incorporating prior constraint
information into lasso. The proposed procedure combines the sample and prior
information, and selects significant variables for responses in a narrower
region where the true parameters lie. It increases the efficiency to choose the
true model correctly. The proposed procedure can be executed by many
constrained quadratic programming methods and the initial estimator can be
found by least square or Monte Carlo method. The proposed procedure also enjoys
good theoretical properties. Moreover, the proposed procedure is not only used
for linear models but also can be used for generalized linear models({\sl
GLM}), Cox models, quantile regression models and many others with the help of
Wang and Leng (2007)'s LSA, which changes these models as the approximation of
linear models. The idea of combining sample and prior constraint information
can be also used for other modified lasso procedures. Some examples are used
for illustration of the idea of incorporating prior constraint information in
variable selection procedures.

<id>
0707.0143v1
<category>
stat.ME
<abstract>
This is an expos\'e on the use of O'Sullivan penalised splines in
contemporary semiparametric regression, including mixed model and Bayesian
formulations. O'Sullivan penalised splines are similar to P-splines, but have
an advantage of being a direct generalisation of smoothing splines. Exact
expressions for the O'Sullivan penalty matrix are obtained. Comparisons between
the two reveals that O'Sullivan penalised splines more closely mimic the
natural boundary behaviour of smoothing splines. Implementation in modern
computing environments such as Matlab, R and BUGS is discussed.

<id>
0707.0246v1
<category>
stat.ME
<abstract>
We present in this paper a new tool for outliers detection in the context of
multiple regression models. This graphical tool is based on recursive
estimation of the parameters. Simulations were carried out to illustrate the
performance of this graphical procedure. As a conclusion, this tool is applied
to real data containing outliers according to the classical available tools.

<id>
0707.0481v3
<category>
stat.ME
<abstract>
In many modern applications, including analysis of gene expression and text
documents, the data are noisy, high-dimensional, and unordered--with no
particular meaning to the given order of the variables. Yet, successful
learning is often possible due to sparsity: the fact that the data are
typically redundant with underlying structures that can be represented by only
a few features. In this paper we present treelets--a novel construction of
multi-scale bases that extends wavelets to nonsmooth signals. The method is
fully adaptive, as it returns a hierarchical tree and an orthonormal basis
which both reflect the internal structure of the data. Treelets are especially
well-suited as a dimensionality reduction and feature selection tool prior to
regression and classification, in situations where sample sizes are small and
the data are sparse with unknown groupings of correlated or collinear
variables. The method is also simple to implement and analyze theoretically.
Here we describe a variety of situations where treelets perform better than
principal component analysis, as well as some common variable selection and
cluster averaging schemes. We illustrate treelets on a blocked covariance model
and on several data sets (hyperspectral image data, DNA microarray data, and
internet advertisements) with highly complex dependencies between variables.

<id>
0707.2158v1
<category>
stat.ME
<abstract>
We express the mean and variance terms in a double exponential regression
model as additive functions of the predictors and use Bayesian variable
selection to determine which predictors enter the model, and whether they enter
linearly or flexibly. When the variance term is null we obtain a generalized
additive model, which becomes a generalized linear model if the predictors
enter the mean linearly. The model is estimated using Markov chain Monte Carlo
simulation and the methodology is illustrated using real and simulated data
sets.

<id>
0707.2257v1
<category>
stat.ME
<abstract>
A class of semi-parametric hazard/failure rates with a bathtub shape is of
interest. It does not only provide a great deal of flexibility over existing
parametric methods in the modeling aspect but also results in a closed and
tractable Bayes estimator for the bathtub-shaped failure rate (BFR). Such an
estimator is derived to be a finite sum over two $\mathbf{S}$-paths due to an
explicit posterior analysis in terms of two (conditionally independent)
$\mathbf{S}$-paths. These, newly discovered, explicit results can be proved to
be a Rao-Blackwellization of counterpart results in terms of partitions that
are readily available by a specialization of James (2005)'s work. We develop
both iterative and non-iterative computational procedures based on existing
efficient Monte Carlo methods for sampling one single $\mathbf{S}$-path.
Nmerical simulations are given to demonstrate the practicality and the
effectiveness of our methodology. Last but not least, two applications of the
proposed method are discussed, of which one is about a Bayesian test for
failure rates and the other is related to modeling with covariates.

<id>
0709.0111v2
<category>
stat.ME
<abstract>
We propose a new method for the Maximum Likelihood Estimator (MLE) of
nonlinear mixed effects models when the variance matrix of Gaussian random
effects has a prescribed pattern of zeros (PPZ). The method consists in
coupling the recently developed Iterative Conditional Fitting (ICF) algorithm
with the Expectation Maximization (EM) algorithm. It provides positive definite
estimates for any sample size, and does not rely on any structural assumption
on the PPZ. It can be easily adapted to many versions of EM.

<id>
0709.0258v2
<category>
stat.ME
<abstract>
We consider Holder smoothness classes of surfaces for which we construct
piecewise polynomial approximation networks, which are graphs with polynomial
pieces as nodes and edges between polynomial pieces that are in `good
continuation' of each other. Little known to the community, a similar
construction was used by Kolmogorov and Tikhomirov in their proof of their
celebrated entropy results for Holder classes.
  We show how to use such networks in the context of detecting geometric
objects buried in noise to approximate the scan statistic, yielding an
optimization problem akin to the Traveling Salesman. In the same context, we
describe an alternative approach based on computing the longest path in the
network after appropriate thresholding.
  For the special case of curves, we also formalize the notion of `good
continuation' between beamlets in any dimension, obtaining more economical
piecewise linear approximation networks for curves.
  We include some numerical experiments illustrating the use of the beamlet
network in characterizing the filamentarity content of 3D datasets, and show
that even a rudimentary notion of good continuity may bring substantial
improvement.

<id>
0709.1616v3
<category>
stat.ME
<abstract>
In the this paper, the contributors propose to estimate the density of a targeted
population with a weighted kernel density estimator (wKDE) based on a weighted
sample. Bandwidth selection for wKDE is discussed. Three mean integrated
squared error based bandwidth estimators are introduced and their performance
is illustrated via Monte Carlo simulation. The least-squares cross-validation
method and the adaptive weight kernel density estimator are also studied. The
contributors also consider the boundary problem for interval bounded data and apply
the new method to a real data set subject to informative censoring.

<id>
0709.3377v1
<category>
stat.ME
<abstract>
The relationship between algebraic geometry and the inferential framework of
the Bayesian Networks with hidden variables has now been fruitfully explored
and exploited by a number of contributors. More recently the algebraic formulation
of Causal Bayesian Networks has also been investigated in this context. After
reviewing these newer relationships, we proceed to demonstrate that many of the
ideas embodied in the concept of a ``causal model'' can be more generally
expressed directly in terms of a partial order and a family of polynomial maps.
The more conventional graphical constructions, when available, remain a
powerful tool.

<id>
0709.3380v1
<category>
stat.ME
<abstract>
Discrete Bayesian Networks have been very successful as a framework both for
inference and for expressing certain causal hypotheses. In this paper we
present a class of graphical models called the chain event graph (CEG) models,
that generalises the class of discrete BN models. It provides a flexible and
expressive framework for representing and analysing the implications of causal
hypotheses, expressed in terms of the effects of a manipulation of the
generating underlying system. We prove that, as for a BN, identifiability
analyses of causal effects can be performed through examining the topology of
the CEG graph, leading to theorems analogous to the back-door theorem for the
BN.

<id>
0709.3545v1
<category>
stat.ME
<abstract>
A nonparametric and locally adaptive Bayesian estimator is proposed for
estimating a binary regression. Flexibility is obtained by modeling the binary
regression as a mixture of probit regressions with the argument of each probit
regression having a thin plate spline prior with its own smoothing parameter
and with the mixture weights depending on the covariates. The estimator is
compared to a single spline estimator and to a recently proposed locally
adaptive estimator. The methodology is illustrated by applying it to both
simulated and real examples.

<id>
0709.3765v1
<category>
stat.ME
<abstract>
This paper summarizes and discusses the methodological research in human
genetic linkage analysis, leading up to and following from the paper of C. A.
B. Smith presented as a Royal Statistical Society discussion paper in 1953.
This paper was given as the Fisher XXVII Memorial Lecture, in Cambridge,
December 4, 2006.

<id>
0709.3860v1
<category>
stat.ME
<abstract>
We describe here a new method to estimate copula measure. From N observations
of two variables X and Y, we draw a huge number m of subsamples (size n<N), and
we compute the joint ranks in these subsamples. Then, for each bivariate rank
(p,q) (0<p,q<n+1), we count the number of subsamples such that there exist an
observation of the subsample with bivariate rank (p,q). This counting gives an
estimate of the density of the copula. The simulation study shows that this
method seems to gives a better than the usual kernel method. The main advantage
of this new method is then we do not need to choose and justify the kernel. In
exchange, we have to choose a subsample size: this is in fact a problem very
similar to the bandwidth choice. We have then reduced the overall difficulty.

<id>
0709.4323v2
<category>
stat.ME
<abstract>
We consider Markov basis arising from fractional factorial designs with
three-level factors. Once we have a Markov basis, $p$ values for various
conditional tests are estimated by the Markov chain Monte Carlo procedure. For
designed experiments with a single count observation for each run, we formulate
a generalized linear model and consider a sample space with the same sufficient
statistics to the observed data. Each model is characterized by a covariate
matrix, which is constructed from the main and the interaction effects we
intend to measure. We investigate fractional factorial designs with $3^{p-q}$
runs noting correspondences to the models for $3^{p-q}$ contingency tables.

<id>
0710.0909v1
<category>
stat.ME
<abstract>
In this paper, insight is given in the techniques used to compute asymptotic
expansions. In a broad fashion the technique is described. Most of the results
apply to the paper "An expansion for the maximum likelihood estimator and its
distribution function", which will be submitted.

<id>
0710.4459v1
<category>
stat.ME
<abstract>
This paper discusses different needs and approaches to establishing
``causation'' that are relevant in legal cases involving statistical input
based on epidemiological (or more generally observational or population-based)
information. We distinguish between three versions of ``cause'': the first
involves negligence in providing or allowing exposure, the second involves
``cause'' as it is shown through a scientifically proved increased risk of an
outcome from the exposure in a population, and the third considers ``cause'' as
it might apply to an individual plaintiff based on the first two. The
population-oriented ``cause'' is that commonly addressed by statisticians, and
we propose a variation on the Bradford Hill approach to testing such causality
in an observational framework, and discuss how such a systematic series of
tests might be considered in a legal context. We review some current legal
approaches to using probabilistic statements, and link these with the
scientific methodology as developed here. In particular, we provide an approach
both to the idea of individual outcomes being caused on a balance of
probabilities, and to the idea of material contribution to such outcomes.
Statistical terminology and legal usage of terms such as ``proof on the balance
of probabilities'' or ``causation'' can easily become confused, largely due to
similar language describing dissimilar concepts; we conclude, however, that a
careful analysis can identify and separate those areas in which a legal
decision alone is required and those areas in which scientific approaches are
useful.

<id>
0710.4614v1
<category>
stat.ME
<abstract>
The mathematical properties of a family of generalized beta distribution,
including beta-normal, skewed-t, log-F, beta-exponential, beta-Weibull
distributions have recently been studied in several publications. This paper
applies these distributions to the modeling of the size distribution of income
and computes the maximum likelihood estimation estimates of parameters. Their
performances are compared to the widely used generalized beta distributions of
the first and second types in terms of measures of goodness of fit.

<id>
0710.4618v1
<category>
stat.ME
<abstract>
The incorporation of unlabeled data in regression and classification analysis
is an increasing focus of the applied statistics and machine learning
literatures, with a number of recent examples demonstrating the potential for
unlabeled data to contribute to improved predictive accuracy. The statistical
basis for this semisupervised analysis does not appear to have been well
delineated; as a result, the underlying theory and rationale may be
underappreciated, especially by nonstatisticians. There is also room for
statisticians to become more fully engaged in the vigorous research in this
important area of intersection of the statistical and computer sciences. Much
of the theoretical work in the literature has focused, for example, on
geometric and structural properties of the unlabeled data in the context of
particular algorithms, rather than probabilistic and statistical questions.
This paper overviews the fundamental statistical foundations for predictive
modeling and the general questions associated with unlabeled data, highlighting
the relevance of venerable concepts of sampling design and prior specification.
This theory, illustrated with a series of central illustrative examples and two
substantial real data analyses, shows precisely when, why and how unlabeled
data matter.

<id>
0710.4622v1
<category>
stat.ME
<abstract>
Hospital profiling involves a comparison of a health care provider's
structure, processes of care, or outcomes to a standard, often in the form of a
report card. Given the ubiquity of report cards and similar consumer ratings in
contemporary American culture, it is notable that these are a relatively recent
phenomenon in health care. Prior to the 1986 release of Medicare hospital
outcome data, little such information was publicly available. We review the
historical evolution of hospital profiling with special emphasis on outcomes;
present a detailed history of cardiac surgery report cards, the paradigm for
modern provider profiling; discuss the potential unintended negative
consequences of public report cards; and describe various statistical
methodologies for quantifying the relative performance of cardiac surgery
programs. Outstanding statistical issues are also described.

<id>
0710.4675v1
<category>
stat.ME
<abstract>
Shoutir Kishore Chatterjee was born in Ranchi, a small hill station in India,
on November 6, 1934. He received his B.Sc. in statistics from the Presidency
College, Calcutta, in 1954, and M.Sc. and Ph.D. degrees in statistics from the
University of Calcutta in 1956 and 1962, respectively. He was appointed a
lecturer in the Department of Statistics, University of Calcutta, in 1960 and
was a member of its faculty until his retirement as a professor in 1997.
Indeed, from the 1970s he steered the teaching and research activities of the
department for the next three decades. Professor Chatterjee was the National
Lecturer in Statistics (1985--1986) of the University Grants Commission, India,
the President of the Section of Statistics of the Indian Science Congress
(1989) and an Emeritus Scientist (1997--2000) of the Council of Scientific and
Industrial Research, India. Professor Chatterjee, affectionately known as SKC
to his students and admirers, is a truly exceptional person who embodies the
spirit of eternal India. He firmly believes that ``fulfillment in man's life
does not come from amassing a lot of money, after the threshold of what is
required for achieving a decent living is crossed. It does not come even from
peer recognition for intellectual achievements. Of course, one has to work and
toil a lot before one realizes these facts.''

<id>
0710.4768v1
<category>
stat.ME
<abstract>
In 1946, Public Law 588 of the 79th Congress established the Office of Naval
Research (ONR). Its mission was to plan, foster and encourage scientific
research in support of Naval problems. The establishment of ONR predates the
National Science Foundation and initiated the refocusing of scientific
infrastructure in the United States following World War II. At the time, ONR
was the only source for federal support of basic research in the United States.
Dorothy Gilford was one of the first Heads of the Probability and Statistics
program at the Office of Naval Research (1955 to 1962), and she went on to
serve as Director of the Mathematical Sciences Division (1962 to 1968). During
her time at ONR, Dorothy influenced many areas of statistics and mathematics
and was ahead of her time in promoting interdisciplinary projects. Dorothy
continued her career at the National Center for Education Statistics (1969 to
1974). She was active in starting international comparisons of education
outcomes in different countries, which has influenced educational policy in the
United States. Dorothy went on to serve in many capacities at the National
Academy of Sciences, including Director of Human Resources Studies (1975 to
1978), Senior Statistician on the Committee on National Statistics (1978 to
1988) and Director of the Board on International Comparative Studies in
Education (1988 to 1994). The following is a conversation we had with Dorothy
Gilford in March of 2004. We found her to be an interesting person and a
remarkable statistician. We hope you agree.

<id>
0710.5005v1
<category>
stat.ME
<abstract>
The general principles of Bayesian data analysis imply that models for survey
responses should be constructed conditional on all variables that affect the
probability of inclusion and nonresponse, which are also the variables used in
survey weighting and clustering. However, such models can quickly become very
complicated, with potentially thousands of poststratification cells. It is then
a challenge to develop general families of multilevel probability models that
yield reasonable Bayesian inferences. We discuss in the context of several
ongoing public health and social surveys. This work is currently open-ended,
and we conclude with thoughts on how research could proceed to solve these
problems.

<id>
0710.5009v1
<category>
stat.ME
<abstract>
Comment: Struggles with Survey Weighting and Regression Modeling
[arXiv:0710.5005]

<id>
0710.5012v1
<category>
stat.ME
<abstract>
Comment: Struggles with Survey Weighting and Regression Modeling
[arXiv:0710.5005]

<id>
0710.5013v1
<category>
stat.ME
<abstract>
Comment: Struggles with Survey Weighting and Regression Modeling
[arXiv:0710.5005]

<id>
0710.5015v1
<category>
stat.ME
<abstract>
Comment: Struggles with Survey Weighting and Regression Modeling
[arXiv:0710.5005]

<id>
0710.5016v1
<category>
stat.ME
<abstract>
Comment: Struggles with Survey Weighting and Regression Modeling
[arXiv:0710.5005]

<id>
math/0107135v2
<category>
stat.TH
<abstract>
We consider two kinds of stochastic volatility models. Both kinds of models
contain a stationary volatility process, the density of which, at a fixed
instant in time, we aim to estimate.
  We discuss discrete time models where for instance a log price process is
modeled as the product of a volatility process and i.i.d. noise. We also
consider samples of certain continuous time diffusion processes. The sampled
time instants will be be equidistant with vanishing distance.
  A Fourier type deconvolution kernel density estimator based on the logarithm
of the squared processes is proposed to estimate the volatility density.
Expansions of the bias and bounds on the variances are derived.

<id>
math/0109002v1
<category>
stat.TH
<abstract>
We show that that the jackknife variance estimator $v_{jack}$ and the the
infinitesimal jackknife variance estimator are asymptotically equivalent if the
functional of interest is a smooth function of the mean or a smooth trimmed
L-statistic. We calculate the asymptotic variance of $v_{jack}$ for these
functionals.

<id>
math/0111152v1
<category>
stat.TH
<abstract>
In this paper an iterated function system on the space of distribution
functions is built. The inverse problem is introduced and studied by convex
optimization problems. Some applications of this method to approximation of
distribution functions and to estimation theory are given.

<id>
math/0111153v1
<category>
stat.TH
<abstract>
A subthreshold signal is transmitted through a channel and may be detected
when some noise -- with known structure and proportional to some level -- is
added to the data. There is an optimal noise level, called stochastic
resonance, that corresponds to the highest Fisher information in the problem of
estimation of the signal. As noise we consider an ergodic diffusion process and
the asymptotic is considered as time goes to infinity. We propose consistent
estimators of the subthreshold signal and we solve further a problem of
hypotheses testing. We also discuss evidence of stochastic resonance for both
estimation and hypotheses testing problems via examples.

<id>
math/0112032v1
<category>
stat.TH
<abstract>
We derive asymptotic normality of kernel type deconvolution estimators of the
density, the distribution function at a fixed point, and of the probability of
an interval. We consider the so called super smooth case where the
characteristic function of the known distribution decreases exponentially.
  It turns out that the limit behavior of the pointwise estimators of the
density and distribution function is relatively straightforward while the
asymptotics of the estimator of the probability of an interval depends in a
complicated way on the sequence of bandwidths.

<id>
math/0112298v1
<category>
stat.TH
<abstract>
In the article we consider accumulated values of annuities-certain with
yearly payments with independent random interest rates. We focus on annuities
with payments varying in arithmetic and geometric progression which are
important basic varying annuities (see Kellison, 1991). They appear to be a
generalization of the types studied recently by Zaks (2001). We derive, via
recursive relationships, mean and variance formulae of the final values of the
annuities. As a consequence, we obtain moments related to the already discussed
cases, which leads to a correction of main results from Zaks (2001).

<id>
math/0202274v1
<category>
stat.TH
<abstract>
This paper is speculated to propose a class of shrinkage estimators for shape
parameter beta in failure censored samples from two-parameter Weibull
distribution when some 'apriori' or guessed interval containing the parameter
beta is available in addition to sample information and analyses their
properties. Some estimators are generated from the proposed class and compared
with the minimum mean squared error (MMSE) estimator. Numerical computations in
terms of percent relative efficiency and absolute relative bias indicate that
certain of these estimators substantially improve the MMSE estimator in some
guessed interval of the parameter space of beta, especially for censored
samples with small sizes. Subsequently, a modified class of shrinkage
estimators is proposed with its properties.

<id>
math/0203080v2
<category>
stat.TH
<abstract>
By the method of Poissonization we confirm some existing results concerning
consistent estimation of the structural distribution function in the situation
of a large number of rare events. Inconsistency of the so called natural
estimator is proved. The method of grouping in cells of equal size is
investigated and its consistency derived. A bound on the mean squared error is
derived.

<id>
math/0206006v2
<category>
stat.TH
<abstract>
We give a visually appealing counterexample to the proposition that unbiased
estimators are better than biased estimators.

<id>
math/0206142v1
<category>
stat.TH
<abstract>
We consider discrete time models for asset prices with a stationary
volatility process. We aim at estimating the multivariate density of this
process at a set of consecutive time instants. A Fourier type deconvolution
kernel density estimator based on the logarithm of the squared process is
proposed to estimate the volatility density. Expansions of the bias and bounds
on the variance are derived.

<id>
math/0207044v1
<category>
stat.TH
<abstract>
We construct an on-line estimator with equidistant design for tracking a
smooth function from Stone-Ibragimov-Khasminskii class. This estimator has the
optimal convergence rate of risk to zero in sample size. The procedure for
setting coefficients of the estimator is controlled by a single parameter and
has a simple numerical solution. The off-line version of this estimator allows
to eliminate a boundary layer. Simulation results are given.

<id>
math/0210425v1
<category>
stat.TH
<abstract>
We consider estimation of the structural distribution function of the cell
probabilities of a multinomial sample in situations where the number of cells
is large. We review the performance of the natural estimator, an estimator
based on grouping the cells and a kernel type estimator. Inconsistency of the
natural estimator and weak consistency of the other two estimators is derived
by Poissonization and other, new, technical devices.

<id>
math/0211079v3
<category>
stat.TH
<abstract>
We construct a density estimator and an estimator of the distribution
function in the uniform deconvolution model. The estimators are based on
inversion formulas and kernel estimators of the density of the observations and
its derivative. Asymptotic normality and the asymptotic biases are derived.

<id>
math/0212007v2
<category>
stat.TH
<abstract>
We derive asymptotic normality of kernel type deconvolution density
estimators. In particular we consider deconvolution problems where the known
component of the convolution has a symmetric lambda-stable distribution,
0<lambda<= 2. It turns out that the limit behavior changes if the exponent
parameter lambda passes the value one, the case of Cauchy deconvolution.

<id>
math/0212350v1
<category>
stat.TH
<abstract>
In this paper we will discuss a procedure to improve the usual estimator of a
linear functional of the unknown regression function in inverse nonparametric
regression models. In Klaassen, Lee, and Ruymgaart (2001) it has been proved
that this traditional estimator is not asymptotically efficient (in the sense
of the H\'{a}jek - Le Cam convolution theorem) except, possibly, when the error
distribution is normal. Since this estimator, however, is still root-n
consistent a procedure in Bickel, Klaassen, Ritov, and Wellner (1993) applies
to construct a modification which is asymptotically efficient. A self-contained
proof of the asymptotic efficiency is included.

<id>
math/0212395v1
<category>
stat.TH
<abstract>
Classical multiscale analysis based on wavelets has a number of successful
applications, e.g. in data compression, fast algorithms, and noise removal.
Wavelets, however, are adapted to point singularities, and many phenomena in
several variables exhibit intermediate-dimensional singularities, such as
edges, filaments, and sheets. This suggests that in higher dimensions, wavelets
ought to be replaced in certain applications by multiscale analysis adapted to
intermediate-dimensional singularities.
  My lecture described various initial attempts in this direction. In
particular, I discussed two approaches to geometric multiscale analysis
originally arising in the work of Harmonic Analysts Hart Smith and Peter Jones
(and others): (a) a directional wavelet transform based on parabolic dilations;
and (b) analysis via anistropic strips. Perhaps surprisingly, these tools have
potential applications in data compression, inverse problems, noise removal,
and signal detection; applied mathematicians, statisticians, and engineers are
eagerly pursuing these leads.

<id>
math/0212410v1
<category>
stat.TH
<abstract>
State space models have long played an important role in signal processing.
The Gaussian case can be treated algorithmically using the famous Kalman
filter. Similarly since the 1970s there has been extensive application of
Hidden Markov models in speech recognition with prediction being the most
important goal. The basic theoretical work here, in the case $X$ and $Y$ finite
(small) providing both algorithms and asymptotic analysis for inference is that
of Baum and colleagues. During the last 30-40 years these general models have
proved of great value in applications ranging from genomics to finance.
  Unless the $X,Y$ are jointly Gaussian or $X$ is finite and small the problem
of calculating the distributions discussed and the likelihood exactly are
numerically intractable and if $Y$ is not finite asymptotic analysis becomes
much more difficult. Some new developments have been the construction of
so-called ``particle filters'' (Monte Carlo type) methods for approximate
calculation of these distributions (see Doucet et al. [4]) for instance and
general asymptotic methods for analysis of statistical methods in HMM [2] and
other contributors.
  We will discuss these methods and results in the light of exponential mixing
properties of the conditional (posterior) distribution of $(X_1,X_2,...)$ given
$(Y_1,Y_2,...)$ already noted by Baum and Petrie and recent work of the contributors
Bickel, Ritov and Ryden, Del Moral and Jacod, Douc and Matias.

<id>
math/0212411v1
<category>
stat.TH
<abstract>
A classical limit theorem of stochastic process theory concerns the sample
cumulative distribution function (CDF) from independent random variables. If
the variables are uniformly distributed then these centered CDFs converge in a
suitable sense to the sample paths of a Brownian Bridge. The so-called
Hungarian construction of Komlos, Major and Tusnady provides a strong form of
this result. In this construction the CDFs and the Brownian Bridge sample paths
are coupled through an appropriate representation of each on the same
measurable space, and the convergence is uniform at a suitable rate.
  Within the last decade several asymptotic statistical-equivalence theorems
for nonparametric problems have been proven, beginning with Brown and Low
(1996) and Nussbaum (1996). The approach here to statistical-equivalence is
firmly rooted within the asymptotic statistical theory created by L. Le Cam but
in some respects goes beyond earlier results.
  This talk demonstrates the analogy between these results and those from the
coupling method for proving stochastic process limit theorems. These two
classes of theorems possess a strong inter-relationship, and technical methods
from each domain can profitably be employed in the other. Results in a recent
paper by Carter, Low, Zhang and myself will be described from this perspective.

<id>
math/0301363v1
<category>
stat.TH
<abstract>
The jackknife variance estimator and the the infinitesimal jackknife variance
estimator are shown to be asymptotically equivalent if the functional of
interest is a smooth function of the mean or a trimmed L-statistic with Hoelder
continuous weight function.

<id>
math/0302079v1
<category>
stat.TH
<abstract>
Log-linear models are a well-established method for describing statistical
dependencies among a set of n random variables. The observed frequencies of the
n-tuples are explained by a joint probability such that its logarithm is a sum
of functions, where each function depends on as few variables as possible. We
obtain for this class a new model selection criterion using nonasymptotic
concepts of statistical learning theory. We calculate the VC dimension for the
class of k-factor log-linear models. In this way we are not only able to select
the model with the appropriate complexity, but obtain also statements on the
reliability of the estimated probability distribution. Furthermore we show that
the selection of the best model among a set of models with the same complexity
can be written as a convex optimization problem.

<id>
math/0305234v1
<category>
stat.TH
<abstract>
Consider estimation of the regression parameter in the accelerated failure
time model, when data are obtained by cross sectional sampling. It is shown
that it is possible under regularity of the model to construct an efficient
estimator of the unknown Euclidean regression parameter if the distribution of
the covariate vector is known and also if it is unknown with vanishing mean.

<id>
math/0305273v2
<category>
stat.TH
<abstract>
This paper introduces a family of recursively defined estimators of the
parameters of a diffusion process. We use ideas of stochastic algorithms for
the construction of the estimators. Asymptotic consistency of these estimators
and asymptotic normality of an appropriate normalization are proved. The
results are applied to two examples from the financial literature; viz.,
Cox-Ingersoll-Ross' model and the constant elasticity of variance (CEV) process
illustrate the use of the technique proposed herein.

<id>
math/0306237v1
<category>
stat.TH
<abstract>
Let $X$ and $Y$ be two independent identically distributed random variables
with density $p(x)$ and $Z=\alpha X+\beta Y$ for some constants $\alpha>0$ and
$\beta>0$. We consider the problem of estimating $p(x)$ by means of the samples
from the distribution of $Z$. Non-parametric estimator based on the sync kernel
is constructed and asymptotic behaviour of the corresponding mean integrated
square error is investigated.

<id>
math/0309355v1
<category>
stat.TH
<abstract>
Let X be a n*p matrix and l_1 the largest eigenvalue of the covariance matrix
X^{*}*X. The "null case" where X_{i,j} are independent Normal(0,1) is of
particular interest for principal component analysis. For this model, when n, p
tend to infinity and n/p tends to gamma in (0,\infty), it was shown in
Johnstone (2001) that l_1, properly centered and scaled, converges to the
Tracy-Widom law. We show that with the same centering and scaling, the result
is true even when p/n or n/p tends to infinity. The derivation uses ideas and
techniques quite similar to the ones presented in Johnstone (2001). Following
Soshnikov (2002), we also show that the same is true for the joint distribution
of the k largest eigenvalues, where k is a fixed integer. Numerical experiments
illustrate the fact that the Tracy-Widom approximation is reasonable even when
one of the dimension is "small".

<id>
math/0310006v3
<category>
stat.TH
<abstract>
The marginalization paradox involves a disagreement between two Bayesians who
use two different procedures for calculating a posterior in the presence of an
improper prior. We show that the argument used to justify the procedure of one
of the Bayesians is inapplicable. There is therefore no reason to expect
agreement, no paradox, and no evidence that improper priors are inherently
inconsistent. We show further that the procedure in question can be interpreted
as the cancellation of infinities in the formal posterior. We suggest that the
implicit use of this formal procedure is the source of the observed
disagreement.

<id>
math/0312056v1
<category>
stat.TH
<abstract>
The subject of robust estimation in time series is widely discussed in
literature. One of the approaches is to use GM-estimation. This method
incorporates a broad class of nonparametric estimators which under suitable
conditions includes estimators robust to outliers in data. For the linear
models the sensitivity of GM-estimators to outliers have been studied in the
work by Martin and Yohai [5], and influence functionals for this estimator were
derived. In this paper we follow this direction and examine the asymptotical
properties of the class of M-estimators, which is narrower than the class of
GM-estimators, but gives more insight into asymptotical properties of such
estimators. This paper gives an asymptotic expansion of the residual weighted
empirical process, which allows to prove asymptotic normality of these
estimators in case of non-smooth objective functions. For simplicity MA(1)
model is considered, but it will be shown that even in this case mathematical
techniques used to derive these asymptotic properties appear to be rather
complicated.However, the approach used in this paper could be applied to
GM-estimators and to more realistic models.

<id>
math/0403373v1
<category>
stat.TH
<abstract>
Grade of membership (GoM) analysis was introduced in 1974 as a means of
analyzing multivariate categorical data. Since then, it has been successfully
applied to many problems. The primary goal of GoM analysis is to derive
properties of individuals based on results of multivariate measurements; such
properties are given in the form of the expectations of a hidden random
variable (state of an individual) conditional on the result of observations.
  In this article, we present a new perspective for the GoM model, based on
considering distribution laws of observed random variables as realizations of
another random variable. It happens that some moments of this new random
variable are directly estimable from observations. Our approach allows us to
establish a number of important relations between estimable moments and values
of interest, which, in turn, provides a basis for a new numerical procedure.

<id>
math/0405511v1
<category>
stat.TH
<abstract>
Vertex direction algorithms have been around for a few decades in the
experimental design and mixture models literature. We briefly review this type
of algorithm and describe a new member of the family: the support reduction
algorithm. The support reduction algorithm is applied to the problem of
computing nonparametric estimates in two inverse problems: convex density
estimation and the Gaussian deconvolution problem. Usually, VD algorithms solve
a finite dimensional (version of the) optimization problem of interest. We
introduce a method to solve the true infinite dimensional optimization problem.

<id>
math/0406424v1
<category>
stat.TH
<abstract>
We describe here a framework for a certain class of multiscale likelihood
factorizations wherein, in analogy to a wavelet decomposition of an L^2
function, a given likelihood function has an alternative representation as a
product of conditional densities reflecting information in both the data and
the parameter vector localized in position and scale. The framework is
developed as a set of sufficient conditions for the existence of such
factorizations, formulated in analogy to those underlying a standard
multiresolution analysis for wavelets, and hence can be viewed as a
multiresolution analysis for likelihoods. We then consider the use of these
factorizations in the task of nonparametric, complexity penalized likelihood
estimation. We study the risk properties of certain thresholding and
partitioning estimators, and demonstrate their adaptivity and near-optimality,
in a minimax sense over a broad range of function spaces, based on squared
Hellinger distance as a loss function. In particular, our results provide an
illustration of how properties of classical wavelet-based estimators can be
obtained in a single, unified framework that includes models for continuous,
count and categorical data types.

<id>
math/0406425v1
<category>
stat.TH
<abstract>
Starting from the observation of an R^n-Gaussian vector of mean f and
covariance matrix \sigma^2 I_n (I_n is the identity matrix), we propose a
method for building a Euclidean confidence ball around f, with prescribed
probability of coverage. For each n, we describe its nonasymptotic property and
show its optimality with respect to some criteria.

<id>
q-bio/0309007v1
<category>
q-bio.BM
<abstract>
We consider the regime in which the bands of the torsional acoustic (TA) and
the hydrogen-bond-stretch (HBS) modes of the DNA interpenetrate each other.
Within the framework of a model that accommodates the structure of the double
helix, we find the three-wave interaction between the TA- and the HBS-modes,
and show that microwave radiation could bring about torsional vibrations that
could serve as a pump mode for maintaining the HBS-one. Rayleigh's threshold
condition for the parametric resonance provides an estimate for the power
density of the mw-field necessary for generating the HBS-mode.

<id>
q-bio/0309017v1
<category>
q-bio.BM
<abstract>
Identifying the driving forces and the mechanism of association of
huntingtin-exon1, a close marker for the progress of Huntington's disease, is
an important prerequisite towards finding potential drug targets, and
ultimately a cure. We introduce here a modelling framework based on a key
analogy of the physico-chemical properties of the exon1 fragment to block
copolymers. We use a systematic mesoscale methodology, based on Dissipative
Particle Dynamics, which is capable of overcoming kinetic barriers, thus
capturing the dynamics of significantly larger systems over longer times than
considered before. Our results reveal that the relative hydrophobicity of the
poly-glutamine block as compared to the rest of the (proline-based) exon1
fragment, ignored to date, constitutes a major factor in the initiation of the
self-assembly process. We find that the assembly is governed by both the
concentration of exon1 and the length of the poly-glutamine stretch, with a low
length threshold for association even at the lowest volume fractions we
considered. Moreover, this self-association occurs irrespective of whether the
glutamine stretch is in random coil or hairpin configuration, leading to
spherical or cylindrical assemblies, respectively. We discuss the implications
of these results for reinterpretation of existing research within this context,
including that the routes towards aggregation of exon1 may be distinct to those
of the widely studied homopolymeric poly-glutamine peptides.

<id>
q-bio/0310013v1
<category>
q-bio.BM
<abstract>
The molecular mechanism of the solvent motion that is required to instigate
the protein structural relaxation above a critical hydration level or
transition temperature has yet to be determined. In this work we use
quasi-elastic neutron scattering (QENS) and molecular dynamics simulation to
investigate hydration water dynamics near a greatly simplified protein surface.
We consider the hydration water dynamics near the completely deuterated
N-acetyl-leucine-methylamide (NALMA) solute, a hydrophobic amino acid side
chain attached to a polar blocked polypeptide backbone, as a function of
concentration between 0.5M-2.0M, under ambient conditions. In this
Communication, we focus our results of hydration dynamics near a model protein
surface on the issue of how enzymatic activity is restored once a critical
hydration level is reached, and provide a hypothesis for the molecular
mechanism of the solvent motion that is required to trigger protein structural
relaxation when above the hydration transition.

<id>
q-bio/0310020v1
<category>
q-bio.BM
<abstract>
We analyze the dependence of thermal denaturation transition and folding
rates of globular proteins on the number of amino acid residues, N. Using
lattice Go models we show that DeltaT/T_F ~ N^-1, where T_F is the folding
transition temperature and DeltaT is the folding transition width. This finding
is consistent with finite size effects expected for the systems undergoing a
phase transition from a disordered to an ordered phase. The dependence of the
folding rates k_F on N for lattice models and the dataset of 57 proteins and
peptides shows that k_F = k_F^0 exp(-CN^beta) provides a good fit, if 0 < beta
<= 2/3 and C is a constant. We find that k_F = k_F^0 exp(-1.1N^0.5) with k_F^0
=(0.4x10^-6 s)^-1 can estimate optimal protein folding rates to within an order
of magnitude in most cases. By using this fit for a set of proteins with
beta-sheet topology we find that k_F^0 is approximately equal to k_U^0, the
prefactor for unfolding rates. The maximum ratio of k_U^0/k_F^0 is 10 for this
class of proteins.

<id>
q-bio/0310023v1
<category>
q-bio.BM
<abstract>
The asymmetry in the shapes of folded and unfolded states are probed using
two parameters, one being a measure of the sphericity and the other that
describes the shape. For the folded states, whose interiors are densely packed,
the radii of gyration (Rg) and these two parameters are calculated using the
coordinates of the experimentally determined structures. Although Rg scales as
expected for maximally compact structures, the distributions of the shape
parameters show that there is considerable asymmetry in the shapes of folded
structures. The degree of asymmetry is greater for proteins that form
oligomers. Analysis of the two- and three-body contacts in the native
structures shows that the presence of near equal number of contacts between
backbone and side-chains and between side-chains gives rise to dense packing.
We suggest that proteins with relatively large values of shape parameters can
tolerate volume mutations without greatly affecting the network of contacts or
their stability. To probe shape characteristics of denatured states we have
developed a model of a WW-like domain. The shape parameters, which are
calculated using Langevin simulations, change dramatically in the course of
coil to globule transition. Comparison of the values of shape parameters
between the globular state and the folded state of WW domain shows that both
energetic (especially dispersion in the hydrophobic interactions) and steric
effects are important in determining packing in proteins.

<id>
q-bio/0310026v1
<category>
q-bio.BM
<abstract>
Instead of conformation states of single residues, refined conformation
states of quintuplets are proposed to reflect conformation correlation. Simple
hidden Markov models combining with sliding window scores are used for
predicting secondary structure of a protein from its amino acid sequence. Since
the length of protein conformation segments varies in a narrow range, we ignore
the duration effect of the length distribution. The window scores for residues
are a window version of the Chou-Fasman propensities estimated under an
approximation of conditional independency. Different window widths are
examined, and the optimal width is found to be 17. A high accuracy about 70% is
achieved.

<id>
q-bio/0310034v1
<category>
q-bio.BM
<abstract>
Is protein secondary structure primarily determined by local interactions
between residues closely spaced along the amino acid backbone, or by non-local
tertiary interactions? To answer this question we have measured the entropy
densities of primary structure and secondary structure sequences, and the local
inter-sequence mutual information density. We find that the important
inter-sequence interactions are short ranged, that correlations between
neighboring amino acids are essentially uninformative, and that only 1/4 of the
total information needed to determine the secondary structure is available from
local inter-sequence correlations. Since the remaining information must come
from non-local interactions, this observation supports the view that the
majority of most proteins fold via a cooperative process where secondary and
tertiary structure form concurrently. To provide a more direct comparison to
existing secondary structure prediction methods, we construct a simple hidden
Markov model (HMM) of the sequences. This HMM achieves a prediction accuracy
comparable to other single sequence secondary structure prediction algorithms,
and can extract almost all of the inter-sequence mutual information. This
suggests that these algorithms are almost optimal, and that we should not
expect a dramatic improvement in prediction accuracy. However, local
correlations between secondary and primary structure are probably of
under-appreciated importance in many tertiary structure prediction methods,
such as threading.

<id>
q-bio/0311008v1
<category>
q-bio.BM
<abstract>
The determination of the folding mechanisms of proteins is critical to
understand the topological change that can propagate Alzheimer and
Creutzfeld-Jakobs diseases, among others. The computational community has paid
considerable attention to this problem; however, the associated time scale,
typically on the order of milliseconds or more, represents a formidable
challenge. Ab initio protein folding from long molecular dynamics (MD)
simulations or ensemble dynamics is not feasible with ordinary computing
facilities and new techniques must be introduced. Here we present a detailed
study of the folding of a 16-residue beta-hairpin, described by a generic
energy model and using the activation-relaxation technique. From a total of 90
trajectories at 300 K, three folding pathways emerge. All involve a
simultaneous optimization of the complete hydrophobic and hydrogen bonding
interactions. The first two follow closely those observed by previous
theoretical studies. The third pathway, never observed by previous all-atom
folding, unfolding and equilibrium simulations, can be described as a reptation
move of one strand of the beta-sheet with respect to the other. This reptation
move indicates that non-native interactions can play a dominant role in the
folding of secondary structures. These results point to a more complex folding
picture than expected for a simple beta-hairpin.

<id>
q-bio/0311010v1
<category>
q-bio.BM
<abstract>
Analytic estimates for the forces and free energy generated by bilayer
deformation reveal a compelling and intuitive model for MscL channel gating
analogous to the nucleation of a second phase. We argue that the competition
between hydrophobic mismatch and tension results in a surprisingly rich story
which can provide both a quantitative comparison to measurements of opening
tension for MscL when reconstituted in bilayers of different thickness and
qualitative insights into the function of the MscL channel and other
transmembrane proteins.

<id>
q-bio/0311011v1
<category>
q-bio.BM
<abstract>
It is important to understand how protein folding and evolution influences
each other. Several studies based on entropy calculation correlating
experimental measurement of residue participation in folding nucleus and
sequence conservation have reached different conclusions. Here we report
analysis of conservation of folding nucleus using an evolutionary model
alternative to entropy based approaches. We employ a continuous time Markov
model of codon substitution to distinguish mutation fixed by evolution and
mutation fixed by chance. This model takes into account bias in codon
frequency, bias favoring transition over transversion, as well as explicit
phylogenetic information. We measure selection pressure using the ratio
$\omega$ of synonymous vs. non-synonymous substitution at individual residue
site. The $\omega$-values are estimated using the {\sc Paml} method, a
maximum-likelihood estimator. Our results show that there is little correlation
between the extent of kinetic participation in protein folding nucleus as
measured by experimental $\phi$-value and selection pressure as measured by
$\omega$-value. In addition, two randomization tests failed to show that
folding nucleus residues are significantly more conserved than the whole
protein. These results suggest that at the level of codon substitution, there
is no indication that folding nucleus residues are significantly more conserved
than other residues. We further reconstruct candidate ancestral residues of the
folding nucleus and suggest possible test tube mutation studies of ancient
folding nucleus.

<id>
q-bio/0311023v2
<category>
q-bio.BM
<abstract>
Many signalling functions in molecular biology require proteins bind to
substrates such as DNA in response to environmental signals such as the
simultaneous binding to a small molecule. Examples are repressor proteins which
may transmit information via a conformational change in response to the ligand
binding. An alternative entropic mechanism of ``allostery'' suggests that the
inducer ligand changes the intramolecular vibrational entropy not just the
static structure. We present a quantitative, coarse-grained model of entropic
allostery that suggests design rules for internal cohesive potentials in
proteins employing this effect. It also addresses the issue of how the signal
information to bind or unbind is transmitted through the protein. The model may
be applicable to a wide range of repressors and also to signalling in
transmembrane proteins.

<id>
q-bio/0311033v1
<category>
q-bio.BM
<abstract>
How DNA repair enzymes find the relatively rare sites of damage is not known
in great detail. Recent experiments and molecular data suggest that the
individual repair enzymes do not work independently of each other, but rather
interact with each other through currents exchanged along DNA. A damaged site
in DNA hinders this exchange and this makes it possible to quickly free up
resources from error free stretches of DNA. Here the size of the speedup gained
from this current exchange mechanism is calculated and the characteristic
length and time scales are identified. In particular for Escherichia coli we
estimate the speedup to be 50000/N, where N is the number of repair enzymes
participating in the current exchange mechanism. Even though N is not exactly
known a speedup of order 10 is not entirely unreasonable. Furthermore upon over
expression of repair enzymes the detection time only varies as one over the
squareroot of N and not as 1/N. This behavior is of interest in assessing the
impact of stress full and radioactive environments on individual cell mutation
rates.

<id>
q-bio/0311038v1
<category>
q-bio.BM
<abstract>
We study DNA adsorption and renaturation in a water-phenol two-phase system,
with or without shaking. In very dilute solutions, single-stranded DNA is
adsorbed at the interface in a salt-dependent manner. At high salt
concentrations the adsorption is irreversible. The adsorption of the
single-stranded DNA is specific to phenol and relies on stacking and hydrogen
bonding. We establish the interfacial nature of a DNA renaturation at a high
salt concentration. In the absence of shaking, this reaction involves an
efficient surface diffusion of the single-stranded DNA chains. In the presence
of a vigorous shaking, the bimolecular rate of the reaction exceeds the
Smoluchowski limit for a three-dimensional diffusion-controlled reaction. DNA
renaturation in these conditions is known as the Phenol Emulsion Reassociation
Technique or PERT. Our results establish the interfacial nature of PERT. A
comparison of this interfacial reaction with other approaches shows that PERT
is the most efficient technique and reveals similarities between PERT and the
renaturation performed by single-stranded nucleic acid binding proteins. Our
results lead to a better understanding of the partitioning of nucleic acids in
two-phase systems, and should help design improved extraction procedures for
damaged nucleic acids. We present arguments in favor of a role of phenol and
water-phenol interface in prebiotic chemistry. The most efficient renaturation
reactions (in the presence of condensing agents or with PERT) occur in
heterogeneous systems. This reveals the limitations of homogeneous approaches
to the biochemistry of nucleic acids. We propose a heterogeneous approach to
overcome the limitations of the homogeneous viewpoint.

<id>
q-bio/0312010v1
<category>
q-bio.BM
<abstract>
Hydrophobicity is thought to be one of the primary forces driving the folding
of proteins. On average, hydrophobic residues occur preferentially in the core,
whereas polar residues tends to occur at the surface of a folded protein. By
analyzing the known protein structures, we quantify the degree to which the
hydrophobicity sequence of a protein correlates with its pattern of surface
exposure. We have assessed the statistical significance of this correlation for
several hydrophobicity scales in the literature, and find that the computed
correlations are significant but far from optimal. We show that this less than
optimal correlation arises primarily from the large degree of mutations that
naturally occurring proteins can tolerate. Lesser effects are due in part to
forces other than hydrophobicity and we quantify this by analyzing the surface
exposure distributions of all amino acids. Lastly we show that our database
findings are consistent with those found from an off-lattice hydrophobic-polar
model of protein folding.

<id>
q-bio/0312034v1
<category>
q-bio.BM
<abstract>
The approach for the description of the DNA conformational transformations on
the mesoscopic scales in the frame of the double helix is presented. Due to
consideration of the joint motions of DNA structural elements along the
conformational pathways the models for different transformations may be
constructed in the unifying two-component form. One component of the model is
the degree of freedom of the elastic rod and another component -- the effective
coordinate of the conformational transformation. The internal and external
model components are interrelated, as it is characteristic for the DNA
structure organization. It is shown that the kinetic energy of the
conformational transformation of heterogeneous DNA may be put in homogeneous
form. In the frame of the developed approach the static excitations of the DNA
structure under the transitions between the stable states are found for
internal and external components. The comparison of the data obtained with the
experiment on intrinsic DNA deformability shows good qualitative agreement. The
conclusion is made that the found excitations in the DNA structure may be
classificated as the static conformational solitons.

<id>
q-bio/0312035v1
<category>
q-bio.BM
<abstract>
Molecular combing is a powerful and simple method for aligning DNA molecules
onto a surface. Using this technique combined with fluorescence microscopy, we
observed that the length of lambda-DNA molecules was extended to about 1.6
times their contour length (unextended length, 16.2 micrometers) by the combing
method on hydrophobic polymethylmetacrylate (PMMA) coated surfaces. The effects
of sodium and magnesium ions and pH of the DNA solution were investigated.
Interestingly, we observed force-induced melting of single DNA molecules.

<id>
q-bio/0312036v2
<category>
q-bio.BM
<abstract>
Using a Brownian dynamics simulation, we numerically studied the interaction
of DNA with histone and proposed an octamer-rotation model to describe the
process of nucleosome formation. Nucleosome disruption under stretching was
also simulated. The theoretical curves of extension versus time as well as of
force versus extension are consistent with previous experimental results.

<id>
q-bio/0312040v1
<category>
q-bio.BM
<abstract>
We propose a two-dimensional model for a complete description of the dynamics
of molecular motors, including both the processive movement along track
filaments and the dissociation from the filaments. The theoretical results on
the distributions of the run length and dwell time at a given ATP
concentration, the dependences of mean run length, mean dwell time and mean
velocity on ATP concentration and load are in good agreement with the previous
experimental results.

<id>
q-bio/0312043v1
<category>
q-bio.BM
<abstract>
Kinesin motors have been studied extensively both experimentally and
theoretically. However, the microscopic mechanism of the processive movement of
kinesin is still an open question. In this paper, we propose a hand-over-hand
model for the processivity of kinesin, which is based on chemical, mechanical,
and electrical couplings. In the model the processive movement does not need to
rely on the two heads' coordination in their ATP hydrolysis and mechanical
cycles. Rather, the ATP hydrolyses at the two heads are independent. The much
higher ATPase rate at the trailing head than the leading head makes the motor
walk processively in a natural way, with one ATP being hydrolyzed per step. The
model is consistent with the structural study of kinesin and the measured
pathway of the kinesin ATPase. Using the model the estimated driving force of ~
5.8 pN is in agreements with the experimental results (5~7.5 pN). The
prediction of the moving time in one step (~10 microseconds) is also consistent
with the measured values of 0~50 microseconds. The previous observation of
substeps within the 8-nm step is explained. The shapes of velocity-load (both
positive and negative) curves show resemblance to previous experimental
results.

<id>
q-bio/0312044v1
<category>
q-bio.BM
<abstract>
Myosin V and myosin VI are two classes of two-headed molecular motors of the
myosin superfamily that move processively along helical actin filaments in
opposite directions. Here we present a hand-over-hand model for their
processive movements. In the model, the moving direction of a dimeric molecular
motor is automatically determined by the relative orientation between its two
heads at free state and its head's binding orientation on track filament. This
determines that myosin V moves toward the barbed end and myosin VI moves toward
the pointed end of actin. During the moving period in one step, one head
remains bound to actin for myosin V whereas two heads are detached for myosin
VI: The moving manner is determined by the length of neck domain. This
naturally explains the similar dynamic behaviors but opposite moving directions
of myosin VI and mutant myosin V (the neck of which is truncated to only
one-sixth of the native length). Because of different moving manners, myosin VI
and mutant myosin V exhibit significantly broader step-size distribution than
native myosin V. However, all three motors give the same mean step size of 36
nm (the pseudo-repeat of actin helix). Using the model we study the dynamics of
myosin V quantitatively, with theoretical results in agreement with previous
experimental ones.

<id>
q-bio/0401011v1
<category>
q-bio.BM
<abstract>
We describe a faster and more accurate algorithm for computing the
statistical mechanics of DNA denaturation according to the Poland-Scheraga
type. Nearest neighbor thermodynamics is included in a complete and general
way. The algorithm represents an optimization with respect to algorithmic
complexity of the partition function algorithm of Yeramian et al.: We reduce
the computation time for a base-pairing probability profile from O(N2) to O(N).
This speed-up comes in addition to the speed-up due to a multiexponential
approximation of the loop entropy factor as introduced by Fixman and Freire.
The speed-up, however, is independent of the multiexponential approximation and
reduces time from O(N3) to O(N2) in the exact case. In addition to calculating
the standard base-pairing probability profiles, we propose to use the algorithm
to calculate various other probabilities (loops, helices, tails) for a more
direct view of the melting regions and their positions and sizes.

<id>
q-bio/0401012v1
<category>
q-bio.BM
<abstract>
A joint experimental / theoretical investigation of the elastin-like
octapeptide GVG(VPGVG) was carried out. In this paper a comprehensive molecular
dynamics study of the temperature dependent folding and unfolding of the
octapeptide is presented. The current study, as well as its experimental
counterpart find that this peptide undergoes an "inverse temperature
transition", ITT, leading to a folding at about 310-330 K. In addition, an
unfolding transition is identified at unusually high temperatures approaching
the boiling point of water. Due to the small size of the system two broad
temperature regimes are found: the "ITT regime" (at about 280-320 K) and the
"unfolding regime" at about T > 330 K, where the peptide has a maximum
probability of being folded at approximately 330 K. A detailed molecular
picture involving a thermodynamic order parameter, or reaction coordinate, for
this process is presented along with a time-correlation function analysis of
the hydrogen bond dynamics within the peptide as well as between the peptide
and solvating water molecules. Correlation with experimental evidence and
ramifications on the properties of elastin are discussed.

<id>
q-bio/0401021v1
<category>
q-bio.BM
<abstract>
A simplified model for the closed circular DNA (ccDNA) is proposed to
describe some specific features of the helix-coil transition in such molecule.
The Hamiltonian of ccDNA is related to the one introduced earlier for the
linear DNA. The basic assumption is that the reduced energy of the hydrogen
bond is not constant through the transition process but depends effectively on
the fraction of already broken bonds. A transformation formula is obtained
which relates the temperature of ccDNA at a given degree of helicity during the
transition to the temperature of the corresponding linear chain at the same
degree of helicity. The formula provides a simple method to calculate the
melting curve for the ccDNA from the experimental melting curve of the linear
DNA with the same nucleotide sequence.

<id>
q-bio/0401034v2
<category>
q-bio.BM
<abstract>
We develop a simple but rigorous model of protein-protein association
kinetics based on diffusional association on free energy landscapes obtained by
sampling configurations within and surrounding the native complex binding
funnels. Guided by results obtained on exactly solvable model problems, we
transform the problem of diffusion in a potential into free diffusion in the
presence of an absorbing zone spanning the entrance to the binding funnel. The
free diffusion problem is solved using a recently derived analytic expression
for the rate of association of asymmetrically oriented molecules. Despite the
required high steric specificity and the absence of long-range attractive
interactions, the computed rates are typically on the order of 10^4-10^6 M-1
s-1, several orders of magnitude higher than rates obtained using a purely
probabilistic model in which the association rate for free diffusion of
uniformly reactive molecules is multiplied by the probability of a correct
alignment of the two partners in a random collision. As the association rates
of many protein-protein complexes are also in the 10^5-10^6 M-1 s-1, our
results suggest that free energy barriers arising from desolvation and/or
side-chain freezing during complex formation or increased ruggedness within the
binding funnel, which are completely neglected in our simple diffusional model,
do not contribute significantly to the dynamics of protein-protein association.
The transparent physical interpretation of our approach that computes
association rates directly from the size and geometry of protein-protein
binding funnels makes it a useful complement to Brownian dynamics simulations.

<id>
q-bio/0401038v1
<category>
q-bio.BM
<abstract>
Functional proteins must fold with some minimal stability to a structure that
can perform a biochemical task. Here we use a simple model to investigate the
relationship between the stability requirement and the capacity of a protein to
evolve the function of binding to a ligand. Although our model contains no
built-in tradeoff between stability and function, proteins evolved function
more efficiently when the stability requirement was relaxed. Proteins with both
high stability and high function evolved more efficiently when the stability
requirement was gradually increased than when there was constant selection for
high stability. These results show that in our model, the evolution of function
is enhanced by allowing proteins to explore sequences corresponding to
marginally stable structures, and that it is easier to improve stability while
maintaining high function than to improve function while maintaining high
stability. Our model also demonstrates that even in the absence of a
fundamental biophysical tradeoff between stability and function, the speed with
which function can evolve is limited by the stability requirement imposed on
the protein.

<id>
q-bio/0402018v1
<category>
q-bio.BM
<abstract>
Using the model for the processive movement of a dimeric kinesin we proposed
before, we study the dynamics of a number of mutant homodimeric and
heterodimeric kinesins that were constructed by Kaseda et al. (Kaseda, K.,
Higuchi, H. and Hirose, K. PNAS 99, 16058 (2002)). The theoretical results of
ATPase rate per head, moving velocity, and stall force of the motors show good
agreement with the experimental results by Kaseda et al.: The puzzling dynamic
behaviors of heterodimeric kinesin that consists of two distinct heads compared
with its parent homodimers can be easily explained by using independent ATPase
rates of the two heads in our model. We also study the collective kinetic
behaviors of kinesins in MT-gliding motility. The results explains well that
the average MT-gliding velocity is independent of the number of bound motors
and is equal to the moving velocity of a single kinesin relative to MT.

<id>
q-bio/0402039v1
<category>
q-bio.BM
<abstract>
The simplest approximation of interaction potential between amino-acids in
proteins is the contact potential, which defines the effective free energy of a
protein conformation by a set of amino acid contacts formed in this
conformation. Finding a contact potential capable of predicting free energies
of protein states across a variety of protein families will aid protein folding
and engineering in silico on a computationally tractable time-scale. We test
the ability of contact potentials to accurately and transferably (across
various protein families) predict stability changes of proteins upon mutations.
We develop a new methodology to determine the contact potentials in proteins
from experimental measurements of changes in protein thermodynamic stabilities
(ddG) upon mutations. We apply our methodology to derive sets of contact
interaction parameters for a hierarchy of interaction models including
solvation and multi-body contact parameters. We test how well our models
reproduce experimental measurements by statistical tests. We evaluate the
maximum accuracy of predictions obtained by using contact potentials and the
correlation between parameters derived from different data-sets of experimental
ddG values. We argue that it is impossible to reach experimental accuracy and
derive fully transferable contact parameters using the contact models of
potentials. However, contact parameters can yield reliable predictions of ddG
for datasets of mutations confined to specific amino-acid positions in the
sequence of a single protein.

<id>
q-bio/0403019v3
<category>
q-bio.BM
<abstract>
We first review how to determine the rate of vibrational energy relaxation
(VER) using perturbation theory. We then apply those theoretical results to the
problem of VER of a CD stretching mode in the protein cytochrome c. We model
cytochrome c in vacuum as a normal mode system with the lowest-order anharmonic
coupling elements. We find that, for the ``lifetime'' width parameter $\gamma=3
\sim 30$ cm$^{-1}$, the VER time is $0.2 \sim 0.3$ ps, which agrees rather well
with the previous classical calculation using the quantum correction factor
method, and is consistent with spectroscopic experiments by Romesberg's group.
We decompose the VER rate into separate contributions from two modes, and find
that the most significant contribution, which depends on the ``lifetime'' width
parameter, comes from those modes most resonant with the CD vibrational mode.

<id>
q-bio/0404005v1
<category>
q-bio.BM
<abstract>
The three-dimensional structures of two common repeat motifs
Val$^1$-Pro$^2$-Gly$^3$-Val$^4$-Gly$^5$ and
Val$^1$-Gly$^2$-Val$^3$-Pro$^4$-Gly$^5$-Val$^6$-Gly$^7$-Val$^8$-Pro$^9$ of
tropoelastin are investigated by using the multicanonical simulation procedure.
By minimizing the energy structures along the trajectory the thermodynamically
most stable low-energy microstates of the molecule are determined. The
structural predictions are in good agreement with X-ray diffraction
experiments.

<id>
q-bio/0404011v1
<category>
q-bio.BM
<abstract>
We address the controversial hot question concerning the validity of the
loose-coupling versus the lever-arm models in the actomyosin dynamics by
re-interpreting and extending the washboard potential model proposed by some of
us in a previous paper. In the new theory, a loose-coupling mechanism co-exists
with the deterministic lever-arm model. The synergetic action of a random
component, originating from the harnessed thermal energy, and of the
power-stroke generated by the lever-arm classical mechanism is seen to yield an
excellent fit of the set of data obtained in T. Yanagida's laboratory on the
sliding of Myosin II heads on actin filaments under various load conditions.
Our theoretical arguments are complemented by accurate numerical simulations,
and the robustness of theory is tested via different combination of parameters
and potential profiles.

<id>
q-bio/0309016v1
<category>
q-bio.CB
<abstract>
We report the cell biological applications of a recently developed
multiphoton fluorescence lifetime imaging microscopy system using a streak
camera (StreakFLIM). The system was calibrated with standard fluorophore
specimens and was shown to have high accuracy and reproducibility. We
demonstrate the applicability of this instrument in living cells for measuring
the effects of protein targeting and point mutations in the protein sequence
which are not obtainable in conventional intensity based fluorescence
microscopy methods. We discuss the relevance of such time resolved information
in quantitative energy transfer microscopy and in measurement of the parameters
characterizing intracellular physiology.

<id>
q-bio/0310014v3
<category>
q-bio.CB
<abstract>
A novel assay based on micropatterning and time-lapse microscopy has been
developed for the study of nuclear migration dynamics in cultured mammalian
cells. When cultured on 10-20 um wide adhesive stripes, the motility of C6
glioma and primary mouse fibroblast cells is diminished. Nevertheless, nuclei
perform an unexpected auto-reverse motion: when a migrating nucleus approaches
the leading edge, it decelerates, changes the direction of motion and
accelerates to move toward the other end of the elongated cell. During this
process cells show signs of polarization closely following the direction of
nuclear movement. The observed nuclear movement requires a functioning
microtubular system, as revealed by experiments disrupting the main
cytoskeletal components with specific drugs. On the basis of our results we
argue that auto-reverse nuclear migration is due to forces determined by the
interplay of microtubule dynamics and the changing position of the microtubule
organizing center as the nucleus reaches the leading edge. Our assay
recapitulates specific features of nuclear migration (cell polarization,
oscillatory nuclear movement), while allows the systematic study of a large
number of individual cells. In particular, our experiments yielded the first
direct evidence of reversive nuclear motion in mammalian cells, induced by
attachment constraints.

<id>
q-bio/0310016v1
<category>
q-bio.CB
<abstract>
We investigate the final stage of cytokinesis in two types of amoeba,
pointing out the existence of biphasic furrow contraction. The first phase is
characterized by a constant contraction rate, is better studied, and seems
universal to a large extent. The second phase is more diverse. In Dictyostelium
discoideum the transition involves a change in the rate of contraction, and
occurs when the width of the cleavage furrow is comparable to the height of the
cell. In Entamoeba invadens the contractile ring carries the cell through the
first phase, but cannot complete the second stage of cytokinesis. As a result,
a cooperative mechanism has evolved in that organism, where a neighboring
amoeba performs directed motion towards the dividing cell, and physically
causes separation by means of extending a pseudopod. We expand here on a
previous report of this novel chemotactic signaling mechanism.

<id>
q-bio/0310030v1
<category>
q-bio.CB
<abstract>
The immune system protects the body against health-threatening entities,
known as antigens, through very complex interactions involving the antigens and
the system's own entities. One remarkable feature resulting from such
interactions is the immune system's ability to improve its capability to fight
antigens commonly found in the individual's environment. This adaptation
process is called the evolution of specificity. In this paper, we introduce a
new mathematical model for the evolution of specificity in humoral immunity,
based on Jerne's functional, or idiotypic, network. The evolution of
specificity is modeled as the dynamic updating of connection weights in a graph
whose nodes are related to the network's idiotypes. At the core of this
weight-updating mechanism are the increase in specificity caused by clonal
selection and the decrease in specificity due to the insertion of uncorrelated
idiotypes by the bone marrow. As we demonstrate through numerous computer
experiments, for appropriate choices of parameters the new model correctly
reproduces, in qualitative terms, several immune functions.

<id>
q-bio/0401032v1
<category>
q-bio.CB
<abstract>
The problem of the onset and growth of solid tumour in homogeneous tissue is
regarded using an approach based on local interaction between the tumoral and
the sane tissue cells. The characteristic sizes and growth rates of spherical
tumours, the points of the beginning and the end of spherical growth, and the
further development of complex structures (elongated outgrowths, dendritic
structures, and metastases) are derived from the assumption that the
reproduction rate of a population of cancer cells is a non-monotone function of
their local concentration. The predicted statistical distribution of the
characteristic tumour sizes, when compared to the clinical data, will make a
basis for checking the validity of the theory.

<id>
q-bio/0406028v1
<category>
q-bio.CB
<abstract>
In this paper we consider a continuous-time anisotropic swarm model with an
attraction/repulsion function and study its aggregation properties. It is shown
that the swarm members will aggregate and eventually form a cohesive cluster of
finite size around the swarm center. We also study the swarm cohesiveness when
the motion of each agent is a combination of the inter-individual interactions
and the interaction of the agent with external environment. Moreover, we extend
our results to more general attraction/repulsion functions. The model in this
paper is more general than isotropic swarms and our results provide further
insight into the effect of the interaction pattern on individual motion in a
swarm system.

<id>
q-bio/0408001v1
<category>
q-bio.CB
<abstract>
Experimental evidence indicates that human brain cancer cells proliferate or
migrate, yet do not display both phenotypes at the same time. Here, we present
a novel computational model simulating this cellular decision-process leading
up to either phenotype based on a molecular interaction network of genes and
proteins. The model's regulatory network consists of the epidermal growth
factor receptor (EGFR), its ligand transforming growth factor-a (TGFa), the
downstream enzyme phospholipaseC-gamma (PLCg) and a mitosis-associated response
pathway. This network is activated by autocrine TGFa secretion, and the
EGFR-dependent downstream signaling this step triggers, as well as modulated by
an extrinsic nutritive glucose gradient. Employing a framework of mass action
kinetics within a multiscale agent-based environment, we analyze both the
emergent multicellular behavior of tumor growth and the single-cell molecular
profiles that change over time and space. Our results show that one can indeed
simulate the dichotomy between cell migration and proliferation based solely on
an EGFR decision network. It turns out that these behavioral decisions on the
single cell level impact the spatial dynamics of the entire cancerous system.
Furthermore, the simulation results yield intriguing experimentally testable
hypotheses also on the sub-cellular level such as spatial cytosolic
polarization of PLCg towards an extrinsic chemotactic gradient. Implications of
these results for future works, both on the modeling and experimental side are
discussed.

<id>
q-bio/0408003v1
<category>
q-bio.CB
<abstract>
Biological systems excel at building spatial structures on scales ranging
from nanometers to kilometers and exhibit temporal patterning from milliseconds
to years. One approach that nature has taken to accomplish this relies on the
harnessing of pattern-forming processes of non-equilibrium physics and
chemistry. For these systems, the study of biological pattern formation starts
with placing a biological phenomenon of interest within the context of the
proper pattern-formation schema and then focusing on the ways in which control
is exerted to adapt the pattern to the needs of the organism. This approach is
illustrated by several examples, notably bacterial colonies (diffusive-growth
schema) and intracellular calcium waves (excitable-media schema).

<id>
q-bio/0411018v1
<category>
q-bio.CB
<abstract>
During embryonic development, a spatial pattern is formed in which
proportions are established precisely. As an early pattern formation step in
Drosophila embryos, an anterior-posterior gradient of Bicoid (Bcd) induces
hunchback (hb) expression (Driever et al. 1989; Tautz et al. 1988). In contrast
to the Bcd gradient, the Hb profile includes information about the scale of the
embryo. Furthermore, the resulting hb expression pattern shows a much lower
embryo-to-embryo variability than the Bcd gradient (Houchmandzadeh et al.
2002). An additional graded posterior repressing activity could theoretically
account for the observed scaling. However, we show that such a model cannot
produce the observed precision in the Hb boundary, such that a fundamentally
different mechanism must be at work. We describe and simulate a model that can
account for the observed precise generation of the scaled Hb profile in a
highly robust manner. The proposed mechanism includes Staufen (Stau), an RNA
binding protein that appears essential to precision scaling (Houchmandzadeh et
al. 2002). In the model, Stau is released from both ends of the embryo and
relocalises hb RNA by increasing its mobility. This leads to an effective
transport of hb away from the respective Stau sources. The balance between
these opposing effects then gives rise to scaling and precision. Considering
the biological importance of robust precision scaling and the simplicity of the
model, the same principle may be employed more often during development.

<id>
q-bio/0501007v2
<category>
q-bio.CB
<abstract>
We investigate noise-induced pattern formation in a model of cancer growth
based on Michaelis-Menten kinetics, subject to additive and multiplicative
noises. We analyse stability properties of the system and discuss the role of
diffusion and noises in the system's dynamics. We find that random dichotomous
fluctuations in the immune response intensity along with Gaussian environmental
noise lead to emergence of a spatial pattern of two phases, in which cancer
cells, or, respectively, immune cells predominate.

<id>
q-bio/0502041v1
<category>
q-bio.CB
<abstract>
Morphogenesis is the ensemble of processes that determines form, shape and
patterns in organisms. Based on a reaction-diffusion theoretical setting and
some prototype reaction schemes, we make a review of the models and experiments
that support possible mechanisms of morphogenesis. We present specific case
studies from chemistry (Belousov-Zhabotinsky reaction) and biology (formation
of wing eyespots patterns in butterflies). We show the importance of
conservation laws in the establishment of patterning in biological systems, and
their relevance to explain phenotypic plasticity in living organisms. Mass
conservation introduces a memory effect in biological development and
phenotypic plasticity in patterns of living organisms can be explained by
differences on the initial conditions occurring during development.

<id>
q-bio/0503016v1
<category>
q-bio.CB
<abstract>
Monte Carlo simulations of the number of stem cells in human colon crypts
allow for fluctuations which kill the population after sufficiently long times.

<id>
q-bio/0506036v1
<category>
q-bio.CB
<abstract>
Although Turing pattern is one of the most universal mechanisms for pattern
formation, in its standard model the number of stripes changes with the system
size, since the wavelength of the pattern is invariant: It fails to preserve
the proportionality of the pattern, i.e., the ratio of the wavelength to the
size, that is often required in biological morphogeneis. To get over this
problem, we show that the Turing pattern can preserve proportionality by
introducing a catalytic chemical whose concentration depends on the system
size. Several plausible mechanisms for such size dependence of the
concentration are discussed. Following this general discussion, two models are
studied in which arising Turing patterns indeed preserve the proportionality.
Relevance of the present mechanism to biological morphogenesis is discussed
from the viewpoint of its generality, robustness, and evolutionary
accessibility.

<id>
q-bio/0507011v1
<category>
q-bio.CB
<abstract>
Many complex systems obey allometric, or power, laws y=Yx^{a}. Here y is the
measured value of some system attribute a, Y is a constant, and x is a
stochastic variable. Remarkably, for many living systems the exponent a is
limited to values +or- n/4, n=0,1,2... Here x is the mass of a randomly
selected creature in the population. These quarter-power laws hold for many
attributes, such as pulse rate (n=-1). Allometry has, in the past, been
theoretically justified on a case-by-case basis. An ultimate goal is to find a
common cause for allometry of all types and for both living and nonliving
systems. The principle I - J = extrem. of Extreme physical information (EPI) is
found to provide such a cause. It describes the flow of Fisher information J =>
I from an attribute value a on the cell level to its exterior observation y.
Data y are formed via a system channel function y = f(x,a), with f(x,a) to be
found. Extremizing the difference I - J through variation of f(x,a) results in
a general allometric law f(x,a)= y = Yx^{a}. Darwinian evolution is presumed to
cause a second extremization of I - J, now with respect to the choice of a. The
solution is a=+or-n/4, n=0,1,2..., defining the particular powers of biological
allometry. Under special circumstances, the model predicts that such biological
systems are controlled by but two distinct intracellular information sources.
These sources are conjectured to be cellular DNA and cellular transmembrane ion
gradients

<id>
q-bio/0507021v1
<category>
q-bio.CB
<abstract>
A key problem of eukaryotic cell motility is the signaling mechanism of
chemoattractant gradient sensing. Recent experiments have revealed the
molecular correlate of gradient sensing: Frontness molecules, such as PI3P and
Rac, localize at the front end of the cell, and backness molecules, such as Rho
and myosin II, accumulate at the back of the cell. Importantly, this
frontness-backness polarization occurs "spontaneously" even if the cells are
exposed to uniform chemoattractant profiles. The spontaneous polarization
suggests that the gradient sensing machinery undergoes a Turing bifurcation.
This has led to several classical activator-inhibitor and activator-substrate
models which identify the frontness molecules with the activator. Conspicuously
absent from these models is any accounting of the backness molecules. This
stands in sharp contrast to experiments which show that the backness pathways
inhibit the frontness pathways. Here, we formulate a model based on the
mutually inhibitory interaction between the frontness and backness pathways.
The model builds upon the mutual inhibition model proposed by Bourne and
coworkers (Xu et al, Cell, 114, 201--214, 2003). We show that mutual inhibition
alone, without the help of any positive feedback, can trigger spontaneous
polarization of the frontness and backness pathways. The spatial distribution
of the frontness and backness molecules in response to inhbition and activation
of the frontness and backness pathways are consistent with those observed in
experiments. Furthermore, depending on the parameter values, the model yields
spatial distributions corresponding to chemoattraction (frontness pathways
in-phase with the external gradient) and chemorepulsion (frontness pathways
out-of-phase with the external gradient).

<id>
q-bio/0507034v1
<category>
q-bio.CB
<abstract>
In this work, we study the in-vitro dynamics of the most malignant form of
the primary brain tumor: Glioblastoma Multiforme. Typically, the growing tumor
consists of the inner dense proliferating zone and the outer less dense
invasive region. Experiments with different types of cells show qualitatively
different behavior. Wild-type cells invade a spherically symmetric manner, but
mutant cells are organized in tenuous branches. We formulate a model for this
sort of growth using two coupled reaction-diffusion equations for the cell and
nutrient concentrations. When the ratio of the nutrient and cell diffusion
coefficients exceeds some critical value, the plane propagating front becomes
unstable with respect to transversal perturbations. The instability threshold
and the full phase-plane diagram in the parameter space are determined. The
results are in a good agreement with experimental findings for the two types of
cells.

<id>
q-bio/0507035v1
<category>
q-bio.CB
<abstract>
We present a discrete stochastic model which represents many of the salient
features of the biological process of wound healing. The model describes fronts
of cells invading a wound. We have numerical results in one and two dimensions.
In one dimension we can give analytic results for the front speed as a power
series expansion in a parameter, p, that gives the relative size of
proliferation and diffusion processes for the invading cells. In two dimensions
the model becomes the Eden model for p near 1. In both one and two dimensions
for small p, front propagation for this model should approach that of the
Fisher-Kolmogorov equation. However, as in other cases, this discrete model
approaches Fisher-Kolmogorov behavior slowly.

<id>
q-bio/0508041v1
<category>
q-bio.CB
<abstract>
Membrane potential in a mathematical model of a cardiac myocyte can be
formulated in different ways. Assuming a spatially homogeneous myocyte that is
strictly charge-conservative and electroneutral as a whole, two methods will be
compared: (1) the differential formulation dV/dt=-I/C_m of membrane potential
used traditionally; and (2) the capacitor formulation, where membrane potential
is defined algebraically by the capacitor equation V=Q/C_m. We examine the
relationship between the formulations, assumptions under which each formulation
is consistent, and show that the capacitor formulation provides a transparent,
physically realistic formulation of membrane potential, whereas use of the
differential formulation may introduce unintended and undesirable behavior,
such as monotonic drift of concentrations. We prove that the drift of
concentrations in the differential formulation arises as a compensation for
failure to assign all currents in concentrations. As an example of these
considerations, we present an electroneutral, explicitly charge-conservative
formulation of Winslow et al. model (1999), and extend it to describe membrane
potentials between intracellular compartments.

<id>
q-bio/0511015v3
<category>
q-bio.CB
<abstract>
Artificial biological pacemakers were developed and tested in canine
ventricles. Next steps will require obtaining oscillations sensitive to
external regulations, and robust with respect to long term drifts of expression
levels of pacemaker currents and gap junctions. We introduce mathematical
models intended to be used in parallel with the experiments.
  The models describe human mesenchymal stem cells ({\it hMSC}) transfected
with HCN2 genes and connected to myocytes. They are intended to mimic
experiments with oscillation induction in a cell pair, in cell culture and in
the cardiac tissue. We give examples of oscillations in a cell pair, in a 1 dim
cell culture, and oscillation dependence on number of pacemaker channels per
cell and number of gap junctions. The models permit to mimic experiments with
levels of gene expressions not achieved yet, and to predict if the work to
achieve this levels will significantly increase the quality of oscillations.
This give arguments for selecting the directions of the experimental work.

<id>
q-bio/0512017v1
<category>
q-bio.CB
<abstract>
In this article we highlight chemotaxis (cellular movement) as a rich source
of potential engineering applications and computational models, highlighting
current research and possible future work. We first give a brief description of
the biological mechanism, before describing recent work on modelling it in
silico. We then propose a methodology for extending existing models and their
possible application as a fundamental tool in engineering cellular pattern
formation. We discuss possible engineering applications of human-defined cell
patterns, as well as the potential for using abstract models of chemotaxis for
generalised computation, before concluding with a brief discussion of future
challenges and opportunities in this field.

<id>
q-bio/0603021v1
<category>
q-bio.CB
<abstract>
We build a model for the hepatic fatty acid metabolism and its metabolic and
genetic regulations. The model has two functioning modes: synthesis and
oxidation of fatty acids. We provide a sufficient condition (the strong
lipolytic condition) for the uniqueness of its equilibrium. Under this
condition, modifications of the glucose input produce equilibrium shifts, which
are gradual changes from one functioning mode to the other. We also discuss the
concentration variations of various metabolites during equilibrium shifts. The
model can explain a certain amount of experimental observations, assess the
role of poly-unsaturated fatty acids in genetic regulation, and predict the
behavior of mutants. The analysis of the model is based on block elimination of
variables and uses a modular decomposition of the system dictated by
mathematical global univalence conditions.

<id>
q-bio/0605046v3
<category>
q-bio.CB
<abstract>
We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.

<id>
q-bio/0607035v1
<category>
q-bio.CB
<abstract>
The reproduction of a living cell requires a repeatable set of chemical
events to be properly coordinated. Such events define a replication cycle,
coupling the growth and shape change of the cell membrane with internal
metabolic reactions. Although the logic of such process is determined by
potentially simple physico-chemical laws, the modeling of a full,
self-maintained cell cycle is not trivial. Here we present a novel approach to
the problem which makes use of so called symmetry breaking instabilities as the
engine of cell growth and division. It is shown that the process occurs as a
consequence of the breaking of spatial symmetry and provides a reliable
mechanism of vesicle growth and reproduction. Our model opens the possibility
of a synthetic protocell lacking information but displaying self-reproduction
under a very simple set of chemical reactions.

<id>
q-bio/0607051v1
<category>
q-bio.CB
<abstract>
Many cell types exhibit oscillatory activity, such as repetitive action
potential firing due to the Hodgkin-Huxley dynamics of ion channels in the cell
membrane or reveal intracellular inositol triphosphate (IP$_3$) mediated
calcium oscillations (CaOs) by calcium-induced calcium release channels
(IP$_3$-receptor) in the membrane of the endoplasmic reticulum (ER). The
dynamics of the excitable membrane and that of the IP$_3$-mediated CaOs have
been the subject of many studies. However, the interaction between the
excitable cell membrane and IP$_3$-mediated CaOs, which are coupled by
cytosolic calcium which affects the dynamics of both, has not been studied.
This study for the first time applied stability analysis to investigate the
dynamic behavior of a model, which includes both an excitable membrane and an
intracellular IP$_3$-mediated calcium oscillator. Taking the IP$_3$
concentration as a control parameter, the model exhibits a novel rich spectrum
of stable and unstable states with hysteresis. The four stable states of the
model correspond in detail to previously reported growth-state dependent states
of the membrane potential of normal rat kidney fibroblasts in cell culture. The
hysteresis is most pronounced for experimentally observed parameter values of
the model, suggesting a functional importance of hysteresis. This study shows
that the four growth-dependent cell states may not reflect the behavior of
cells that have differentiated into different cell types with different
properties, but simply reflect four different states of a single cell type,
that is characterized by a single model.

<id>
q-bio/0608025v1
<category>
q-bio.CB
<abstract>
Various molecules exclusively accumulate at the front or back of migrating
eukaryotic cells in response to a shallow gradient of extracellular signals.
Directional sensing and signal amplification highlight the essential properties
in the migrating cells, known as cell polarity. In addition to these, such
properties of cell polarity involve unique determination of migrating direction
(uniqueness of axis) and localized gradient sensing at the front edge
(localization of sensitivity), both of which may be required for smooth
migration. Here we provide the mass conservation system based on the
reaction-diffusion system with two components, where the mass of the two
components is always conserved. Using two models belonging to this mass
conservation system, we demonstrate through both numerical simulation and
analytical approximations that the spatial pattern with a single peak
(uniqueness of axis) can be generally observed and that the existent peak
senses a gradient of parameters at the peak position, which guides the movement
of the peak. We extended this system with multiple components, and we developed
a multiple-component model in which cross-talk between members of the Rho
family of small GTPases is involved. This model also exhibits the essential
properties of the two models with two components. Thus, the mass conservation
system shows properties similar to those of cell polarity, such as uniqueness
of axis and localization of sensitivity, in addition to directional sensing and
signal amplification.

<id>
q-bio/0608039v1
<category>
q-bio.CB
<abstract>
During the aging of the red-blood cell, or under conditions of extreme
echinocytosis, membrane is shed from the cell plasma membrane in the form of
nano-vesicles. We propose that this process is the result of the
self-adaptation of the membrane surface area to the elastic stress imposed by
the spectrin cytoskeleton, via the local buckling of membrane under increasing
cytoskeleton stiffness. This model introduces the concept of force balance as a
regulatory process at the cell membrane, and quantitatively reproduces the rate
of area loss in aging red-blood cells.

<id>
q-bio/0609019v1
<category>
q-bio.CB
<abstract>
We derive a physiologically structured multiscale model for biofilm
development. The model has components on two spatial scales, which induce
different time scales into the problem. The macroscopic behavior of the system
is modeled using growth-induced flow in a domain with a moving boundary.
Cell-level processes are incorporated into the model using a so-called
physiologically structured variable to represent cell senescence, which in turn
affects cell division and mortality. We present computational results for our
models which shed light on modeling the combined role senescence and the
biofilm state play in the defense strategy of bacteria.

<id>
q-bio/0609043v1
<category>
q-bio.CB
<abstract>
All materials enter or exit the cell nucleus through nuclear pore complexes
(NPCs), efficient transport devices that combine high selectivity and
throughput. A central feature of this transport is the binding of
cargo-carrying soluble transport factors to flexible, unstructured
proteinaceous filaments called FG-nups that line the NPC. We have modeled the
dynamics of transport factors and their interaction with the flexible FG-nups
as diffusion in an effective potential, using both analytical theory and
computer simulations. We show that specific binding of transport factors to the
FG-nups facilitates transport and provides the mechanism of selectivity. We
show that the high selectivity of transport can be accounted for by competition
for both binding sites and space inside the NPC, which selects for transport
factors over other macromolecules that interact only non-specifically with the
NPC. We also show that transport is relatively insensitive to changes in the
number and distribution of FG-nups in the NPC, due mainly to their flexibility;
this accounts for recent experiments where up to half of the total mass of the
NPC has been deleted, without abolishing the transport. Notably, we demonstrate
that previously established physical and structural properties of the NPC can
account for observed features of nucleocytoplasmic transport. Finally, our
results suggest strategies for creation of artificial nano-molecular sorting
devices.

<id>
q-bio/0610005v1
<category>
q-bio.CB
<abstract>
Morphologies of moving amoebae are categorized into two types. One is the
``neutrophil'' type in which the long axis of cell roughly coincides with its
moving direction. This type of cell extends a leading edge at the front and
retracts a narrow tail at the rear, whose shape has been often drawn as a
typical amoeba in textbooks. The other one is the ``keratocyte'' type with
widespread lamellipodia along the front side arc. Short axis of cell in this
type roughly coincides with its moving direction. In order to understand what
kind of molecular feature causes conversion between two types of morphologies,
and how two typical morphologies are maintained, a mathematical model of
amoebic cells is developed. This model describes movement of cell and
intracellular reactions of activator, inhibitor and actin filaments in a
unified way. It is found that the producing rate of activator is a key factor
of conversion between two types. This model also explains the observed data
that the keratocye type cells tend to rapidly move along a straight line. The
neutrophil type cells move along a straight line when the moving velocity is
small, but they show fluctuated motions deviating from a line when they move as
fast as the keratocye type cells. Efficient energy consumption in the
neutrophil type cells is predicted.

<id>
q-bio/0610015v1
<category>
q-bio.CB
<abstract>
We present a stochastic model which describes fronts of cells invading a
wound. In the model cells can move, proliferate, and experience cell-cell
adhesion. We find several qualitatively different regimes of front motion and
analyze the transitions between them. Above a critical value of adhesion and
for small proliferation large isolated clusters are formed ahead of the front.
This is mapped onto the well-known ferromagnetic phase transition in the Ising
model. For large adhesion, and larger proliferation the clusters become
connected (at some fixed time). For adhesion below the critical value the
results are similar to our previous work which neglected adhesion. The results
are compared with experiments, and possible directions of future work are
proposed.

<id>
q-bio/0309020v1
<category>
q-bio.GN
<abstract>
We identify a set of 575 human genes that are expressed in all conditions
tested in a publicly available database of microarray results. Based on this
common occurrence, the set is expected to be rich in "housekeeping" genes,
showing constitutive expression in all tissues. We compare selected aspects of
their genomic structure with a set of background genes. We find that the
introns, untranslated regions and coding sequences of the housekeeping genes
are shorter, indicating a selection for compactness in these genes.

<id>
q-bio/0310022v1
<category>
q-bio.GN
<abstract>
We propose a new kernel for biological sequences which borrows ideas and
techniques from information theory and data compression. This kernel can be
used in combination with any kernel method, in particular Support Vector
Machines for protein classification. By incorporating prior biological
assumptions on the properties of amino-acid sequences and using a Bayesian
averaging framework, we compute the value of this kernel in linear time and
space, benefiting from previous achievements proposed in the field of universal
coding. Encouraging classification results are reported on a standard protein
homology detection experiment.

<id>
q-bio/0310027v1
<category>
q-bio.GN
<abstract>
We introduce a random bit-string model of post-transcriptional genetic
regulation based on sequence matching. The model spontaneously yields a scale
free network with power law scaling with $ \gamma=-1$ and also exhibits
log-periodic behaviour. The in-degree distribution is much narrower, and
exhibits a pronounced peak followed by a Gaussian distribution. The network is
of the smallest world type, with the average minimum path length independent of
the size of the network, as long as the network consists of one giant cluster.
The percolation threshold depends on the system size.

<id>
q-bio/0310040v2
<category>
q-bio.GN
<abstract>
BACKGROUND: Transcriptional regulation is a key mechanism in the functioning
of the cell, and is mostly effected through transcription factors binding to
specific recognition motifs located upstream of the coding region of the
regulated gene. The computational identification of such motifs is made easier
by the fact that they often appear several times in the upstream region of the
regulated genes, so that the number of occurrences of relevant motifs is often
significantly larger than expected by pure chance. RESULTS: To exploit this
fact, we construct sets of genes characterized by the statistical
overrepresentation of a certain motif in their upstream regions. Then we study
the functional characterization of these sets by analyzing their annotation to
Gene Ontology terms. For the sets showing a statistically significant specific
functional characterization, we conjecture that the upstream motif
characterizing the set is a binding site for a transcription factor involved in
the regulation of the genes in the set. CONCLUSIONS: The method we propose is
able to identify many known binding sites in S. cerevisiae and new candidate
targets of regulation by known transcription factors. Its application to less
well studied organisms is likely to be valuable in the exploration of their
regulatory interaction network.

<id>
q-bio/0311018v1
<category>
q-bio.GN
<abstract>
We describe a new global multiple alignment program capable of aligning a
large number of genomic regions. Our progressive alignment approach
incorporates the following ideas: maximum-likelihood inference of ancestral
sequences, automatic guide-tree construction, protein based anchoring of
ab-initio gene predictions, and constraints derived from a global homology map
of the sequences. We have implemented these ideas in the MAVID program, which
is able to accurately align multiple genomic regions up to megabases long.
MAVID is able to effectively align divergent sequences, as well as incomplete
unfinished sequences. We demonstrate the capabilities of the program on the
benchmark CFTR region which consists of 1.8Mb of human sequence and 20
orthologous regions in marsupials, birds, fish, and mammals. Finally, we
describe two large MAVID alignments: an alignment of all the available HIV
genomes and a multiple alignment of the entire human, mouse and rat genomes.

<id>
q-bio/0312006v1
<category>
q-bio.GN
<abstract>
The Relevance Vector Machine (RVM) is a recently developed machine learning
framework capable of building simple models from large sets of candidate
features. Here, we describe a protocol for using the RVM to explore very large
numbers of candidate features, and a family of models which apply the power of
the RVM to classifying and detecting interesting points and regions in
biological sequence data. The models described here have been used successfully
for predicting transcription start sites and other features in genome
sequences.

<id>
q-bio/0312007v1
<category>
q-bio.GN
<abstract>
Background: In addition to known protein-coding genes, large amount of
apparently non-coding sequence are conserved between the human and mouse
genomes. It seems reasonable to assume that these conserved regions are more
likely to contain functional elements than less-conserved portions of the
genome. Here we used a motif-oriented machine learning method to extract the
strongest signal from a set of non-coding conserved sequences.
  Results: We successfully fitted models to reflect the non-coding sequences,
and showed that the results were quite consistent for repeated training runs.
Using the learned model to scan genomic sequence, we found that it often made
predictions close to the start of annotated genes. We compared this method with
other published promoter-prediction systems, and show that the set of promoters
which are detected by this method seems to be substantially similar to that
detected by existing methods.
  Conclusions: The results presented here indicate that the promoter signal is
the strongest single motif-based signal in the non-coding functional fraction
of the genome. They also lend support to the belief that there exists a
substantial subset of promoter regions which share common features and are
detectable by a variety of computational methods.

<id>
q-bio/0312017v1
<category>
q-bio.GN
<abstract>
This paper studies sequencing and mapping methods that rely solely on pooling
and shotgun sequencing of clones. First, we scrutinize and improve the recently
proposed Clone-Array Pooled Shotgun Sequencing (CAPSS) method, which delivers a
BAC-linked assembly of a whole genome sequence. Secondly, we introduce a novel
physical mapping method, called Clone-Array Pooled Shotgun Mapping (CAPS-MAP),
which computes the physical ordering of BACs in a random library. Both CAPSS
and CAPS-MAP construct subclone libraries from pooled genomic BAC clones.
  We propose algorithmic and experimental improvements that make CAPSS a viable
option for sequencing a set of BACs. We provide the first probabilistic model
of CAPSS sequencing progress. The model leads to theoretical results supporting
previous, less formal arguments on the practicality of CAPSS. We demonstrate
the usefulness of CAPS-MAP for clone overlap detection with a probabilistic
analysis, and a simulated assembly of the Drosophila melanogaster genome. Our
analysis indicates that CAPS-MAP is well-suited for detecting BAC overlaps in a
highly redundant library, relying on a low amount of shotgun sequence
information. Consequently, it is a practical method for computing the physical
ordering of clones in a random library, without requiring additional clone
fingerprinting. Since CAPS-MAP requires only shotgun sequence reads, it can be
seamlessly incorporated into a sequencing project with almost no experimental
overhead.

<id>
q-bio/0312027v1
<category>
q-bio.GN
<abstract>
Using Monte Carlo methods, we simulated the effects of bias in generation and
elimination of paralogs on the size distribution of paralog groups. It was
found that the function describing the decay of the number of paralog groups
with their size depends on the ratio between the probability of duplications of
genes and their deletions, which corresponds to different selection pressures
on the genome size. Slightly different slopes of curves describing the decay of
the number of paralog groups with their size were also observed when the
threshold of homology between paralogous sequences was changed.

<id>
q-bio/0312039v1
<category>
q-bio.GN
<abstract>
To test whether X-chromosome has unique genomic characteristics, X-chromosome
and 22 autosomes were compared for RNA binding density. Nucleotide sequences on
the chromosomes were divided into 50kb per segment that was recoded as a set of
frequency values of 7-nucleotide (7nt) strings using all possible 7nt strings
(47=16384). 120 genes highly expressed in tonsil germinal center B cells were
selected for calculating 7nt string frequency values of all introns (RNAs). The
binding density of DNA segments and RNAs was determined by the amount of
complement sequences. It was shown for the first time that gene-poor and low
gene expression X-chromosome had the lowest percentage of the DNA segments that
can highly bind RNAs, whereas gene-rich and high gene expression chromosome 19
had the highest percentage of the segments. On the basis of these results, it
is proposed that the nonrandom properties of distribution of RNA highly binding
DNA segments on the chromosomes provide strong evidence that lack of RNA highly
binding segments may be a cause of X-chromosome inactivation

<id>
q-bio/0402044v1
<category>
q-bio.GN
<abstract>
In this paper, we study the equilibrium behavior of Eigen's quasispecies
equations for an arbitrary gene network. We consider a genome consisting of $ N
$ genes, so that each gene sequence $ \sigma $ may be written as $ \sigma =
\sigma_1 \sigma_2 ... \sigma_N $. We assume a single fitness peak (SFP) model
for each gene, so that gene $ i $ has some ``master'' sequence $ \sigma_{i, 0}
$ for which it is functioning. The fitness landscape is then determined by
which genes in the genome are functioning, and which are not. The equilibrium
behavior of this model may be solved in the limit of infinite sequence length.
The central result is that, instead of a single error catastrophe, the model
exhibits a series of localization to delocalization transitions, which we term
an ``error cascade.'' As the mutation rate is increased, the selective
advantage for maintaining functional copies of certain genes in the network
disappears, and the population distribution delocalizes over the corresponding
sequence spaces. The network goes through a series of such transitions, as more
and more genes become inactivated, until eventually delocalization occurs over
the entire genome space, resulting in a final error catastrophe. This model
provides a criterion for determining the conditions under which certain genes
in a genome will lose functionality due to genetic drift. It also provides
insight into the response of gene networks to mutagens. In particular, it
suggests an approach for determining the relative importance of various genes
to the fitness of an organism, in a more accurate manner than the standard
``deletion set'' method. The results in this paper also have implications for
mutational robustness and what C.O. Wilke termed ``survival of the flattest.''

<id>
q-bio/0403024v1
<category>
q-bio.GN
<abstract>
Background: Exonic splice enhancers are sequences embedded within exons which
promote and regulate the splicing of the transcript in which they are located.
A class of exonic splice enhancers are the SR proteins, which are thought to
mediate interactions between splicing factors bound to the 5' and 3' splice
sites. Method and results: We present a novel strategy for analysing
protein-coding sequence by first randomizing the codons used at each position
within the coding sequence, then applying a motif-based machine learning
algorithm to compare the true and randomized sequences. This strategy
identified a collection of motifs which can successfully discriminate between
real and randomized coding sequence, including -- but not restricted to --
several previously reported splice enhancer elements. As well as successfully
distinguishing coding exons from randomized sequences, we show that our model
is able to recognize non-coding exons. Conclusions: Our strategy succeeded in
detecting signals in coding exons which seem to be orthogonal to the sequences'
primary function of coding for proteins. We believe that many of the motifs
detected here may represent binding sites for previously unrecognized proteins
which influence RNA splicing. We hope that this development will lead to
improved knowledge of exonic splice enhancers, and new developments in the
field of computational gene prediction.

<id>
q-bio/0403032v1
<category>
q-bio.GN
<abstract>
The genetic code is considered to be universal. In order to test if some
statistical properties of the coding bacterial genome were due to inherent
properties of the genetic code, we compared the autocorrelation function, the
scaling properties and the maximum entropy of the distribution of distances of
amino acids in sequences obtained by translating protein-coding regions from
the genome of Borrelia burgdorferi, under different genetic codes. Overall our
results indicate that these properties are very stable to perturbations made by
altering the genetic code. We also discuss the evolutionary likely implications
of the present results.

<id>
q-bio/0404024v1
<category>
q-bio.GN
<abstract>
Much of the on-going statistical analysis of DNA sequences is focused on the
estimation of characteristics of coding and non-coding regions that would
possibly allow discrimination of these regions. In the current approach, we
concentrate specifically on genes and intergenic regions. To estimate the level
and type of correlation in these regions we apply various statistical methods
inspired from nonlinear time series analysis, namely the probability
distribution of tuplets, the Mutual Information and the Identical Neighbour
Fit. The methods are suitably modified to work on symbolic sequences and they
are first tested for validity on sequences obtained from well--known simple
deterministic and stochastic models. Then they are applied to the DNA sequence
of chromosome 1 of {\em arabidopsis thaliana}. The results suggest that
correlations do exist in the DNA sequence but they are weak and that intergenic
sequences tend to be more correlated than gene sequences. The use of
statistical tests with surrogate data establish these findings in a rigorous
statistical manner.

<id>
q-bio/0408014v1
<category>
q-bio.GN
<abstract>
Herein it is shown that in order to study the statistical properties of DNA
sequences in bacterial chromosomes it suffices to consider only one half of the
chromosome because they are similar to its corresponding complementary sequence
in the other half. This is due to the inverse bilateral symmetry of bacterial
chromosomes. Contrary to the classical result that DNA coding regions of
bacterial genomes are purely uncorrelated random sequences, here it is shown,
via a renormalization group approach, that DNA random fluctuations of single
bases are modulated by log-periodic variations. Distance series of triplets
display long-range correlations in each half of the intact chromosome and in
intronless protein-coding sequences, or both long-range correlations and
log-periodic modulations along the whole chromosome. Hence scaling analyses of
distance series of DNA sequences have to consider the functional units of
bacterial chromosomes.

<id>
q-bio/0408017v1
<category>
q-bio.GN
<abstract>
The rules that specify how the information contained in DNA codes amino
acids, is called "the genetic code". Using a simplified version of the Penna
nodel, we are using computer simulations to investigate the importance of the
genetic code and the number of amino acids in Nature on population dynamics. We
find that the genetic code is not a random pairing of codons to amino acids and
the number of amino acids in Nature is an optimum under mutations.

<id>
q-bio/0409011v1
<category>
q-bio.GN
<abstract>
Small Ubiquitin-related modifier (SUMO) proteins are widely expressed in
eukaryotic cells, which are reversibly coupled to their substrates by motif
recognition, called sumoylation. Two interesting questions are 1) how many
potential SUMO substrates may be included in mammalian proteomes, such as human
and mouse, 2) and given a SUMO substrate, can we recognize its sumoylation
sites? To answer these two questions, previous prediction systems of SUMO
substrates mainly adopted the pattern recognition methods, which could get high
sensitivity with relatively too many potential false positives. So we use
phylogenetic conservation between mouse and human to reduce the number of
potential false positives.

<id>
q-bio/0410008v1
<category>
q-bio.GN
<abstract>
With the sponsorship of ``Fundacio La Caixa'' we met in Barcelona, November
21st and 22nd, to analyze the reasons why, after the completion of the human
genome sequence, the identification all protein coding genes and their variants
remains a distant goal. Here we report on our discussions and summarize some of
the major challenges that need to be overcome in order to complete the human
gene catalog.

<id>
q-bio/0411024v1
<category>
q-bio.GN
<abstract>
Gene sequences in the vicinity of splice sites are found to possess
dinucleotide periodicities, especially RR and YY, with the period close to the
pitch of nucleosome DNA. This confirms previously reported finding about
preferential positioning of splice junctions within the nucleosomes. The RR and
YY dinucleotides oscillate counterphase, i.e., their respective preferred
positions are shifted about half-period one from another, as it was observed
earlier for AA and TT dinucleotides. Species specificity of nucleosome
positioning DNA pattern is indicated by predominant use of the periodical
GG(CC) dinucleotides in human and mouse genes, as opposed to predominant AA(TT)
dinucleotides in Arabidopsis and C.elegans.
  Keywords: chromatin; gene splicing; intron; exon; dinucleotide; periodical
pattern

<id>
q-bio/0411045v1
<category>
q-bio.GN
<abstract>
RNA editing by members of the double-stranded RNA-specific ADAR family leads
to site-specific conversion of adenosine to inosine (A-to-I) in precursor
messenger RNAs. Editing by ADARs is believed to occur in all metazoa, and is
essential for mammalian development. Currently, only a limited number of human
ADAR substrates are known, while indirect evidence suggests a substantial
fraction of all pre-mRNAs being affected. Here we describe a computational
search for ADAR editing sites in the human transcriptome, using millions of
available expressed sequences. 12,723 A-to-I editing sites were mapped in 1,637
different genes, with an estimated accuracy of 95%, raising the number of known
editing sites by two orders of magnitude. We experimentally validated our
method by verifying the occurrence of editing in 26 novel substrates. A-to-I
editing in humans primarily occurs in non-coding regions of the RNA, typically
in Alu repeats. Analysis of the large set of editing sites indicates the role
of editing in controlling dsRNA stability.

<id>
q-bio/0412037v1
<category>
q-bio.GN
<abstract>
Shannon information (SI) and its special case, divergence, are defined for a
DNA sequence in terms of probabilities of chemical words in the sequence and
are computed for a set of complete genomes highly diverse in length and
composition. We find the following: SI (but not divergence) is inversely
proportional to sequence length for a random sequence but is length-independent
for genomes; the genomic SI is always greater and, for shorter words and longer
sequences, hundreds to thousands times greater than the SI in a random sequence
whose length and composition match those of the genome; genomic SIs appear to
have word-length dependent universal values. The universality is inferred to be
an evolution footprint of a universal mode for genome growth.

<id>
q-bio/0412043v1
<category>
q-bio.GN
<abstract>
A-To-I RNA editing is common to all eukaryotes, associated with various
neurological functions. Recently, A-to-I editing was found to occur abundantly
in the human transcriptome. Here we show that the frequency of A-to-I editing
in humans is at least an order of magnitude higher as that of mouse, rat,
chicken or fly. The extraordinary frequency of RNA editing in human is
explained by the dominance of the primate-specific Alu element in the human
transcriptome, which increases the number of double-stranded RNA substrates.

<id>
q-bio/0501012v1
<category>
q-bio.GN
<abstract>
We proposed a probabilistic algorithm to solve the Multiple Sequence
Alignment problem. The algorithm is a Simulated Annealing (SA) that exploits
the representation of the Multiple Alignment between $D$ sequences as a
directed polymer in $D$ dimensions. Within this representation we can easily
track the evolution in the configuration space of the alignment through local
moves of low computational cost. At variance with other probabilistic
algorithms proposed to solve this problem, our approach allows for the creation
and deletion of gaps without extra computational cost. The algorithm was tested
aligning proteins from the kinases family. When D=3 the results are consistent
with those obtained using a complete algorithm. For $D>3$ where the complete
algorithm fails, we show that our algorithm still converges to reasonable
alignments. Moreover, we study the space of solutions obtained and show that
depending on the number of sequences aligned the solutions are organized in
different ways, suggesting a possible source of errors for progressive
algorithms.

<id>
q-bio/0501018v1
<category>
q-bio.GN
<abstract>
The presence of neighbor dependencies generated a specific pattern of
dinucleotide frequencies in all organisms. Especially, the
CpG-methylation-deamination process is the predominant substitution process in
vertebrates and needs to be incorporated into a more realistic model for
nucleotide substitutions. Based on a general framework of nucleotide
substitutions we develop a method that is able to identify the most relevant
neighbor dependent substitution processes, measure their strength, and judge
their importance to be included into the modeling. Starting from a model for
neighbor independent nucleotide substitution we successively add neighbor
dependent substitution processes in the order of their ability to increase the
likelihood of the model describing given data. The analysis of neighbor
dependent nucleotide substitutions in human, zebrafish and fruit fly is
presented. A web server to perform the presented analysis is publicly
available.

<id>
q-bio/0501020v1
<category>
q-bio.GN
<abstract>
This study presents the first global, 1 Mbp level analysis of patterns of
nucleotide substitutions along the human lineage. The study is based on the
analysis of a large amount of repetitive elements deposited into the human
genome since the mammalian radiation, yielding a number of results that would
have been difficult to obtain using the more conventional comparative method of
analysis. This analysis revealed substantial and consistent variability of
rates of substitution, with the variability ranging up to 2-fold among
different regions. The rates of substitutions of C or G nucleotides with A or T
nucleotides vary much more sharply than the reverse rates suggesting that much
of that variation is due to differences in mutation rates rather than in the
probabilities of fixation of C/G vs. A/T nucleotides across the genome. For all
types of substitution we observe substantially more hotspots than coldspots,
with hotspots showing substantial clustering over tens of Mbp's. Our analysis
revealed that GC-content of surrounding sequences is the best predictor of the
rates of substitution. The pattern of substitution appears very different near
telomeres compared to the rest of the genome and cannot be explained by the
genome-wide correlations of the substitution rates with GC content or exon
density. The telomere pattern of substitution is consistent with natural
selection or biased gene conversion acting to increase the GC-content of the
sequences that are within 10-15 Mbp away from the telomere.

<id>
q-bio/0501030v2
<category>
q-bio.GN
<abstract>
In molecular phylogeny, relationships among organisms are reconstructed using
DNA or protein sequences and are displayed as trees. A linear increase in the
number of sequences results in an exponential increase of possible trees. Thus,
inferring trees from molecular data was shown to be NP-hard. This causes
problems, if large data sets are used. This review gives an introduction to
molecular phylogenetic methods and to the problems biologists are facing in
molecular phylogenetic analyses.

<id>
q-bio/0501033v3
<category>
q-bio.GN
<abstract>
We model the competition between recombination and point mutation in
microbial genomes, and present evidence for two distinct phases, one uniform,
the other genetically diverse. Depending on the specifics of homologous
recombination, we find that global sequence divergence can be mediated by
fronts propagating along the genome, whose characteristic signature on genome
structure is elucidated, and apparently observed in closely-related {\it
Bacillus} strains. Front propagation provides an emergent, generic mechanism
for microbial "speciation", and suggests a classification of microorganisms on
the basis of their propensity to support propagating fronts.

<id>
q-bio/0502009v1
<category>
q-bio.GN
<abstract>
The human genome contains repetitive DNA at different level of sequence
length, number and dispersion. Highly repetitive DNA is particularly rich in
homo-- and di--nucleotide repeats, while middle repetitive DNA is rich of
families of interspersed, mobile elements hundreds of base pairs (bp) long,
among which the Alu families. A link between homo- and di-polymeric tracts and
mobile elements has been recently highlighted. In particular, the mobility of
Alu repeats, which form 10% of the human genome, has been correlated with the
length of poly(A) tracts located at one end of the Alu. These tracts have a
rigid and non-bendable structure and have an inhibitory effect on nucleosomes,
which normally compact the DNA. We performed a statistical analysis of the
genome-wide distribution of lengths and inter--tract separations of poly(X) and
poly(XY) tracts in the human genome. Our study shows that in humans the length
distributions of these sequences reflect the dynamics of their expansion and
DNA replication. By means of general tools from linguistics, we show that the
latter play the role of highly-significant content-bearing terms in the DNA
text. Furthermore, we find that such tracts are positioned in a non-random
fashion, with an apparent periodicity of 150 bases. This allows us to extend
the link between repetitive, highly mobile elements such as Alus and
low-complexity words in human DNA. More precisely, we show that Alus are
sources of poly(X) tracts, which in turn affect in a subtle way the combination
and diversification of gene expression and the fixation of multigene families.

<id>
q-bio/0502045v1
<category>
q-bio.GN
<abstract>
A-to-I RNA editing by ADARs is a post-transcriptional mechanism for expanding
the proteomic repertoire. Genetic recoding by editing was so far observed for
only a few mammalian RNAs that are predominantly expressed in nervous tissues.
However, as these editing targets fail to explain the broad and severe
phenotypes of ADAR1 knockout mice, additional targets for editing by ADARs were
always expected. Using comparative genomics and expressed sequence analysis, we
identified and experimentally verified four additional candidate human
substrates for ADAR-mediated editing: FLNA, BLCAP, CYFIP2 and IGFBP7.
Additionally, editing of three of these substrates was verified in the mouse
while two of them were validated in chicken. Interestingly, none of these
substrates encodes a receptor protein but two of them are strongly expressed in
the CNS and seem important for proper nervous system function. The editing
pattern observed suggests that some of the affected proteins might have altered
physiological properties leaving the possibility that they can be related to
the phenotypes of ADAR1 knockout mice.

<id>
q-bio/0504012v2
<category>
q-bio.GN
<abstract>
The initial analysis of the recently sequenced genome of Acanthamoeba
polyphaga Mimivirus, the largest known double-stranded DNA virus, predicted a
proteome of size and complexity more akin to small parasitic bacteria than to
other nucleo-cytoplasmic large DNA viruses, and identified numerous functions
never before described in a virus. It has been proposed that the Mimivirus
lineage could have emerged before the individualization of cellular organisms
from the 3 domains of life. An exhaustive in silico analysis of the non-coding
moiety of all known viral genomes, now uncovers the unprecedented perfect
conservation of a AAAATTGA motif in close to 50% of the Mimivirus genes. This
motif preferentially occurs in genes transcribed from the predicted leading
strand and is associated with functions required early in the viral infectious
cycle, such as transcription and protein translation. A comparison with the
known promoter of unicellular eukaryotes, in particular amoebal protists,
strongly suggests that the AAAATTGA motif is the structural equivalent of the
TATA box core promoter element. This element is specific to the Mimivirus
lineage, and may correspond to an ancestral promoter structure predating the
radiation of the eukaryotic kingdoms. This unprecedented conservation of core
promoter regions is another exceptional features of Mimivirus, that again
raises the question of its evolutionary origin.

<id>
q-bio/0310018v1
<category>
q-bio.MN
<abstract>
We study a recent model for calcium signal transduction. This model displays
spiking, bursting and chaotic oscillations in accordance with experimental
results. We calculate bifurcation diagrams and study the bursting behaviour in
detail. This behaviour is classified according to the dynamics of separated
slow and fast subsystems. It is shown to be of the Fold-Hopf type, a type which
was previously only described in the context of neuronal systems, but not in
the context of signal transduction in the cell.

<id>
q-bio/0310021v1
<category>
q-bio.MN
<abstract>
Former work on an application of order-disorder theory is recalled as a
vehicle to add further development and significance to the recent paper on
motifs in protein interactions.

<id>
q-bio/0310041v1
<category>
q-bio.MN
<abstract>
We study genetic switches formed from pairs of mutually repressing operons.
The switch stability is characterised by a well defined lifetime which grows
sub-exponentially with the number of copies of the most-expressed transcription
factor, in the regime accessible by our numerical simulations. The stability
can be markedly enhanced by a suitable choice of overlap between the upstream
regulatory domains. Our results suggest that robustness against biochemical
noise can provide a selection pressure that drives operons, that regulate each
other, together in the course of evolution.

<id>
q-bio/0311019v1
<category>
q-bio.MN
<abstract>
Expression of the Drosophila segment polarity genes is initiated by a
prepattern of pair-rule gene products and maintained by a network of regulatory
interactions throughout several stages of embryonic development. Analysis of a
model of gene interactions based on differential equations showed that
wild-type expression patterns of these genes can be obtained for a wide range
of kinetic parameters, which suggests that the steady states are determined by
the topology of the network and the type of regulatory interactions between
components, not the detailed form of the rate laws. To investigate this, we
propose and analyze a Boolean model of this network which is based on a binary
ON/OFF representation of transcription and protein levels, and in which the
interactions are formulated as logical functions. In this model the spatial and
temporal patterns of gene expression are determined by the topology of the
network and whether components are present or absent, rather than the absolute
levels of the mRNAs and proteins and the functional details of their
interactions. The model is able to reproduce the wild type gene expression
patterns, as well as the ectopic expression patterns observed in
over-expression experiments and various mutants. Furthermore, we compute
explicitly all steady states of the network and identify the basin of
attraction of each steady state. The model gives important insights into the
functioning of the segment polarity gene network, such as the crucial role of
the wingless and sloppy paired genes, and the network's ability to correct
errors in the prepattern.

<id>
q-bio/0311031v1
<category>
q-bio.MN
<abstract>
We consider a model of large regulatory gene expression networks where the
thresholds activating the sigmoidal interactions between genes and the signs of
these interactions are shuffled randomly. Such an approach allows for a
qualitative understanding of network dynamics in a lack of empirical data
concerning the large genomes of living organisms. Local dynamics of network
nodes exhibits the multistationarity and oscillations and depends crucially
upon the global topology of a "maximal" graph (comprising of all possible
interactions between genes in the network). The long time behavior observed in
the network defined on the homogeneous "maximal" graphs is featured by the
fraction of positive interactions ($0\leq \eta\leq 1$) allowed between genes.
There exists a critical value $\eta_c<1$ such that if $\eta<\eta_c$, the
oscillations persist in the system, otherwise, when $\eta>\eta_c,$ it tends to
a fixed point (which position in the phase space is determined by the initial
conditions and the certain layout of switching parameters). In networks defined
on the inhomogeneous directed graphs depleted in cycles, no oscillations arise
in the system even if the negative interactions in between genes present
therein in abundance ($\eta_c=0$). For such networks, the bidirectional edges
(if occur) influence on the dynamics essentially. In particular, if a number of
edges in the "maximal" graph is bidirectional, oscillations can arise and
persist in the system at any low rate of negative interactions between genes
($\eta_c=1$). Local dynamics observed in the inhomogeneous scalable regulatory
networks is less sensitive to the choice of initial conditions. The scale free
networks demonstrate their high error tolerance.

<id>
q-bio/0401029v1
<category>
q-bio.MN
<abstract>
Theoretical physics is used for a toy model of molecular biology to assess
conditions that lead to the edge of chaos (EOC) in a network of biomolecules.
Results can enhance our ability to understand complex diseases and their
treatment or cure.

<id>
q-bio/0402017v1
<category>
q-bio.MN
<abstract>
Recent genomic and bioinformatic advances have motivated the development of
numerous random network models purporting to describe graphs of biological,
technological, and sociological origin. The success of a model has been
evaluated by how well it reproduces a few key features of the real-world data,
such as degree distributions, mean geodesic lengths, and clustering
coefficients. Often pairs of models can reproduce these features with
indistinguishable fidelity despite being generated by vastly different
mechanisms. In such cases, these few target features are insufficient to
distinguish which of the different models best describes real world networks of
interest; moreover, it is not clear a priori that any of the presently-existing
algorithms for network generation offers a predictive description of the
networks inspiring them. To derive discriminative classifiers, we construct a
mapping from the set of all graphs to a high-dimensional (in principle
infinite-dimensional) ``word space.'' This map defines an input space for
classification schemes which allow us for the first time to state unambiguously
which models are most descriptive of the networks they purport to describe. Our
training sets include networks generated from 17 models either drawn from the
literature or introduced in this work, source code for which is freely
available. We anticipate that this new approach to network analysis will be of
broad impact to a number of communities.

<id>
q-bio/0402027v2
<category>
q-bio.MN
<abstract>
We employed the random graph theory approach to analyze the protein-protein
interaction database DIP (Feb. 2004), for seven species (S. cerevisiae, H.
pylori, E. coli, C. elegans, H. sapiens, M. musculus and D. melanogaster).
Several global topological parameters (such as node connectivity, average
diameter, node connectivity correlation) were used to characterize these
protein-protein interaction networks (PINs). The logarithm of the connectivity
distribution vs. the logarithm of connectivity study indicated that PINs follow
a power law (P(k) ~ k-\gamma) behavior. Using the regression analysis method we
determined that \gamma lies between 1.5 and 2.4, for the seven species.
Correlation analysis provides good evidence supporting the fact that the seven
PINs form a scale-free network. The average diameters of the networks and their
randomized version are found to have large difference. We also demonstrated
that the interaction networks are quite robust when subject to random
perturbation. Average node connectivity correlation study supports the earlier
results that nodes of low connectivity are correlated, whereas nodes of high
connectivity are not directly linked. These results provided some evidence
suggesting such correlation relations might be a general feature of the PINs
across different species.

<id>
q-bio/0403012v1
<category>
q-bio.MN
<abstract>
We perform a reverse engineering from the ``extended Spellman data'',
consisting of 6178 mRNA levels measured by microarrays at 73 instances in four
time series during the cell cycle of the yeast Saccharomyces cerevisae. By
assuming a linear model of the genetic regulatory network, and imposing an
extra constraint (the Lasso), we obtain a unique inference of coupling
parameters. These parameters are transfered into an adjacent matrix, which is
analyzed with respect to topological properties and biological relevance. We
find a very broad distribution of outdegrees in the network, compatible with
earlier findings for biological systems and totally incompatible with a random
graph, and also indications of modules in the network. Finally, we show there
is an excess of genes coding for transcription factors among the genes of
highest outdegrees, a fact which indicates that our approach has biological
relevance.

<id>
q-bio/0403027v1
<category>
q-bio.MN
<abstract>
We prove the global asymptotic stability of a well-known delayed
negative-feedback model of testosterone dynamics, which has been proposed as a
model of oscillatory behavior. We establish stability (and hence the
impossibility of oscillations) even in the presence of delays of arbitrary
length.

<id>
q-bio/0403033v1
<category>
q-bio.MN
<abstract>
We discuss properties which must be satisfied by a genetic network in order
for it to allow differentiation.
  These conditions are expressed as follows in mathematical terms. Let $F$ be a
differentiable mapping from a finite dimensional real vector space to itself.
The signs of the entries of the Jacobian matrix of $F$ at a given point $a$
define an interaction graph, i.e. a finite oriented finite graph $G(a)$ where
each edge is equipped with a sign. Ren\'e Thomas conjectured twenty years ago
that, if $F$ has at least two non degenerate zeroes, there exists $a$ such that
$G(a)$ contains a positive circuit. Different contributors proved this in special
cases, and we give here a general proof of the conjecture. In particular, we
get this way a necessary condition for genetic networks to lead to
multistationarity, and therefore to differentiation.
  We use for our proof the mathematical literature on global univalence, and we
show how to derive from it several variants of Thomas' rule, some of which had
been anticipated by Kaufman and Thomas.

<id>
q-bio/0403045v2
<category>
q-bio.MN
<abstract>
Many real networks can be understood as two complementary networks with two
kind of nodes. This is the case of metabolic networks where the first network
has chemical compounds as nodes and the second one has nodes as reactions. The
second network can be related to the first one by a technique called line graph
transformation (i.e., edges in an initial network are transformed into nodes).
Recently, the main topological properties of the metabolic networks have been
properly described by means of a hierarchical model. In our work, we apply the
line graph transformation to a hierarchical network and the clustering
coefficient $C(k)$ is calculated for the transformed network, where $k$ is the
node degree. While $C(k)$ follows the scaling law $C(k)\sim k^{-1.1}$ for the
initial hierarchical network, $C(k)$ scales weakly as $k^{0.08}$ for the
transformed network. These results indicate that the reaction network can be
identified as a degree-independent clustering network.

<id>
q-bio/0404002v2
<category>
q-bio.MN
<abstract>
Biochemical networks are the analog computers of life. They allow living
cells to control a large number of biological processes, such as gene
expression and cell signalling. In biochemical networks, the concentrations of
the components are often low. This means that the discrete nature of the
reactants and the stochastic character of their interactions have to be taken
into account. Moreover, the spatial distribution of the components can be of
crucial importance. However, the current numerical techniques for simulating
biochemical networks either ignore the particulate nature of matter or treat
the spatial fluctuations in a mean-field manner. We have developed a new
technique, called Green's Function Reaction Dynamics (GFRD), that makes it
possible to simulate biochemical networks at the particle level and in both
time and space. In this scheme, a maximum time step is chosen such that only
single particles or pairs of particles have to be considered. For these
particles, the Smoluchowski equation can be solved analytically using Green's
functions. The main idea of GFRD is to exploit the exact solution of the
Smoluchoswki equation to set up an event-driven algorithm. This allows GFRD to
make large jumps in time when the particles are far apart from each other.
Here, we apply the technique to a simple model of gene expression. The
simulations reveal that the scheme is highly efficient. Under biologically
relevant conditions, GFRD is up to six orders of magnitude faster than
conventional particle-based techniques for simulating biochemical networks in
time and space.

<id>
q-bio/0404006v1
<category>
q-bio.MN
<abstract>
We study the bifurcations of a set of nine nonlinear ordinary differential
equations that describe the regulation of the cyclin-dependent kinase that
triggers DNA synthesis and mitosis in the budding yeast, Saccharomyces
cerevisiae. We show that Clb2-dependent kinase exhibits bistability (stable
steady states of high or low kinase activity). The transition from low to high
Clb2-dependent kinase activity is driven by transient activation of
Cln2-dependent kinase, and the reverse transition is driven by transient
activation of the Clb2 degradation machinery. We show that a four-variable
model retains the main features of the nine-variable model. In a three-variable
model exhibiting birhythmicity (two stable oscillatory states), we explore
possible effects of extrinsic fluctuations on cell cycle progression.

<id>
q-bio/0404017v1
<category>
q-bio.MN
<abstract>
Networks have been used to model many real-world phenomena to better
understand the phenomena and to guide experiments in order to predict their
behavior. Since incorrect models lead to incorrect predictions, it is vital to
have a correct model. As a result, new techniques and models for analyzing and
modeling real-world networks have recently been introduced. One example of
large and complex networks involves protein-protein interaction (PPI) networks.
We demonstrate that the currently popular scale-free model of PPI networks
fails to fit the data in several respects. We show that a random geometric
model provides a much more accurate model of the PPI data.

<id>
q-bio/0405020v1
<category>
q-bio.MN
<abstract>
Signal processing (SP) techniques convert DNA and protein sequences into
information that lead to successful drug discovery. One must, however, be aware
about the difference between information and entropy1. Eight other physical
properties of DNA and protein segments are suggested for SP analysis other than
ones already used in the literature. QSAR formulations of these properties are
suggested for ranking the amino acids that maximize efficiency of the amino
acids in proteins. Multiobjective programs are suggested for constraining or
searching the components of such sequences. Geometric maps of the networks of
proteins are preferable to scale-free descriptions in most cases. The genetic
code is presented as graphlets which show interesting correspondence to each
other, leading to possible new revelations.

<id>
q-bio/0406006v1
<category>
q-bio.MN
<abstract>
Bistable biochemical switches are ubiquitous in gene regulatory networks and
signal transduction pathways. Their switching dynamics, however, are difficult
to study directly in experiments or conventional computer simulations, because
switching events are rapid, yet infrequent. We present a simulation technique
that makes it possible to predict the rate and mechanism of flipping of
biochemical switches. The method uses a series of interfaces in phase space
between the two stable steady states of the switch to generate transition
trajectories in a ratchet-like manner. We demonstrate its use by calculating
the spontaneous flipping rate of a symmetric model of a genetic switch
consisting of two mutually repressing genes. The rate constant can be obtained
orders of magnitude more efficiently than using brute-force simulations. For
this model switch, we show that the switching mechanism, and consequently the
switching rate, depends crucially on whether the binding of one regulatory
protein to the DNA excludes the binding of the other one. Our technique could
also be used to study rare events and non-equilibrium processes in soft
condensed matter systems.

<id>
q-bio/0406043v1
<category>
q-bio.MN
<abstract>
It is becoming increasingly appreciated that the signal transduction systems
used by eukaryotic cells to achieve a variety of essential responses represent
highly complex networks rather than simple linear pathways. While significant
effort is being made to experimentally measure the rate constants for
individual steps in these signaling networks, many of the parameters required
to describe the behavior of these systems remain unknown, or at best,
estimates. With these goals and caveats in mind, we use methods of statistical
mechanics to extract useful predictions for complex cellular signaling
networks. To establish the usefulness of our approach, we have applied our
methods towards modeling the nerve growth factor (NGF)-induced differentiation
of neuronal cells. Using our approach, we are able to extract predictions that
are highly specific and accurate, thereby enabling us to predict the influence
of specific signaling modules in determining the integrated cellular response
to the two growth factors. We show that extracting biologically relevant
predictions from complex signaling models appears to be possible even in the
absence of measurements of all the individual rate constants. Our methods also
raise some interesting insights into the design and possible evolution of
cellular systems, highlighting an inherent property of these systems wherein
particular ''soft'' combinations of parameters can be varied over wide ranges
without impacting the final output and demonstrating that a few ''stiff''
parameter combinations center around the paramount regulatory steps of the
network. We refer to this property -- which is distinct from robustness -- as
''sloppiness.''

<id>
q-bio/0406046v1
<category>
q-bio.MN
<abstract>
A new approach to the modular, complex systems analysis of nonlinear dynamics
in cell cycling network transformations involved in carcinogenesis is proposed.
Carcinogenesis is a complex process that involves dynamically inter-connected
biomolecules in the intercellular, membrane, cytosolic, nuclear and nucleolar
compartments that form numerous inter-related pathways. One such family of
pathways contains the cell cyclins. Cyclins are proteins that link several
critical pro-apoptotic and other cell cycling/division components, including
the tumor suppressor gene TP53 and its product, the Thomsen-Friedenreich
antigen (T antigen), Rb, mdm2, c-Myc, p21, p27, Bax, Bad and Bcl-2, which all
play major roles in neoplastic transformation of many tissues. This novel
theoretical analysis based on recently published studies of cyclin signaling,
with special emphasis placed on the roles of cyclins D1 and E, suggests novel
clinical trials and rational therapies of cancer through reestablishment of
cell cycling inhibition in metastatic cancer cells.

<id>
q-bio/0407031v1
<category>
q-bio.MN
<abstract>
A four-node network consisting of a negative loop controlling a positive one
is studied. It models some of the features of the p53 gene network. Using
piecewise linear dynamics with thresholds, the allowed dynamical classes are
fully characterized and coded. The biologically relevant situations are
identified and conclusions drawn concerning the effectiveness of the p53
network as a tumour inhibitor mechanism.

<id>
q-bio/0409020v1
<category>
q-bio.MN
<abstract>
Immune cells coordinate their efforts for the correct and efficient
functioning of the immune system (IS). Each cell type plays a distinct role and
communicates with other cell types through mediators such as cytokines,
chemokines and hormones, among others, that are crucial for the functioning of
the IS and its fine tuning. Nevertheless, a quantitative analysis of the
topological properties of an immunological network involving this complex
interchange of mediators among immune cells is still lacking. Here we present a
method for quantifying the relevance of different mediators in the immune
network, which exploits a definition of centrality based on the concept of
efficient communication. The analysis, applied to the human immune system,
indicates that its mediators significantly differ in their network relevance.
We found that cytokines involved in innate immunity and inflammation and some
hormones rank highest in the network, revealing that the most prominent
mediators of the IS are molecules involved in these ancestral types of defence
mechanisms highly integrated with the adaptive immune response, and at the
interplay among the nervous, the endocrine and the immune systems.

<id>
q-bio/0410003v1
<category>
q-bio.MN
<abstract>
We study by mean-field analysis and stochastic simulations chemical models
for genetic toggle switches formed from pairs of genes that mutually repress
each other. In order to determine the stability of the genetic switches, we
make a connection with reactive flux theory and transition state theory. The
switch stability is characterised by a well defined lifetime $\tau$. We find
that $\tau$ grows exponentially with the mean number $\Nmean$ of transcription
factor molecules involved in the switching. In the regime accessible to direct
numerical simulations, the growth law is well characterised by
$\tau\sim\Nmean{}^{\alpha}\exp(b\Nmean)$, where $\alpha$ and $b$ are
parameters. The switch stability is decreased by phenomena that increase the
noise in gene expression, such as the production of multiple copies of a
protein from a single mRNA transcript (shot noise), and fluctuations in the
number of proteins produced per transcript. However, robustness against
biochemical noise can be drastically enhanced by arranging the transcription
factor binding domains on the DNA such that competing transcription factors
mutually exclude each other on the DNA. We also elucidate the origin of the
enhanced stability of the exclusive switch with respect to that of the general
switch: while the kinetic prefactor is roughly the same for both switches, the
`barrier' for flipping the switch is significantly higher for the exclusive
switch than for the general switch.

<id>
q-bio/0410009v1
<category>
q-bio.MN
<abstract>
Recent progress has clarified many features of the global architecture of
biological metabolic networks, which have highly organized and optimized
tolerances and tradeoffs (HOT) for functional requirements of flexibility,
efficiency, robustness, and evolvability, with constraints on conservation of
energy, redox, and many small moieties. One consequence of this architecture is
a highly structured modularity that is self-dissimilar and scale-rich, with
extremes in low and high variability, including power laws, in both metabolite
and reaction degree distributions. This paper illustrates these features using
the well-understood stoichiometry of metabolic networks in bacteria, and a
simple model of an abstract metabolism.

<id>
q-bio/0410017v1
<category>
q-bio.MN
<abstract>
Many genes have been identified as driving cellular differentiation, but
because of their complex interactions, the understanding of their collective
behaviour requires mathematical modelling. Intriguingly, it has been observed
in numerous developmental contexts, and particularly hematopoiesis, that genes
regulating differentiation are initially co-expressed in progenitors despite
their antagonism, before one is upregulated and others downregulated. We
characterise conditions under which 3 classes of generic "master regulatory
networks", modelled at the molecular level after experimentally-observed
interactions (including bHLH protein dimerisation), and including an arbitrary
number of antagonistic components, can behave as a "multi-switch", directing
differentiation in an all-or-none fashion to a specific cell-type chosen among
more than 2 possible outcomes. bHLH dimerisation networks can readily display
coexistence of many antagonistic factors when competition is low (a simple
characterisation is derived). Decision-making can be forced by a transient
increase in competition, which could correspond to some unexplained
experimental observations related to Id proteins; the speed of response varies
with the initial conditions the network is subjected to, which could explain
some aspects of cell behaviour upon reprogramming. The coexistence of
antagonistic factors at low levels, early in the differentiation process or in
pluripotent stem cells, could be an intrinsic property of the interaction
between those factors, not requiring a specific regulatory system.

<id>
q-bio/0411035v1
<category>
q-bio.MN
<abstract>
The evolutionary reason for the increase in gene length from archaea to
prokaryotes to eukaryotes observed in large scale genome sequencing efforts has
been unclear. We propose here that the increasing complexity of protein-protein
interactions has driven the selection of longer proteins, as longer proteins
are more able to distinguish among a larger number of distinct interactions due
to their greater average surface area. Annotated protein sequences available
from the SWISS-PROT database were analyzed for thirteen eukaryotes, eight
bacteria, and two archaea species. The number of subcellular locations to which
each protein is associated is used as a measure of the number of interactions
to which a protein participates. Two databases of yeast protein-protein
interactions were used as another measure of the number of interactions to
which each \emph{S. cerevisiae} protein participates. Protein length is shown
to correlate with both number of subcellular locations to which a protein is
associated and number of interactions as measured by yeast two-hybrid
experiments. Protein length is also shown to correlate with the probability
that the protein is encoded by an essential gene. Interestingly, average
protein length and number of subcellular locations are not significantly
different between all human proteins and protein targets of known, marketed
drugs. Increased protein length appears to be a significant mechanism by which
the increasing complexity of protein-protein interaction networks is
accommodated within the natural evolution of species. Consideration of protein
length may be a valuable tool in drug design, one that predicts different
strategies for inhibiting interactions in aberrant and normal pathways.

<id>
q-bio/0412045v2
<category>
q-bio.MN
<abstract>
Despite considerable progress in genome- and proteome-based high-throughput
screening methods and rational drug design, the number of successful single
target drugs did not increase appreciably during the past decade. Network
models suggest that partial inhibition of a surprisingly small number of
targets can be more efficient than the complete inhibition of a single target.
This and the success stories of multi-target drugs and combinatorial therapies
led us to suggest that systematic drug design strategies should be directed
against multiple targets. We propose that the final effect of partial, but
multiple drug actions might often surpass that of complete drug action at a
single target. The future success of this novel drug design paradigm will
depend not only on a new generation of computer models to identify the correct
multiple hits and their multi-fitting, low-affinity drug candidates but also on
more efficient in vivo testing.

<id>
q-bio/0501016v1
<category>
q-bio.MN
<abstract>
Complex dynamical networks consisting of many components that interact and
produce each other are difficult to understand, especially, when new components
may appear. In this paper we outline a theory to deal with such systems. The
theory consists of two parts. The first part introduces the concept of a
chemical organization as a closed and mass-maintaining set of components. This
concept allows to map a complex (reaction) network to the set of organizations,
providing a new view on the system's structure. The second part connects
dynamics with the set of organizations, which allows to map a movement of the
system in state space to a movement in the set of organizations.

<id>
q-bio/0501037v1
<category>
q-bio.MN
<abstract>
Interactions between genes and gene products give rise to complex circuits
that enable cells to process information and respond to external signals.
Theoretical studies often describe these interactions using continuous,
stochastic, or logical approaches. We propose a new modeling framework for gene
regulatory networks, that combines the intuitive appeal of a qualitative
description of gene states with a high flexibility in incorporating
stochasticity in the duration of cellular processes. We apply our methods to
the regulatory network of the segment polarity genes, thus gaining novel
insights into the development of gene expression patterns. For example, we show
that very short synthesis and decay times can perturb the wild type pattern. On
the other hand, separation of timescales between pre- and posttranslational
processes and a minimal prepattern ensure convergence to the wild type
expression pattern regardless of fluctuations.

<id>
q-bio/0502018v1
<category>
q-bio.MN
<abstract>
Thanks to recent progress in high-throughput experimental techniques, the
datasets of large-scale protein interactions of prototypical multicellular
species, the nematode worm Caenorhabditis elegans and the fruit fly Drosophila
melanogaster, have been assayed. The datasets are obtained mainly by using the
yeast hybrid method, which contains false-positive and false-negative
simultaneously. Accordingly, while it is desirable to test such datasets
through further wet experiments, here we invoke recent developed network theory
to test such high throughput datasets in a simple way. Based on the fact that
the key biological processes indispensable to maintaining life are universal
across eukaryotic species, and the comparison of structural properties of the
protein interaction networks (PINs) of the two species with those of the yeast
PIN, we find that while the worm and the yeast PIN datasets exhibit similar
structural properties, the current fly dataset, though most comprehensively
screened ever, does not reflect generic structural properties correctly as it
is. The modularity is suppressed and the connectivity correlation is lacking.
Addition of interlogs to the current fly dataset increases the modularity and
enhances the occurrence of triangular motifs as well. The connectivity
correlation function of the fly, however, remains distinct under such interlogs
addition, for which we present a possible scenario through an in silico
modeling.

<id>
q-bio/0502033v1
<category>
q-bio.MN
<abstract>
The structure of a genetic network is uncovered by studying its response to
external stimuli (input signals). We present a theory of propagation of an
input signal through a linear stochastic genetic network. It is found that
there are important advantages in using oscillatory signals over step or
impulse signals, and that the system may enter into a pure fluctuation
resonance for a specific input frequency.

<id>
q-bio/0309024v1
<category>
q-bio.NC
<abstract>
We study some mechanisms responsible for synchronous oscillations and loss of
synchrony at physiologically relevant frequencies (10-200 Hz) in a network of
heterogeneous inhibitory neurons. We focus on the factors that determine the
level of synchrony and frequency of the network response, as well as the
effects of mild heterogeneity on network dynamics. With mild heterogeneity,
synchrony is never perfect and is relatively fragile. In addition, the effects
of inhibition are more complex in mildly heterogeneous networks than in
homogeneous ones. In the former, synchrony is broken in two distinct ways,
depending on the ratio of the synaptic decay time to the period of repetitive
action potentials ($\tau_s/T$), where $T$ can be determined either from the
network or from a single, self-inhibiting neuron. With $\tau_s/T > 2$,
corresponding to large applied current, small synaptic strength or large
synaptic decay time, the effects of inhibition are largely tonic and
heterogeneous neurons spike relatively independently. With $\tau_s/T < 1$,
synchrony breaks when faster cells begin to suppress their less excitable
neighbors; cells that fire remain nearly synchronous. We show numerically that
the behavior of mildly heterogeneous networks can be related to the behavior of
single, self-inhibiting cells, which can be studied analytically.

<id>
q-bio/0309025v1
<category>
q-bio.NC
<abstract>
We analyze the control of frequency for a synchronized inhibitory neuronal
network. The analysis is done for a reduced membrane model with a
biophysically-based synaptic influence. We argue that such a reduced model can
quantitatively capture the frequency behavior of a larger class of neuronal
models. We show that in different parameter regimes, the network frequency
depends in different ways on the intrinsic and synaptic time constants. Only in
one portion of the parameter space, called `phasic', is the network period
proportional to the synaptic decay time. These results are discussed in
connection with previous work of the contributors, which showed that for mildly
heterogeneous networks, the synchrony breaks down, but coherence is preserved
much more for systems in the phasic regime than in the other regimes. These
results imply that for mildly heterogeneous networks, the existence of a
coherent rhythm implies a linear dependence of the network period on synaptic
decay time, and a much weaker dependence on the drive to the cells. We give
experimental evidence for this conclusion.

<id>
q-bio/0309026v1
<category>
q-bio.NC
<abstract>
We would like to know whether the statistics of neuronal responses vary
across cortical areas. We examined stimulus-elicited spike count response
distributions in V1 and IT cortices of awake monkeys. In both areas the
distribution of spike counts for each stimulus was well-described by a
Gaussian, with the log of the variance in the spike count linearly related to
the log of the mean spike count. Two significant differences in response
characteristics were found: both the range of spike counts and the slope of the
log(variance) vs. log(mean) regression were larger in V1 than in IT. However,
neurons in the two areas transmitted approximately the same amount of
information about the stimuli, and had about the same channel capacity (the
maximum possible transmitted information given noise in the responses). These
results suggest that neurons in V1 use more variable signals over a larger
dynamic range than neurons in IT, which use less variable signals over a
smaller dynamic range. The two coding strategies are approximately as effective
in transmitting information.

<id>
q-bio/0309027v1
<category>
q-bio.NC
<abstract>
We review recent developments in the measurement of the dynamics of the
response properties of auditory cortical neurons to broadband sounds, which is
closely related to the perception of timbre. The emphasis is on a method that
characterizes the spectro-temporal properties of single neurons to dynamic,
broadband sounds, akin to the drifting gratings used in vision. The method
treats the spectral and temporal aspects of the response on an equal footing.

<id>
q-bio/0309030v1
<category>
q-bio.NC
<abstract>
Here we analyze synaptic transmission from an information-theoretic
perspective. We derive closed-form expressions for the lower-bounds on the
capacity of a simple model of a cortical synapse under two explicit coding
paradigms. Under the ``signal estimation'' paradigm, we assume the signal to be
encoded in the mean firing rate of a Poisson neuron. The performance of an
optimal linear estimator of the signal then provides a lower bound on the
capacity for signal estimation. Under the ``signal detection'' paradigm, the
presence or absence of the signal has to be detected. Performance of the
optimal spike detector allows us to compute a lower bound on the capacity for
signal detection. We find that single synapses (for empirically measured
parameter values) transmit information poorly but significant improvement can
be achieved with a small amount of redundancy.

<id>
q-bio/0309033v2
<category>
q-bio.NC
<abstract>
Studies of insect olfactory processing indicate that odors are represented by
rich spatio-temporal patterns of neural activity. These patterns are very
difficult to predict a priori, yet they are stimulus specific and reliable upon
repeated stimulation with the same input. We formulate here a theoretical
framework in which we can interpret these experimental results. We propose a
paradigm of ``dynamic competition'' in which inputs (odors) are represented by
internally competing neural assemblies. Each pattern is the result of dynamical
motion within the network and does not involve a ``winner'' among competing
possibilities. The model produces spatio-temporal patterns with strong
resemblance to those observed experimentally and possesses many of the general
features one desires for pattern classifiers: large information capacity,
reliability, specific responses to specific inputs, and reduced sensitivity to
initial conditions or influence of noise. This form of neural processing may
thus describe the organizational principles of neural information processing in
sensory systems and go well beyond the observations on insect olfactory
processing which motivated its development.

<id>
q-bio/0309034v1
<category>
q-bio.NC
<abstract>
A number of cortical structures are reported to have elevated single unit
firing rates sustained throughout the memory period of a working memory task.
How the nervous system forms and maintains these memories is unknown but
reverberating neuronal network activity is thought to be important. We studied
the temporal structure of single unit (SU) activity and simultaneously recorded
local field potential (LFP) activity from area LIP in the inferior parietal
lobe of two awake macaques during a memory-saccade task. Using multitaper
techniques for spectral analysis, which play an important role in obtaining the
present results, we find elevations in spectral power in a 50--90 Hz (gamma)
frequency band during the memory period in both SU and LFP activity. The
activity is tuned to the direction of the saccade providing evidence for
temporal structure that codes for movement plans during working memory. We also
find SU and LFP activity are coherent during the memory period in the 50--90 Hz
gamma band and no consistent relation is present during simple fixation.
Finally, we find organized LFP activity in a 15--25 Hz frequency band that may
be related to movement execution and preparatory aspects of the task. Neuronal
activity could be used to control a neural prosthesis but SU activity can be
hard to isolate with cortical implants. As the LFP is easier to acquire than SU
activity, our finding of rich temporal structure in LFP activity related to
movement planning and execution may accelerate the development of this medical
application.

<id>
q-bio/0310005v2
<category>
q-bio.NC
<abstract>
Generalized language-of-thought arguments appropriate to interacting
cognitive modules permit exploration of how disease states interact with
medical treatment. The interpenetrating feedback between treatment and response
to it creates a kind of idiotypic hall-of-mirrors generating a synergistic
pattern of efficacy, treatment failure, adverse reactions, and patient
noncompliance which, from a Rate Distortion perspective, embodies a distorted
image of externally-imposed structured psychosocial stress. For the US,
accelerating spatial and social diffusion of such stress enmeshes both dominant
and subordinate populations in a linked system which will express itself, not
only in an increasingly unhealthy society, but in the diffusion of therapeutic
failure, including, but not limited to, drug-based treatments.

<id>
q-bio/0310039v4
<category>
q-bio.NC
<abstract>
In animal experiments, the observed orientation preference (OP) and ocular
dominance (OD) columns in the visual cortex of the brain show various pattern
types. Here, we show that the different visual map formations in various
species are due to the crossover behavior in anisotropic systems composed of
orientational and scalar components such as easy-plane Heisenberg models. We
predict the transition boundary between different pattern types with the
anisotropy as a main bifurcation parameter, which is consistent with
experimental observations.

<id>
q-bio/0311006v1
<category>
q-bio.NC
<abstract>
This paper proposes an approach to framing and answering fundamental
questions about consciousness. It argues that many of the more theoretical
debates about consciousness, such as debates about "when does it begin?", are
misplaced and meaningless, in part because "consciousness" as a word has many
valid and interesting definitions, and in part because consciousness qua mind
or intelligence (the main focus here)is a matter of degree or level, not a
binary variable. It proposes that new mathematical work related to functional
neural network designs -- designs so functional that they can be used in
engineering -- is essential to a functional understanding of intelligence as
such, and outlines some key mathematics as of 1999, citing earlier work for
more details. Quantum theory is relevant, but not in the simple ways proposed
in more popular philosophies.

<id>
q-bio/0311026v1
<category>
q-bio.NC
<abstract>
The question: whether quantum coherent states can sustain decoherence,
heating and dissipation over time scales comparable to the dynamical timescales
of the brain neurons, is actively discussed in the last years. Positive answer
on this question is crucial, in particular, for consideration of brain neurons
as quantum computers. This discussion was mainly based on theoretical
arguments. In present paper nonlinear statistical properties of the Ventral
Tegmental Area (VTA) of genetically depressive limbic brain are studied {\it in
vivo} on the Flinders Sensitive Line of rats (FSL). VTA plays a key role in
generation of pleasure and in development of psychological drug addiction. We
found that the FSL VTA (dopaminergic) neuron signals exhibit multifractal
properties for interspike frequencies on the scales where healthy VTA
dopaminergic neurons exhibit bursting activity. For high moments the observed
multifractal (generalized dimensions) spectrum coincides with the generalized
dimensions spectrum calculated for a spectral measure of a {\it quantum} system
(so-called kicked Harper model, actively used as a model of quantum chaos).
This observation can be considered as a first experimental ({\it in vivo})
indication in the favour of the quantum (at least partially) nature of the
brain neurons activity.

<id>
q-bio/0311027v1
<category>
q-bio.NC
<abstract>
We study a mathematical model for ocular dominance patterns (ODPs) in primary
visual cortex. This model is based on the premise that ODP is an adaptation to
minimize the length of intra-cortical wiring. Thus we attempt to understand the
existing ODPs by solving a wire length minimization problem. We divide all the
neurons into two classes: left- and right-eye dominated. We find that
segregation of neurons into monocular regions reduces wire length if the number
of connections to the neurons of the same class (intraocular) differs from the
number of interocular connections. The shape of the regions depends on the
relative fraction of neurons in the two classes. We find that if both classes
are almost equally represented, the optimal ODP consists of interdigitating
stripes. If one class is less numerous than the other, the optimal ODP consists
of patches of the less abundant class surrounded by the neurons of the other
class. We predict that the transition from stripes to patches occurs when the
fraction of neurons dominated by the underrepresented eye is about 40%. This
prediction agrees with the data in macaque and Cebus monkeys. We also study the
dependence of the periodicity of ODP on the parameters of our model.

<id>
q-bio/0311030v1
<category>
q-bio.NC
<abstract>
Two rate code models -- a reconstruction network model and a control model --
of the hippocampal-entorhinal loop are merged. The hippocampal-entorhinal loop
plays a double role in the unified model, it is part of a reconstruction
network and a controller, too. This double role turns the bottom-up information
flow into top-down control like signals. The role of bottom-up filtering is
information maximization, noise filtering, temporal integration and prediction,
whereas the role of top-down filtering is emphasizing, i.e., highlighting or
`paving of the way' as well as context based pattern completion. In the joined
model, the control task is performed by cortical areas, whereas reconstruction
networks can be found between cortical areas. While the controller is highly
non-linear, the reconstruction network is an almost linear architecture, which
is optimized for noise estimation and noise filtering. A conjecture of the
reconstruction network model -- that the long-term memory of the visual stream
is the linear feedback connections between neocortical areas -- is reinforced
by the joined model. Falsifying predictions are presented; some of them have
recent experimental support. Connections to attention and to awareness are
made.

<id>
q-bio/0312025v1
<category>
q-bio.NC
<abstract>
Artificial spike-based computation, inspired by models of computations in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. In this paper,
we study new models for two common instances of such computation,
winner-take-all and coincidence detection. In both cases, very fast convergence
is achieved independent of initial conditions, and network complexity is linear
in the number of inputs.

<id>
q-bio/0312038v1
<category>
q-bio.NC
<abstract>
Random walk methods are used to calculate the moments of negative image
equilibrium distributions in synaptic weight dynamics governed by spike-timing
dependent plasticity (STDP). The neural architecture of the model is based on
the electrosensory lateral line lobe (ELL) of mormyrid electric fish, which
forms a negative image of the reafferent signal from the fish's own electric
discharge to optimize detection of sensory electric fields. Of particular
behavioral importance to the fish is the variance of the equilibrium
postsynaptic potential in the presence of noise, which is determined by the
variance of the equilibrium weight distribution. Recurrence relations are
derived for the moments of the equilibrium weight distribution, for arbitrary
postsynaptic potential functions and arbitrary learning rules. For the case of
homogeneous network parameters, explicit closed form solutions are developed
for the covariances of the synaptic weight and postsynaptic potential
distributions.

<id>
q-bio/0401001v1
<category>
q-bio.NC
<abstract>
Artificial spike-based computation, inspired by models of computation in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. This paper
describes very simple network architectures for k-winners-take-all and
soft-winner-take-all computation using neural oscillators. Fast convergence is
achieved from arbitrary initial conditions, which makes the networks
particularly suitable to track time-varying inputs.

<id>
q-bio/0401009v4
<category>
q-bio.NC
<abstract>
Effects of distractions such as noises and parameter heterogeneity have been
studied on the firing activity of ensemble neurons, each of which is described
by the extended Morris-Lecar model showing the graded persisting firings with
the aid of an included ${\rm Ca}^{2+}$-dependent cation current. Although the
sustained activity of {\it single} neurons is rather robust in a sense that the
activity is realized even in the presence of the distractions, the graded
frequency of sustained firings is vulnerable to them. It has been shown,
however, that the graded persisting activity of {\it ensemble} neurons becomes
much robust to the distractions by the pooling (ensemble) effect. When the
coupling is introduced, the synchronization of firings in ensemble neurons is
enhanced, which is beneficial to firings of target neurons.

<id>
q-bio/0401013v2
<category>
q-bio.NC
<abstract>
The response of a neural cell to an external stimulus can follow one of the
two patterns: Nonresonant neurons monotonously relax to the resting state after
excitation while resonant ones show subthreshold oscillations. We investigate
how do these subthreshold properties of neurons affect their suprathreshold
response. Vice versa we ask: Can we distinguish between both types of neuronal
dynamics using suprathreshold spike trains? The dynamics of neurons is given by
stochastic FitzHugh-Nagumo and Morris-Lecar models with either having a focus
or a node as the stable fixpoint. We determine numerically the spectral power
density as well as the interspike interval density in response to a random
(noise-like) signals. We show that the information about the type of dynamics
obtained from power spectra is of limited validity. In contrast, the interspike
interval density gives a very sensitive instrument for the diagnostics of
whether the dynamics has resonant or nonresonant properties. For the latter
value we formulate a fit formula and use it to reconstruct theoretically the
spectral power density, which coincides with the numerically obtained spectra.
We underline that the renewal theory is applicable to analysis of
suprathreshold responses even of resonant neurons.

<id>
q-bio/0401019v1
<category>
q-bio.NC
<abstract>
The cognitive frame in which most neuropsychological research on the neural
basis of behavior is conducted contains the assumption that brain mechanisms
per se fully suffice to explain all psychologically described phenomena. This
assumption stems from the idea that the brain is made up entirely of material
particles and fields, and that all causal mechanisms must therefore be
formulated solely in terms of properties of these elements. One consequence of
this stance is that psychological terms having intrinsic mentalistic and/or
experiential content (terms such as "feeling," "knowing," and "effort") have
not been included as primary causal factors in neuropsychological research:
insofar as properties are not described in material terms they are deemed
irrelevant to the causal mechanisms underlying brain function. However, the
origin of this demand that experiential realities be excluded from the causal
base is a theory of nature that has been known for more that three quarters of
a century to be fundamentally incorrect. It is explained here why it is
consequently scientifically unwarranted to assume that material factors alone
can in principle explain all causal mechanisms relevant to neuroscience. More
importantly, it is explained how a key quantum effect can be introduced into
brain dynamics in a simple and practical way that provides a rationally
coherent, causally formulated, physics-based way of understanding and using the
psychological and physical data derived from the growing set of studies of the
capacity of directed attention and mental effort to systematically alter brain
function.

<id>
q-bio/0402022v1
<category>
q-bio.NC
<abstract>
We study the spike statistics of neurons in a network with dynamically
balanced excitation and inhibition. Our model, intended to represent a generic
cortical column, comprises randomly connected excitatory and inhibitory leaky
integrate-and-fire neurons, driven by excitatory input from an external
population. The high connectivity permits a mean-field description in which
synaptic currents can be treated as Gaussian noise, the mean and
autocorrelation function of which are calculated self-consistently from the
firing statistics of single model neurons. Within this description, we find
that the irregularity of spike trains is controlled mainly by the strength of
the synapses relative to the difference between the firing threshold and the
post-firing reset level of the membrane potential. For moderately strong
synapses we find spike statistics very similar to those observed in primary
visual cortex.

<id>
q-bio/0402023v1
<category>
q-bio.NC
<abstract>
We review the use of mean field theory for describing the dynamics of dense,
randomly connected cortical circuits. For a simple network of excitatory and
inhibitory leaky integrate-and-fire neurons, we can show how the firing
irregularity, as measured by the Fano factor, increases with the strength of
the synapses in the network and with the value to which the membrane potential
is reset after a spike. Generalizing the model to include conductance-based
synapses gives insight into the connection between the firing statistics and
the high-conductance state observed experimentally in visual cortex. Finally,
an extension of the model to describe an orientation hypercolumn provides
understanding of how cortical interactions sharpen orientation tuning, in a way
that is consistent with observed firing statistics.

<id>
q-bio/0402026v1
<category>
q-bio.NC
<abstract>
Measured responses from visual cortical neurons show that spike times tend to
be correlated rather than exactly Poisson distributed. Fano factors vary and
are usually greater than 1 due to the tendency of spikes being clustered into
bursts. We show that this behavior emerges naturally in a balanced cortical
network model with random connectivity and conductance-based synapses. We
employ mean field theory with correctly colored noise to describe temporal
correlations in the neuronal activity. Our results illuminate the connection
between two independent experimental findings: high conductance states of
cortical neurons in their natural environment, and variable non-Poissonian
spike statistics with Fano factors greater than 1.

<id>
q-bio/0402035v1
<category>
q-bio.NC
<abstract>
Recent experiments revealed that a certain class of inhibitory neurons in the
cerebral cortex make synapses not onto cell bodies but at distal parts of
dendrites of the target neurons, mediating highly nonlinear dendritic
inhibition. We propose a novel form of competitive neural network model that
realizes such dendritic inhibition. Contrary to the conventional lateral
inhibition in neural networks, our dendritic inhibition models don't always
show winner-take-all behaviors; instead, they converge to "I don't know" states
when unknown input patterns are presented. We derive reduced two-dimensional
dynamics for the network, showing that a drastic shift of the fixed point from
a winner-take-all state to an "I don't know" state occurs in accordance with
the increase in noise added to the stored patterns. By preventing
misrecognition in such a way, dendritic inhibition networks achieve fine
pattern discrimination, which could be one of the basic computations by
inhibitory connected recurrent neural networks in the brain.

<id>
q-bio/0402045v1
<category>
q-bio.NC
<abstract>
The performance of the Hopfield neural network model is numerically studied
on various complex networks, such as the Watts-Strogatz network, the
Barab{\'a}si-Albert network, and the neuronal network of the C. elegans.
Through the use of a systematic way of controlling the clustering coefficient,
with the degree of each neuron kept unchanged, we find that the networks with
the lower clustering exhibit much better performance. The results are discussed
in the practical viewpoint of application, and the biological implications are
also suggested.

<id>
q-bio/0403031v1
<category>
q-bio.NC
<abstract>
The melodic consonance of a sequence of tones is explained using the overtone
series: the overtones form "flow lines" that link the tones melodically; the
strength of these flow lines determines the melodic consonance. This hypothesis
admits of psychoacoustical and neurophysiological interpretations that fit well
with the place theory of pitch perception. The hypothesis is used to create a
model for how the auditory system judges melodic consonance, which is used to
algorithmically construct melodic sequences of tones.

<id>
q-bio/0403037v1
<category>
q-bio.NC
<abstract>
We present a complete mean field theory for a balanced state of a simple
model of an orientation hypercolumn. The theory is complemented by a
description of a numerical procedure for solving the mean-field equations
quantitatively. With our treatment, we can determine self-consistently both the
firing rates and the firing correlations, without being restricted to specific
neuron models. Here, we solve the analytically derived mean-field equations
numerically for integrate-and-fire neurons. Several known key properties of
orientation selective cortical neurons emerge naturally from the description:
Irregular firing with statistics close to -- but not restricted to -- Poisson
statistics; an almost linear gain function (firing frequency as a function of
stimulus contrast) of the neurons within the network; and a contrast-invariant
tuning width of the neuronal firing. We find that the irregularity in firing
depends sensitively on synaptic strengths. If Fano factors are bigger than 1,
then they are so for all stimulus orientations that elicit firing. We also find
that the tuning of the noise in the input current is the same as the tuning of
the external input, while that for the mean input current depends on both the
external input and the intracortical connectivity.

<id>
q-bio/0404021v1
<category>
q-bio.NC
<abstract>
We analyze two pulse-coupled resonate-and-fire neurons. Numerical simulation
reveals that an anti-phase state is an attractor of this model. We can
analytically explain the stability of anti-phase states by means of a return
map of firing times, which we propose in this paper. The resultant stability
condition turns out to be quite simple. The phase diagram based on our theory
shows that there are two types of anti-phase states. One of these cannot be
seen in coupled integrate-and-fire models and is peculiar to resonate-and-fire
models. The results of our theory coincide with those of numerical simulations.

<id>
q-bio/0405002v1
<category>
q-bio.NC
<abstract>
We have designed a toy brain and have written computer code that simulates
it. This toy brain is flexible, modular, has hierarchical learning and
recognition, has short and long term memory, is distributed (i.e. has no
central control), is asynchronous, and includes parallel and series processing.
We have simulated the neurons calculating their internal voltages as a function
of time. We include in the simulation the ion pumps of the neurons, the
synapses with glutamate or GABA neurotransmitters, and the delays of the action
pulses in axons and dendrites. We have used known or plausible circuits of real
brains. The toy brain reads books and learns languages using the Hebb
mechanism. Finally, we have related the toy brain with what might be occurring
in a real brain.

<id>
q-bio/0405027v3
<category>
q-bio.NC
<abstract>
We exhibit a mathematical framework to represent the neural dynamics at
cortical level. Our description of neural dynamics with columnar and functional
modularity, named fibre bundle representation (FBM) method, is based both on
neuroscience and informatics, whereas they correspond with the conventional
formulas in statistical physics. In spite of complex interactions in neural
circuitry and various cortical modification rules per models, some significant
factors determine the typical phenomena in cortical dynamics. The FBM
representation method reveals them plainly and gives profit in building or
analyzing the cortical dynamic models. Not only the similarity in formulas, the
cortical dynamics can share the statistical properties with other physical
systems, which validated in primary visual maps. We apply our method to
proposed models in visual map formations, in addition our suggestion using the
lateral interaction scheme. In this paper, we will show that the neural dynamic
procedures can be treated through conventional physics expressions and
theories.

<id>
q-bio/0406050v1
<category>
q-bio.NC
<abstract>
In a recent paper on Cortical Dynamics, Francis and Grossberg raise the
question how visual forms and motion information are integrated to generate a
coherent percept of moving forms? In their investigation of illusory contours
(which are, like Kanizsa squares, mental constructs rather than stimuli on the
retina) they quantify the subjective impression of apparent motion between
illusory contours that are formed by two subsequent stimuli with delay times of
about 0.2 second (called the interstimulus interval ISI). The impression of
apparent motion is due to a back referral of a later experience to an earlier
time in the conscious representation. A model is developed which describes the
state of awareness in the observer in terms of a time dependent Schroedinger
equation to which a second order time derivative is added. This addition
requires as boundary conditions the values of the solution both at the
beginning and after the process. Satisfactory quantitative agreement is found
between the results of the model and the experimental results. We recall that
in the von Neumann interpretation of the collapse of the quantum mechanical
wave-function, the collapse was associated with an observer's awareness. Some
questions of causality and determinism that arise from later-time boundary
conditions are touched upon.

<id>
q-bio/0312024v1
<category>
q-bio.OT
<abstract>
It has been suggested by several contributors that nonlinear excitations, in
particular solitary waves, could play a fundamental functional role in the
process of DNA transcription, effecting the opening of the double chain needed
for RNA Polymerase to be able to copy the genetic code. Some models have been
proposed to model the relevant DNA dynamics in terms of a reduced number of
effective degrees of freedom. Here I discuss advantages and disadvantages of
such an approach, and discuss in more detail one of the models, i.e. the one
proposed by Yakushevich.

<id>
q-bio/0312031v2
<category>
q-bio.OT
<abstract>
Numerous problems connected with an assumption of the life origin on the
Earth do not arise on Galilean satellites. Here, in presence of a practically
non-salt water and of a great deal (~5-10%) of abiogenic organics, a great
diversity of conditions, which are unthinkable for the Earth, were realized
more than once. They were caused by global electrochemical processes in the
magnetic field presence what could entail an absolute enantiomeric synthesis.
The subsequent explosions of the satellites' icy envelopes saturated by the
electrolysis products resulted in appearance of hot massive atmospheres and
warm deep oceans and ejection of the dirty ice fragments (=comet nuclei), what
led to the material exchange with other bodies, etc.

<id>
q-bio/0406026v1
<category>
q-bio.OT
<abstract>
A healthy human brain is perfused with blood flowing laminarly through
cerebral vessels, providing brain tissue with substrates such as oxygen and
glucose. Under normal conditions, cerebral blood flow is controlled by
autoregulation as well as metabolic, chemical and neurogenic regulation.
Physiological complexity of these mechanisms invariably leads to a question as
to what are the relations between the statistical properties of arterial and
intracranial pressure fluctuations. To shed new light on cerebral hemodynamics,
we employ a complex continuous wavelet transform to determine the instantaneous
phase difference between the arterial blood pressure (ABP) and intracranial
pressure (ICP) in patients with traumatic brain injuries or spontaneous
cerebral hemorrhage. For patients with mild to moderate injury, the phase
difference slowly evolves in time. However, severe neurological injury with
elevated ICP are herein associated with synchronization of arterial and
intracranial pressure. We use Shannon entropy to quantify the stability of
ABP-ICP phase difference and discuss the clinical applicability of such measure
to assessment of cerebrovascular reactivity and autoregulation integrity.

<id>
q-bio/0406045v2
<category>
q-bio.OT
<abstract>
Carcinogenesis is a complex process that involves dynamically inter-connected
modular sub-networks that evolve under the influence of micro-environmentally
induced perturbations, in non-random, pseudo-Markov chain processes. An
appropriate n-stage model of carcinogenesis involves therefore n-valued Logic
treatments of nonlinear dynamic transformations of complex functional genomes
and cell interactomes. Lukasiewicz Algebraic Logic models of genetic networks
and signaling pathways in cells are formulated in terms of nonlinear dynamic
systems with n-state components that allow for the generalization of previous,
Boolean or "fuzzy", logic models of genetic activities in vivo. Such models are
then applied to cell transformations during carcinogenesis based on very
extensive genomic transcription and translation data from the CGAP databases
supported by NCI. Such models are represented in a Lukasiewicz-Topos with an
n-valued Lukasiewicz Algebraic Logics subobject classifier description that
represents non-random and nonlinear network activities as well as their
transformations in carcinogeness. Specific models for different types of cancer
are then derived from representations of the dynamic state-space of LT
non-random, pseudo-Markov chain process, network models in terms of cDNA and
proteomic, high throughput analyses by ultra-sensitive techniques. This novel
theoretical analysis is based on extensive CGAP genomic data for human tumors,
as well as recently published studies of cyclin signaling. Several such
specific models suggest novel clinical trials and rational therapies of cancer
through re-establishment of cell cycling inhibition in stage III cancers.

<id>
q-bio/0406047v1
<category>
q-bio.OT
<abstract>
Selected applications of novel techniques in Agricultural Biotechnology,
Health Food formulations and Medical Biotechnology are being reviewed with the
aim of unraveling future developments and policy changes that are likely to
open new niches for Biotechnology and prevent the shrinking or closing the
existing ones. Amongst the selected novel techniques with applications to both
Agricultural and Medical Biotechnology are: immobilized bacterial cells and
enzymes, microencapsulation and liposome production, genetic manipulation of
microorganisms, development of novel vaccines from plants, epigenomics of
mammalian cells and organisms, as well as biocomputational tools for molecular
modeling related to disease and Bioinformatics. Both fundamental and applied
aspects of the emerging new techniques are being discussed in relation to their
anticipated impact on future biotechnology applications together with policy
changes that are needed for continued success in both Agricultural and Medical
Biotechnology. Several novel techniques are illustrated in an attempt to convey
the most representative and powerful tools that are currently being developed
for both immediate and long term applications in Agriculture, Health Food
formulation and production, pharmaceuticals and Medicine. The research aspects
are naturally emphasized in our review as they are key to further developments
in Medical and Agricultural Biotechnology.

<id>
q-bio/0407005v1
<category>
q-bio.OT
<abstract>
The response of the immune system to different vaccination patterns is
studied with a simple model. It is argued that the history and characteristics
of the pattern defines very different secondary immune responses in the case of
infection. The memory function of the immune response can be set to work in
very different modes depending on the pattern followed during immunizations. It
is argued that the history and pattern of immunizations can be a decisive (and
experimentally accessible) factor to tailor the effectiveness of a specific
vaccine.

<id>
q-bio/0409001v1
<category>
q-bio.OT
<abstract>
This paper traces the seminal roles that physicists and mathematicians have
played in the conceptual development of the biological sciences in the past,
and especially in the 19th and 20th centuries.

<id>
q-bio/0410001v1
<category>
q-bio.OT
<abstract>
We present the first systematic evidence for the origins of 1/f-type temporal
scaling in human heart rate. The heart rate is regulated by the activity of two
branches of the autonomic nervous system: the parasympathetic (PNS) and the
sympathetic (SNS) nervous systems. We examine alterations in the scaling
property when the balance between PNS and SNS activity is modified, and find
that the relative PNS suppression by congestive heart failure results in a
substantial increase in the Hurst exponent H towards random walk scaling
$1/f^{2}$ and a similar breakdown is observed with relative SNS suppression by
primary autonomic failure. These results suggest that 1/f scaling in heart rate
requires the intricate balance between the antagonistic activity of PNS and
SNS.

<id>
q-bio/0503002v1
<category>
q-bio.OT
<abstract>
In this article we use global and regional data from the SARS epidemic in
conjunction with a model of susceptible, exposed, infective, diagnosed, and
recovered classes of people (``SEIJR'') to extract average properties and rate
constants for those populations. The model is fitted to data from the Ontario
(Toronto) in Canada, Hong Kong in China and Singapore outbreaks and predictions
are made based on various assumptions and observations, including the current
effect of isolating individuals diagnosed with SARS. The epidemic dynamics for
Hong Kong and Singapore appear to be different from the dynamics in Toronto,
Ontario. Toronto shows a very rapid increase in the number of cases between
March 31st and April 6th, followed by a {\it significant} slowing in the number
of new cases. We explain this as the result of an increase in the diagnostic
rate and in the effectiveness of patient isolation after March 26th. Our best
estimates are consistent with SARS eventually being contained in Toronto,
although the time of containment is sensitive to the parameters in our model.
It is shown that despite the empirically modeled heterogeneity in transmission,
SARS' average reproductive number is 1.2, a value quite similar to that
computed for some strains of influenza \cite{CC2}. Although it would not be
surprising to see levels of SARS infection higher than ten per cent in some
regions of the world (if unchecked), lack of data and the observed
heterogeneity and sensitivity of parameters prevent us from predicting the
long-term impact of SARS.

<id>
q-bio/0503006v1
<category>
q-bio.OT
<abstract>
Despite improved control measures, Ebola remains a serious public health risk
in African regions where recurrent outbreaks have been observed since the
initial epidemic in 1976. Using epidemic modeling and data from two
well-documented Ebola outbreaks (Congo 1995 and Uganda 2000), we estimate the
number of secondary cases generated by an index case in the absence of control
interventions ($R_0$). Our estimate of $R_0$ is 1.83 (SD 0.06) for Congo (1995)
and 1.34 (SD 0.03) for Uganda (2000). We model the course of the outbreaks via
an SEIR (susceptible-exposed-infectious-removed) epidemic model that includes a
smooth transition in the transmission rate after control interventions are put
in place. We perform an uncertainty analysis of the basic reproductive number
$R_0$ to quantify its sensitivity to other disease-related parameters. We also
analyze the sensitivity of the final epidemic size to the time interventions
begin and provide a distribution for the final epidemic size. The control
measures implemented during these two outbreaks (including education and
contact tracing followed by quarantine) reduce the final epidemic size by a
factor of 2 relative the final size with a two-week delay in their
implementation.

<id>
q-bio/0505028v2
<category>
q-bio.OT
<abstract>
Data discretization, also known as binning, is a frequently used technique in
computer science, statistics, and their applications to biological data
analysis. We present a new method for the discretization of real-valued data
into a finite number of discrete values. Novel aspects of the method are the
incorporation of an information-theoretic criterion and a criterion to
determine the optimal number of values. While the method can be used for data
clustering, the motivation for its development is the need for a discretization
algorithm for several multivariate time series of heterogeneous data, such as
transcript, protein, and metabolite concentration measurements. As several
modeling methods for biochemical networks employ discrete variable states, the
method needs to preserve correlations between variables as well as the dynamic
features of the time series. A C++ implementation of the algorithm is available
from the contributors at http://polymath.vbi.vt.edu/discretization .

<id>
q-bio/0506042v1
<category>
q-bio.OT
<abstract>
Sleep is one of the most noticeable and widespread phenomena occurring in
multicellular animals. Nevertheless, no consensus for a theory of its origins
has emerged. In particular, no explicit, quantitative theory exists that
elucidates or distinguishes between the myriad hypotheses proposed for sleep.
Here, we develop a general, quantitative theory for mammalian sleep that
relates many of its fundamental parameters to metabolic rate and body size.
Most mechanisms suggested for the function of sleep can be placed in this
framework, e.g., cellular repair of damage caused by metabolic processes and
cortical reorganization to process sensory input. Our theory leads to
predictions for sleep time, sleep cycle time, and REM (rapid-eye-movement) time
as functions of body and brain mass, and explains, for example, why mice sleep
\~14 hours per day relative to the 3.5 hours per day that elephants sleep. Data
for 96 species of mammals, spanning six orders of magnitude in body size, are
consistent with these predictions and provide strong evidence that time scales
for sleep are set by the brain's, not the whole-body, metabolic rate.

<id>
q-bio/0508024v2
<category>
q-bio.OT
<abstract>
Sensory systems pass information about an animal's environment to higher
nervous system units through sequences of action potentials. When these action
potentials have essentially equivalent waveforms, all information is contained
in the interspike intervals (ISIs) of the spike sequence. We address the
question: How do neural circuits recognize and read these ISI sequences?
  Our answer is given in terms of a biologically inspired neural circuit that
we construct using biologically realistic neurons. The essential ingredients of
the ISI Reading Unit (IRU) are (i) a tunable time delay circuit modelled after
one found in the anterior forebrain pathway of the birdsong system and (ii) a
recently observed rule for inhibitory synaptic plasticity. We present a circuit
that can both learn the ISIs of a training sequence using inhibitory synaptic
plasticity and then recognize the same ISI sequence when it is presented on
subsequent occasions. We investigate the ability of this IRU to learn in the
presence of two kinds of noise: jitter in the time of each spike and random
spikes occurring in the ideal spike sequence. We also discuss how the circuit
can be detuned by removing the selected ISI sequence and replacing it by an ISI
sequence with ISIs drawn from a probability distribution.
  We have investigated realizations of the time delay circuit using
Hodgkin-Huxley conductance based neurons connected by realistic excitatory and
inhibitory synapses. Our models for the time delay circuit are tunable from
about 10 ms to 100 ms allowing one to learn and recognize ISI sequences within
that range of ISIs. ISIs down to a few ms and longer than 100 ms are possible
with other intrinsic and synaptic currents in the component neurons.

<id>
q-bio/0511001v1
<category>
q-bio.OT
<abstract>
Global assessments have shown that future climate change is likely to
significantly impact forest ecosystems. The present study makes an assessment
of the impact of projected climate change on forest ecosystems in India. This
assessment is based on climate projections of Regional Climate Model of the
Hadley Centre (HadRM3) using the A2 (740 ppm CO2) and B2 (575 ppm CO2)
scenarios of Special Report on Emissions Scenarios and the BIOME4 vegetation
response model. The main conclusion is that under the climate projection for
the year 2085, 77% and 68% of the forested grids in India are likely to
experience shift in forest types under A2 and B2 scenario, respectively.
Indications are a shift towards wetter forest types in the northeastern region
and drier forest types in the northwestern region in the absence of human
influence. Increasing atmospheric CO2 concentration and climate warming could
also result in a doubling of net primary productivity under the A2 scenario and
nearly 70% increase under the B2 scenario. The trends of impacts could be
considered as robust but the magnitudes should be viewed with caution, due to
the uncertainty in climate projections. Given the projected trends of likely
impacts of climate change on forest ecosystems, it is important to incorporate
climate change consideration in forest sector long-term planning process.

<id>
q-bio/0512016v1
<category>
q-bio.OT
<abstract>
Intrinsic transcriptional noise induced by operator fluctuations is
investigated with a simple spin like stochastic model. The effects of
transcriptional fluctuations in protein synthesis is probed by coupling
transcription and translation by an amplificative interaction. In the presence
of repression a new term contributes to the noise which depends on the rate of
mRNA production. If the switching time is small compared with the mRNA life
time the noise is also small. In general the dumping of protein production by a
repressive agent occurs linearly but the fluctuations can show a maxima at
intermediate repression. The discrepancy between the switching time, the mRNA
degradation and protein degradation is crucial for the repressive control in
translation without large fluctuations. The noise profiles obtained here are in
quantitative agreement with recent experiments.

<id>
q-bio/0601016v1
<category>
q-bio.OT
<abstract>
The effects of pure multiplicative noise on stochastic resonance in an
anti-tumor system modulated by a seasonal external field are investigated by
using theoretical analyses of the generalized potential and numerical
simulations. For optimally selected values of the multiplicative noise
intensity quasi-symmetry of two potential minima and stochastic resonance are
observed. Theoretical results and numerical simulations are in good
quantitative agreement.

<id>
q-bio/0601040v1
<category>
q-bio.OT
<abstract>
We have proposed the neck linker swing model to investigate the mechanism of
mechanochemical coupling of kinesin. The Michaelis-Menten-like curve for
velocity vs ATP concentration at different loads has been obtained, which is in
agreement with experiments. We have predicted that Michaelis constant doesn't
increase monotonically and an elastic instability will happen with increasing
of applied force.

<id>
q-bio/0602017v1
<category>
q-bio.OT
<abstract>
It has been observed that male mice who are consistently winning fights with
conspecifics can raise their tail, which is very similar to a morphine-induced
Straub tail response. Since this response is a typical index of opiate
activation, it has been proposed that the opioidergic systems of such mice are
chronically activated. This activation appeared to be a potent factor, which
leads to addiction to aggression. To check this hypothesis, we exposed the mice
who had won 20 fights in succession with conspecifics to a behavioral
sensitization procedure. The effects of the mu-opioid receptor agonist morphine
(10 mg/kg, i.p.) on the behavior of winners were examined in an open-field test
before and after 5- and 14-day deprivation of aggression. Morphine had a much
stronger stimulating effect on the open-field behavior of 60 % of the winners
deprived of aggression for 14 days than on that of the control mice. Morphine
did not stimulate behavioral activity in the winners before or after
deprivation for 5 days. The aggression level in the winners was higher after
than before deprivation. It has been concluded that, in the winners, the
mu-opioid receptors became tolerant to the effects of morphine and became
sensitized after long aggression deprivation. The development of addiction to
aggression due to repeated victories is discussed in the light of the theory of
addiction (Robinson,Berridge, 2003).

<id>
q-bio/0605029v1
<category>
q-bio.OT
<abstract>
Ammonoids are a group of extinct mollusks belonging to the same class of the
living genus Nautilus (Cephalopoda). In both Nautili and ammonoids, the
(usually planospiral) shell is divided into chambers separated by septa that
during the lifetime were filled with gas at atmospheric pressure. The
intersection of septa with the external shell generates a curve called the
suture line, which in living and most fossil Nautili is fairly uncomplicated.
In contrast, suture lines of ancient ammonoid were gently curved and during the
evolution of the group became highly complex, in some cases so extensively
frilled to be considerable as fractal curves. Numerous theories have been put
forward to explain the complexity of suture ammonoid lines. Calculations
presented here lend support to the hypothesis that complex suture lines aided
in counteracting the effect of the external water pressure. Additionally, it is
found that complex suture lines diminished shell shrinkage caused by water
pressure, and thus aided improve buoyancy. Understanding the reason for complex
sutures in ammonoids does not only represent an important issue in
paleobiology, but is also a challenging problem in the resistance of complex
mechanical structures subjected to high pressure.

<id>
q-bio/0606024v4
<category>
q-bio.OT
<abstract>
In the last century mercury levels in the global environment have tripled as
a result of increased pollution from industrial, occupational, medicinal and
domestic uses \cite{BaMe03}. Glutathione is known to be the main agent
responsible for the excretion of mercury (we refer to \cite{Thim05},
\cite{ZalBar99} and \cite{Lyn02}). It has also been shown that mercury inhibits
glutathione synthetase (an enzyme acting in the synthesization of Glutathione),
therefore leading to decreased glutathione levels
  (we refer to \cite{Thim05}, \cite{GeGe05}, \cite{GeGe06} and \cite{RDeth04}).
Mercury also interferes with the production of heme in the porphyrin pathway
\cite{WoMaEc93}. Heme is needed for biological energy production and ability to
detox organic toxins via the P450 enzymes \cite{Boy06}. The purpose of this
paper is to show that body's response to mercury exposure is hysteretic, i.e.
when this feedback of mercury on its main detoxifying agents is strong enough
then mercury body burden has two points of equilibrium: one with normal
abilities to detoxify and low levels of mercury and one with inhibited
abilities to detoxify and high levels of mercury. Furthermore, a small increase
of body's mercury burden may not be sufficient to trigger observable neurotoxic
effects but it may be sufficient to act as a switch leading to an accumulation
of mercury in the body through environmental exposure until its toxicity
becomes manifest.

<id>
q-bio/0606038v2
<category>
q-bio.OT
<abstract>
The Metropolis implementation of the Monte Carlo algorithm has been developed
to study the equilibrium thermodynamics of many-body systems. Choosing small
trial moves, the trajectories obtained applying this algorithm agree with those
obtained by Langevin's dynamics. Applying this procedure to a simplified
protein model, it is possible to show that setting a threshold of 1 degree on
the movement of the dihedrals of the protein backbone in a single Monte Carlo
step, the mean quantities associated with the off-equilibrium dynamics (e.g.,
energy, RMSD, etc.) are well reproduced, while the good description of higher
moments requires smaller moves. An important result is that the time duration
of a Monte Carlo step depends linearly on the temperature, something which
should be accounted for when doing simulations at different temperatures.

<id>
q-bio/0608011v1
<category>
q-bio.OT
<abstract>
Cicadas (Homoptera:Cicadidae) are insects able to produce loudly songs and it
is known that the mechanism to produce sound of tymballing cicadas works as a
Helmholtz resonator. In this work we offer evidence on the participation of the
wings in a high quality resonating process which defines the details of the
acoustic properties of the calling song. The study is carry on \textit{Quesada
gigas} species and it is dividied in three stages: (i) the acoustical
characterization of the abdominal cavity, (ii) the record and calculation of
frequency spectrum of the calling song, and (iii) the measurement of the
vibration modes of the wings. The comparison between all the results
unequivocally show the dramatic influence of the wings in the moment in which
the insect emits its calling song.

<id>
q-bio/0610044v1
<category>
q-bio.OT
<abstract>
In a certain way, this paper presents the continuation of the previous one
which discussed the harmonic structure of the genetic code (Rakocevic, 2004).
Several new harmonic structures presented in this paper, through specific unity
and coherence, together with the previously presented (Rakocevic, 2004), show
that it makes sense to understand genetic code as a set of several different
harmonic structures. Thereby, the harmonicity itself represents a specific
unity and coherence of physico-chemical properties of amino acid molecules and
the number of atoms and/or nucleons in the molecules themselves (in the form of
typical balances). A specific Gauss' arithmetical algorithm has the central
position among all these structures and it corresponds to the patterns of the
number of atoms within the side chains of amino acid molecules in the following
sense: G+V = 11; P+I = 21; S+T+L+A+G = 31; D+E+M+C+P = 41; K+R+Q+N+V = 61;
F+Y+W+H+I = 71; (L+M+Q+W) + (A+C+N+H) = 81; (S+D+K+F) + (T+E+R+Y) = 91;
(F+L+M+S+P) = (T+A+Y+H+I) = (Q+N+K+D+V) = (E+C+W+R+G) = 51. Bearing in mind all
these regularities it make sense to talk about genetic code as a harmonic
system. On the other hand, such an order provides new evidence supporting the
hypothesis established in the previous paper (Rakocevic, 2004) that genetic
code has been complete from the very beginning and as such was the condition
for the origin and evolution of life.

<id>
q-bio/0611009v1
<category>
q-bio.OT
<abstract>
The possible role of quantum effects in transfer of genetic information is
studied. It's argued that the nucleotides selection during DNA replication is
performed by means of proton tunneling between nucleotide and DNA-polimerase
bound by hydrogen bonds. Such mechanism is sensitive to the structure of
nucleotide hydrogen bonds, consequently only one nucleotide sort is captured by
DNA-polimerase in each event.The algorithm of this multistep selection
mechanism is also analysed from the point of its optimality. It's shown that
it's equivalent to Grover algorithm of data base search.

<id>
q-bio/0611034v1
<category>
q-bio.OT
<abstract>
In order to face environmental constraints, trees are able to re-orient their
axes by controlling the stress level in the newly formed wood layers.
Angiosperms and gymnosperms evolved into two distinct mechanisms: the former
produce a wood with large tension pre-stress on the upper side of the tilted
axis, while the latter produce a wood with large compression pre-stress on the
lower side. In both cases, the difference between this stress level and that of
the opposite side, in light tension, generates the bending of the axis.
However, light values of compression were sometimes measured in the opposite
side of angiosperms. By analysing old data on chestnut and mani and new data on
poplar, this study shows that these values were not measurement artefacts. This
reveals that generating light compression stress in opposite wood contributes
to improve the performance of the re-orientation mechanism.

<id>
q-bio/0611041v1
<category>
q-bio.OT
<abstract>
This work presents a mathematical model that establishes an interesting
connection between nucleotide frequencies in human single-stranded DNA and the
famous Fibonacci's numbers. The model relies on two assumptions. First,
Chargaff's second parity rule should be valid, and, second, the nucleotide
frequencies should approach limit values when the number of bases is
sufficiently large. Under these two hypotheses, it is possible to predict the
human nucleotide frequencies with accuracy. It is noteworthy, that the
predicted values are solutions of an optimization problem, which is commonplace
in many nature's phenomena.

<id>
q-bio/0611068v2
<category>
q-bio.OT
<abstract>
We perform geometrization of genetics by representing genetic information by
points of the 4-adic {\it information space.} By well known theorem of number
theory this space can also be represented as the 2-adic space. The process of
DNA-reproduction is described by the action of a 4-adic (or equivalently
2-adic) dynamical system. As we know, the genes contain information for
production of proteins. The genetic code is a degenerate map of codons to
proteins. We model this map as functioning of a polynomial dynamical system.
The purely mathematical problem under consideration is to find a dynamical
system reproducing the degenerate structure of the genetic code. We present one
of possible solutions of this problem.

<id>
q-bio/0612022v2
<category>
q-bio.OT
<abstract>
We discuss the similarity of the degeneration structure of the genetic code
with a pure number theoretic -- ``divisors code.'' The most interesting thing
about our observation is not that there is a connection between number theory
and the genetic code, but the simplicity of the rule. We hope that the
observation and the naive model presented in this paper will serve for ideas to
other models of the degeneracy of the genetic code. Maybe, the ideas of this
article can also be used in the area of artificial life to syntesize artificial
genetic codes.

<id>
q-bio/0701016v1
<category>
q-bio.OT
<abstract>
The contributors study the short-time dynamics of helix-forming polypeptide chains
using an all-atom representation of the molecules and an implicit solvation
model to approximate the interaction with the surrounding solvent. The results
confirm earlier observations that the helix-coil transition in proteins can be
described by a set of critical exponents. The high statistics of the
simulations allows the contributors to determine the exponents values with increased
precision and support universality of the helix-coil transition in homopolymers
and (helical) proteins.

<id>
q-bio/0701033v1
<category>
q-bio.OT
<abstract>
Understanding the cause of the synchronization of population evolution is an
important issue for ecological improvement. Here we present a
Lotka-Volterra-type model driven by two correlated environmental noises and
show, via theoretical analysis and direct simulation, that noise correlation
can induce a synchronization of the mutualists. The time series of mutual
species exhibit a chaotic-like fluctuation, which is independent to the noise
correlation, however, the chaotic fluctuation of mutual species ratio decreases
with the noise correlation. A quantitative parameter defined for characterizing
chaotic fluctuation provides a good approach to measure when the complete
synchronization happens.

<id>
q-bio/0310003v2
<category>
q-bio.PE
<abstract>
The defense response in plants challenged with pathogens is characterized by
the activation of a diverse set of genes. Many of the same genes are induced in
the defense responses of a wide range of plant species. How plant defense gene
families evolve may therefore provide an important clue to our understanding of
how disease resistance evolves. Because studies usually focus on a single host
species, little data are available regarding changes in defense gene expression
patterns as species diverge. The expression of defense-induced genes PR10,
chitinase and chalcone synthase was assayed in four pea species (Pisum sativum,
P. humile, P. elatius and P. fulvum) and two Lathyrus species (L. sativus and
L. tingitanus) which exhibited a range of infection phenotypes with Fusarium
solani . In P. sativum, resistance was accompanied by a strong induction of
defense genes at 8 hr. post-inoculation. Weaker induction was seen in
susceptible interactions in wild species. Divergence in the timing of PR10
expression was most striking between P. sativum and its closest realtive, P.
humile. Two members of this multigene family, designated PR10.1 and PR10.2, are
strongly-expressed in response to Fusarium, while the PR10.3 gene is more
weakly expressed, among Pisum species. The rapidity with which PR10 expression
evolves raises the question, is divergence of defense gene expression a part of
the phenotypic diversity underlying plant/pathogen coevolution?

<id>
q-bio/0310004v1
<category>
q-bio.PE
<abstract>
Hierarchical structure is an essential part of complexity, important notion
relevant for a wide range of applications ranging from biological population
dynamics through robotics to social sciences. In this paper we propose a simple
cellular-automata tool for study of hierarchical population dynamics.

<id>
q-bio/0310006v1
<category>
q-bio.PE
<abstract>
We investigate the process of fixation of advantageous mutations in an
asexual population. We assume that the effect of each beneficial mutation is
exponentially distributed with mean value $\omega_{med}=1/\beta$. The model
also considers that the effect of each new deleterious mutation reduces the
fitness of the organism independent on the previous number of mutations. We use
the branching process formulation and also extensive simulations to study the
model. The agreement between the analytical predictions and the simulational
data is quite satisfactory. Surprisingly, we observe that the dependence of the
probability of fixation $P_{fix}$ on the parameter $\omega_{med}$ is precisely
described by a power-law relation, $P_{fix} \sim \omega_{med}^{\gamma}$. The
exponent $\gamma$ is an increase function of the rate of deleterious mutations
$U$, whereas the probability $P_{fix}$ is a decreasing function of $U$. The
mean value $\omega_{fix}$ of the beneficial mutations which reach ultimate
fixation depends on $U$ and $\omega_{med}$. The ratio
$\omega_{fix}/\omega_{med}$ increases as we consider higher values of mutation
value $U$ in the region of intermediate to large values of $\omega_{med}$,
whereas for low $\omega_{med}$ we observe the opposite behavior.

<id>
q-bio/0310017v1
<category>
q-bio.PE
<abstract>
We study a four species ecological system with cyclic dominance whose
individuals are distributed on a square lattice. Randomly chosen individuals
migrate to one of the neighboring sites if it is empty or invade this site if
occupied by their prey. The cyclic dominance maintains the coexistence of all
the four species if the concentration of vacant sites is lower than a threshold
value. Above the treshold, a symmetry breaking ordering occurs via growing
domains containing only two neutral species inside. These two neutral species
can protect each other from the external invaders (predators) and extend their
common territory. According to our Monte Carlo simulations the observed phase
transition is equivalent to those found in spreading models with two equivalent
absorbing states although the present model has continuous sets of absorbing
states with different portions of the two neutral species. The selection
mechanism yielding symmetric phases is related to the domain growth process
whith wide boundaries where the four species coexist.

<id>
q-bio/0310031v1
<category>
q-bio.PE
<abstract>
Animal mitochondrial genomes usually have two transfer RNAs for Leucine: one,
with anticodon UAG, translates the four-codon family CUN, whilst the other,
with anticodon UAA, translates the two-codon family UUR. These two genes must
differ at the third anticodon position, but in some species the genes differ at
many additional sites, indicating that these genes have been independent for a
long time. Duplication and deletion of genes in mitochondrial genomes occurs
frequently during the evolution of the Metazoa. If a tRNA-Leu gene were
duplicated and a substitution occurred in the anticodon, this would effectively
turn one type of tRNA into the other. The original copy of the second tRNA type
might then be lost by a deletion elsewhere in the genome. There are several
groups of species in which the two tRNA-Leu genes occur next to one another (or
very close) on the genome, which suggests that tandem duplication has occurred.
Here we use RNA-specific phylogenetic methods to determine evolutionary trees
for both genes. We present evidence that the process of duplication, anticodon
mutation and deletion of tRNA-Leu genes has occurred at least five times during
the evolution of the Metazoa - once in the common ancestor of all Protostomes,
once in the common ancestor of Echinoderms and Hemichordates, once in the
hermit crab, and twice independently in Molluscs.

<id>
q-bio/0311002v1
<category>
q-bio.PE
<abstract>
We propose a generic model of eco-systems, with a {\it hierarchical} food web
structure. In our computer simulations we let the eco-system evolve
continuously for so long that that we can monitor extinctions as well as
speciations over geological time scales. {\it Speciation} leads not only to
horizontal diversification of species at any given trophic level but also to
vertical bio-diversity that accounts for the emergence of complex species from
simpler forms of life. We find that five or six trophic levels appear as the
eco-system evolves for sufficiently long time, starting initially from just one
single level. Moreover, the time intervals between the successive collections
of ecological data is so short that we could also study ``micro''-evolution of
the eco-system, i.e., the birth, ageing and death of individual organisms.

<id>
q-bio/0311020v2
<category>
q-bio.PE
<abstract>
Recent work on mutation-selection models has revealed that, under specific
assumptions on the fitness function and the mutation rates, asymptotic
estimates for the leading eigenvalue of the mutation-reproduction matrix may be
obtained through a low-dimensional maximum principle in the limit N to infinity
(where N is the number of types). In order to extend this variational principle
to a larger class of models, we consider here a family of reversible N by N
matrices and identify conditions under which the high-dimensional Rayleigh-Ritz
variational problem may be reduced to a low-dimensional one that yields the
leading eigenvalue up to an error term of order 1/N. For a large class of
mutation-selection models, this implies estimates for the mean fitness, as well
as a concentration result for the ancestral distribution of types.

<id>
q-bio/0311035v1
<category>
q-bio.PE
<abstract>
A simulation approach to the stochastic growth of bacterial towers is
presented, in which a non-uniform and finite nutrient supply essentially
determines the emerging structure through elementary chemotaxis. The method is
based on cellular automata and we use simple, microscopic, local rules for
bacterial division in nutrient-rich surroundings. Stochastic nutrient
diffusion, while not crucial to the dynamics of the total population, is
influential in determining the porosity of the bacterial tower and the
roughness of its surface. As the bacteria run out of food, we observe an
exponentially rapid saturation to a carrying capacity distribution, similar in
many respects to that found in a recently proposed phenomenological
hierarchical population model, which uses heuristic parameters and macroscopic
rules. Complementary to that phenomenological model, the simulation aims at
giving more microscopic insight into the possible mechanisms for one of the
recently much studied bacterial morphotypes, known as "towering biofilm",
observed experimentally using confocal laser microscopy. A simulation
suggesting a mechanism for biofilm resistance to antibiotics is also shown.

<id>
q-bio/0312014v3
<category>
q-bio.PE
<abstract>
The evolutionary persistence of symbiotic associations is a puzzle.
Adaptation should eliminate cooperative traits if it is possible to enjoy the
advantages of cooperation without reciprocating - a facet of cooperation known
in game theory as the Prisoner's Dilemma. Despite this barrier, symbioses are
widespread, and may have been necessary for the evolution of complex life. The
discovery of strategies such as tit-for-tat has been presented as a general
solution to the problem of cooperation. However, this only holds for
within-species cooperation, where a single strategy will come to dominate the
population. In a symbiotic association each species may have a different
strategy, and the theoretical analysis of the single species problem is no
guide to the outcome. We present basic analysis of two-species cooperation and
show that a species with a fast adaptation rate is enslaved by a slowly
evolving one. Paradoxically, the rapidly evolving species becomes highly
cooperative, whereas the slowly evolving one gives little in return. This helps
understand the occurrence of endosymbioses where the host benefits, but the
symbionts appear to gain little from the association.

<id>
q-bio/0312016v1
<category>
q-bio.PE
<abstract>
A simplified susceptible-infected-recovered (SIR) epidemic model and a
small-world model are applied to analyse the spread and control of Severe Acute
Respiratory Syndrome (SARS) for Hong Kong in early 2003. From data available in
mid April 2003, we predict that SARS would be controlled by June and nearly
1700 persons would be infected based on the SIR model. This is consistent with
the known data. A simple way to evaluate the development and efficacy of
control is described and shown to provide a useful measure for the future
evolution of an epidemic. This may contribute to improve strategic response
from the government. The evaluation process here is universal and therefore
applicable to many similar homogeneous epidemic diseases within a fixed
population. A novel model consisting of map systems involving the Small-World
network principle is also described. We find that this model reproduces
qualitative features of the random disease propagation observed in the true
data. Unlike traditional deterministic models, scale-free phenomena are
observed in the epidemic network. The numerical simulations provide theoretical
support for current strategies and achieve more efficient control of some
epidemic diseases, including SARS.

<id>
q-bio/0312029v1
<category>
q-bio.PE
<abstract>
Using daily infection data for Hong Kong we explore the validity of a variety
of models of disease propagation when applied to the SARS epidemic. Surrogate
data methods show that simple random models are insufficient and that the
standard epidemic susceptible-infected-removed model does not fully account for
the underlying variability in the observed data. As an alternative, we consider
a more complex small world network model and show that such a structure can be
applied to reliably produce simulations quantitative similar to the true data.
The small world network model not only captures the apparently random
fluctuation in the reported data, but can also reproduces mini-outbreaks such
as those caused by so-called ``super-spreaders'' and in the Hong Kong housing
estate Amoy Gardens.

<id>
q-bio/0401023v1
<category>
q-bio.PE
<abstract>
We analyze the properties of model food webs and of fifteen community food
webs from a variety of environments. We first perform a theoretical analysis of
the niche model of Williams and Martinez. We derive analytical expressions for
the distributions of species' number of prey, number of predators, and total
number of trophic links and find that they follow universal functional forms.
We also derive expressions for a number of other biologically relevant
parameters, including the fraction of top, intermediate, basal, and cannibal
species, the standard deviations of generality and vulnerability, the
correlation coefficient between species' number of prey and number of
predators, and assortativity. We show that our findings are robust under rather
general conditions. We then use our analytical predictions as a guide to the
analysis of fifteen of the most complete empirical food webs available. We
uncover quantitative unifying patterns that describe the properties of the
model food webs and most of the trophic webs considered. Our results support a
strong new hypothesis that the empirical distributions of number of prey and
number of predators follow universal functional forms that, without free
parameters, match our analytical predictions. Further, we find that the
empirically observed correlation coefficient, assortativity, and fraction of
cannibal species are consistent with our analytical expressions and simulations
of the niche model. Finally, we show that the average distance between nodes
and the average clustering coefficient show a high degree of regularity for
both the empirical data and simulations of the niche model. Our findings
suggest that statistical physics concepts such as scaling and universality may
be useful in the description of natural ecosystems.

<id>
q-bio/0401035v1
<category>
q-bio.PE
<abstract>
A fundamental problem in evolutionary ecology research is to explain how
different species coexist in natural ecosystems. This question is directly
related with species trophic competition. However, competition theory, based on
the classical logistic Lotka-Volterra equations, leads to erroneous conclusions
about species coexistence. The reason for this is incorrectly interpreted
interspecific interactions, expressed in the form of the competition
coefficients. Here I use the logistic Lotka-Volterra type competition equations
derived from the so called resource competition models to obtain the necessary
conditions for species coexistence. These models show that only species with
identical competitive abilities may coexist. Due to such relations between
competing species ecosystems biodiversity decreases in the course of evolution.

<id>
q-bio/0402007v4
<category>
q-bio.PE
<abstract>
It is possible to consider stochastic models of sequence evolution in
phylogenetics in the context of a dynamical tensor description inspired from
physics. Approaching the problem in this framework allows for the well
developed methods of mathematical physics to be exploited in the biological
arena. We present the tensor description of the homogeneous continuous time
Markov chain model of phylogenetics with branching events generated by
dynamical operations. Standard results from phylogenetics are shown to be
derivable from the tensor framework. We summarize a powerful approach to
entanglement measures in quantum physics and present its relevance to
phylogenetic analysis. Entanglement measures are found to give distance
measures that are equivalent to, and expand upon, those already known in
phylogenetics. In particular we make the connection between the group invariant
functions of phylogenetic data and phylogenetic distance functions. We
introduce a new distance measure valid for three taxa based on the group
invariant function known in physics as the "tangle". All work is presented for
the homogeneous continuous time Markov chain model with arbitrary rate
matrices.

<id>
q-bio/0402009v3
<category>
q-bio.PE
<abstract>
In large asexual populations, beneficial mutations have to compete with each
other for fixation. Here, I derive explicit analytic expressions for the rate
of substitution and the mean beneficial effect of fixed mutations, under the
assumptions that the population size N is large, that the mean effect of new
beneficial mutations is smaller than the mean effect of new deleterious
mutations, and that new beneficial mutations are exponentially distributed. As
N increases, the rate of substitution approaches a constant, which is equal to
the mean effect of new beneficial mutations. The mean effect of fixed mutations
continues to grow logarithmically with N. The speed of adaptation, measured as
the change of log fitness over time, also grows logarithmically with N for
moderately large N, and it grows double-logarithmically for extremely large N.
Moreover, I derive a simple formula that determines whether at given N
beneficial mutations are expected to compete with each other or go to fixation
independently. Finally, I verify all results with numerical simulations.

<id>
q-bio/0402010v1
<category>
q-bio.PE
<abstract>
RNA viruses are a widely used tool to study evolution experimentally. Many
standard protocols of virus propagation and competition are done at nominally
low multiplicity of infection (m.o.i.), but lead during one passage to two or
more rounds of infection, of which the later ones are at high m.o.i. Here, we
develop a model of the competition between wild type (wt) and a mutant under a
regime of alternating m.o.i. We assume that the mutant is deleterious when it
infects cells on its own, but derives a selective advantage when rare and
coinfecting with wt, because it can profit from superior protein products
created by the wt. We find that, under these assumptions, replication at
alternating low and high m.o.i. may lead to the stable coexistence of wt and
mutant for a wide range of parameter settings. The predictions of our model are
consistent with earlier observations of frequency-dependent selection in VSV
and HIV-1. Our results suggest that frequency-dependent selection may be common
in typical evolution experiments with viruses.

<id>
q-bio/0402011v2
<category>
q-bio.PE
<abstract>
Trends in human longevity are puzzling, especially when considering the
limits of human longevity. Partially, the conflicting assertions are based upon
demographic evidence and the interpretation of survival and mortality curves
using the Gompertz model and the Weibull model; these models are sometimes
considered to be incomplete in describing the entire curves. In this paper a
new model is proposed to take the place of the traditional models. We directly
analysed the rectangularity (the parts of the curves being shaped like a
rectangle) of survival curves for 17 countries and for 1876-2001 in Switzerland
(it being one of the longest-lived countries) with a new model. This model is
derived from the Weibull survival function and is simply described by two
parameters, in which the shape parameter indicates 'rectangularity' and
characteristic life indicates the duration for survival to be 'exp(-1)'. The
shape parameter is essentially a function of age and it distinguishes humans
from technical devices. We find that although characteristic life has increased
up to the present time, the slope of the shape parameter for middle age has
been saturated in recent decades and that the rectangularity above
characteristic life has been suppressed, suggesting there are ultimate limits
to human longevity. The new model and subsequent findings will contribute
greatly to the interpretation and comprehension of our knowledge on the human
ageing processes.

<id>
q-bio/0402013v1
<category>
q-bio.PE
<abstract>
General functions for human survival and mortality may support a possibility
of general mechanisms in human ageing. We discovered that the survival and
mortality curves could be described very simply and accurately by the Weibull
survival function with age-dependent shape parameter. The age-dependence of
shape parameter determines the shape of the survival and mortality curves and
tells the nature of the ageing rate. Especially, the progression of shape
parameter with age may be explained by the increase of interaction among vital
processes or the evolution of susceptibility to faults with age. Age-related
diseases may be attributed to the evolution of susceptibility to faults with
age.

<id>
q-bio/0402016v1
<category>
q-bio.PE
<abstract>
In this paper the Penna model is reconsidered. With computer simulations we
check how the control parameters of the model influence the size of the stable
population.

<id>
q-bio/0402019v2
<category>
q-bio.PE
<abstract>
In this paper we study the household-structure SIS epidemic spreading on
general complex networks. The household structure gives us the way to
distinguish inner and the outer infection rate. Unlike household-structure
models on homogenous networks, such as regular and random networks, here we
consider heterogeneous networks with arbitrary degree distribution p(k). First
we introduce the epidemic model. Then rate equations under mean field
appropriation and computer simulations are used here to analyze our model. Some
unique phenomena only existing in divergent network with household structure is
found, while we also get some similar conclusions that some simple geometrical
quantities of networks have important impression on infection property of
infectous disease. It seems that in our model even when local cure rate is
greater than inner infection rate in every household, disease still can spread
on scale-free network. It implies that no disease is spreading in every single
household, but for the whole network, disease is spreading. Since our society
network seems like this structure, maybe this conclusion remind us that during
disease spreading we should pay more attention on network structure than local
cure condition.

<id>
q-bio/0402028v1
<category>
q-bio.PE
<abstract>
We study the effect of mutations in a simple model of colonization, based on
Montecarlo simulations. When the population colonizes the whole available
habitat, a maximum population density is reached, which depends on the mutation
rate. Depending on the values of other parameters, such as selection pressure,
fecundity and mobility, there is an optimal value for the mutation rate for
which the colonization reaches the highest density. We also investigate the
survival probabilities under different conditions and its relation to the
mutation rate.

<id>
q-bio/0402034v2
<category>
q-bio.PE
<abstract>
We wish to verify that the mortality deceleration (or decrease) is a
consequence of the bending of the shape parameter at old ages. This
investigation is based upon the Weon model (the Weibull model with an
age-dependent shape parameter) for human survival and mortality curves.
According to the Weon model, we are well able to describe the mortality
decrease after the mortality plateau, including the mortality deceleration.
Furthermore, we are able to simply define the mathematical limit of longevity
by the mortality decrease. From the demographic analysis of the historical
trends in Switzerland (1876-2001) and Sweden (1861-2001), and the most recent
trends in the other eleven developed countries (1996-2001), we confirm that the
bending of the shape parameter after characteristic life is correlated with the
mortality deceleration (or decrease). As a consequence, this bending of the
shape parameters and the mortality deceleration is associated with the
mathematical limit on longevity. These results suggest that the mathematical
limit of longevity can be induced by the mortality deceleration (or decrease)
in nature. These findings will give us a breakthrough for studying the
mortality dynamics at the highest ages.

<id>
q-bio/0402038v2
<category>
q-bio.PE
<abstract>
A microscopic model is developed, within the frame of the theory of
quantitative traits, to study both numerically and analytically the combined
effect of competition and assortativity on the sympatric speciation process,
i.e. speciation in the absence of geographical barriers. Two components of
fitness are considered: a static one that describes adaptation to environmental
factors not related to the population itself, and a dynamic one that accounts
for interactions between organisms, e.g. competition. The effects of finiteness
of population size on survival of coexisting species are also accounted for.
The simulations show that both in the case of flat and ripid static fitness
landscapes, competition and assortativity do exert a synergistic effect on
speciation. We also show that competition acts as a stabilizing force against
extinction due to random sampling in a finite population. Finally, evidence is
shown that speciation can be seen as a phase transition.

<id>
q-bio/0402042v3
<category>
q-bio.PE
<abstract>
In this paper, we introduce a modified epidemic model on regular and
scale-free networks respectively. We consider the birth rate $\delta$, cure
rate $\gamma$, infection rate $\lambda$, $\alpha$ from the infectious disease,
and death rate $\beta$ from other factors. Through mean-field analysis, we find
that on regular network there is an epidemic threshold $\lambda_{c}$ dependent
on the parameters $\delta$, $\gamma$, $\alpha$, and $\beta$; while for power
law degree distribution network epidemic threshold is absent in the
thermodynamic limit. The result is the same as that of the standard SIS model.
This reminds us the structure of the networks plays a very important role in
the spreading property of the infectious disease.

<id>
q-bio/0403004v2
<category>
q-bio.PE
<abstract>
Noise, through its interaction with the nonlinearity of the living systems,
can give rise to counter-intuitive phenomena such as stochastic resonance,
noise-delayed extinction, temporal oscillations, and spatial patterns. In this
paper we briefly review the noise-induced effects in three different
ecosystems: (i) two competing species; (ii) three interacting species, one
predator and two preys, and (iii) N-interacting species. The transient dynamics
of these ecosystems are analyzed through generalized Lotka-Volterra equations
in the presence of multiplicative noise, which models the interaction between
the species and the environment. The interaction parameter between the species
is random in cases (i) and (iii), and a periodical function, which accounts for
the environmental temperature, in case (ii). We find noise-induced phenomena
such as quasi-deterministic oscillations, stochastic resonance, noise-delayed
extinction, and noise-induced pattern formation with nonmonotonic behaviors of
patterns areas and of the density correlation as a function of the
multiplicative noise intensity. The asymptotic behavior of the time average of
the \emph{$i^{th}$} population when the ecosystem is composed of a great number
of interacting species is obtained and the effect of the noise on the
asymptotic probability distributions of the populations is discussed.

<id>
q-bio/0403010v2
<category>
q-bio.PE
<abstract>
In recent we introduced, developed and established a new concept, model,
methodology and principle for studying human longevity in terms of demographic
basis. We call the new model the "Weon model", which is a general model
modified from the Weibull model with an age-dependent shape parameter to
describe human survival and mortality curves. We demonstrate the application of
the Weon model to the mortality dynamics and the mathematical limit of
longevity (the mortality rate to be mathematically zero, implying a maximum
longevity) in the Section I. The mathematical limit of longevity can be induced
by the mortality dynamics in nature. As a result, we put forward the
complementarity principle, which explains the recent paradoxical trends that
the mathematical limit decreases as the longevity increases, in the Section II.
Our findings suggest that the human longevity can be limited by the
complementarity principle.

<id>
q-bio/0403014v1
<category>
q-bio.PE
<abstract>
We present a model for the growth of West Nile virus in mosquito and bird
populations based on observations of the initial epidemic in the U.S. Increase
of bird mortality as a result of infection, which is a feature of the epidemic,
is found to yield an effect which is observable in principle, viz., periodic
variations in the extent of infection. The vast difference between mosquito and
bird lifespans, another peculiarity of the system, is shown to lead to
interesting consequences regarding delay in the onset of the steady-state
infection. An outline of a framework is provided to treat mosquito diffusion
and bird migration.

<id>
q-bio/0403017v1
<category>
q-bio.PE
<abstract>
Accurate demographic functions help scientists define and understand
longevity. We summarize a new demographic model, the Weon model, and show the
application to the demographic data for Switzerland (1876-2002). Particularly,
the Weon model simply defines the maximum longevity, which is induced in nature
by the mortality dynamics. In this study, we reconsider the definition of the
maximum longevity and the effectiveness for longevity by the combined effect of
the survival and mortality functions. The results suggest that the mortality
function should be zero at the maximum longevity, since the density function is
zero but the survival function is not zero. Furthermore, the effectiveness for
longevity can be maximized at the characteristic life by the complementarity
between the survival and mortality functions, which suggests that there may be
two parts of rectangularization for longevity. The historical trends for
Switzerland (1876-2002) implies that there may be a fundamental limiting force
to restrict the increase of the effectiveness. As a result, it seems that the
density function is essential to define and understand the mortality dynamics,
the maximum longevity, the effectiveness for longevity, the paradigm of
rectangularization and the historical trends of the effectiveness by the
complementarity between the survival and mortality functions.

<id>
q-bio/0403030v1
<category>
q-bio.PE
<abstract>
In this paper, we investigate fitness landscapes (under point mutation and
recombination) from the standpoint of whether the induced evolutionary dynamics
have a "fast-slow" time scale associated with the differences in relaxation
time between local quasi-equilibria and the global equilibrium. This dynamical
behavior has been formally described in the econometrics literature in terms of
the spectral properties of the appropriate operator matrices by Simon and Ando
(1961), and we use the relations they derive to ask which fitness functions and
mutation/recombination operators satisfy these properties. It turns out that
quite a wide range of landscapes satisfy the condition (at least trivially)
under point mutation given a sufficiently low mutation rate, while the property
appears to be difficult to satisfy under genetic recombination. In spite of the
fact that Simon-Ando decomposability can be realized over fairly wide range of
parameters, it imposes a number of restrictions on which landscape
partitionings are possible. For these reasons, the Simon-Ando formalism doesn't
appear to be applicable to other forms of decomposition and aggregation of
variables that are important in evolutionary systems.

<id>
q-bio/0403035v1
<category>
q-bio.PE
<abstract>
A fundamental question in aging research concerns the demographic
trajectories at the highest ages, especially for supercentenarians (persons
aged 110 or more). We wish to demonstrate that the Weon model enables
scientists to describe the demographic trajectories for supercentenarians. We
evaluate the average survival data from the modern eight countries and the
valid and complete data for supercentenarians from the International Database
on Longevity (Robine and Vaupel, (2002) North American Actuarial Journal 6,
54-63). The results suggest that the Weon model predicts the maximum longevity
to exist around ages 120-130, which indicates that there is an intrinsic limit
to human longevity, and that the Weon model allows the best possible
description of the demographic trajectories for supercentenarians.

<id>
q-bio/0309006v1
<category>
q-bio.QM
<abstract>
Background: The availability of high throughput methods for measurement of
mRNA concentrations makes the reliability of conclusions drawn from the data
and global quality control of samples and hybridization important issues. We
address these issues by an information theoretic approach, applied to
discretized expression values in replicated gene expression data.
  Results: Our approach yields a quantitative measure of two important
parameter classes: First, the probability $P(\sigma | S)$ that a gene is in the
biological state $\sigma$ in a certain variety, given its observed expression
$S$ in the samples of that variety. Second, sample specific error probabilities
which serve as consistency indicators of the measured samples of each variety.
The method and its limitations are tested on gene expression data for
developing murine B-cells and a $t$-test is used as reference. On a set of
known genes it performs better than the $t$-test despite the crude
discretization into only two expression levels. The consistency indicators,
i.e. the error probabilities, correlate well with variations in the biological
material and thus prove efficient.
  Conclusions: The proposed method is effective in determining differential
gene expression and sample reliability in replicated microarray data. Already
at two discrete expression levels in each sample, it gives a good explanation
of the data and is comparable to standard techniques.

<id>
q-bio/0309021v1
<category>
q-bio.QM
<abstract>
We report the development and detailed calibration of a multiphoton
fluorescence lifetime imaging system (FLIM) using a streak camera. The present
system is versatile with high spatial (0.2 micron) and temporal (50 psec)
resolution and allows rapid data acquisition and reliable and reproducible
lifetime determinations. The system was calibrated with standard fluorescent
dyes and the lifetime values obtained were in very good agreement with values
reported in literature for these dyes. We also demonstrate the applicability of
the system to FLIM studies in cellular specimens including stained pollen
grains and fibroblast cells expressing green fluorescent protein. The lifetime
values obtained matched well with those reported earlier by other groups for
these same specimens. Potential applications of the present system include the
measurement of intracellular physiology and Fluorescence Resonance Energy
Transfer (FRET) imaging which are discussed in the context of live cell
imaging.

<id>
q-bio/0309023v1
<category>
q-bio.QM
<abstract>
We make available a library of documented IDL .pro files as well as a
shareable object library that allows IDL to call routines from LAPACK. The
routines are for use in the spectral analysis of time series data. The primary
focus of these routines are David Thomson's multitaper methods but a whole
range of functions will be made available in future revisions of the
submission. At present routines are provided to carry out the following
operations: calculate prolate spheroidal sequences and eigenvalues, project
time-series into frequency bands, calculate spectral estimates with or without
moving windows, and calculate the cross-coherence between two time series as a
function of frequency as well as the coherence between frequencies for a single
time series.

<id>
q-bio/0310007v1
<category>
q-bio.QM
<abstract>
We examined the changes in swimming behaviour of the bacterium Rhodobacter
sphaeroides in response to stepwise changes in a nutrient (propionate),
following the prestimulus motion, the initial response and the adaptation to
the sustained concentration of the chemical. This was carried out by tethering
motile cells by their flagella to glass slides and following the rotational
behaviour of their cell bodies in response to the nutrient change. Computerised
motion analysis was used to analyse the behaviour. Distributions of run and
stop times were obtained from rotation data for tethered cells. Exponential and
Weibull fits for these distributions, and variability in individual responses
are discussed. In terms of parameters derived from the run and stop time
distributions, we compare the responses to stepwise changes in the nutrient
concentration and the long-term behaviour of 84 cells under twelve propionate
concentration levels from 1 nM to 25 mM. We discuss traditional assumptions for
the random walk approximation to bacterial swimming and compare them with the
observed R. sphaeroides motile behaviour.

<id>
q-bio/0311022v1
<category>
q-bio.QM
<abstract>
An analysis of the protein content of several crystal forms of proteins has
been performed. We apply a new numerical technique, the Independent Component
Analysis (ICA), to determine the volume fraction of the asymmetric unit
occupied by the protein. This technique requires only the crystallographic data
of structure factors as input.

<id>
q-bio/0311024v1
<category>
q-bio.QM
<abstract>
We introduce, analyze, and implement a new method for parameter
identification for system of ordinary differential equations that are used to
model sets of biochemical reactions. Our method relies on the integral
formulation of the ODE system and a method of linear least squares applied to
the integral equations. Certain variants of this method are also introduced in
this paper.

<id>
q-bio/0402024v1
<category>
q-bio.QM
<abstract>
Microelectromagnet devices, a ring trap and a matrix, were developed for the
microscopic control of biological systems. The ring trap is a circular Au wire
with an insulator on top. The matrix has two arrays of straight Au wires, one
array perpendicular to the other, that are separated and topped by insulating
layers. Microelectromagnets can produce strong magnetic fields to stably
manipulate magnetically tagged biological systems in a fluid. Moreover, by
controlling the currents flowing through the wires, a microelectromagnet matrix
can move a peak in the magnetic field magnitude continuously over the surface
of the device, generate multiple peaks simultaneously and control them
independently. These capabilities of a matrix can be used to trap, continuously
transport, assemble, separate and sort biological samples on micrometer length
scales. Combining microelectromagnets with microfluidic systems, chip-based
experimental systems can be realized for novel applications in biological and
biomedical studies.

<id>
q-bio/0402030v1
<category>
q-bio.QM
<abstract>
A barrier penetration model has been proposed to explain the spontaneous
melting of the DNA oligomers into two separate single strands whereas the
partially melted intermediate states are shown to be the bound state solution
of the same effective potential that generates the barrier.

<id>
q-bio/0406002v1
<category>
q-bio.QM
<abstract>
Large numbers of MS/MS peptide spectra generated in proteomics experiments
require efficient, sensitive and specific algorithms for peptide
identification. In the Open Mass Spectrometry Search Algorithm [OMSSA],
specificity is calculated by a classic probability score using an explicit
model for matching experimental spectra to sequences. At default thresholds,
OMSSA matches more spectra from a standard protein cocktail than a comparable
algorithm. OMSSA is designed to be faster than published algorithms in
searching large MS/MS datasets.

<id>
q-bio/0406009v1
<category>
q-bio.QM
<abstract>
Sequence comparison is a widely used computational technique in modern
molecular biology. In spite of the frequent use of sequence comparisons the
important problem of assigning statistical significance to a given degree of
similarity is still outstanding. Analytical approaches to filling this gap
usually make use of an approximation that neglects certain correlations in the
disorder underlying the sequence comparison algorithm. Here, we use the longest
common subsequence problem, a prototype sequence comparison problem, to
analytically establish that this approximation does make a difference to
certain sequence comparison statistics. In the course of establishing this
difference we develop a method that can systematically deal with these disorder
correlations.

<id>
q-bio/0406016v2
<category>
q-bio.QM
<abstract>
We present a novel classification-based algorithm called GeneClass for
learning to predict gene regulatory response. Our approach is motivated by the
hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can
learn a decision rule for predicting whether a gene is up- or down-regulated in
a particular experiment based on (1) the presence of binding site subsequences
(``motifs'') in the gene's regulatory region and (2) the expression levels of
regulators such as transcription factors in the experiment (``parents''). Thus
our learning task integrates two qualitatively different data sources:
genome-wide cDNA microarray data across multiple perturbation and mutant
experiments along with motif profile data from regulatory sequences. Rather
than focusing on the regression task of predicting real-valued gene expression
measurements, GeneClass performs the classification task of predicting +1 and
-1 labels, corresponding to up- and down-regulation beyond the levels of
biological and measurement noise in microarray measurements. GeneClass uses the
Adaboost learning algorithm with a margin-based generalization of decision
trees called alternating decision trees. In computational experiments based on
the Gasch S. cerevisiae dataset, we show that the GeneClass method predicts up-
and down-regulation on held-out experiments with high accuracy. We explore a
range of experimental setups related to environmental stress response, and we
retrieve important regulators, binding site motifs, and relationships between
regulators and binding sites that are known to be associated to specific stress
response pathways. Our method thus provides predictive hypotheses, suggests
biological experiments, and provides interpretable insight into the structure
of genetic regulatory networks.

<id>
q-bio/0407038v1
<category>
q-bio.QM
<abstract>
Action potential duration (APD) restitution, which relates APD to the
preceding diastolic interval (DI), is a useful tool for predicting the onset of
abnormal cardiac rhythms. However, it is known that different pacing protocols
lead to different APD restitution curves (RCs). This phenomenon, known as APD
rate-dependence, is a consequence of memory in the tissue. In addition to APD
restitution, conduction velocity restitution also plays an important role in
the spatiotemporal dynamics of cardiac tissue. We present new results
concerning rate-dependent restitution in the velocity of propagating action
potentials in a one-dimensional fiber. Our numerical simulations show that,
independent of the amount of memory in the tissue, waveback velocity exhibits
pronounced rate-dependence and the wavefront velocity does not. Moreover, the
discrepancy between waveback velocity RCs is most significant for small DI. We
provide an analytical explanation of these results, using a system of coupled
maps to relate the wavefront and waveback velocities. Our calculations show
that waveback velocity rate-dependence is due to APD restitution, not memory.

<id>
q-bio/0408012v1
<category>
q-bio.QM
<abstract>
A commonly employed measure of the signal amplification properties of an
input/output system is its induced L2 norm, sometimes also known as "H
infinity" gain. In general, however, it is extremely difficult to compute the
numerical value for this norm, or even to check that it is finite, unless the
system being studied is linear. This paper describes a class of systems for
which it is possible to reduce this computation to that of finding the norm of
an associated linear system. In contrast to linearization approaches, a precise
value, not an estimate, is obtained for the full nonlinear model. The class of
systems that we study arose from the modeling of certain biological
intracellular signaling cascades, but the results should be of wider
applicability.

<id>
q-bio/0409010v1
<category>
q-bio.QM
<abstract>
We analyze a microscopic RNA model, which includes two widely used models as
limiting cases, namely it contains terms for bond as well as for stacking
energies. We numerically investigate possible changes in the qualitative and
quantitative behaviour while going from one model to the other; in particular
we test, whether a transition occurs, when continuously moving from one model
to the other. For this we calculate various thermodynamic quantities, both at
zero temperature as well as at finite temperatures. All calculations can be
done efficiently in polynomial time by a dynamic programming algorithm. We do
not find a sign for transition between the models, but the critical exponent
$\nu$ of the correlation length, describing the phase transition in all models
to an ordered low-temperature phase, seems to depend continuously on the model.
Finally, we apply the epsilon-Coupling method, to study low excitations. The
exponent $\theta$ describing the energy-scaling of the excitations seems to
depend not much on the energy model.

<id>
q-bio/0410010v1
<category>
q-bio.QM
<abstract>
An analysis of the RR-interval time series, $t_i$, is presented for the case
in which the average time, $\bar{t}$, changes slowly. In particular, $\bar{t}$
and a short-time scale variability parameter, $V$, are simultaneously measured
while $\bar{t}$ decreases for subjects in the reclined position. The initial
decrease in $\bar{t}$ is usually linear with $V$ yielding parameters that can
be related to physiological quantities.

<id>
q-bio/0411028v1
<category>
q-bio.QM
<abstract>
We present a novel classification-based method for learning to predict gene
regulatory response. Our approach is motivated by the hypothesis that in simple
organisms such as Saccharomyces cerevisiae, we can learn a decision rule for
predicting whether a gene is up- or down-regulated in a particular experiment
based on (1) the presence of binding site subsequences (``motifs'') in the
gene's regulatory region and (2) the expression levels of regulators such as
transcription factors in the experiment (``parents''). Thus our learning task
integrates two qualitatively different data sources: genome-wide cDNA
microarray data across multiple perturbation and mutant experiments along with
motif profile data from regulatory sequences. We convert the regression task of
predicting real-valued gene expression measurement to a classification task of
predicting +1 and -1 labels, corresponding to up- and down-regulation beyond
the levels of biological and measurement noise in microarray measurements. The
learning algorithm employed is boosting with a margin-based generalization of
decision trees, alternating decision trees. This large-margin classifier is
sufficiently flexible to allow complex logical functions, yet sufficiently
simple to give insight into the combinatorial mechanisms of gene regulation. We
observe encouraging prediction accuracy on experiments based on the Gasch S.
cerevisiae dataset, and we show that we can accurately predict up- and
down-regulation on held-out experiments. Our method thus provides predictive
hypotheses, suggests biological experiments, and provides interpretable insight
into the structure of genetic regulatory networks.

<id>
q-bio/0412021v1
<category>
q-bio.QM
<abstract>
We introduce a new heuristic for the multiple alignment of a set of
sequences. The heuristic is based on a set cover of the residue alphabet of the
sequences, and also on the determination of a significant set of blocks
comprising subsequences of the sequences to be aligned. These blocks are
obtained with the aid of a new data structure, called a suffix-set tree, which
is constructed from the input sequences with the guidance of the
residue-alphabet set cover and generalizes the well-known suffix tree of the
sequence set. We provide performance results on selected BAliBASE amino-acid
sequences and compare them with those yielded by some prominent approaches.

<id>
q-bio/0503023v2
<category>
q-bio.QM
<abstract>
We present multiscale models of cancer tumor invasion with components at the
molecular, cellular, and tissue levels. We provide biological justifications
for the model components, present computational results from the model, and
discuss the scientific-computing methodology used to solve the model equations.
The models and methodology presented in this paper form the basis for
developing and treating increasingly complex, mechanistic models of tumor
invasion that will be more predictive and less phenomenological. Because many
of the features of the cancer models, such as taxis, aging and growth, are seen
in other biological systems, the models and methods discussed here also provide
a template for handling a broader range of biological problems.

<id>
q-bio/0503025v2
<category>
q-bio.QM
<abstract>
Random forest is a classification algorithm well suited for microarray data:
it shows excellent performance even when most predictive variables are noise,
can be used when the number of variables is much larger than the number of
observations, and returns measures of variable importance. Thus, it is
important to understand the performance of random forest with microarray data
and its use for gene selection.
  We first show the effects of changes in parameters of random forest on the
prediction error. Then we present an approach for gene selection that uses
measures of variable importance and error rate, and is targeted towards the
selection of small sets of genes. Using simulated and real microarray data, we
show that the gene selection procedure yields small sets of genes while
preserving predictive accuracy.
  Availability: All code is available as an R package, varSelRF, from CRAN,
http://cran.r-project.org/src/contrib/PACKAGES.html, or from the supplementary
material page.
  Supplementary information:
http://ligarto.org/rdiaz/Papers/rfVS/randomForestVarSel.html

<id>
q-bio/0504007v1
<category>
q-bio.QM
<abstract>
We introduce a new methodology for the determination of amino-acid
substitution matrices for use in the alignment of proteins. The new methodology
is based on a pre-existing set cover on the set of residues and on the
undirected graph that describes residue exchangeability given the set cover.
For fixed functional forms indicating how to obtain edge weights from the set
cover and, after that, substitution-matrix elements from weighted distances on
the graph, the resulting substitution matrix can be checked for performance
against some known set of reference alignments and for given gap costs. Finding
the appropriate functional forms and gap costs can then be formulated as an
optimization problem that seeks to maximize the performance of the substitution
matrix on the reference alignment set. We give computational results on the
BAliBASE suite using a genetic algorithm for optimization. Our results indicate
that it is possible to obtain substitution matrices whose performance is either
comparable to or surpasses that of several others, depending on the particular
scenario under consideration.

<id>
q-bio/0504028v1
<category>
q-bio.QM
<abstract>
We introduce a model for describing the dynamics of large numbers of
interacting cells. The fundamental dynamical variables in the model are
sub-cellular elements, which interact with each other through phenomenological
intra- and inter-cellular potentials. Advantages of the model include i)
adaptive cell-shape dynamics, ii) flexible accommodation of additional
intra-cellular biology, and iii) the absence of an underlying grid. We present
here a detailed description of the model, and use successive mean-field
approximations to connect it to more coarse-grained approaches, such as
discrete cell-based algorithms and coupled partial differential equations. We
also discuss efficient algorithms for encoding the model, and give an example
of a simulation of an epithelial sheet. Given the biological flexibility of the
model, we propose that it can be used effectively for modeling a range of
multi-cellular processes, such as tumor dynamics and embryogenesis.

<id>
q-bio/0505027v2
<category>
q-bio.QM
<abstract>
A model of growth of icosahedral viral capsids is proposed. It takes into
account the diversity of hexamers' compositions, leading to definite capsid
size. We show that the observed yield of capsid production implies a very high
level of self-organization of elementary building blocks. The exact number of
different protein dimers composing hexamers is related to the size of a given
capsid, labeled by its T-number. Simple rules determining these numbers for
each value of T are deduced and certain consequences are discussed.

<id>
q-bio/0507009v1
<category>
q-bio.QM
<abstract>
A quantitative measure of stability in stochastic dynamics starts to emerge
in recent experiments on bioswitches. This quantity, similar to the potential
function in mathematics, is deeply rooted in biology, dated back at the
beginning of quantitative description of biological processes: the adaptive
landscape of Wright (1932) and the development landscape of Waddington (1940).
Nevertheless, its quantitative implication has been frequently challenged by
biologists. Recent progresses in quantitative biology begin to meet those
outstanding challenges.

<id>
q-bio/0508038v2
<category>
q-bio.QM
<abstract>
The presented previously indirect optimization method (IOM) developed within
biochemical systems theory (BST) provides a versatile and mathematically
tractable optimization strategy for biochemical systems. However, due to the
local approximations nature of the BST formalism, the iterative version of this
technique possibly does not yield the true optimum solution. In this work, an
algorithm is proposed to obtain the correct and consistent optimum steady-state
operating point of biochemical systems. The existing linear optimization
problem of the direct IOM approach is modified by adding an equality constraint
of describing the consistency of solutions between the S-system and the
original model. Lagrangian analysis is employed to derive the first order
necessary optimality conditions for the above modified optimization problem.
This leads to a procedure that may be regarded as a modified iterative IOM
approach in which the optimization objective function includes an extra linear
term. The extra term contains a comparison of metabolite concentration
derivatives with respect to the enzyme activities between the S-system and the
original model and ensures that the new algorithm is still carried out within
linear programming techniques. The presented framework is applied to several
biochemical systems and shown to the tractability and effectiveness of the
method. The simulation is also studied to investigate the convergence
properties of the algorithm and to give a performance comparison of standard
and modified iterative IOM approach.

<id>
q-bio/0510031v1
<category>
q-bio.QM
<abstract>
Ecological systems are governed by complex interactions which are mainly
nonlinear. In order to capture this complexity and nonlinearity, statistical
models recently gained popularity. However, although these models are commonly
applied in ecology, there are no studies to date aiming to assess the
applicability and performance. We provide an overview for nature of the wide
range of the data sets and predictive variables, from both aquatic and
terrestrial ecosystems with different scales of time-dependent dynamics, and
the applicability and robustness of predictive modeling methods on such data
sets by comparing different statistical modeling approaches. The methods
considered k-NN, LDA, QDA, generalized linear models (GLM) feedforward
multilayer backpropagation networks and pseudo-supervised network ARTMAP. For
ecosystems involving time-dependent dynamics and periodicities whose frequency
are possibly less than the time scale of the data considered, GLM and
connectionist neural network models appear to be most suitable and robust,
provided that a predictive variable reflecting these time-dependent dynamics
included in the model either implicitly or explicitly. For spatial data, which
does not include any time-dependence comparable to the time scale covered by
the data, on the other hand, neighborhood based methods such as k-NN and ARTMAP
proved to be more robust than other methods considered in this study. In
addition, for predictive modeling purposes, first a suitable, computationally
inexpensive method should be applied to the problem at hand a good predictive
performance of which would render the computational cost and efforts associated
with complex variants unnecessary.

<id>
q-bio/0510032v1
<category>
q-bio.QM
<abstract>
Support vector machines and kernel methods are increasingly popular in
genomics and computational biology, due to their good performance in real-world
applications and strong modularity that makes them suitable to a wide range of
problems, from the classification of tumors to the automatic annotation of
proteins. Their ability to work in high dimension, to process non-vectorial
data, and the natural framework they provide to integrate heterogeneous data
are particularly relevant to various problems arising in computational biology.
In this chapter we survey some of the most prominent applications published so
far, highlighting the particular developments in kernel methods triggered by
problems in biology, and mention a few promising research directions likely to
expand in the future.

<id>
q-bio/0511034v1
<category>
q-bio.QM
<abstract>
A central task in the study of molecular sequence data from present-day
species is the reconstruction of the ancestral relationships. The most
established approach to tree reconstruction is the maximum likelihood (ML)
method. In this method, evolution is described in terms of a discrete-state
continuous-time Markov process on a phylogenetic tree. The substitution rate
matrix, that determines the Markov process, can be estimated using the
expectation maximization (EM) algorithm. Unfortunately, an exhaustive search
for the ML phylogenetic tree is computationally prohibitive for large data
sets. In such situations, the neighbor-joining (NJ) method is frequently used
because of its computational speed. The NJ method reconstructs trees by
clustering neighboring sequences recursively, based on pairwise comparisons
between the sequences. The NJ method can be generalized such that
reconstruction is based on comparisons of subtrees rather than pairwise
distances. In this paper, we present an algorithm for simultaneous substitution
rate estimation and phylogenetic tree reconstruction. The algorithm iterates
between the EM algorithm for estimating substitution rates and the generalized
NJ method for tree reconstruction. Preliminary results of the approach are
encouraging.

<id>
q-bio/0511042v2
<category>
q-bio.QM
<abstract>
This technical report provides the supplementary material for a paper
entitled "Information based clustering", to appear shortly in Proceedings of
the National Academy of Sciences (USA). In Section I we present in detail the
iterative clustering algorithm used in our experiments and in Section II we
describe the validation scheme used to determine the statistical significance
of our results. Then in subsequent sections we provide all the experimental
results for three very different applications: the response of gene expression
in yeast to different forms of environmental stress, the dynamics of stock
prices in the Standard and Poor's 500, and viewer ratings of popular movies. In
particular, we highlight some of the results that seem to deserve special
attention. All the experimental results and relevant code, including a freely
available web application, can be found at
http://www.genomics.princeton.edu/biophysics-theory .

<id>
q-bio/0511043v1
<category>
q-bio.QM
<abstract>
In an age of increasingly large data sets, investigators in many different
disciplines have turned to clustering as a tool for data analysis and
exploration. Existing clustering methods, however, typically depend on several
nontrivial assumptions about the structure of data. Here we reformulate the
clustering problem from an information theoretic perspective which avoids many
of these assumptions. In particular, our formulation obviates the need for
defining a cluster "prototype", does not require an a priori similarity metric,
is invariant to changes in the representation of the data, and naturally
captures non-linear relations. We apply this approach to different domains and
find that it consistently produces clusters that are more coherent than those
extracted by existing algorithms. Finally, our approach provides a way of
clustering based on collective notions of similarity rather than the
traditional pairwise measures.

<id>
q-bio/0512023v1
<category>
q-bio.QM
<abstract>
A salient feature of stationary patterns in tip-growing cells is the key role
played by the symports and antiports, membrane proteins that translocate two
ionic species at the same time. It is shown that these co-transporters
destabilize generically the membrane voltage if the two translocated ions
diffuse differently and carry a charge of opposite (same) sign for symports
(antiports). Orders of magnitude obtained for the time and lengthscale are in
agreement with experiments. A weakly nonlinear analysis characterizes the
bifurcation.

<id>
q-bio/0312028v2
<category>
q-bio.SC
<abstract>
The recent discovery of a lateral organization in cell membranes due to small
structures called 'rafts' has motivated a lot of biological and
physico-chemical studies. A new experiment on a model system has shown a
spectacular budding process with the expulsion of one or two rafts when one
introduces proteins on the membrane. In this paper, we give a physical
interpretation of the budding of the raft phase. An approach based on the
energy of the system including the presence of proteins is used to derive a
shape equation and to study possible instabilities. This model shows two
different situations which are strongly dependent on the nature of the
proteins: a regime of easy budding when the proteins are strongly coupled to
the membrane and a regime of difficult budding.

<id>
q-bio/0402040v1
<category>
q-bio.SC
<abstract>
We show that at the onset of a cyclic fold bifurcation, a birhythmic medium
composed of glycolytic oscillators displays turbulent dynamics. By computing
the largest Lyapunov exponent, the spatial correlation function, and the
average transient lifetime, we classify it as a weak turbulence with transient
nature. Virtual heterogeneities generating unstable fast oscillations are the
mechanism of the transient turbulence. In the presence of wavenumber
instability, unstable oscillations can be reinjected leading to stationary
turbulence. We also find similar turbulence in a cell cycle model. These
findings suggest that weak turbulence may be universal in biochemical
birhythmic media exhibiting cyclic fold bifurcations.

<id>
q-bio/0405024v2
<category>
q-bio.SC
<abstract>
We focused our attention on Ca$^{2+}$ release from the endoplasmic reticulum
through a cluster of inositol 1,4,5-trisphosphate (IP$_3$) receptor channels.
The random opening and closing of these receptors introduce stochastic effects
that have been observed experimentally. Here, we present a stochastic version
of Othmer-Tang model for IP$_3$ receptor clusters. We address the average
behavior of the channels in response to IP$_3$ stimuli. We found, by stochastic
simulation, that the shape of the receptor response to IP$_3$ (fraction of open
channels versus [IP$_3$]), is affected by the cytosolic Ca$^{2+}$ level. We
also study several aspects of the stochastic properties of Ca${2+}$ release and
we compare with experimental observations.

<id>
q-bio/0407011v3
<category>
q-bio.SC
<abstract>
By observing reconstituted chromatin by fluorescence microscopy (FM) and
atomic force microscopy (AFM), we found that the density of nucleosomes
exhibits a bimodal profile, i.e., there is a large transition between the dense
and dispersed states in reconstituted chromatin. Based on an analysis of the
spatial distribution of nucleosome cores, we deduced an effective thermodynamic
potential as a function of the nucleosome-nucleosome distance. This enabled us
to interpret the folding transition of chromatin in terms of a first-order
phase transition. This mechanism for the condensation of chromatin is discussed
in terms of its biological significance.

<id>
q-bio/0409006v1
<category>
q-bio.SC
<abstract>
In E. coli, accurate cell division depends upon the oscillation of Min
proteins from pole to pole. We provide a model for the polar localization of
MinD based only on diffusion, a delay for nucleotide exchange, and different
rates of attachment to the bare membrane and the occupied membrane. We derive
analytically the probability density, and correspondingly the length scale, for
MinD attachment zones. Our simple analytical model illustrates the processes
giving rise to the observed localization of cellular MinD zones.

<id>
q-bio/0409007v1
<category>
q-bio.SC
<abstract>
The charges in live cells interact with or produce electric fields, which
results in enormous dielectric responses, flexoelectricity, and related
phenomena. Here we report on a contraction of schizosacchraoymces pombe
(fission yeast) cells induced by magnetic fields, as observed using a phase
sensitive projection image technique. Unlike electric fields, magnetic fields
only act on moving charges. The observed behavior is quite remarkable, and may
result from a contractile Lorentz force acting on diamagnetic screening
currents. This would indicate extremely high intracellular charge mobilities.
Besides, we observed a large electro - optical response from fission yeast
cells.

<id>
q-bio/0411038v1
<category>
q-bio.SC
<abstract>
A significant part of the thin layers of counter-ions adjacent to the
exterior and interior surfaces of a cell membrane form quasi-two-dimensional
(2D) layers of mobile charge. Collective charge density oscillations, known as
plasmon modes, in these 2D charged systems of counter-ions are predicted in the
present paper. This is based on a calculation of the self-consistent response
of this system to a fast electric field fluctuation. The possibility that the
membrane channels might be using these excitations to carry out fast
communication is suggested and experiments are proposed to reveal the existence
of such excitations.

<id>
q-bio/0412023v1
<category>
q-bio.SC
<abstract>
In E. coli the determination of the middle of the cell and the proper
placement of the septum is essential to the division of the cell. This step
depends on the proteins MinC, MinD, and MinE. Exposure to a constant external
field e.g., an electric field or magnetic field may cause the bacteria cell
division mechanism to change resulting in an abnormal cytokinesis. To have
insight into the effects of an external field on this process, we model the
process using a set of the deterministic reaction diffusion equations, which
incorporate the influence of an external field, min protein reactions, and
diffusion of all species. Using the numerical method, we have found some
changes in the dynamics of the oscillations of the min proteins from pole to
pole when compared that of without the external field. The results show some
interesting effects, which are qualitatively in good agreement with some
experimental results.

<id>
q-bio/0501024v1
<category>
q-bio.SC
<abstract>
Based on experimental observations it is known that various biological cells
exhibit a persistent random walk during migration on flat substrates. The
persistent random walk is characterized by `stop-and-go' movements :
unidirectional motions over distances of the order of several cell diameter are
separated by localized short time erratic movements. Using computer simulations
the reasons for this phenomena had been unveiled and shown to be attributed to
two antagonistic nucleation processes during the polymerization of the cell's
actin cytoskeleton : the (ordinary) spontaneous nucleation and the dendritic
nucleation processes. Whereas spontaneous nucleations generate actin filaments
growing in different directions and hence create motions in random directions,
dendritic nucleations provide a unidirectional growth. Since dendritic growth
exhibits stochastic fluctuations, spontaneous nucleation may eventually compete
or even dominate, which results in a reorientation of filament growth and hence
a new direction of cell motion. The event of reorientation takes place at
instants of vanishing polarity of the actin skeleton.

<id>
q-bio/0501029v1
<category>
q-bio.SC
<abstract>
Trajectories of on-off events are the output of many single molecule
experiments. Usually, one describes the underlying mechanism that generates the
trajectory using a kinetic scheme, and by analyzing the trajectory aims at
deducing this scheme. In a previous work [O. Flomenbom, J. Klafter, and A.
Szabo, submitted (2004)], we showed that when successive events along a
trajectory are uncorrelated, all the information in the trajectory is contained
in two basic functions, which are the waiting time probability functions (PDFs)
of the on state and of the off state. The kinetic schemes that lead to such
uncorrelated trajectories were termed reducible. Here we discuss the reasons
that lead to reducible schemes. In particular, the topology of reducible
schemes is characterized and proven.

<id>
q-bio/0501032v1
<category>
q-bio.SC
<abstract>
Motor proteins that specifically interact with the ends of cytoskeletal
filaments can induce filament depolymerization. A phenomenological description
of this process is presented. We show that under certain conditions motors
dynamically accumulate at the filament ends. We compare simulations of two
microscopic models to the phenomenological description. The depolymerization
rate can exhibit maxima and dynamic instabilities as a function of the bulk
motor density for processive depolymerization. We discuss our results in
relation to experimental studies of Kin-13 family motor proteins.

<id>
q-bio/0504008v1
<category>
q-bio.SC
<abstract>
L-selectin mediated tethers result in leukocyte rolling only above a
threshold in shear. Here we present biophysical modeling based on recently
published data from flow chamber experiments (Dwir et al., J. Cell Biol. 163:
649-659, 2003) which supports the interpretation that L-selectin mediated
tethers below the shear threshold correspond to single L-selectin carbohydrate
bonds dissociating on the time scale of milliseconds, whereas L-selectin
mediated tethers above the shear threshold are stabilized by multiple bonds and
fast rebinding of broken bonds, resulting in tether lifetimes on the timescale
of $10^{-1}$ seconds. Our calculations for cluster dissociation suggest that
the single molecule rebinding rate is of the order of $10^4$ Hz. A similar
estimate results if increased tether dissociation for tail-truncated L-selectin
mutants above the shear threshold is modeled as diffusive escape of single
receptors from the rebinding region due to increased mobility. Using computer
simulations, we show that our model yields first order dissociation kinetics
and exponential dependence of tether dissociation rates on shear stress. Our
results suggest that multiple contacts, cytoskeletal anchorage of L-selectin
and local rebinding of ligand play important roles in L-selectin tether
stabilization and progression of tethers into persistent rolling on endothelial
surfaces.

<id>
q-bio/0504023v1
<category>
q-bio.SC
<abstract>
We address the controversial hot question concerning the validity of the
loose coupling versus the lever-arm theories in the actomyosin dynamics by
re-interpreting and extending the phenomenological washboard potential model
proposed by some of us in a previous paper. In this new model a Brownian motion
harnessing thermal energy is assumed to co-exist with the deterministic swing
of the lever-arm, to yield an excellent fit of the set of data obtained by some
of us on the sliding of Myosin II heads on immobilized actin filaments under
various load conditions. Our theoretical arguments are complemented by accurate
numerical simulations, and the robustness of the model is tested via different
choices of parameters and potential profiles.

<id>
q-bio/0510050v1
<category>
q-bio.SC
<abstract>
Motivated by the formation of ring-like filament structures in the cortex of
plant and animal cells, we study the dynamics of a two-dimensional layer of
cytoskeletal filaments and motor proteins near a surface by a general continuum
theory. As a result of active processes, dynamic patterns of filament
orientation and density emerge via instabilities. We show that
self-organization phenomena can lead to the formation of stationary and
oscillating rings. We present state diagrams which reveal a rich scenario of
asymptotic behaviors and discuss the role of boundary conditions.

<id>
q-bio/0511008v1
<category>
q-bio.SC
<abstract>
The conditions of the chromosomes inside the nucleus in the Rabl
configuration have been modelled as self-avoiding polymer chains under
restraining conditions. To ensure that the chromosomes remain stretched out and
lined up, we fixed their end points to two opposing walls. The numbers of
segments $N$, the distances $d_1$ and $d_2$ between the fixpoints, and the
wall-to-wall distance $z$ (as measured in segment lengths) determine an
approximate value for the Kuhn segment length $k_l$. We have simulated the
movement of the chromosomes using molecular dynamics to obtain the expected
distance distribution between the genetic loci in the absence of further
attractive or repulsive forces. A comparison to biological experiments on
\textit{Drosophila Melanogaster} yields information on the parameters for our
model. With the correct parameters it is possible to draw conclusions on the
strength and range of the attraction that leads to pairing.

<id>
q-bio/0512027v2
<category>
q-bio.SC
<abstract>
During the eukaryotic cell cycle, chromatin undergoes several conformational
changes, which are believed to play key roles in gene expression regulation
during interphase, and in genome replication and division during mitosis. In
this paper, we propose a scenario for chromatin structural reorganization
during mitosis, which bridges all the different scales involved in chromatin
architecture, from nucleosomes to chromatin loops. We build a model for
chromatin, based on available data, taking into account both physical and
topological constraints DNA has to deal with. Our results suggest that the
mitotic chromosome condensation/decondensation process is induced by a
structural change at the level of the nucleosome itself.

<id>
q-bio/0601013v1
<category>
q-bio.SC
<abstract>
The influence of intrinsic channel noise on the spontaneous spiking activity
of poisoned excitable membrane patches is studied by use of a stochastic
generalization of the Hodgkin-Huxley model. Internal noise stemming from the
stochastic dynamics of individual ion channels is known to affect the
collective properties of the whole ion channel cluster. For example, there
exists an optimal size of the membrane patch for which the internal noise alone
causes a regular spontaneous generation of action potentials. In addition to
varying the size of ion channel clusters, living organisms may adapt the
densities of ion channels in order to optimally regulate the spontaneous
spiking activity. The influence of channel block on the excitability of a
membrane patch of certain size is twofold: First, a variation of ion channel
densities primarily yields a change of the conductance level. Second, a
down-regulation of working ion channels always increases the channel noise.
While the former effect dominates in the case of sodium channel block resulting
in a reduced spiking activity, the latter enhances the generation of
spontaneous action potentials in the case of a tailored potassium channel
blocking. Moreover, by blocking some portion of either potassium or sodium ion
channels, it is possible to either increase or to decrease the regularity of
the spike train.

<id>
q-bio/0601022v1
<category>
q-bio.SC
<abstract>
Morphogens are proteins, often produced in a localised region, whose
concentrations spatially demarcate regions of differing gene expression in
developing embryos. The boundaries of expression must be set accurately and in
proportion to the size of the one-dimensional developing field; this cannot be
accomplished by a single gradient. Here, we show how a pair of morphogens
produced at opposite ends of a developing field can solve the pattern-scaling
problem. In the most promising scenario, the morphogens effectively interact
according to the annihilation reaction $A+B\to\emptyset$ and the switch occurs
according to the absolute concentration of $A$ or $B$. In this case embryonic
markers across the entire developing field scale approximately with system
size; this cannot be achieved with a pair of non-interacting gradients that
combinatorially regulate downstream genes. This scaling occurs in a window of
developing-field sizes centred at a few times the morphogen decay length.

<id>
q-bio/0603019v1
<category>
q-bio.SC
<abstract>
There is a correspondence between the circulation of blood in all higher
animals and the circulation of sap in all higher plants - up to heights h of
140 m - through the xylem and phloem vessels. Plants suck in water from the
soil, osmotically through the roothair zone, and subsequently lift it
osmotically again, and by capillary suction (via their buds, leaves, and
fruits) into their crowns. In between happens a reverse osmosis - the
endodermis jump - realized by two layers of subcellular mechanical pumps in the
endodermis walls which are powered by ATP, or in addition by two analogous
layers of such pumps in the exodermis. The thus established root pressure helps
forcing the absorbed ground water upward, through the whole plant, and often
out again, in the form of guttation, or exudation.

<id>
q-bio/0605017v3
<category>
q-bio.SC
<abstract>
Using a theoretical model for spontaneous partial DNA unwrapping from
histones, we study the transient exposure of protein-binding DNA sites within
nucleosomes. We focus on the functional dependence of the rates for site
exposure and reburial on the site position, which is measurable experimentally
and pertinent to gene regulation. We find the dependence to be roughly
described by a random walker model. Close inspection reveals a surprising
physical effect of flexibility-assisted barrier crossing, which we characterize
within a toy model, the "semiflexible Brownian rotor."

<id>
q-bio/0608030v3
<category>
q-bio.SC
<abstract>
In this paper we use a simple toy model to explore the function of the gene
Osteosarcoma-9. We are in particular interested in understanding the role of
this gene as a potent anti-apoptotic factor. The theoretical description is
constrained by experimental data from induction of apoptosis in cells where
OS-9 is overexpressed. The data available suggest that OS-9 promotes cell
viability and confers resistance to apoptosis, potentially implicating OS-9 in
the survival of cancer cells. Three different apoptosis inducing mechanisms
were tested and are here modelled. More complex and realistic models are also
discussed.

<id>
q-bio/0611085v1
<category>
q-bio.SC
<abstract>
The distribution of inclusion-rich domains in membranes with active two-state
inclusions is studied by simulations. Our study shows that typical size of
inclusion-rich domains ($L$) can be controlled by inclusion activities in
several ways. When there is effective attraction between state-1 inclusions, we
find: (i) Small domains with only several inclusions are observed for
inclusions with time scales ($\sim 10^{-3} {\rm s}$) and interaction energy
[$\sim \mathcal{O}({\rm k_BT})$] comparable to motor proteins. (ii) $L$ scales
as 1/3 power of the lifetime of state-1 for a wide range of parameters. (iii)
$L$ shows a switch-like dependence on state-2 lifetime $k_{12}^{-1}$. That is,
$L$ depends weakly on $k_{12}$ when $k_{12} < k_{12}^*$ but increases rapidly
with $k_{12}$ when $k_{12} > k_{12}^*$, the crossover $k_{12}^*$ occurs when
the diffusion length of a typical state-2 inclusion within its lifetime is
comparable to $L$. (iv) Inclusion-curvature coupling provides another length
scale that competes with the effects of transition rates.

<id>
q-bio/0702021v2
<category>
q-bio.SC
<abstract>
It is shown that the dual to the linear programming problem that arises in
constraint-based models of metabolism can be given a thermodynamic
interpretation in which the shadow prices are chemical potential analogues, and
the objective is to minimise free energy consumption given a free energy drain
corresponding to growth. The interpretation is distinct from conventional
non-equilibrium thermodynamics, although it does satisfy a minimum entropy
production principle. It can be used to motivate extensions of constraint-based
modelling, for example to microbial ecosystems.

<id>
q-bio/0703008v2
<category>
q-bio.SC
<abstract>
Viral infection requires the binding of receptors on the target cell membrane
to glycoproteins, or ``spikes,'' on the viral membrane. The initial entry is
usually classified as fusogenic or endocytotic. However, binding of viral
spikes to cell surface receptors not only initiates the viral adhesion and the
wrapping process necessary for internalization, but can simultaneously initiate
direct fusion with the cell membrane. Both fusion and internalization have been
observed to be viable pathways for many viruses. We develop a stochastic model
for viral entry that incorporates a competition between receptor mediated
fusion and endocytosis. The relative probabilities of fusion and endocytosis of
a virus particle initially nonspecifically adsorbed on the host cell membrane
are computed as functions of receptor concentration, binding strength, and
number of spikes. We find different parameter regimes where the entry pathway
probabilities can be analytically expressed. Experimental tests of our
mechanistic hypotheses are proposed and discussed.

<id>
q-bio/0703055v1
<category>
q-bio.SC
<abstract>
I point out the similarity between the microtubule experiment reported by
Priel et al [Biophys. J. 90, 4639 (2006)] and the ZnO nanowire experiment of
Wang et al [Nanolett. 6, 2768 (2006)]. It is quite possible that MTs are
similar to a piezoelectric field effect transistor for which the role of the
control gate electrode is played by the piezo-induced electric field across the
width of the MT walls and their elastic bending features

<id>
0801.1392v1
<category>
q-bio.SC
<abstract>
Mechanochemical coupling was studied for two different types of myosin motors
in cells: myosin V, which carries cargo over long distances by as a single
molecule; and myosin II, which generates a contracting force in cooperation
with other myosin II molecules. Both mean and variance of myosin V velocity at
various [ATP] obeyed Michaelis-Menten mechanics, consistent with tight
mechanochemical coupling. Myosin II, working in an ensemble, however, was
explained by a loose coupling mechanism, generating variable step sizes
depending on the ATP concentration and realizing a much larger step (200 nm)
per ATP hydrolysis than myosin V through its cooperative nature at zero load.
These different mechanics are ideal for the respective myosin's physiological
functions.

<id>
0801.2730v1
<category>
q-bio.SC
<abstract>
In mammals, dosage compensation of X linked genes in female cells is achieved
by inactivation of one of their two X chromosomes which is randomly chosen. The
earliest steps in X-inactivation (XCI), namely the mechanism whereby cells
count their X chromosomes and choose between two equivalent X, remain
mysterious. Starting from the recent discovery of X chromosome colocalization
at the onset of X-inactivation, we propose a Statistical Mechanics model of
XCI, which is investigated by computer simulations and checked against
experimental data. Our model describes how a `blocking factor' complex is
self-assembled and why only one is formed out of many diffusible molecules,
resulting in a spontaneous symmetry breaking (SB) in the binding to two
identical chromosomes. These results are used to derive a scenario of
biological implications describing all current experimental evidences, e.g.,
the importance of colocalization.

<id>
0807.0499v1
<category>
q-bio.SC
<abstract>
Intracellular transport is based on molecular motors that pull cargos along
cytoskeletal filaments. One motor species always moves in one direction, e.g.
conventional kinesin moves to the microtubule plus end, while cytoplasmic
dynein moves to the microtubule minus end. However, many cellular cargos are
observed to move bidirectionally, involving both plus-end and minus-end
directed motors. The presumably simplest mechanism for such bidirectional
transport is provided by a tug-of-war between the two motor species. This
mechanism is studied theoretically using the load-dependent transport
properties of individual motors as measured in single-molecule experiments. In
contrast to previous expectations, such a tug-of-war is found to be highly
cooperative and to exhibit seven different motility regimes depending on the
precise values of the single motor parameters. The sensitivity of the transport
process to small parameter changes can be used by the cell to regulate its
cargo traffic.

<id>
0809.0773v3
<category>
q-bio.SC
<abstract>
While ordinary differential equations (ODEs) form the conceptual framework
for modelling many cellular processes, specific situations demand stochastic
models to capture the influence of noise. The most common formulation of
stochastic models for biochemical networks is the chemical master equation
(CME). While stochastic simulations are a practical way to realise the CME,
analytical approximations offer more insight into the influence of noise.
Towards that end, the two-moment approximation (2MA) is a promising addition to
the established analytical approaches including the chemical Langevin equation
(CLE) and the related linear noise approximation (LNA). The 2MA approach
directly tracks the mean and (co)variance which are coupled in general. This
coupling is not obvious in CME and CLE and ignored by LNA and conventional ODE
models. We extend previous derivations of 2MA by allowing a) non-elementary
reactions and b) relative concentrations. Often, several elementary reactions
are approximated by a single step. Furthermore, practical situations often
require the use relative concentrations. We investigate the applicability of
the 2MA approach to the well established fission yeast cell cycle model. Our
analytical model reproduces the clustering of cycle times observed in
experiments. This is explained through multiple resettings of MPF, caused by
the coupling between mean and (co)variance, near the G2/M transition.

<id>
0809.4788v1
<category>
q-bio.SC
<abstract>
A general model for the early recognition and colocalization of homologous
DNA sequences is proposed. We show, on a thermodynamic ground, how the distance
between two homologous DNA sequences is spontaneously regulated by the
concentration and affinity of diffusible mediators binding them, which act as a
switch between two phases corresponding to independence or colocalization of
pairing regions.

<id>
q-bio/0312041v1
<category>
q-bio.TO
<abstract>
We propose a travelling-wave perturbation method to control the
spatiotemporal dynamics in a cardiac model. It is numerically demonstrated that
the method can successfully suppress the wave instability (alternans in action
potential duration) in the one-dimensional case and convert spiral waves and
turbulent states to the normal travelling wave states in the two-dimensional
case. An experimental scheme is suggested which may provide a new design for a
cardiac defibrillator.

<id>
q-bio/0402025v1
<category>
q-bio.TO
<abstract>
The purpose of research was to check up the influence of decrease of
nonequality of ventilating (after bronchodilator (berotec) inhalation (BI)) on
the magnitude of dynamic compliance of lungs (Cdyn) at asthma patients with
ventilating infringements. Methods and materials: 20 patients (with 2 and 3
degrees of ventilating infringements (VC<73%, FEV1<51%, MVV<56%), without
restrictive disease of lungs, suffering from bronchial asthma were studied
before and after BI by plotting volume, rate flow, against the transpulmonare
pressure. About the change of nonequality of ventilating we consider by the
change after BI of Cdyn, Cdyn at once after flow interruption (Cdyn1), tissue
resistance at inhalation (Rti in) and exhalation (Rti ex), parameters of
ventilating and general parameters of respiratory mechanics. Results: the
parameters of ventilating were improved (P < 0,05). General parameters of
respiratory mechanics also improved. Rti in and Rti ex are made 0,48+0,16;
1,05+0,25 kPa/l/s before BI and decreased 0,09+0,04; 0,28+0,09 kPa/l/s after BI
(P < 0,05; P < 0,05). But Cdyn and Cdyn1 are not changed after BI. Conclusions:
1. The decrease of ventilation nonequality and tissue friction after BI do not
influence on the initially reduced dynamic compliance of lungs at asthma
patients without any restrictive diseases of lungs. 2. The cause of not
increasing of dynamic compliance after BI probably due by changes in elastic
component of parenchyma of lungs, insensitive to berotec.

<id>
q-bio/0404034v1
<category>
q-bio.TO
<abstract>
When the body is infected, it mounts an acute inflammatory response to rid
itself of the pathogens and restore health. Uncontrolled acute inflammation due
to infection is defined clinically as Sepsis and can culminate in organ failure
and death. We consider a three dimensional ordinary differential equation model
of inflammation consisting of a pathogen, and two inflammatory mediators. The
model reproduces the healthy outcome and diverse negative outcomes, depending
on initial conditions and parameters.when key parameters are changed and
suggest various therapeutic strategies. We suggest that the clinical condition
of sepsis can arise from several distinct physiological states, each of which
requires a different treatment approach. We analyze the various bifurcations
between the different outcomes

<id>
q-bio/0406001v1
<category>
q-bio.TO
<abstract>
A growth of malignant neoplasm is considered as a fractional transport
approach. We suggested that the main process of the tumor development through a
lymphatic net is fractional transport of cells. In the framework of this
fractional kinetics we were able to show that the mean size of main growth is
due to subdiffusion, while the appearance of metaphases is determined by
superdiffusion.

<id>
q-bio/0409002v1
<category>
q-bio.TO
<abstract>
We develop some techniques to prove analytically the existence and stability
of long period oscillations of stem cell populations in the case of periodic
chronic myelogenous leukemia. Such a periodic oscillation $p_\infty $ can be
analytically constructed when the hill coefficient involved in the nonlinear
feedback is infinite, and we show it is possible to obtain a contractive
returning map (for the semiflow defined by the modeling functional differential
equation) in a closed and convex cone containing $p_\infty $ when the hill
coefficient is large, and the fixed point of such a contractive map gives the
long period oscillation previously observed both numerically and
experimentally.

<id>
q-bio/0409003v1
<category>
q-bio.TO
<abstract>
In this paper we use a continuous model to describe the development of a
single cell lineage following the committal of stem cells. Three separate
controls are implemented in the model, namely the proliferative control of stem
cells, the proliferative control of developing blast cells, and the peripheral
control of stem cell committal by circulating blood cell density. We show that
variation of parameters in all three control systems can cause oscillations,
and that the characters of these oscillations are very different. This allows
us some potential insight into the mechanisms that may be operative in some of
the dynamic blood diseases like cyclical neutropenia and periodic chronic
myelogenous leukemia.

<id>
q-bio/0409039v1
<category>
q-bio.TO
<abstract>
We demonstrate the robust scale-invariance in the probability density
function (PDF) of detrended healthy human heart rate increments, which is
preserved not only in a quiescent condition, but also in a dynamic state where
the mean level of heart rate is dramatically changing. This scale-independent
and fractal structure is markedly different from the scale-dependent PDF
evolution observed in a turbulent-like, cascade heart rate model. These results
strongly support the view that healthy human heart rate is controlled to
converge continually to a critical state.

<id>
q-bio/0410034v1
<category>
q-bio.TO
<abstract>
This work introduces an in vivo marker in mice from the altitude of the well,
determined by this cancer.The duration of development and the altitude of the
cancer well are obtained from the synchronization of the mechanics and the
kinetics of the processes of normal and anomalous transport under tumor mass
growth.The mechanics and the kinetics of these two transports are found from
the cancer fugacity.The cancer fugacity is defined as a photograph of the rate
of the scaling exponent under tumor mass growth.

<id>
q-bio/0412040v1
<category>
q-bio.TO
<abstract>
A macroscopic model of the tumor Gompertzian growth is proposed. This
approach is based on the energetic balance among the different cell activities,
described by methods of statistical mechanics and related to the growth
inhibitor factors. The model is successfully applied to the multicellular tumor
spheroid data.

<id>
q-bio/0502004v1
<category>
q-bio.TO
<abstract>
Most human carcinomas exhibit telomere abnormalities early in the
carcinogenesis process suggesting that crisis caused by telomere shortening may
be a necessary event leading to human carcinomas. Epidemiological records of
the age at which each patient in a population develops carcinoma are known as
age-incidence data; these provide a quantitative measure of human tumor
initiation and dynamics. If crisis brought on by telomere shortening is
necessary for most human carcinomas, it may also be the rate limiting step. To
test this, we compared a mathematical model in which telomere loss is the rate
limiting step during carcinogenesis with age-incidence data compiled by the
Surveillance, Epidemiology and End Results (SEER) program. We found that this
model adequately explains the age-incidence data. The model also implies that
two distinct paths exist for carcinoma to develop in prostate, breast, and
ovary tissues. We conclude that a single step, crisis brought on by telomere
shortening, limits the rate of formation of human carcinomas.

<id>
q-bio/0502038v1
<category>
q-bio.TO
<abstract>
Eyespots are concentric motifs with contrasting colours on butterfly wings.
Eyespots have intra- and inter-specific visual signalling functions with
adaptive and selective roles. We propose a reaction-diffusion model that
accounts for eyespot development. The model considers two diffusive morphogens
and three non-diffusive pigment precursors. The first morphogen is produced in
the focus and determines the differentiation of the first eyespot ring. A
second morphogen is then produced, modifying the chromatic properties of the
wing background pigment precursor, inducing the differentiation of a second
ring. The model simulates the general structural organisation of eyespots,
their phenotypic plasticity and seasonal variability, and predicts effects from
microsurgical manipulations on pupal wings as reported in the literature.

<id>
q-bio/0503021v2
<category>
q-bio.TO
<abstract>
We make comments on the paper by Miller [{\it J. Theor. Biol.} {\bf 234}
(2005) 511].

<id>
q-bio/0503033v1
<category>
q-bio.TO
<abstract>
Colour parameters of European beech were measured using CIELab system. 103
logs from 87 trees in 9 sites were cut into boards to study the variations of
wood colour parameters. Both site and tree effect on colour were observed.
Patterns of red heartwood occurrence were defined. When excepting red heartwood
there was still a highly significant effect of site and tree; differences
remained after veneer processing. Axial variations were small, except very near
the pith or in red heartwood, suggesting possible early selection at periphery
under colour criteria. Red heartwood is darker, redder and more yellow than
normal peripheral wood.

<id>
q-bio/0506023v2
<category>
q-bio.TO
<abstract>
Remodelling is defined as an evolution of microstructure or variations in the
configuration of the underlying manifold. The manner in which a biological
tissue and its subsystems remodel their structure is treated in a continuum
mechanical setting. While some examples of remodelling are conveniently
modelled as evolution of the reference configuration (Case I), others are more
suited to an internal variable description (Case II). In this paper we explore
the applicability of stationary energy states to remodelled systems. A
variational treatment is introduced by assuming that stationary energy states
are attained by changes in microstructure via one of the two mechanisms--Cases
I and II. An example is presented to illustrate each case. The example
illustrating Case II is further studied in the context of the thermodynamic
dissipation inequality.

<id>
q-bio/0507042v1
<category>
q-bio.TO
<abstract>
The impulse response function (IRF) of a localized bolus in cerebral blood
flow codes important information on the tissue type. It is indirectly
accessible both from MR- and CT-imaging methods, at least in principle. In
practice, however, noise and limited signal resolution render standard
deconvolution techniques almost useless. Parametric signal descriptions look
more promising, and it is the aim of this contribution to develop some
improvements along this line.

<id>
q-bio/0507043v1
<category>
q-bio.TO
<abstract>
A theoretical model based on the molecular interactions between a growing
tumor and a dynamically evolving blood vessel network describes the
transformation of the regular vasculature in normal tissues into a highly
inhomogeneous tumor specific capillary network. The emerging morphology,
characterized by the compartmentalization of the tumor into several regions
differing in vessel density, diameter and necrosis, is in accordance with
experimental data for human melanoma. Vessel collapse due to a combination of
severely reduced blood flow and solid stress exerted by the tumor, leads to a
correlated percolation process that is driven towards criticality by the
mechanism of hydrodynamic vessel stabilization.

<id>
q-bio/0510044v1
<category>
q-bio.TO
<abstract>
The RR series extracted from human electrocardiogram signal (ECG) is
considered as a fractal stochastic process. The manifestation of long-range
dependencies is the presence of power laws in scale dependent process
characteristics. Exponents of these laws: $\beta$ - describing power spectrum
decay, $\alpha$ - responsible for decay of detrended fluctuations or $H$
related to, so-called, roughness of a signal, are known to differentiate hearts
of healthy people from hearts with congestive heart failure. There is a strong
expectation that resolution spectrum of exponents, so-called, local exponents
in place of global exponents allows to study differences between hearts in
details. The arguments are given that local exponents obtained in multifractal
analysis by the two methods: wavelet transform modulus maxima (WTMM) and
multifractal detrended fluctuation analysis (MDFA), allow to recognize the
following four stages of the heart: healthy and young, healthy and advance in
years, subjects with left ventricle systolic dysfunction (NYHA I--III class)
and characterized by severe congestive heart failure (NYHA III-IV class).

<id>
q-bio/0601021v1
<category>
q-bio.TO
<abstract>
We argue that volumetric growth dynamics of a solid cancer depend on the
tumor system's overall surface extension. While this at first may seem evident,
to our knowledge, so far no theoretical argument has been presented explaining
this relationship explicitly. In here, we therefore develop a conceptual
framework based on the universal scaling law and then support our conjecture
through evaluation with experimental data.

<id>
q-bio/0603016v1
<category>
q-bio.TO
<abstract>
Experiments that discuss influence of noise to H. Seidel and H. Herzel
dynamics model of human cardiovascular system are presented. Noise is
introduced by considering stochastic delays in response to the sympathetic
system. It appears that in the presence of the noise 10 s heart rate
oscillations connected with Mayer waves are preserved. Moreover the heart rate
becomes approximately normally distributed (even in unstable phase of original
Seidel Herzel model), similarly like the real RR intervals data.

<id>
q-bio/0604008v1
<category>
q-bio.TO
<abstract>
We model the interaction between the immune system and tumor cells including
a time delay to simulate the time needed by the latter to develop a chemical
and cell mediated response to the presence of the tumor. The results are
compared with those of a previous paper, concluding that the delay introduces
new instabilities in the system leading to an uncontrolable growth of the
tumour. Then a cytokine based periodic immunotherapy treatment is included in
the model and the effects of its dossage are studied for the case of a weak
immune system and a growing tumour. We find the existence of metastable states
(that may last for tens of years) induced by the treatment, and also of
potentially adverse effects of the dossage frequency on the stabilization of
the tumour. These two effects depend on the delay, the cytokine dose burden and
other parameters considered in the model.

<id>
q-bio/0605035v1
<category>
q-bio.TO
<abstract>
The differential Adhesion Hypothesis (DAH) is a theory of the organization of
cells within a tissue. In this study we introduce a stochastic model supporting
the DAH, that can be seen as a continuous version of a discrete model of Graner
and Glazier. Our approach is based on the mathematical framework of Gibbsian
marked point processes. We provide a Markov chain Monte Carlo algorithm that
can reproduce classical biological patterns, and we propose an estimation
procedure for a parameter that quantifies the strength of adhesion between
cells. This procedure is tested through simulations.

<id>
q-bio/0607019v1
<category>
q-bio.TO
<abstract>
We investigate, both experimentally and theoretically, the bifurcation to
alternans in heart tissue. Previously, this phenomenon has been modeled either
as a smooth or as border-collision period-doubling bifurcation. Using a new
experimental technique, we find a hybrid behavior: very close to the
bifurcation point the dynamics are smooth-like, whereas further away they are
border-collision-like. This behavior is captured by a new type of model, called
an unfolded border-collision bifurcation.

<id>
q-bio/0607047v2
<category>
q-bio.TO
<abstract>
We describe an asymptotic approach to gated ionic models of single-cell
cardiac excitability. It has a form essentially different from the Tikhonov
fast-slow form assumed in standard asymptotic reductions of excitable systems.
This is of interest since the standard approaches have been previously found
inadequate to describe phenomena such as the dissipation of cardiac wave fronts
and the shape of action potential at repolarization. The proposed asymptotic
description overcomes these deficiencies by allowing, among other non-Tikhonov
features, that a dynamical variable may change its character from fast to slow
within a single solution. The general asymptotic approach is best demonstrated
on an example which should be both simple and generic. The classical model of
Purkinje fibers (Noble, 1962) has the simplest functional form of all cardiac
models but according to the current understanding it assigns a physiologically
incorrect role to the Na current. This leads us to suggest an ``Archetypal
Model'' with the simplicity of the Noble model but with a structure more
typical to contemporary cardiac models. We demonstrate that the Archetypal
Model admits a complete asymptotic solution in quadratures. To validate our
asymptotic approach, we proceed to consider an exactly solvable ``caricature''
of the Archetypal Model and demonstrate that the asymptotic of its exact
solution coincides with the solutions obtained by substituting the
``caricature'' right-hand sides into the asymptotic solution of the generic
Archetypal Model. This is necessary, because, unlike in standard asymptotic
descriptions, no general results exist which can guarantee the proximity of the
non-Tikhonov asymptotic solutions to the solutions of the corresponding
detailed ionic model.

<id>
q-bio/0610027v2
<category>
q-bio.TO
<abstract>
Assuming that there is feedback between an expanding cancer system and its
organ-typical microenvironment, we argue here that such local tumor growth is
guided by co-existence rather than competition with the surrounding tissue. We
then present a novel concept that understands cancer dissemination as a
biological mechanism to evade the specific carrying capacity limit of its host
organ. This conceptual framework allows us to relate the tumor system's
volumetric growth rate to the host organ's functionality-conveying composite
infrastructure, and, intriguingly, already provides useful insights into
several clinical findings.

<id>
q-bio/0611048v1
<category>
q-bio.TO
<abstract>
Muscles crossing a joint usually outnumber its degrees of freedom, which
renders the motor system underdetermined. Typically, optimization laws are
postulated to cope with this redundancy. A natural question then arises whether
all muscular load sharing patterns can be regarded as results of optimization.
To answer it, we propose a method of constructing an objective function whose
minimization yields a given load sharing pattern. We give necessary conditions
for this construction to be feasible and investigate its uniqueness. For linear
load sharing patterns the Crowninshield and Brand objective function is
reproduced, nonlinear ones require a more general formula.

<id>
q-bio/0701003v4
<category>
q-bio.TO
<abstract>
In this paper we address some modelling issues related to biological growth.
Our treatment is based on a recently-proposed, general formulation for growth
within the context of Mixture Theory (Journal of the Mechanics and Physics of
Solids, 52, 2004, 1595--1625). We aim to enhance this treatment by making it
more appropriate for the biophysics of growth in porous soft tissue,
specifically tendon. This involves several modifications to the mathematical
formulation to represent the reactions, transport and mechanics, and their
interactions. We also reformulate the governing differential equations for
reaction-transport to represent the incompressibility constraint on the fluid
phase of the tissue. This revision enables a straightforward implementation of
numerical stabilisation for the hyperbolic, or advection-dominated, limit. A
finite element implementation employing an operator splitting scheme is used to
solve the coupled, non-linear partial differential equations that arise from
the theory. Motivated by our experimental model, an in vitro scaffold-free
engineered tendon formed by self-assembly of tendon fibroblasts (Tissue
Engineering, 10, 2004, 755--761), we solve several numerical examples
demonstrating biophysical aspects of tissue growth, and the improved numerical
performance of the models.

<id>
q-bio/0703015v1
<category>
q-bio.TO
<abstract>
Employing a novel two-dimensional computational model we have simulated the
feedback between angiogenesis and tumor growth dynamics. Analyzing vessel
formation and elongation towards the concentration gradient of the
tumor-derived angiogenetic basic fibroblast growth factor, bFGF, we assumed
that prior to the blood vessels reaching the tumor surface, the resulting
pattern of tumor growth is symmetric, circular with a common center point.
However, after the vessels reach the tumor surface, we assumed that the growth
rate of that particular cancer region is accelerated compared to the tumor
surface section that lacks neo-vascularization. Therefore, the resulting
asymmetric tumor growth pattern is biased towards the site of the nourishing
vessels. The simulation results show over time an increase in vessel density, a
decrease in vessel branching length, and an increase in fracticality of the
vascular branching architecture. Interestingly, over time the fractal dimension
displayed a sigmoidal pattern with a reduced rate increase at earlier and later
tumor growth stages due to distinct characteristics in vessel length and
density. The finding that, at later stages, higher vascular fracticality
resulted in a marked increase of tumor slice volume provides further in silico
evidence for a functional impact of vascular patterns on cancer growth.

<id>
0712.1970v1
<category>
q-bio.TO
<abstract>
Recently, much attention has been given to a noteworthy property of some soft
tissues: their ability to grow. Many attempts have been made to model this
behaviour in biology, chemistry and physics. Using the theory of finite
elasticity, Rodriguez has postulated a multiplicative decomposition of the
geometric deformation gradient into a growth-induced part and an elastic one
needed to ensure compatibility of the body. In order to fully explore the
consequences of this hypothesis, the equations describing thin elastic objects
under finite growth are derived. Under appropriate scaling assumptions for the
growth rates, the proposed model is of the Foppl-von Karman type. As an
illustration, the circumferential growth of a free hyperelastic disk is
studied.

<id>
0712.4001v1
<category>
q-bio.TO
<abstract>
We consider a system of differential equations the behavior of which
solutions possesses several properties characteristic of the blood pressure
distribution. The system can be used for a compartmental modeling of the
cardiovascular system. It admits a unique bounded solution such that all
coordinates of the solution are separated from zero by positive numbers, and
which is periodic, eventually periodic or almost periodic depending on the
moments of heart contraction. Appropriate numerical simulations are provided.

<id>
0801.0654v2
<category>
q-bio.TO
<abstract>
Hot spots in tumors are regions of high vascular density in the center of the
tumor and their analysis is an important diagnostic tool in cancer treatment.
We present a model for vascular remodeling in tumors predicting that the
formation of hot spots correlates with local inhomogeneities of the original
arterio-venous vasculature of the healthy tissue. Probable locations for hot
spots in the late stages of the tumor are locations of increased blood pressure
gradients. The developing tumor vasculature is non-hierarchical but still
complex displaying algebraically decaying density distributions.

<id>
cs/9810011v1
<category>
cs.AR
<abstract>
As the one-chip integration of HW-modules designed by different companies
becomes more and more popular reliability of a HW-design and evaluation of the
timing behavior during the prototype stage are absolutely necessary. One way to
guarantee reliability is the use of robust design styles, e.g.,
delay-insensitivity. For early timing evaluation two aspects must be
considered: a) The timing needs to be proportional to technology variations and
b) the implemented architecture should be identical for prototype and target.
The first can be met also by delay-insensitive implementation. The latter one
is the key point. A unified architecture is needed for prototyping as well as
implementation. Our new approach to rapid prototyping of signal processing
tasks is based on a configurable, delay-insensitive implemented processor
called Flysig. In essence, the Flysig processor can be understood as a complex
FPGA where the CLBs are substituted by bit-serial operators. In this paper the
general concept is detailed and first experimental results are given for
demonstration of the main advantages: delay-insensitive design style, direct
correspondence between prototyping and target architecture, high performance
and reasonable shortening of the design cycle.

<id>
cs/0111029v1
<category>
cs.AR
<abstract>
Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson
National Accelerator Facility (Jefferson Lab) with versatile VME-based data
acquisition and control interfaces with minimal development times. FPGA designs
have been used to interface to VME and provide control logic for numerous
systems. The building blocks of these logic designs can be tailored to the
individual needs of each system and provide system operators with read-backs
and controls via a VME interface to an EPICS based computer. This versatility
allows the system developer to choose components and define operating
parameters and options that are not readily available commercially. Jefferson
Lab has begun developing standard FPGA libraries that result in quick turn
around times and inexpensive designs.

<id>
cs/0111030v1
<category>
cs.AR
<abstract>
A Dual Digital Signal Processing VME Board was developed for the Continuous
Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at
Jefferson Lab. It is a versatile general-purpose digital signal processing
board using an open architecture, which allows for adaptation to various
applications. The base design uses two independent Texas Instrument (TI)
TMS320C6711, which are 900 MFLOPS floating-point digital signal processors
(DSP). Applications that require a fixed point DSP can be implemented by
replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The
design can be manufactured with a reduced chip set without redesigning the
printed circuit board. For example it can be implemented as a single-channel
DSP with no analog I/O.

<id>
cs/0111032v1
<category>
cs.AR
<abstract>
This poster describes the timing system being designed for Spallation Neutron
Source being built at Oak Ridge National lab.

<id>
cs/0207012v1
<category>
cs.AR
<abstract>
This paper introduces a novel method for synthesizing digital circuits
derived from Binary Decision Diagrams (BDDs) that can yield to reduction in
power dissipation. The power reduction is achieved by decreasing the switching
activity in a circuit while paying close attention to information measures as
an optimization criterion. We first present the technique of efficient
BDD-based computation of information measures which are used to guide the power
optimization procedures. Using this technique, we have developed an algorithm
of BDD reordering which leads to reducing the power consumption of the circuits
derived from BDDs. Results produced by the synthesis on the ISCAS benchmark
circuits are very encouraging.

<id>
cs/0207014v1
<category>
cs.AR
<abstract>
This paper addresses a new approach to find a spectrum of information
measures for the process of digital circuit synthesis. We consider the problem
from the information engine point of view. The circuit synthesis as a whole and
different steps of the design process (an example of decision diagram is given)
are presented via such measurements as entropy, logical work and information
vitality. We also introduce new information measures to provide better
estimates of synthesis criteria. We show that the basic properties of
information engine, such as the conservation law of information flow and the
equilibrium law of information can be formulated.

<id>
cs/0405015v1
<category>
cs.AR
<abstract>
Reconfigurable computing refers to the use of processors, such as Field
Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to
take on different processing tasks. A reconfigurable computing platform
describes the hardware and software base on top of which modular extensions can
be created, depending on the desired application. Such reconfigurable computing
platforms can take on varied designs and implementations, according to the
constraints imposed and features desired by the scope of applications. This
paper introduces a PC-based reconfigurable computing platform software
frameworks that is flexible and extensible enough to abstract the different
hardware types and functionality that different PCs may have. The requirements
of the software platform, architectural issues addressed, rationale behind the
decisions made, and frameworks design implemented are discussed.

<id>
cs/0407019v2
<category>
cs.AR
<abstract>
A standard approach to building a fuzzy controller based on stochastic logic
uses binary random signals with an average (expected value of a random
variable) in the range [0, 1]. A different approach is presented, founded on a
representation of the membership functions with the probability density
functions.

<id>
cs/0407032v1
<category>
cs.AR
<abstract>
Many reconfigurable platforms require that applications be written
specifically to take advantage of the reconfigurable hardware. In a PC-based
environment, this presents an undesirable constraint in that the many already
available applications cannot leverage on such hardware. Greatest benefit can
only be derived from reconfigurable devices if even native OS applications can
transparently utilize reconfigurable devices as they would normal full-fledged
hardware devices. This paper presents how Proteus Virtual Devices are used to
expose reconfigurable hardware in a transparent manner for use by typical
native OS applications.

<id>
cs/0409025v1
<category>
cs.AR
<abstract>
In the paper we define and characterize the asynchronous systems from the
point of view of their autonomy, determinism, order, non-anticipation, time
invariance, symmetry, stability and other important properties. The study is
inspired by the models of the asynchronous circuits.

<id>
cs/0412040v1
<category>
cs.AR
<abstract>
This paper presents a data stationary architecture in which each word has an
attached address field. Address fields massively update in parallel to record
data interchanges. Words do not move until memory is read for post processing.
A sea of such cells can test large-scale quantum algorithms, although other
programming is possible.

<id>
cs/0503066v1
<category>
cs.AR
<abstract>
Management of communication by on-line routing in new FPGAs with a large
amount of logic resources and partial reconfigurability is a new challenging
problem. A Network-on-Chip
  (NoC) typically uses packet routing mechanism, which has often unsafe data
transfers, and network interface overhead. In this paper, circuit routing for
such dynamic NoCs is investigated, and a practical 1-dimensional network with
an efficient routing algorithm is proposed and implemented. Also, this concept
has been extended to the 2-dimensional case. The implementation results show
the low area overhead and high performance of this network.

<id>
cs/0508038v1
<category>
cs.AR
<abstract>
Wiring diagrams are given for a quantum algorithm processor in CMOS to
compute, in parallel, all divisors of an n-bit integer. Lines required in a
wiring diagram are proportional to n. Execution time is proportional to the
square of n.

<id>
cs/0510039v1
<category>
cs.AR
<abstract>
A new paradigm to support the communication among modules dynamically placed
on a reconfigurable device at run-time is presented. Based on the network on
chip (NoC) infrastructure, we developed a dynamic communication infrastructure
as well as routing methodologies capable to handle routing in a NoC with
obstacles created by dynamically placed components. We prove the unrestricted
reachability of components and pins, the deadlock-freeness and we finally show
the feasibility of our approach by means on real life example applications.

<id>
cs/0602096v1
<category>
cs.AR
<abstract>
This paper reviews various engineering hurdles facing the field of quantum
computing. Specifically, problems related to decoherence, state preparation,
error correction, and implementability of gates are considered.

<id>
cs/0603088v1
<category>
cs.AR
<abstract>
IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. This
paper proposes two novel BCD adders called carry skip and carry look-ahead BCD
adders respectively. Furthermore, in the recent years, reversible logic has
emerged as a promising technology having its applications in low power CMOS,
quantum computing, nanotechnology, and optical computing. It is not possible to
realize quantum computing without reversible logic. Thus, this paper also paper
provides the reversible logic implementation of the conventional BCD adder as
the well as the proposed Carry Skip BCD adder using a recently proposed TSG
gate. Furthermore, a new reversible gate called TS-3 is also being proposed and
it has been shown that the proposed reversible logic implementation of the BCD
Adders is much better compared to recently proposed one, in terms of number of
reversible gates used and garbage outputs produced. The reversible BCD circuits
designed and proposed here form the basis of the decimal ALU of a primitive
quantum CPU.

<id>
cs/0603091v1
<category>
cs.AR
<abstract>
In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. This paper proposes a new 4 * 4 reversible gate called TSG
gate. The proposed gate is used to design efficient adder units. The most
significant aspect of the proposed gate is that it can work singly as a
reversible full adder i.e reversible full adder can now be implemented with a
single gate only. The proposed gate is then used to design reversible ripple
carry and carry skip adders. It is demonstrated that the adder architectures
designed using the proposed gate are much better and optimized, compared to
their existing counterparts in literature; in terms of number of reversible
gates and garbage outputs. Thus, this paper provides the initial threshold to
building of more complex system which can execute more complicated operations
using reversible logic.

<id>
cs/0603092v1
<category>
cs.AR
<abstract>
In recent years, reversible logic has emerged as a promising computing
paradigm having its applications in low power computing, quantum computing,
nanotechnology, optical computing and DNA computing. The classical set of gates
such as AND, OR, and EXOR are not reversible. Recently, it has been shown how
to encode information in DNA and use DNA amplification to implement Fredkin
gates. Furthermore, in the past Fredkin gates have been constructed using DNA,
whose outputs are used as inputs for other Fredkin gates. Thus, it can be
concluded that arbitrary circuits of Fredkin gates can be constructed using
DNA. This paper provides the initial threshold to building of more complex
system having reversible sequential circuits and which can execute more
complicated operations. The novelty of the paper is the reversible designs of
sequential circuits using Fredkin gate. Since, Fredkin gate has already been
realized using DNA, it is expected that this work will initiate the building of
complex systems using DNA. The reversible circuits designed here are highly
optimized in terms of number of gates and garbage outputs. The modularization
approach that is synthesizing small circuits and thereafter using them to
construct bigger circuits is used for designing the optimal reversible
sequential circuits.

<id>
cs/0605004v1
<category>
cs.AR
<abstract>
In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. Recently a 4 * 4 reversible gate called TSG is proposed. The
most significant aspect of the proposed gate is that it can work singly as a
reversible full adder, that is reversible full adder can now be implemented
with a single gate only. This paper proposes a NXN reversible multiplier using
TSG gate. It is based on two concepts. The partial products can be generated in
parallel with a delay of d using Fredkin gates and thereafter the addition can
be reduced to log2N steps by using reversible parallel adder designed from TSG
gates. Similar multiplier architecture in conventional arithmetic (using
conventional logic) has been reported in existing literature, but the proposed
one in this paper is totally based on reversible logic and reversible cells as
its building block. A 4x4 architecture of the proposed reversible multiplier is
also designed. It is demonstrated that the proposed multiplier architecture
using the TSG gate is much better and optimized, compared to its existing
counterparts in literature; in terms of number of reversible gates and garbage
outputs. Thus, this paper provides the initial threshold to building of more
complex system which can execute more complicated operations using reversible
logic.

<id>
cs/0605125v1
<category>
cs.AR
<abstract>
We detail a procedure for the computation of the polynomial form of an
electronic combinational circuit from the design equations in a truth table.
The method uses the Buchberger algorithm rather than current traditional
methods based on search algorithms. We restrict the analysis to a single
output, but the procedure can be generalized to multiple outputs. The procedure
is illustrated with the design of a simple arithmetic and logic unit with two
3-bit operands and two control bits.

<id>
cs/0605142v1
<category>
cs.AR
<abstract>
The systems supporting signal and image applications process large amount of
data. That involves an intensive use of the memory which becomes the bottleneck
of systems. Memory limits performances and represents a significant proportion
of total consumption. In the development high level synthesis tool called GAUT
Low Power, we are interested in the synthesis of the memory unit. In this work,
we integrate the data storage and data transfert to constraint the high level
synthesis of the datapath's execution unit.

<id>
cs/0605143v1
<category>
cs.AR
<abstract>
The design of complex Systems-on-Chips implies to take into account
communication and memory access constraints for the integration of dedicated
hardware accelerator. In this paper, we present a methodology and a tool that
allow the High-Level Synthesis of DSP algorithm, under both I/O timing and
memory constraints. Based on formal models and a generic architecture, this
tool helps the designer to find a reasonable trade-off between both the
required I/O timing behavior and the internal memory access parallelism of the
circuit. The interest of our approach is demonstrated on the case study of a
FFT algorithm.

<id>
cs/0605144v1
<category>
cs.AR
<abstract>
We introduce a new approach to take into account the memory architecture and
the memory mapping in High- Level Synthesis for data intensive applications. We
formalize the memory mapping as a set of constraints for the synthesis, and
defined a Memory Constraint Graph and an accessibility criterion to be used in
the scheduling step. We use a memory mapping file to include those memory
constraints in our HLS tool GAUT. It is possible, with the help of GAUT, to
explore a wide range of solutions, and to reach a good tradeoff between time,
power-consumption, and area.

<id>
cs/0605145v1
<category>
cs.AR
<abstract>
We introduce a new approach to take into account the memory architecture and
the memory mapping in the High- Level Synthesis of Real-Time embedded systems.
We formalize the memory mapping as a set of constraints used in the scheduling
step. We use a memory mapping file to include those memory constraints in our
HLS tool GAUT. Our scheduling algorithm exhibits a relatively low complexity
that permits to tackle complex designs in a reasonable time. Finally, we show
how to explore, with the help of GAUT, a wide range of solutions, and to reach
a good tradeoff between time, power-consumption, and area.

<id>
cs/0605146v1
<category>
cs.AR
<abstract>
The design of complex Digital Signal Processing systems implies to minimize
architectural cost and to maximize timing performances while taking into
account communication and memory accesses constraints for the integration of
dedicated hardware accelerator. Unfortunately, the traditional Matlab/ Simulink
design flows gather not very flexible hardware blocs. In this paper, we present
a methodology and a tool that permit the High-Level Synthesis of DSP
applications, under both I/O timing and memory constraints. Based on formal
models and a generic architecture, our tool GAUT helps the designer in finding
a reasonable trade-off between the circuit's performance and its architectural
complexity. The efficiency of our approach is demonstrated on the case study of
a FFT algorithm.

<id>
cs/0608075v1
<category>
cs.AR
<abstract>
Media-processing applications, such as signal processing, 2D and 3D graphics
rendering, and image compression, are the dominant workloads in many embedded
systems today. The real-time constraints of those media applications have
taxing demands on today's processor performances with low cost, low power and
reduced design delay. To satisfy those challenges, a fast and efficient
strategy consists in upgrading a low cost general purpose processor core. This
approach is based on the personalization of a general RISC processor core
according the target multimedia application requirements. Thus, if the extra
cost is justified, the general purpose processor GPP core can be enforced with
instruction level coprocessors, coarse grain dedicated hardware, ad hoc
memories or new GPP cores. In this way the final design solution is tailored to
the application requirements. The proposed approach is based on three main
steps: the first one is the analysis of the targeted application using
efficient metrics. The second step is the selection of the appropriate
architecture template according to the first step results and recommendations.
The third step is the architecture generation. This approach is experimented
using various image and video algorithms showing its feasibility.

<id>
cs/0609023v1
<category>
cs.AR
<abstract>
In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. This paper utilizes a new 4 * 4 reversible
gate called TSG gate to build the components of a primitive reversible/quantum
ALU. The most significant aspect of the TSG gate is that it can work singly as
a reversible full adder, that is reversible full adder can now be implemented
with a single gate only. A Novel reversible 4:2 compressor is also designed
from the TSG gate which is later used to design a novel 8x8 reversible Wallace
tree multiplier. It is proved that the adder, 4:2 compressor and multiplier
architectures designed using the TSG gate are better than their counterparts
available in literature, in terms of number of reversible gates and garbage
outputs. This is perhaps, the first attempt to design a reversible 4:2
compressor and a reversible Wallace tree multiplier as far as existing
literature and our knowledge is concerned. Thus, this paper provides an initial
threshold to build more complex systems which can execute complicated
operations using reversible logic.

<id>
cs/0609028v1
<category>
cs.AR
<abstract>
This paper proposes the hardware implementation of RSA encryption/decryption
algorithm using the algorithms of Ancient Indian Vedic Mathematics that have
been modified to improve performance. The recently proposed hierarchical
overlay multiplier architecture is used in the RSA circuitry for multiplication
operation. The most significant aspect of the paper is the development of a
division architecture based on Straight Division algorithm of Ancient Indian
Vedic Mathematics and embedding it in RSA encryption/decryption circuitry for
improved efficiency. The coding is done in Verilog HDL and the FPGA synthesis
is done using Xilinx Spartan library. The results show that RSA circuitry
implemented using Vedic division and multiplication is efficient in terms of
area/speed compared to its implementation using conventional multiplication and
division architectures

<id>
cs/0609029v1
<category>
cs.AR
<abstract>
In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. In this paper, the contributors have proposed
reversible programmable logic array (RPLA) architecture using reversible
Fredkin and Feynman gates. The proposed RPLA has n inputs and m outputs and can
realize m functions of n variables. In order to demonstrate the design of RPLA,
a 3 input RPLA is designed which can perform any 28 functions using the
combination of 8 min terms (23). Furthermore, the application of the designed 3
input RPLA is shown by implementing the full adder and full subtractor
functions through it.

<id>
cs/0609036v1
<category>
cs.AR
<abstract>
IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. Firstly,
this paper proposes novel two transistor AND and OR gates. The proposed AND
gate has no power supply, thus it can be referred as the Powerless AND gate.
Similarly, the proposed two transistor OR gate has no ground and can be
referred as Groundless OR. Secondly for IEEE 754r format, two novel BCD adders
called carry skip and carry look-ahead BCD adders are also proposed in this
paper. In order to design the carry look-ahead BCD adder, a novel 4 bit carry
look-ahead adder called NCLA is proposed which forms the basic building block
of the proposed carry look-ahead BCD adder. Finally, the proposed two
transistors AND and OR gates are used to provide the optimized small area low
power high throughput circuitries of the proposed BCD adders.

<id>
cs/9308101v1
<category>
cs.AI
<abstract>
Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches.

<id>
cs/9308102v1
<category>
cs.AI
<abstract>
Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms.

<id>
cs/9309101v1
<category>
cs.AI
<abstract>
We describe an extensive study of search in GSAT, an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search: rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems,
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase, the
average gradient of this phase, and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms.

<id>
cs/9311101v1
<category>
cs.AI
<abstract>
As real logic programmers normally use cut (!), an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning, clauses containing cut cannot be
learned using an extensional evaluation method, as is done in most learning
systems. On the other hand, searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples, and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused, in general, by the need for intensional evaluation. As a conclusion,
the analysis of this paper suggests, on precise and technical grounds, that
learning cut is difficult, and current induction techniques should probably be
restricted to purely declarative logic languages.

<id>
cs/9311102v1
<category>
cs.AI
<abstract>
To support the goal of allowing users to record and retrieve information,
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First, it actively predicts what the user is
going to write. Second, it automatically constructs a custom, button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix: People like to record information. Doing this
on paper is initially efficient, but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre, the user records information directly on a computer. Behind the
interface, an agent acts for the user. To help, it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime.

<id>
cs/9312101v1
<category>
cs.AI
<abstract>
Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied, often required in practical applications, can be summarized
in three main points. First, we consider a highly expressive terminological
language, called ALCNR, including general complements of concepts, number
restrictions and role conjunction. Second, we allow to express inclusion
statements between general concepts, and terminological cycles as a particular
case. Third, we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability, subsumption and instance checking) through a
sound, complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof, we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles, if descriptive semantics is
adopted.

<id>
cs/9401101v1
<category>
cs.AI
<abstract>
A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback, T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition, T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots.

<id>
cs/9402101v1
<category>
cs.AI
<abstract>
Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms.

<id>
cs/9402102v1
<category>
cs.AI
<abstract>
The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data, multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar, but not identical, instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix: This is a compressed tar file containing the SUBDUE discovery
system, written in C. The program accepts as input databases represented in
graph form, and will output discovered substructures with their corresponding
value.

<id>
cs/9402103v1
<category>
cs.AI
<abstract>
The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here, called
PTR, uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.

<id>
cs/9403101v1
<category>
cs.AI
<abstract>
We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees.

<id>
cs/9406101v1
<category>
cs.AI
<abstract>
This paper analyzes the correctness of the subsumption algorithm used in
CLASSIC, a description logic-based knowledge representation system that is
being used in practical applications. In order to deal efficiently with
individuals in CLASSIC descriptions, the developers have had to use an
algorithm that is incomplete with respect to the standard, model-theoretic
semantics for description logics. We provide a variant semantics for
descriptions with respect to which the current implementation is complete, and
which can be independently motivated. The soundness and completeness of the
polynomial-time subsumption algorithm is established using description graphs,
which are an abstracted version of the implementation structures used in
CLASSIC, and are of independent interest.

<id>
cs/9406102v1
<category>
cs.AI
<abstract>
In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time, without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far.

<id>
cs/9408101v1
<category>
cs.AI
<abstract>
Given a knowledge base KB containing first-order and statistical facts, we
consider a principled method, called the random-worlds method, for computing a
degree of belief that some formula Phi holds given KB. If we are reasoning
about a world or system consisting of N individuals, then we can consider all
possible worlds, or first-order models, with domain {1,...,N} that satisfy KB,
and compute the fraction of them in which Phi is true. We define the degree of
belief to be the asymptotic value of this fraction as N grows large. We show
that when the vocabulary underlying Phi and KB uses constants and unary
predicates only, we can naturally associate an entropy with each world. As N
grows larger, there are many more worlds with higher entropy. Therefore, we can
use a maximum-entropy computation to compute the degree of belief. This result
is in a similar spirit to previous work in physics and artificial intelligence,
but is far more general. Of equal interest to the result itself are the
limitations on its scope. Most importantly, the restriction to unary predicates
seems necessary. Although the random-worlds method makes sense in general, the
connection to maximum entropy seems to disappear in the non-unary case. These
observations suggest unexpected limitations to the applicability of
maximum-entropy methods.

<id>
cs/9408102v1
<category>
cs.AI
<abstract>
Information extraction is the task of automatically picking up information of
interest from an unconstrained text. Information of interest is usually
extracted in two steps. First, sentence level processing locates relevant
pieces of information scattered throughout the text; second, discourse
processing merges coreferential information to generate the output. In the
first step, pieces of information are locally identified without recognizing
any relationships among them. A key word search or simple pattern search can
achieve this purpose. The second step requires deeper knowledge in order to
understand relationships among separately identified pieces of information.
Previous information extraction systems focused on the first step, partly
because they were not required to link up each piece of information with other
pieces. To link the extracted pieces of information and map them onto a
structured output format, complex discourse processing is essential. This paper
reports on a Japanese information extraction system that merges information
using a pattern matcher and discourse processor. Evaluation results show a high
level of system performance which approaches human performance.

<id>
cs/9408103v1
<category>
cs.AI
<abstract>
This article describes a new system for induction of oblique decision trees.
This system, OC1, combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric, although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies, using both real and artificial data, that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees.

<id>
cs/9409101v1
<category>
cs.AI
<abstract>
This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems, a plan can be presented and verified in a reasonable time. However,
coming up algorithmically with a plan, even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes, and show that, in most natural cases, the verification (projection)
part can be carried out in an efficient algorithmic manner.

<id>
cs/9412101v1
<category>
cs.AI
<abstract>
The vast amounts of on-line text now available have led to renewed interest
in information extraction (IE) systems that analyze unrestricted text,
producing a structured representation of selected information from the text.
This paper presents a novel approach that uses machine learning to acquire
knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE
discourse component that makes intersentential inferences and identifies
logical relations among information extracted from the text. Previous
corpus-based approaches were limited to lower level processing such as
part-of-speech tagging, lexical disambiguation, and dictionary construction.
Wrap-Up is fully trainable, and not only automatically decides what classifiers
are needed, but even derives the feature set for each classifier automatically.
Performance equals that of a partially trainable discourse module requiring
manual customization for each domain.

<id>
cs/9412102v1
<category>
cs.AI
<abstract>
This paper is a multidisciplinary review of empirical, statistical learning
from a graphical model perspective. Well-known examples of graphical models
include Bayesian networks, directed graphs representing a Markov chain, and
undirected networks representing a Markov field. These graphical models are
extended to model data analysis and empirical learning using the notation of
plates. Graphical operations for simplifying and manipulating a problem are
provided including decomposition, differentiation, and the manipulation of
probability models from the exponential family. Two standard algorithm schemas
for learning are reviewed in a graphical framework: Gibbs sampling and the
expectation maximization algorithm. Using these operations and schemas, some
popular algorithms can be synthesized from their graphical specification. This
includes versions of linear regression, techniques for feed-forward networks,
and learning Gaussian and discrete Bayesian networks from data. The paper
concludes by sketching some implications for data analysis and summarizing how
some popular algorithms fall within the framework presented. The main original
contributions here are the decomposition techniques and the demonstration that
graphical models provide a framework for understanding and developing complex
learning algorithms.

<id>
cs/9412103v1
<category>
cs.AI
<abstract>
For many years, the intuitions underlying partial-order planning were largely
taken for granted. Only in the past few years has there been renewed interest
in the fundamental principles underlying this paradigm. In this paper, we
present a rigorous comparative analysis of partial-order and total-order
planning by focusing on two specific planners that can be directly compared. We
show that there are some subtle assumptions that underly the wide-spread
intuitions regarding the supposed efficiency of partial-order planning. For
instance, the superiority of partial-order planning can depend critically upon
the search strategy and the structure of the search space. Understanding the
underlying assumptions is crucial for constructing efficient planners.

<id>
cs/9501101v1
<category>
cs.AI
<abstract>
Multiclass learning problems involve finding a definition for an unknown
function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k
``classes''). The definition is acquired by studying collections of training
examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning
problems include direct application of multiclass algorithms such as the
decision-tree algorithms C4.5 and CART, application of binary concept learning
algorithms to learn individual binary functions for each of the k classes, and
application of binary concept learning algorithms with distributed output
representations. This paper compares these three approaches to a new technique
in which error-correcting codes are employed as a distributed output
representation. We show that these output representations improve the
generalization performance of both C4.5 and backpropagation on a wide range of
multiclass learning tasks. We also demonstrate that this approach is robust
with respect to changes in the size of the training sample, the assignment of
distributed representations to particular classes, and the application of
overfitting avoidance techniques such as decision-tree pruning. Finally, we
show that---like the other methods---the error-correcting code technique can
provide reliable class probability estimates. Taken together, these results
demonstrate that error-correcting output codes provide a general-purpose method
for improving the performance of inductive learning programs on multiclass
problems.

<id>
cs/9501102v1
<category>
cs.AI
<abstract>
The paradigms of transformational planning, case-based planning, and plan
debugging all involve a process known as plan adaptation - modifying or
repairing an old plan so it solves a new problem. In this paper we provide a
domain-independent algorithm for plan adaptation, demonstrate that it is sound,
complete, and systematic, and compare it to other adaptation algorithms in the
literature. Our approach is based on a view of planning as searching a graph of
partial plans. Generative planning starts at the graph's root and moves from
node to node using plan-refinement operators. In planning by adaptation, a
library plan - an arbitrary node in the plan graph - is the starting point for
the search, and the plan-adaptation algorithm can apply both the same
refinement operators available to a generative planner and can also retract
constraints and steps from the plan. Our algorithm's completeness ensures that
the adaptation algorithm will eventually search the entire graph and its
systematicity ensures that it will do so without redundantly searching any
parts of the graph.

<id>
cs/9501103v1
<category>
cs.AI
<abstract>
Temporal difference (TD) methods constitute a class of methods for learning
predictions in multi-step prediction problems, parameterized by a recency
factor lambda. Currently the most important application of these methods is to
temporal credit assignment in reinforcement learning. Well known reinforcement
learning algorithms, such as AHC or Q-learning, may be viewed as instances of
TD learning. This paper examines the issues of the efficient and general
implementation of TD(lambda) for arbitrary lambda, for use with reinforcement
learning algorithms optimizing the discounted sum of rewards. The traditional
approach, based on eligibility traces, is argued to suffer from both
inefficiency and lack of generality. The TTD (Truncated Temporal Differences)
procedure is proposed as an alternative, that indeed only approximates
TD(lambda), but requires very little computation per action and can be used
with arbitrary function representation methods. The idea from which it is
derived is fairly simple and not new, but probably unexplored so far.
Encouraging experimental results are presented, suggesting that using lambda
&gt 0 with the TTD procedure allows one to obtain a significant learning
speedup at essentially the same cost as usual TD(0) learning.

<id>
cs/9503102v1
<category>
cs.AI
<abstract>
This paper introduces ICET, a new algorithm for cost-sensitive
classification. ICET uses a genetic algorithm to evolve a population of biases
for a decision tree induction algorithm. The fitness function of the genetic
algorithm is the average cost of classification when using the decision tree,
including both the costs of tests (features, measurements) and the costs of
classification errors. ICET is compared here with three other algorithms for
cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,
which classifies without regard to cost. The five algorithms are evaluated
empirically on five real-world medical datasets. Three sets of experiments are
performed. The first set examines the baseline performance of the five
algorithms on the five datasets and establishes that ICET performs
significantly better than its competitors. The second set tests the robustness
of ICET under a variety of conditions and shows that ICET maintains its
advantage. The third set looks at ICET's search in bias space and discovers a
way to improve the search.

<id>
cs/9504101v1
<category>
cs.AI
<abstract>
Theory revision integrates inductive learning and background knowledge by
combining training examples with a coarse domain theory to produce a more
accurate theory. There are two challenges that theory revision and other
theory-guided systems face. First, a representation language appropriate for
the initial theory may be inappropriate for an improved theory. While the
original representation may concisely express the initial theory, a more
accurate theory forced to use that same representation may be bulky,
cumbersome, and difficult to reach. Second, a theory structure suitable for a
coarse domain theory may be insufficient for a fine-tuned theory. Systems that
produce only small, local changes to a theory have limited value for
accomplishing complex structural alterations that may be required.
Consequently, advanced theory-guided learning systems require flexible
representation and flexible structure. An analysis of various theory revision
systems and theory-guided learning systems reveals specific strengths and
weaknesses in terms of these two desired properties. Designed to capture the
underlying qualities of each system, a new system uses theory-guided
constructive induction. Experiments in three domains show improvement over
previous theory-guided systems. This leads to a study of the behavior,
limitations, and potential of theory-guided constructive induction.

<id>
cs/9505101v1
<category>
cs.AI
<abstract>
Many studies have been carried out in order to increase the search efficiency
of constraint satisfaction problems; among them, some make use of structural
properties of the constraint network; others take into account semantic
properties of the constraints, generally assuming that all the constraints
possess the given property. In this paper, we propose a new decomposition
method benefiting from both semantic properties of functional constraints (not
bijective constraints) and structural properties of the network; furthermore,
not all the constraints need to be functional. We show that under some
conditions, the existence of solutions can be guaranteed. We first characterize
a particular subset of the variables, which we name a root set. We then
introduce pivot consistency, a new local consistency which is a weak form of
path consistency and can be achieved in O(n^2d^2) complexity (instead of
O(n^3d^3) for path consistency), and we present associated properties; in
particular, we show that any consistent instantiation of the root set can be
linearly extended to a solution, which leads to the presentation of the
aforementioned new method for solving by decomposing functional CSPs.

<id>
cs/9505102v1
<category>
cs.AI
<abstract>
We study the process of multi-agent reinforcement learning in the context of
load balancing in a distributed system, without use of either central
coordination or explicit communication. We first define a precise framework in
which to study adaptive load balancing, important features of which are its
stochastic nature and the purely local information available to individual
agents. Given this framework, we show illuminating results on the interplay
between basic adaptive behavior parameters and their effect on system
efficiency. We then investigate the properties of adaptive load balancing in
heterogeneous populations, and address the issue of exploration vs.
exploitation in that context. Finally, we show that naive use of communication
may not improve, and might even harm system efficiency.

<id>
cs/9505103v1
<category>
cs.AI
<abstract>
Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory.

<id>
cs/9505104v1
<category>
cs.AI
<abstract>
We present algorithms that learn certain classes of function-free recursive
logic programs in polynomial time from equivalence queries. In particular, we
show that a single k-ary recursive constant-depth determinate clause is
learnable. Two-clause programs consisting of one learnable recursive clause and
one constant-depth determinate non-recursive clause are also learnable, if an
additional ``basecase'' oracle is assumed. These results immediately imply the
pac-learnability of these classes. Although these classes of learnable
recursive programs are very constrained, it is shown in a companion paper that
they are maximally general, in that generalizing either class in any natural
way leads to a computationally difficult learning problem. Thus, taken together
with its companion paper, this paper establishes a boundary of efficient
learnability for recursive logic programs.

<id>
cs/9505105v1
<category>
cs.AI
<abstract>
In a companion paper it was shown that the class of constant-depth
determinate k-ary recursive clauses is efficiently learnable. In this paper we
present negative results showing that any natural generalization of this class
is hard to learn in Valiant's model of pac-learnability. In particular, we show
that the following program classes are cryptographically hard to learn:
programs with an unbounded number of constant-depth linear recursive clauses;
programs with one constant-depth determinate clause containing an unbounded
number of recursive calls; and programs with one linear recursive clause of
constant locality. These results immediately imply the non-learnability of any
more general class of programs. We also show that learning a constant-depth
determinate program with either two linear recursive clauses or one linear
recursive clause and one non-recursive clause is as hard as learning boolean
DNF. Together with positive results from the companion paper, these negative
results establish a boundary of efficient learnability for recursive
function-free clauses.

<id>
cs/9809020v1
<category>
cs.CL
<abstract>
We present a new method for discovering a segmental discourse structure of a
document while categorizing segment function. We demonstrate how retrieval of
noun phrases and pronominal forms, along with a zero-sum weighting scheme,
determines topicalized segmentation. Futhermore, we use term distribution to
aid in identifying the role that the segment performs in the document. Finally,
we present results of evaluation in terms of precision and recall which surpass
earlier approaches.

<id>
cs/9809022v1
<category>
cs.CL
<abstract>
We outline how utterances in dialogs can be interpreted using a partial first
order logic. We exploit the capability of this logic to talk about the truth
status of formulae to define a notion of coherence between utterances and
explain how this coherence relation can serve for the construction of AND/OR
trees that represent the segmentation of the dialog. In a BDI model we
formalize basic assumptions about dialog and cooperative behaviour of
participants. These assumptions provide a basis for inferring speech acts from
coherence relations between utterances and attitudes of dialog participants.
Speech acts prove to be useful for determining dialog segments defined on the
notion of completing expectations of dialog participants. Finally, we sketch
how explicit segmentation signalled by cue phrases and performatives is covered
by our dialog model.

<id>
cs/9809024v2
<category>
cs.CL
<abstract>
This document describes a sizable grammar of English written in the TAG
formalism and implemented for use with the XTAG system. This report and the
grammar described herein supersedes the TAG grammar described in an earlier
1995 XTAG technical report. The English grammar described in this report is
based on the TAG formalism which has been extended to include lexicalization,
and unification-based feature structures. The range of syntactic phenomena that
can be handled is large and includes auxiliaries (including inversion), copula,
raising and small clause constructions, topicalization, relative clauses,
infinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO
constructions, noun-noun modifications, extraposition, determiner sequences,
genitives, negation, noun-verb contractions, sentential adjuncts and
imperatives. This technical report corresponds to the XTAG Release 8/31/98. The
XTAG grammar is continuously updated with the addition of new analyses and
modification of old ones, and an online version of this report can be found at
the XTAG web page at http://www.cis.upenn.edu/~xtag/

<id>
cs/9809026v1
<category>
cs.CL
<abstract>
Language models for speech recognition typically use a probability model of
the form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other
hand, are typically used to assign structure to utterances. A language model of
the above form is constructed from such grammars by computing the prefix
probability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all
possible terminations of the prefix a_1 ... a_n. The main result in this paper
is an algorithm to compute such prefix probabilities given a stochastic Tree
Adjoining Grammar (TAG). The algorithm achieves the required computation in
O(n^6) time. The probability of subderivations that do not derive any words in
the prefix, but contribute structurally to its derivation, are precomputed to
achieve termination. This algorithm enables existing corpus-based estimation
techniques for stochastic TAGs to be used for language modelling.

<id>
cs/9809027v1
<category>
cs.CL
<abstract>
Much of the power of probabilistic methods in modelling language comes from
their ability to compare several derivations for the same string in the
language. An important starting point for the study of such cross-derivational
properties is the notion of _consistency_. The probability model defined by a
probabilistic grammar is said to be _consistent_ if the probabilities assigned
to all the strings in the language sum to one. From the literature on
probabilistic context-free grammars (CFGs), we know precisely the conditions
which ensure that consistency is true for a given CFG. This paper derives the
conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can
be shown to be consistent. It gives a simple algorithm for checking consistency
and gives the formal justification for its correctness. The conditions derived
here can be used to ensure that probability models that use TAGs can be checked
for _deficiency_ (i.e. whether any probability mass is assigned to strings that
cannot be generated).

<id>
cs/9809028v1
<category>
cs.CL
<abstract>
In this paper we present a new tree-rewriting formalism called Link-Sharing
Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using
LSTAG we define an approach towards coordination where linguistic dependency is
distinguished from the notion of constituency. Such an approach towards
coordination that explicitly distinguishes dependencies from constituency gives
a better formal understanding of its representation when compared to previous
approaches that use tree-rewriting systems which conflate the two issues.

<id>
cs/9809029v1
<category>
cs.CL
<abstract>
This paper describes the incremental generation of parse tables for the
LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented
handles modifications to the input grammar by updating the parser generated so
far. In this paper, a lazy generation of LR-type parsers for TALs is defined in
which parse tables are created by need while parsing. We then describe an
incremental parser generator for TALs which responds to modification of the
input grammar by updating parse tables built so far.

<id>
cs/9809050v1
<category>
cs.CL
<abstract>
In this paper we present Morphy, an integrated tool for German morphology,
part-of-speech tagging and context-sensitive lemmatization. Its large lexicon
of more than 320,000 word forms plus its ability to process German compound
nouns guarantee a wide morphological coverage. Syntactic ambiguities can be
resolved with a standard statistical part-of-speech tagger. By using the output
of the tagger, the lemmatizer can determine the correct root even for ambiguous
word forms. The complete package is freely available and can be downloaded from
the World Wide Web.

<id>
cs/9809106v1
<category>
cs.CL
<abstract>
The lexical acquisition system presented in this paper incrementally updates
linguistic properties of unknown words inferred from their surrounding context
by parsing sentences with an HPSG grammar for German. We employ a gradual,
information-based concept of ``unknownness'' providing a uniform treatment for
the range of completely known to maximally unknown lexical entries. ``Unknown''
information is viewed as revisable information, which is either generalizable
or specializable. Updating takes place after parsing, which only requires a
modified lexical lookup. Revisable pieces of information are identified by
grammar-specified declarations which provide access paths into the parse
feature structure. The updating mechanism revises the corresponding places in
the lexical feature structures iff the context actually provides new
information. For revising generalizable information, type union is required. A
worked-out example demonstrates the inferential capacity of our implemented
system.

<id>
cs/9809107v1
<category>
cs.CL
<abstract>
This paper describes a computational, declarative approach to prosodic
morphology that uses inviolable constraints to denote small finite candidate
sets which are filtered by a restrictive incremental optimization mechanism.
The new approach is illustrated with an implemented fragment of Modern Hebrew
verbs couched in MicroCUF, an expressive constraint logic formalism. For
generation and parsing of word forms, I propose a novel off-line technique to
eliminate run-time optimization. It produces a finite-state oracle that
efficiently restricts the constraint interpreter's search space. As a
byproduct, unknown words can be analyzed without special mechanisms. Unlike
pure finite-state transducer approaches, this hybrid setup allows for more
expressivity in constraints to specify e.g. token identity for reduplication or
arithmetic constraints for phonetics.

<id>
cs/9809112v1
<category>
cs.CL
<abstract>
This paper addresses the issue of {\sc pos} tagger evaluation. Such
evaluation is usually performed by comparing the tagger output with a reference
test corpus, which is assumed to be error-free. Currently used corpora contain
noise which causes the obtained performance to be a distortion of the real
value. We analyze to what extent this distortion may invalidate the comparison
between taggers or the measure of the improvement given by a new system. The
main conclusion is that a more rigorous testing experimentation
setting/designing is needed to reliably evaluate and compare tagger accuracies.

<id>
cs/9809113v1
<category>
cs.CL
<abstract>
We present a bootstrapping method to develop an annotated corpus, which is
specially useful for languages with few available resources. The method is
being applied to develop a corpus of Spanish of over 5Mw. The method consists
on taking advantage of the collaboration of two different POS taggers. The
cases in which both taggers agree present a higher accuracy and are used to
retrain the taggers.

<id>
cs/9810014v1
<category>
cs.CL
<abstract>
We report on two corpora to be used in the evaluation of component systems
for the tasks of (1) linear segmentation of text and (2) summary-directed
sentence extraction. We present characteristics of the corpora, methods used in
the collection of user judgments, and an overview of the application of the
corpora to evaluating the component system. Finally, we discuss the problems
and issues with construction of the test set which apply broadly to the
construction of evaluation resources for language technologies.

<id>
cs/9810015v1
<category>
cs.CL
<abstract>
Several methods are known for parsing languages generated by Tree Adjoining
Grammars (TAGs) in O(n^6) worst case running time. In this paper we investigate
which restrictions on TAGs and TAG derivations are needed in order to lower
this O(n^6) time complexity, without introducing large runtime constants, and
without losing any of the generative power needed to capture the syntactic
constructions in natural language that can be handled by unrestricted TAGs. In
particular, we describe an algorithm for parsing a strict subclass of TAG in
O(n^5), and attempt to show that this subclass retains enough generative power
to make it useful in the general case.

<id>
cs/9811008v1
<category>
cs.CL
<abstract>
This paper argues that an interlingual representation must explicitly
represent some parts of the meaning of a situation as possibilities (or
preferences), not as necessary or definite components of meaning (or
constraints). Possibilities enable the analysis and generation of nuance,
something required for faithful translation. Furthermore, the representation of
the meaning of words, especially of near-synonyms, is crucial, because it
specifies which nuances words can convey in which contexts.

<id>
cs/9811009v1
<category>
cs.CL
<abstract>
This paper presents a partial solution to a component of the problem of
lexical choice: choosing the synonym most typical, or expected, in context. We
apply a new statistical approach to representing the context of a word through
lexical co-occurrence networks. The implementation was trained and evaluated on
a large corpus, and results show that the inclusion of second-order
co-occurrence relations improves the performance of our implemented lexical
choice program.

<id>
cs/9811016v1
<category>
cs.CL
<abstract>
In this paper we present the results of comparing a statistical tagger for
German based on decision trees and a rule-based Brill-Tagger for German. We
used the same training corpus (and therefore the same tag-set) to train both
taggers. We then applied the taggers to the same test corpus and compared their
respective behavior and in particular their error rates. Both taggers perform
similarly with an error rate of around 5%. From the detailed error analysis it
can be seen that the rule-based tagger has more problems with unknown words
than the statistical tagger. But the results are opposite for tokens that are
many-ways ambiguous. If the unknown words are fed into the taggers with the
help of an external lexicon (such as the Gertwol system) the error rate of the
rule-based tagger drops to 4.7%, and the respective rate of the statistical
taggers drops to around 3.7%. Combining the taggers by using the output of one
tagger to help the other did not lead to any further improvement.

<id>
cs/9811022v2
<category>
cs.CL
<abstract>
The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words--binary-parse-structure with headword annotation and
operates in a left-to-right manner --- therefore usable for automatic speech
recognition. The model, its probabilistic parameterization, and a set of
experiments meant to evaluate its predictive power are presented; an
improvement over standard trigram modeling is achieved.

<id>
cs/9811025v2
<category>
cs.CL
<abstract>
The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words - binary-parse-structure with headword annotation. The
model, its probabilistic parametrization, and a set of experiments meant to
evaluate its predictive power are presented.

<id>
cs/9812001v3
<category>
cs.CL
<abstract>
In this thesis, I address the problem of automatically acquiring lexical
semantic knowledge, especially that of case frame patterns, from large corpus
data and using the acquired knowledge in structural disambiguation. The
approach I adopt has the following characteristics: (1) dividing the problem
into three subproblems: case slot generalization, case dependency learning, and
word clustering (thesaurus construction). (2) viewing each subproblem as that
of statistical estimation and defining probability models for each subproblem,
(3) adopting the Minimum Description Length (MDL) principle as learning
strategy, (4) employing efficient learning algorithms, and (5) viewing the
disambiguation problem as that of statistical prediction. Major contributions
of this thesis include: (1) formalization of the lexical knowledge acquisition
problem, (2) development of a number of learning methods for lexical knowledge
acquisition, and (3) development of a high-performance disambiguation method.

<id>
cs/9812005v1
<category>
cs.CL
<abstract>
There exist several methods of calculating a similarity curve, or a sequence
of similarity values, representing the lexical cohesion of successive text
constituents, e.g., paragraphs. Methods for deciding the locations of fragment
boundaries are, however, scarce. We propose a fragmentation method based on
dynamic programming. The method is theoretically sound and guaranteed to
provide an optimal splitting on the basis of a similarity curve, a preferred
fragment length, and a cost function defined. The method is especially useful
when control on fragment size is of importance.

<id>
cs/9812018v1
<category>
cs.CL
<abstract>
In order to support the efficient development of NL generation systems, two
orthogonal methods are currently pursued with emphasis: (1) reusable, general,
and linguistically motivated surface realization components, and (2) simple,
task-oriented template-based techniques. In this paper we argue that, from an
application-oriented perspective, the benefits of both are still limited. In
order to improve this situation, we suggest and evaluate shallow generation
methods associated with increased flexibility. We advise a close connection
between domain-motivated and linguistic ontologies that supports the quick
adaptation to new tasks and domains, rather than the reuse of general
resources. Our method is especially designed for generating reports with
limited linguistic variations.

<id>
cs/9901005v1
<category>
cs.CL
<abstract>
Scheduling dialogs, during which people negotiate the times of appointments,
are common in everyday life. This paper reports the results of an in-depth
empirical investigation of resolving explicit temporal references in scheduling
dialogs. There are four phases of this work: data annotation and evaluation,
model development, system implementation and evaluation, and model evaluation
and analysis. The system and model were developed primarily on one set of data,
and then applied later to a much more complex data set, to assess the
generalizability of the model for the task being performed. Many different
types of empirical methods are applied to pinpoint the strengths and weaknesses
of the approach. Detailed annotation instructions were developed and an
intercoder reliability study was performed, showing that naive annotators can
reliably perform the targeted annotations. A fully automatic system has been
developed and evaluated on unseen test data, with good results on both data
sets. We adopt a pure realization of a recency-based focus model to identify
precisely when it is and is not adequate for the task being addressed. In
addition to system results, an in-depth evaluation of the model itself is
presented, based on detailed manual annotations. The results are that few
errors occur specifically due to the model of focus being used, and the set of
anaphoric relations defined in the model are low in ambiguity for both data
sets.

<id>
cs/9902001v1
<category>
cs.CL
<abstract>
Treebanks, such as the Penn Treebank (PTB), offer a simple approach to
obtaining a broad coverage grammar: one can simply read the grammar off the
parse trees in the treebank. While such a grammar is easy to obtain, a
square-root rate of growth of the rule set with corpus size suggests that the
derived grammar is far from complete and that much more treebanked text would
be required to obtain a complete grammar, if one exists at some limit. However,
we offer an alternative explanation in terms of the underspecification of
structures within the treebank. This hypothesis is explored by applying an
algorithm to compact the derived grammar by eliminating redundant rules --
rules whose right hand sides can be parsed by other rules. The size of the
resulting compacted grammar, which is significantly less than that of the full
treebank grammar, is shown to approach a limit. However, such a compacted
grammar does not yield very good performance figures. A version of the
compaction algorithm taking rule probabilities into account is proposed, which
is argued to be more linguistically motivated. Combined with simple
thresholding, this method can be used to give a 58% reduction in grammar size
without significant change in parsing performance, and can produce a 69%
reduction with some gain in recall, but a loss in precision.

<id>
cs/9902029v1
<category>
cs.CL
<abstract>
The paper argues that Fodor and Lepore are misguided in their attack on
Pustejovsky's Generative Lexicon, largely because their argument rests on a
traditional, but implausible and discredited, view of the lexicon on which it
is effectively empty of content, a view that stands in the long line of
explaining word meaning (a) by ostension and then (b) explaining it by means of
a vacuous symbol in a lexicon, often the word itself after typographic
transmogrification. (a) and (b) both share the wrong belief that to a word must
correspond a simple entity that is its meaning. I then turn to the semantic
rules that Pustejovsky uses and argue first that, although they have novel
features, they are in a well-established Artificial Intelligence tradition of
explaining meaning by reference to structures that mention other structures
assigned to words that may occur in close proximity to the first. It is argued
that Fodor and Lepore's view that there cannot be such rules is without
foundation, and indeed systems using such rules have proved their practical
worth in computational systems. Their justification descends from line of
argument, whose high points were probably Wittgenstein and Quine that meaning
is not to be understood by simple links to the world, ostensive or otherwise,
but by the relationship of whole cultural representational structures to each
other and to the world as a whole.

<id>
cs/9902030v1
<category>
cs.CL
<abstract>
This paper compares the tasks of part-of-speech (POS) tagging and
word-sense-tagging or disambiguation (WSD), and argues that the tasks are not
related by fineness of grain or anything like that, but are quite different
kinds of task, particularly becuase there is nothing in POS corresponding to
sense novelty. The paper also argues for the reintegration of sub-tasks that
are being separated for evaluation

<id>
cs/9903003v1
<category>
cs.CL
<abstract>
`Linguistic annotation' covers any descriptive or analytic notations applied
to raw language data. The basic data may be in the form of time functions --
audio, video and/or physiological recordings -- or it may be textual. The added
notations may include transcriptions of all sorts (from phonetic features to
discourse structures), part-of-speech and sense tagging, syntactic analysis,
`named entity' identification, co-reference annotation, and so on. While there
are several ongoing efforts to provide formats and tools for such annotations
and to publish annotated linguistic databases, the lack of widely accepted
standards is becoming a critical problem. Proposed standards, to the extent
they exist, have focussed on file formats. This paper focuses instead on the
logical structure of linguistic annotations. We survey a wide variety of
existing annotation formats and demonstrate a common conceptual core, the
annotation graph. This provides a formal framework for constructing,
maintaining and searching linguistic annotations, while remaining consistent
with many alternative data structures and file formats.

<id>
cs/9903008v1
<category>
cs.CL
<abstract>
Recent technological advances have made it possible to build real-time,
interactive spoken dialogue systems for a wide variety of applications.
However, when users do not respect the limitations of such systems, performance
typically degrades. Although users differ with respect to their knowledge of
system limitations, and although different dialogue strategies make system
limitations more apparent to users, most current systems do not try to improve
performance by adapting dialogue behavior to individual users. This paper
presents an empirical evaluation of TOOT, an adaptable spoken dialogue system
for retrieving train schedules on the web. We conduct an experiment in which 20
users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,
resulting in a corpus of 80 dialogues. The values for a wide range of
evaluation measures are then extracted from this corpus. Our results show that
adaptable TOOT generally outperforms non-adaptable TOOT, and that the utility
of adaptation depends on TOOT's initial dialogue strategies.

<id>
cs/9904008v1
<category>
cs.CL
<abstract>
Context sensitive rewrite rules have been widely used in several areas of
natural language processing, including syntax, morphology, phonology and speech
processing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various
algorithms to compile such rewrite rules into finite-state transducers. The
present paper extends this work by allowing a limited form of backreferencing
in such rules. The explicit use of backreferencing leads to more elegant and
general solutions.

<id>
cs/9904009v1
<category>
cs.CL
<abstract>
The two principal areas of natural language processing research in pragmatics
are belief modelling and speech act processing. Belief modelling is the
development of techniques to represent the mental attitudes of a dialogue
participant. The latter approach, speech act processing, based on speech act
theory, involves viewing dialogue in planning terms. Utterances in a dialogue
are modelled as steps in a plan where understanding an utterance involves
deriving the complete plan a speaker is attempting to achieve. However,
previous speech act based approaches have been limited by a reliance upon
relatively simplistic belief modelling techniques and their relationship to
planning and plan recognition. In particular, such techniques assume
precomputed nested belief structures. In this paper, we will present an
approach to speech act processing based on novel belief modelling techniques
where nested beliefs are propagated on demand.

<id>
cs/9301111v1
<category>
cs.CC
<abstract>
A special case of the satisfiability problem, in which the clauses have a
hierarchical structure, is shown to be solvable in linear time, assuming that
the clauses have been represented in a convenient way.

<id>
cs/9301113v1
<category>
cs.CC
<abstract>
We discuss properties of recursive schemas related to McCarthy's ``91
function'' and to Takeuchi's triple recursion. Several theorems are proposed as
interesting candidates for machine verification, and some intriguing open
questions are raised.

<id>
cs/9808002v1
<category>
cs.CC
<abstract>
Hemaspaandra et al. proved that, for $m > 0$ and $0 < i < k - 1$: if
$\Sigma_i^p \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed under complementation,
then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$. This sharply asymmetric
result fails to apply to the case in which the hypothesis is weakened by
allowing the $\Sigma_i^p$ to be replaced by any class in its difference
hierarchy. We so extend the result by proving that, for $s,m > 0$ and $0 < i <
k - 1$: if $DIFF_s(\Sigma_i^p) \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed
under complementation, then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$.

<id>
cs/9809001v1
<category>
cs.CC
<abstract>
Ko [RAIRO 24, 1990] and Bruschi [TCS 102, 1992] showed that in some
relativized world, PSPACE (in fact, ParityP) contains a set that is immune to
the polynomial hierarchy (PH). In this paper, we study and settle the question
of (relativized) separations with immunity for PH and the counting classes PP,
C_{=}P, and ParityP in all possible pairwise combinations. Our main result is
that there is an oracle A relative to which C_{=}P contains a set that is
immune to BPP^{ParityP}. In particular, this C_{=}P^A set is immune to PH^{A}
and ParityP^{A}. Strengthening results of Tor\'{a}n [J.ACM 38, 1991] and Green
[IPL 37, 1991], we also show that, in suitable relativizations, NP contains a
C_{=}P-immune set, and ParityP contains a PP^{PH}-immune set. This implies the
existence of a C_{=}P^{B}-simple set for some oracle B, which extends results
of Balc\'{a}zar et al. [SIAM J.Comp. 14, 1985; RAIRO 22, 1988] and provides the
first example of a simple set in a class not known to be contained in PH. Our
proof technique requires a circuit lower bound for ``exact counting'' that is
derived from Razborov's [Mat. Zametki 41, 1987] lower bound for majority.

<id>
cs/9809002v1
<category>
cs.CC
<abstract>
We study the question of whether every P set has an easy (i.e.,
polynomial-time computable) census function. We characterize this question in
terms of unlikely collapses of language and function classes such as the
containment of #P_1 in FP, where #P_1 is the class of functions that count the
witnesses for tally NP sets. We prove that every #P_{1}^{PH} function can be
computed in FP^{#P_{1}^{#P_{1}}}. Consequently, every P set has an easy census
function if and only if every set in the polynomial hierarchy does. We show
that the assumption of #P_1 being contained in FP implies P = BPP and that PH
is contained in MOD_{k}P for each k \geq 2, which provides further evidence
that not all sets in P have an easy census function. We also relate a set's
property of having an easy census function to other well-studied properties of
sets, such as rankability and scalability (the closure of the rankable sets
under P-isomorphisms). Finally, we prove that it is no more likely that the
census function of any set in P can be approximated (more precisely, can be
n^{\alpha}-enumerated in time n^{\beta} for fixed \alpha and \beta) than that
it can be precisely computed in polynomial time.

<id>
cs/9809114v1
<category>
cs.CC
<abstract>
Building upon the known generalized-quantifier-based first-order
characterization of LOGCFL, we lay the groundwork for a deeper investigation.
Specifically, we examine subclasses of LOGCFL arising from varying the arity
and nesting of groupoidal quantifiers. Our work extends the elaborate theory
relating monoidal quantifiers to NC1 and its subclasses. In the absence of the
BIT predicate, we resolve the main issues: we show in particular that no single
outermost unary groupoidal quantifier with FO can capture all the context-free
languages, and we obtain the surprising result that a variant of Greibach's
``hardest context-free language'' is LOGCFL-complete under quantifier-free
BIT-free projections. We then prove that FO with unary groupoidal quantifiers
is strictly more expressive with the BIT predicate than without. Considering a
particular groupoidal quantifier, we prove that first-order logic with majority
of pairs is strictly more expressive than first-order with majority of
individuals. As a technical tool of independent interest, we define the notion
of an aperiodic nondeterministic finite automaton and prove that FO
translations are precisely the mappings computed by single-valued aperiodic
nondeterministic finite transducers.

<id>
cs/9809115v1
<category>
cs.CC
<abstract>
A notion of generalized quantifier in computational complexity theory is
explored and used to give a unified treatment of leaf language definability,
oracle separations, type 2 operators, and circuits with monoidal gates.
Relations to Lindstroem quantifiers are pointed out.

<id>
cs/9809116v1
<category>
cs.CC
<abstract>
We consider the problems of finding the lexicographically minimal (or
maximal) satisfying assignment of propositional formulae for different
restricted formula classes. It turns out that for each class from our
framework, the above problem is either polynomial time solvable or complete for
OptP. We also consider the problem of deciding if in the optimal assignment the
largest variable gets value 1. We show that this problem is either in P or P^NP
complete.

<id>
cs/9809117v2
<category>
cs.CC
<abstract>
We propose an algorithm of generating hard instances for the Satisfying
Assignment Search Problem (in short, SAT). The algorithm transforms instances
of the integer factorization problem into SAT instances efficiently by using
the Chinese Remainder Theorem. For example, it is possible to construct SAT
instances with about 5,600 variables that is as hard as factorizing 100 bit
integers.

<id>
cs/9906017v1
<category>
cs.CC
<abstract>
Let L be an infinite regular language on a totally ordered alphabet (A,<).
Feeding a finite deterministic automaton (with output) with the words of L
enumerated lexicographically with respect to < leads to an infinite sequence
over the output alphabet of the automaton. This process generalizes the concept
of k-automatic sequence for abstract numeration systems on a regular language
(instead of systems in base k). Here, I study the first properties of these
sequences and their relations with numeration systems.

<id>
cs/9906028v1
<category>
cs.CC
<abstract>
In the early 1980s, Selman's seminal work on positive Turing reductions
showed that positive Turing reduction to NP yields no greater computational
power than NP itself. Thus, positive Turing and Turing reducibility to NP
differ sharply unless the polynomial hierarchy collapses.
  We show that the situation is quite different for DP, the next level of the
boolean hierarchy. In particular, positive Turing reduction to DP already
yields all (and only) sets Turing reducibility to NP. Thus, positive Turing and
Turing reducibility to DP yield the same class. Additionally, we show that an
even weaker class, P(NP[1]), can be substituted for DP in this context.

<id>
cs/9906033v1
<category>
cs.CC
<abstract>
We continue the study of robust reductions initiated by Gavalda and Balcazar.
In particular, a 1991 paper of Gavalda and Balcazar claimed an optimal
separation between the power of robust and nondeterministic strong reductions.
Unfortunately, their proof is invalid. We re-establish their theorem.
  Generalizing robust reductions, we note that robustly strong reductions are
built from two restrictions, robust underproductivity and robust
overproductivity, both of which have been separately studied before in other
contexts. By systematically analyzing the power of these reductions, we explore
the extent to which each restriction weakens the power of reductions. We show
that one of these reductions yields a new, strong form of the Karp-Lipton
Theorem.

<id>
cs/9907033v1
<category>
cs.CC
<abstract>
It is known that for any class C closed under union and intersection, the
Boolean closure of C, the Boolean hierarchy over C, and the symmetric
difference hierarchy over C all are equal. We prove that these equalities hold
for any complexity class closed under intersection; in particular, they thus
hold for unambiguous polynomial time (UP). In contrast to the NP case, we prove
that the Hausdorff hierarchy and the nested difference hierarchy over UP both
fail to capture the Boolean closure of UP in some relativized worlds.
  Karp and Lipton proved that if nondeterministic polynomial time has sparse
Turing-complete sets, then the polynomial hierarchy collapses. We establish the
first consequences from the assumption that unambiguous polynomial time has
sparse Turing-complete sets: (a) UP is in Low_2, where Low_2 is the second
level of the low hierarchy, and (b) each level of the unambiguous polynomial
hierarchy is contained one level lower in the promise unambiguous polynomial
hierarchy than is otherwise known to be the case.

<id>
cs/9907034v1
<category>
cs.CC
<abstract>
We introduce a generalization of Selman's P-selectivity that yields a more
flexible notion of selectivity, called (polynomial-time) multi-selectivity, in
which the selector is allowed to operate on multiple input strings. Since our
introduction of this class, it has been used to prove the first known (and
optimal) lower bounds for generalized selectivity-like classes in terms of
EL_2, the second level of the extended low hierarchy. We study the resulting
selectivity hierarchy, denoted by SH, which we prove does not collapse. In
particular, we study the internal structure and the properties of SH and
completely establish, in terms of incomparability and strict inclusion, the
relations between our generalized selectivity classes and Ogihara's P-mc
(polynomial-time membership-comparable) classes. Although SH is a strictly
increasing infinite hierarchy, we show that the core results that hold for the
P-selective sets and that prove them structurally simple also hold for SH. In
particular, all sets in SH have small circuits; the NP sets in SH are in Low_2,
the second level of the low hierarchy within NP; and SAT cannot be in SH unless
P = NP. Finally, it is known that P-Sel, the class of P-selective sets, is not
closed under union or intersection. We provide an extended selectivity
hierarchy that is based on SH and that is large enough to capture those
closures of the P-selective sets, and yet, in contrast with the P-mc classes,
is refined enough to distinguish them.

<id>
cs/9907035v1
<category>
cs.CC
<abstract>
Can easy sets only have easy certificate schemes? In this paper, we study the
class of sets that, for all NP certificate schemes (i.e., NP machines), always
have easy acceptance certificates (i.e., accepting paths) that can be computed
in polynomial time. We also study the class of sets that, for all NP
certificate schemes, infinitely often have easy acceptance certificates.
  In particular, we provide equivalent characterizations of these classes in
terms of relative generalized Kolmogorov complexity, showing that they are
robust. We also provide structural conditions---regarding immunity and class
collapses---that put upper and lower bounds on the sizes of these two classes.
Finally, we provide negative results showing that some of our positive claims
are optimal with regard to being relativizable. Our negative results are proven
using a novel observation: we show that the classical ``wide spacing'' oracle
construction technique yields instant non-bi-immunity results. Furthermore, we
establish a result that improves upon Baker, Gill, and Solovay's classical
result that NP \neq P = NP \cap coNP holds in some relativized world.

<id>
cs/9907036v1
<category>
cs.CC
<abstract>
In 1876, Lewis Carroll proposed a voting system in which the winner is the
candidate who with the fewest changes in voters' preferences becomes a
Condorcet winner---a candidate who beats all other candidates in pairwise
majority-rule elections. Bartholdi, Tovey, and Trick provided a lower
bound---NP-hardness---on the computational complexity of determining the
election winner in Carroll's system. We provide a stronger lower bound and an
upper bound that matches our lower bound. In particular, determining the winner
in Carroll's system is complete for parallel access to NP, i.e., it is complete
for $\thetatwo$, for which it becomes the most natural complete problem known.
It follows that determining the winner in Carroll's elections is not
NP-complete unless the polynomial hierarchy collapses.

<id>
cs/9907037v1
<category>
cs.CC
<abstract>
We prove that the join of two sets may actually fall into a lower level of
the extended low hierarchy than either of the sets. In particular, there exist
sets that are not in the second level of the extended low hierarchy, EL_2, yet
their join is in EL_2. That is, in terms of extended lowness, the join operator
can lower complexity. Since in a strong intuitive sense the join does not lower
complexity, our result suggests that the extended low hierarchy is unnatural as
a complexity measure. We also study the closure properties of EL_ and prove
that EL_2 is not closed under certain Boolean operations. To this end, we
establish the first known (and optimal) EL_2 lower bounds for certain notions
generalizing Selman's P-selectivity, which may be regarded as an interesting
result in its own right.

<id>
cs/9907038v1
<category>
cs.CC
<abstract>
Rice's Theorem states that every nontrivial language property of the
recursively enumerable sets is undecidable. Borchert and Stephan initiated the
search for complexity-theoretic analogs of Rice's Theorem. In particular, they
proved that every nontrivial counting property of circuits is UP-hard, and that
a number of closely related problems are SPP-hard.
  The present paper studies whether their UP-hardness result itself can be
improved to SPP-hardness. We show that their UP-hardness result cannot be
strengthened to SPP-hardness unless unlikely complexity class containments
hold. Nonetheless, we prove that every P-constructibly bi-infinite counting
property of circuits is SPP-hard. We also raise their general lower bound from
unambiguous nondeterminism to constant-ambiguity nondeterminism.

<id>
cs/9907039v1
<category>
cs.CC
<abstract>
A decade ago, a beautiful paper by Wagner developed a ``toolkit'' that in
certain cases allows one to prove problems hard for parallel access to NP.
However, the problems his toolkit applies to most directly are not overly
natural. During the past year, problems that previously were known only to be
NP-hard or coNP-hard have been shown to be hard even for the class of sets
solvable via parallel access to NP. Many of these problems are longstanding and
extremely natural, such as the Minimum Equivalent Expression problem (which was
the original motivation for creating the polynomial hierarchy), the problem of
determining the winner in the election system introduced by Lewis Carroll in
1876, and the problem of determining on which inputs heuristic algorithms
perform well. In the present article, we survey this recent progress in raising
lower bounds.

<id>
cs/9907041v1
<category>
cs.CC
<abstract>
One way of suggesting that an NP problem may not be NP-complete is to show
that it is in the class UP. We suggest an analogous new approach---weaker in
strength of evidence but more broadly applicable---to suggesting that
concrete~NP problems are not NP-complete. In particular we introduce the class
EP, the subclass of NP consisting of those languages accepted by NP machines
that when they accept always have a number of accepting paths that is a power
of two. Since if any NP-complete set is in EP then all NP sets are in EP, it
follows---with whatever degree of strength one believes that EP differs from
NP---that membership in EP can be viewed as evidence that a problem is not
NP-complete.
  We show that the negation equivalence problem for OBDDs (ordered binary
decision diagrams) and the interchange equivalence problem for 2-dags are in
EP. We also show that for boolean negation the equivalence problem is in
EP^{NP}, thus tightening the existing NP^{NP} upper bound. We show that FewP,
bounded ambiguity polynomial time, is contained in EP, a result that is not
known to follow from the previous SPP upper bound. For the three problems and
classes just mentioned with regard to EP, no proof of membership/containment in
UP is known, and for the problem just mentioned with regard to EP^{NP}, no
proof of membership in UP^{NP} is known. Thus, EP is indeed a tool that gives
evidence against NP-completeness in natural cases where UP cannot currently be
applied.

<id>
cs/9908018v1
<category>
cs.CC
<abstract>
A generalization of numeration system in which the set N of the natural
numbers is recognizable by finite automata can be obtained by describing a
lexicographically ordered infinite regular language. Here we show that if P
belonging to Q[x] is a polynomial such that P(N) is a subset of N then we can
construct a numeration system in which the set of representations of P(N) is
regular. The main issue in this construction is to setup a regular language
with a density function equals to P(n+1)-P(n) for n large enough.

<id>
cs/9909020v1
<category>
cs.CC
<abstract>
We study the effect of query order on computational power, and show that
$\pjk$-the languages computable via a polynomial-time machine given one query
to the jth level of the boolean hierarchy followed by one query to the kth
level of the boolean hierarchy-equals $\redttnp{j+2k-1}$ if j is even and k is
odd, and equals $\redttnp{j+2k}$ otherwise. Thus, unless the polynomial
hierarchy collapses, it holds that for each $1\leq j \leq k$: $\pjk = \pkj \iff
(j=k) \lor (j{is even} \land k=j+1)$. We extend our analysis to apply to more
general query classes.

<id>
cs/9910002v1
<category>
cs.CC
<abstract>
During the past decade, nine papers have obtained increasingly strong
consequences from the assumption that boolean or bounded-query hierarchies
collapse. The final four papers of this nine-paper progression actually achieve
downward collapse---that is, they show that high-level collapses induce
collapses at (what beforehand were thought to be) lower complexity levels. For
example, for each $k\geq 2$ it is now known that if $\psigkone=\psigktwo$ then
$\ph=\sigmak$. This article surveys the history, the results, and the
technique---the so-called easy-hard method---of these nine papers.

<id>
cs/9910003v1
<category>
cs.CC
<abstract>
Do complexity classes have many-one complete sets if and only if they have
Turing-complete sets? We prove that there is a relativized world in which a
relatively natural complexity class-namely a downward closure of NP, \rsnnp -
has Turing-complete sets but has no many-one complete sets. In fact, we show
that in the same relativized world this class has 2-truth-table complete sets
but lacks 1-truth-table complete sets. As part of the groundwork for our
result, we prove that \rsnnp has many equivalent forms having to do with
ordered and parallel access to $\np$ and $\npinterconp$.

<id>
cs/9910004v1
<category>
cs.CC
<abstract>
Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the following
questions: If one is allowed one question to each of two different information
sources, does the order in which one asks the questions affect the class of
problems that one can solve with the given access? If so, which order yields
the greater computational power?
  The answers to these questions have been learned-inasfar as they can be
learned without resolving whether or not the polynomial hierarchy collapses-for
both the polynomial hierarchy and the boolean hierarchy. In the polynomial
hierarchy, query order never matters. In the boolean hierarchy, query order
sometimes does not matter and, unless the polynomial hierarchy collapses,
sometimes does matter. Furthermore, the study of query order has yielded
dividends in seemingly unrelated areas, such as bottleneck computations and
downward translation of equality.
  In this article, we present some of the central results on query order. The
article is written in such a way as to encourage the reader to try his or her
own hand at proving some of these results. We also give literature pointers to
the quickly growing set of related results and applications.

<id>
cs/9910005v2
<category>
cs.CC
<abstract>
Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] initiated the field of
query order, which studies the ways in which computational power is affected by
the order in which information sources are accessed. The present paper studies,
for the first time, query order as it applies to the levels of the polynomial
hierarchy. We prove that the levels of the polynomial hierarchy are
order-oblivious. Yet, we also show that these ordered query classes form new
levels in the polynomial hierarchy unless the polynomial hierarchy collapses.
We prove that all leaf language classes - and thus essentially all standard
complexity classes - inherit all order-obliviousness results that hold for P.

<id>
cs/9910006v1
<category>
cs.CC
<abstract>
We study the computational power of machines that specify their own
acceptance types, and show that they accept exactly the languages that
$\manyonesharp$-reduce to NP sets. A natural variant accepts exactly the
languages that $\manyonesharp$-reduce to P sets. We show that these two classes
coincide if and only if $\psone = \psnnoplusbigohone$, where the latter class
denotes the sets acceptable via at most one question to $\sharpp$ followed by
at most a constant number of questions to $\np$.

<id>
cs/9910007v1
<category>
cs.CC
<abstract>
Downward collapse (a.k.a. upward separation) refers to cases where the
equality of two larger classes implies the equality of two smaller classes. We
provide an unqualified downward collapse result completely within the
polynomial hierarchy. In particular, we prove that, for k > 2, if $\psigkone =
\psigktwo$ then $\sigmak = \pik = \ph$. We extend this to obtain a more general
downward collapse result.

<id>
cs/9910008v2
<category>
cs.CC
<abstract>
Downward translation of equality refers to cases where a collapse of some
pair of complexity classes would induce a collapse of some other pair of
complexity classes that (a priori) one expects are smaller. Recently, the first
downward translation of equality was obtained that applied to the polynomial
hierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. In
this paper, we provide a much broader downward translation that extends not
only that downward translation but also that translation's elegant enhancement
by Buhrman and Fortnow. Our work also sheds light on previous research on the
structure of refined polynomial hierarchies, and strengthens the connection
between the collapse of bounded query hierarchies and the collapse of the
polynomial hierarchy.

<id>
cs/9911002v2
<category>
cs.CC
<abstract>
Generalizations of numeration systems in which N is recognizable by a finite
automaton are obtained by describing a lexicographically ordered infinite
regular language L over a finite alphabet A. For these systems, we obtain a
characterization of recognizable sets of integers in terms of rational formal
series. We also show that, if the complexity of L is Theta (n^q) (resp. if L is
the complement of a polynomial language), then multiplication by an integer k
preserves recognizability only if k=t^{q+1} (resp. if k is not a power of the
cardinality of A) for some integer t. Finally, we obtain sufficient conditions
for the notions of recognizability and U-recognizability to be equivalent,
where U is some positional numeration system related to a sequence of integers.

<id>
cs/0011014v1
<category>
cs.CE
<abstract>
Chip-level CMP modeling is investigated to obtain the post-CMP film profile
thickness across a die from its design layout file and a few film deposition
and CMP parameters. The work covers both HDP and conformal CVD film. The
experimental CMP results agree well with the modeled results. Different
algorithms for filling of dummy structure are compared. A smart algorithm for
dummy filling is presented, which achieves maximal pattern-density uniformity
and CMP planarity.

<id>
cs/0012013v16
<category>
cs.CE
<abstract>
Taxes have major costs beyond the collected revenue: deadweight from
distorted incentives, compliance and enforcement costs, etc. A simple market
mechanism, the Equity Tax, avoids these problems for the trickiest cases:
corporate, dividend, and capital gains taxes.
  It exploits the ability of the share prices to reflect the expected true
annual return (as perceived by investors, not as defined by law) and works only
for publicly held corporations. Since going or staying public cannot be forced,
and for some constitutional reasons too, the conversion to equity tax must be a
voluntary contract. Repeated reconversions would be costly (all capital gains
are realized) and thus rare. The converts and their shareholders pay no income,
dividend, or capital gain taxes. Instead, they give the IRS, say, 2% of stock
per year to auction promptly. Debts are the lender's assets: its status, not
the debtor's, determines their equity-tax or income-tax treatment.
  The system looks too simple to be right. However, it does have no loopholes
(thus lowering the revenue-neutral tax rate), no compliance costs, requires
little regulation, and leaves all business decisions tax neutral. The total
capital the equity taxed sector absorbs is the only thing the tax could
possibly distort. The rates should match so as to minimize this distortion. The
equity tax enlarges the pre-tax profit since this is what the taxpayers
maximize, not a different after-tax net. The wealth shelter is paid for by
efficiency, not by lost tax.

<id>
cs/0102003v1
<category>
cs.CE
<abstract>
This paper develops three polynomial-time pricing techniques for European
Asian options with provably small errors, where the stock prices follow
binomial trees or trees of higher-degree. The first technique is the first
known Monte Carlo algorithm with analytical error bounds suitable for pricing
single-stock options with meaningful confidence and speed. The second technique
is a general recursive bucketing-based scheme that can use the
Aingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and
possibly others as the base-case subroutine. This scheme enables robust
trade-offs between accuracy and time over subtrees of different sizes. For
long-term options or high frequency price averaging, it can price single-stock
options with smaller errors in less time than the base-case algorithms
themselves. The third technique combines Fast Fourier Transform with
bucketing-based schemes for pricing basket options. This technique takes
polynomial time in the number of days and the number of stocks, and does not
add any errors to those already incurred in the companion bucketing scheme.
This technique assumes that the price of each underlying stock moves
independently.

<id>
cs/0104013v1
<category>
cs.CE
<abstract>
The real monetary economy is grounded upon monetary flow equilibration or the
activity of actualizing monetary flow continuity at each economic agent except
for the central bank. Every update of monetary flow continuity at each agent
constantly causes monetary flow equilibration at the neighborhood agents. Every
monetary flow equilibration as the activity of shooting the mark identified as
monetary flow continuity turns out to be off the mark, and constantly generate
the similar activities in sequence. Monetary flow equilibration ceaselessly
reverberating in the economy performs two functions. One is to seek an
organization on its own, and the other is to perturb the ongoing organization.
Monetary flow equilibration as the agency of seeking and perturbing its
organization also serves as a means of predicting its behavior. The likely
organizational behavior could be the one that remains most robust against
monetary flow equilibration as an agency of applying perturbations.

<id>
cs/0104014v1
<category>
cs.CE
<abstract>
Any economic agent constituting the monetary economy maintains the activity
of monetary flow equilibration for fulfilling the condition of monetary flow
continuity in the record, except at the central bank. At the same time,
monetary flow equilibration at one economic agent constantly induces at other
agents in the economy further flow disequilibrium to be eliminated
subsequently. We propose the rate of monetary flow disequilibration as a figure
measuring the progressive movement of the economy. The rate of disequilibration
was read out of both the Japanese and the United States monetary economy
recorded over the last fifty years.

<id>
cs/0105004v1
<category>
cs.CE
<abstract>
This paper describes the parallel implementation of the TRANSIMS traffic
micro-simulation. The parallelization method is domain decomposition, which
means that each CPU of the parallel computer is responsible for a different
geographical area of the simulated region. We describe how information between
domains is exchanged, and how the transportation network graph is partitioned.
An adaptive scheme is used to optimize load balancing. We then demonstrate how
computing speeds of our parallel micro-simulations can be systematically
predicted once the scenario and the computer architecture are known. This makes
it possible, for example, to decide if a certain study is feasible with a
certain computing budget, and how to invest that budget. The main ingredients
of the prediction are knowledge about the parallel implementation of the
micro-simulation, knowledge about the characteristics of the partitioning of
the transportation network graph, and knowledge about the interaction of these
quantities with the computer system. In particular, we investigate the
differences between switched and non-switched topologies, and the effects of 10
Mbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel
computing, transportation planning, TRANSIMS

<id>
cs/0110067v1
<category>
cs.CE
<abstract>
The optimal planning trajectory is analyzed on the basis of the growth model
with effectiveness. The saving per capital value has to be rather high
initially with smooth decrement in the future years.

<id>
cs/0201026v1
<category>
cs.CE
<abstract>
In a seminal paper in 1973, Black and Scholes argued how expected
distributions of stock prices can be used to price options. Their model assumed
a directed random motion for the returns and consequently a lognormal
distribution of asset prices after a finite time. We point out two problems
with their formulation. First, we show that the option valuation is not
uniquely determined; in particular, stratergies based on the delta-hedge and
CAMP (Capital Asset Pricing Model) are shown to provide different valuations of
an option. Second, asset returns are known not to be Gaussian distributed.
Empirically, distributions of returns are seen to be much better approximated
by an exponential distribution. This exponential distribution of asset prices
can be used to develop a new pricing model for options that is shown to provide
valuations that agree very well with those used by traders. We show how the
Fokker-Planck formulation of fluctuations (i.e., the dynamics of the
distribution) can be modified to provide an exponential distribution for
returns. We also show how a singular volatility can be used to go smoothly from
exponential to Gaussian returns and thereby illustrate why exponential returns
cannot be reached perturbatively starting from Gaussian ones, and explain how
the theory of 'stochastic volatility' can be obtained from our model by making
a bad approximation. Finally, we show how to calculate put and call prices for
a stretched exponential density.

<id>
cs/0203023v1
<category>
cs.CE
<abstract>
New services based on the best-effort paradigm could complement the current
deterministic services of an electronic financial exchange. Four crucial
aspects of such systems would benefit from a hybrid stance: proper use of
processing resources, bandwidth management, fault tolerance, and exception
handling. We argue that a more refined view on Quality-of-Service control for
exchange systems, in which the principal ambition of upholding a fair and
orderly marketplace is left uncompromised, would benefit all interested
parties.

<id>
cs/0204051v1
<category>
cs.CE
<abstract>
On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders.

<id>
cs/0204056v1
<category>
cs.CE
<abstract>
Some roaming users need services to manipulate autonomous processes. Trading
agents running on agent trade servers are used as a case in point. We present a
solution that provides the agent owners with means to upkeeping their desktop
environment, and maintaining their agent trade server processes, via a
briefcase service.

<id>
cs/0208022v1
<category>
cs.CE
<abstract>
Currently statistical and artificial neural network methods dominate in
financial data mining. Alternative relational (symbolic) data mining methods
have shown their effectiveness in robotics, drug design and other applications.
Traditionally symbolic methods prevail in the areas with significant
non-numeric (symbolic) knowledge, such as relative location in robot
navigation. At first glance, stock market forecast looks as a pure numeric area
irrelevant to symbolic methods. One of our major goals is to show that
financial time series can benefit significantly from relational data mining
based on symbolic methods. The paper overviews relational data mining
methodology and develops this techniques for financial data mining.

<id>
cs/0208040v1
<category>
cs.CE
<abstract>
This paper presents a statistical framework for assessing wireless systems
performance using hierarchical data mining techniques. We consider WCDMA
(wideband code division multiple access) systems with two-branch STTD (space
time transmit diversity) and 1/2 rate convolutional coding (forward error
correction codes). Monte Carlo simulation estimates the bit error probability
(BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A
performance database of simulation runs is collected over a targeted space of
system configurations. This database is then mined to obtain regions of the
configuration space that exhibit acceptable average performance. The shape of
the mined regions illustrates the joint influence of configuration parameters
on system performance. The role of data mining in this application is to
provide explainable and statistically valid design conclusions. The research
issue is to define statistically meaningful aggregation of data in a manner
that permits efficient and effective data mining algorithms. We achieve a good
compromise between these goals and help establish the applicability of data
mining for characterizing wireless systems performance.

<id>
cs/0210005v1
<category>
cs.CE
<abstract>
In mathematical modeling of the non-squared frequency-dependent diffusions,
also known as the anomalous diffusions, it is desirable to have a positive real
Fourier transform for the time derivative of arbitrary fractional or odd
integer order. The Fourier transform of the fractional time derivative in the
Riemann-Liouville and Caputo senses, however, involves a complex power function
of the fractional order. In this study, a positive time derivative of
fractional or odd integer order is introduced to respect the positivity in
modeling the anomalous diffusions.

<id>
cs/0302034v2
<category>
cs.CE
<abstract>
We show that, for the purpose of pricing Swaptions, the Swap rate and the
corresponding Forward rates can be considered lognormal under a single
martingale measure. Swaptions can then be priced as options on a basket of
lognormal assets and an approximation formula is derived for such options. This
formula is centered around a Black-Scholes price with an appropriate
volatility, plus a correction term that can be interpreted as the expected
tracking error. The calibration problem can then be solved very efficiently
using semidefinite programming.

<id>
cs/0302035v2
<category>
cs.CE
<abstract>
When interest rate dynamics are described by the Libor Market Model as in
BGM97, we show how some essential risk-management results can be obtained from
the dual of the calibration program. In particular, if the objetive is to
maximize another swaption's price, we show that the optimal dual variables
describe a hedging portfolio in the sense of \cite{Avel96}. In the general
case, the local sensitivity of the covariance matrix to all market movement
scenarios can be directly computed from the optimal dual solution. We also show
how semidefinite programming can be used to manage the Gamma exposure of a
portfolio.

<id>
cs/0304009v1
<category>
cs.CE
<abstract>
Standard quantitative models of the stock market predict a log-normal
distribution for stock returns (Bachelier 1900, Osborne 1959), but it is
recognised (Fama 1965) that empirical data, in comparison with a Gaussian,
exhibit leptokurtosis (it has more probability mass in its tails and centre)
and fat tails (probabilities of extreme events are underestimated). Different
attempts to explain this departure from normality have coexisted. In
particular, since one of the strong assumptions of the Gaussian model concerns
the volatility, considered finite and constant, the new models were built on a
non finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985)
volatility. We investigate in this thesis a very recent model (Dragulescu et
al. 2002) based on a Brownian motion process for the returns, and a stochastic
mean-reverting process for the volatility. In this model, the forward
Kolmogorov equation that governs the time evolution of returns is solved
analytically. We test this new theory against different stock indexes (Dow
Jones Industrial Average, Standard and Poor s and Footsie), over different
periods (from 20 to 105 years). Our aim is to compare this model with the
classical Gaussian and with a simple Neural Network, used as a benchmark. We
perform the usual statistical tests on the kurtosis and tails of the expected
distributions, paying particular attention to the outliers. As claimed by the
contributors, the new model outperforms the Gaussian for any time lag, but is
artificially too complex for medium and low frequencies, where the Gaussian is
preferable. Moreover this model is still rejected for high frequencies, at a
0.05 level of significance, due to the kurtosis, incorrectly handled.

<id>
cs/0305036v4
<category>
cs.CE
<abstract>
As in the car industry for quite some time, dynamic simulation of complete
vehicles is being practiced more and more in the development of off-road
machinery. However, specific questions arise due not only to company structure
and size, but especially to the type of product. Tightly coupled, non-linear
subsystems of different domains make prediction and optimisation of the
complete system's dynamic behaviour a challenge. Furthermore, the demand for
versatile machines leads to sometimes contradictory target requirements and can
turn the design process into a hunt for the least painful compromise. This can
be avoided by profound system knowledge, assisted by simulation-driven product
development. This paper gives an overview of joint research into this issue by
Volvo Wheel Loaders and Linkoping University on that matter, lists the results
of a related literature review and introduces the term "operateability". Rather
than giving detailed answers, the problem space for ongoing and future research
is examined and possible solutions are sketched.

<id>
cs/0305055v1
<category>
cs.CE
<abstract>
An analytical formula for the probability distribution of stock-market
returns, derived from the Heston model assuming a mean-reverting stochastic
volatility, was recently proposed by Dragulescu and Yakovenko in Quantitative
Finance 2002. While replicating their results, we found two significant
weaknesses in their method to pre-process the data, which cast a shadow over
the effective goodness-of-fit of the model. We propose a new method, more truly
capturing the market, and perform a Kolmogorov-Smirnov test and a Chi Square
test on the resulting probability distribution. The results raise some
significant questions for large time lags -- 40 to 250 days -- where the
smoothness of the data does not require such a complex model; nevertheless, we
also provide some statistical evidence in favour of the Heston model for small
time lags -- 1 and 5 days -- compared with the traditional Gaussian model
assuming constant volatility.

<id>
cs/0306105v1
<category>
cs.CE
<abstract>
This paper gives an overview of a reconstruction algorithm for muon events in
ATLAS experiment at CERN. After a short introduction on ATLAS Muon
Spectrometer, we will describe the procedure performed by the algorithms
Muonbox and Muonboy (last version) in order to achieve correctly the
reconstruction task. These algorithms have been developed in Fortran language
and are working in the official C++ framework Athena, as well as in stand alone
mode. A description of the interaction between Muonboy and Athena will be
given, together with the reconstruction performances (efficiency and momentum
resolution) obtained with MonteCarlo data.

<id>
cs/0307039v1
<category>
cs.CE
<abstract>
Business concepts are studied using a metamodel-based approach, using UML
2.0. The Notation Independent Business concepts metamodel is introduced. The
approach offers a mapping between different business modeling notations which
could be used for bridging BM tools and boosting the MDA approach.

<id>
cs/0307053v1
<category>
cs.CE
<abstract>
We present a C++ implementation of a fifth order semi-implicit Runge-Kutta
algorithm for solving Ordinary Differential Equations. This algorithm can be
used for studying many different problems and in particular it can be applied
for computing the evolution of any system whose Hamiltonian is known. We
consider in particular the problem of calculating the neutrino oscillation
probabilities in presence of matter interactions. The time performance and the
accuracy of this implementation is competitive with respect to the other
analytical and numerical techniques used in literature. The algorithm design
and the salient features of the code are presented and discussed and some
explicit examples of code application are given.

<id>
cs/0307064v1
<category>
cs.CE
<abstract>
An experimental server for stock trading autonomous agents is presented and
made available, together with an agent shell for swift development. The server,
written in Java, was implemented as proof-of-concept for an agent trade server
for a real financial exchange.

<id>
cs/0406021v3
<category>
cs.CE
<abstract>
We examine the problem of approximating, in the Frobenius-norm sense, a
positive, semidefinite symmetric matrix by a rank-one matrix, with an upper
bound on the cardinality of its eigenvector. The problem arises in the
decomposition of a covariance matrix into sparse factors, and has wide
applications ranging from biology to finance. We use a modification of the
classical variational representation of the largest eigenvalue of a symmetric
matrix, where cardinality is constrained, and derive a semidefinite programming
based relaxation for our problem. We also discuss Nesterov's smooth
minimization technique applied to the SDP arising in the direct sparse PCA
method.

<id>
cs/0407029v1
<category>
cs.CE
<abstract>
We compare static arbitrage price bounds on basket calls, i.e. bounds that
only involve buy-and-hold trading strategies, with the price range obtained
within a multi-variate generalization of the Black-Scholes model. While there
is no gap between these two sets of prices in the univariate case, we observe
here that contrary to our intuition about model risk for at-the-money calls,
there is a somewhat large gap between model prices and static arbitrage prices,
hence a similarly large set of prices on which a multivariate Black-Scholes
model cannot be calibrated but where no conclusion can be drawn on the presence
or not of a static arbitrage opportunity.

<id>
cs/0410064v1
<category>
cs.CE
<abstract>
The paper describes a new CNC control unit for machining centres with
learning ability and automatic intelligent generating of NC programs on the
bases of a neural network, which is built-in into a CNC unit as special device.
The device performs intelligent and completely automatically the NC part
programs only on the bases of 2D, 2,5D or 3D computer model of prismatic part.
Intervention of the operator is not needed. The neural network for milling,
drilling, reaming, threading and operations alike has learned to generate NC
programs in the learning module, which is a part of intelligent CAD/CAM system.

<id>
cs/0412031v1
<category>
cs.CE
<abstract>
The features of designing of reconstruction of the acting plant by its design
department are considered: the results of work are drawings corresponding with
the national standards; large number of the small projects for different acting
objects; variety of the types of the drawings in one project; large paper
archive. The models and methods of developing of the complex CAD system with
friend uniform environment of designing, with setting a profile of operations,
with usage of the general parts of the project, with a series of
problem-oriented subsystems are described on an example of a CAD system
TechnoCAD GlassX

<id>
cs/0412033v1
<category>
cs.CE
<abstract>
The parametric model of build constructions and features of design operations
are described for making drawings, which are the common component of the
different parts of the projects of renovation of enterprises. The key moment of
the deep design automation is the using of so-called units in the drawings,
which are joining a visible graphic part and invisible parameters. The model
has passed check during designing of several hundreds of drawings

<id>
cs/0412034v1
<category>
cs.CE
<abstract>
The characteristic of design works on firm at its renovation and of the
common directions of their informatization is given. The implantation of a CAD
is selected as the key direction, and the requirements to a complex CAD-system
are stated. The methods of such a CAD-system development are featured, and the
connectedness of this development with the process of integration of
information space of design department of the firm is characterized. The
experience of development and implantation of a complex CAD of renovation of
firms TechnoCAD GlassX lies in a basis of this reviewing

<id>
cs/0503084v1
<category>
cs.CE
<abstract>
In this paper the distribution of charged particles is constructed under the
approximation of ambipolar diffusion. The results of mathematical modelling in
two-dimensional case taking into account the velocities of the system are
presented.

<id>
cs/9808008v1
<category>
cs.CG
<abstract>
Problems presented at the open-problem session of the 14th Annual ACM
Symposium on Computational Geometry are listed.

<id>
cs/9809038v2
<category>
cs.CG
<abstract>
We present an algorithm for maintaining the width of a planar point set
dynamically, as points are inserted or deleted. Our algorithm takes time
O(kn^epsilon) per update, where k is the amount of change the update causes in
the convex hull, n is the number of points in the set, and epsilon is any
arbitrarily small constant. For incremental or decremental update sequences,
the amortized time per update is O(n^epsilon).

<id>
cs/9809081v1
<category>
cs.CG
<abstract>
We study the problem of moving a vertex in an unstructured mesh of
triangular, quadrilateral, or tetrahedral elements to optimize the shapes of
adjacent elements. We show that many such problems can be solved in linear time
using generalized linear programming. We also give efficient algorithms for
some mesh smoothing problems that do not fit into the generalized linear
programming paradigm.

<id>
cs/9809109v1
<category>
cs.CG
<abstract>
We show that any polyhedron forming a topological ball with an even number of
quadrilateral sides can be partitioned into O(n) topological cubes, meeting
face to face. The result generalizes to non-simply-connected polyhedra
satisfying an additional bipartiteness condition. The same techniques can also
be used to reduce the geometric version of the hexahedral mesh generation
problem to a finite case analysis amenable to machine solution.

<id>
cs/9810007v2
<category>
cs.CG
<abstract>
We use here the results on the influence graph by Boissonnat et al. to adapt
them for particular cases where additional information is available. In some
cases, it is possible to improve the expected randomized complexity of
algorithms from O(n log n) to O(n log star n).
  This technique applies in the following applications: triangulation of a
simple polygon, skeleton of a simple polygon, Delaunay triangulation of points
knowing the EMST (euclidean minimum spanning tree).

<id>
cs/9901013v1
<category>
cs.CG
<abstract>
We present an empirical analysis of data structures for approximate nearest
neighbor searching. We compare the well-known optimized kd-tree splitting
method against two alternative splitting methods. The first, called the
sliding-midpoint method, which attempts to balance the goals of producing
subdivision cells of bounded aspect ratio, while not producing any empty cells.
The second, called the minimum-ambiguity method is a query-based approach. In
addition to the data points, it is also given a training set of query points
for preprocessing. It employs a simple greedy algorithm to select the splitting
plane that minimizes the average amount of ambiguity in the choice of the
nearest neighbor for the training points. We provide an empirical analysis
comparing these two methods against the optimized kd-tree construction for a
number of synthetically generated data and query sets. We demonstrate that for
clustered data and query sets, these algorithms can provide significant
improvements over the standard kd-tree construction for approximate nearest
neighbor searching.

<id>
cs/9906023v1
<category>
cs.CG
<abstract>
The subquadratic algorithm of Kapoor for finding shortest paths on a
polyhedron is described.

<id>
cs/9907023v1
<category>
cs.CG
<abstract>
This paper presents how the space of spheres and shelling may be used to
delete a point from a $d$-dimensional triangulation efficiently. In dimension
two, if k is the degree of the deleted vertex, the complexity is O(k log k),
but we notice that this number only applies to low cost operations, while time
consuming computations are only done a linear number of times.
  This algorithm may be viewed as a variation of Heller's algorithm, which is
popular in the geographic information system community. Unfortunately, Heller
algorithm is false, as explained in this paper.

<id>
cs/9907024v1
<category>
cs.CG
<abstract>
We propose a new data structure to compute the Delaunay triangulation of a
set of points in the plane. It combines good worst case complexity, fast
behavior on real data, and small memory occupation.
  The location structure is organized into several levels. The lowest level
just consists of the triangulation, then each level contains the triangulation
of a small sample of the levels below. Point location is done by marching in a
triangulation to determine the nearest neighbor of the query at that level,
then the march restarts from that neighbor at the level below. Using a small
sample (3%) allows a small memory occupation; the march and the use of the
nearest neighbor to change levels quickly locate the query.

<id>
cs/9907025v1
<category>
cs.CG
<abstract>
We provide a lower bound construction showing that the union of unit balls in
three-dimensional space has quadratic complexity, even if they all contain the
origin. This settles a conjecture of Sharir.

<id>
cs/9907028v1
<category>
cs.CG
<abstract>
An efficient technique to solve precision problems consists in using exact
computations. For geometric predicates, using systematically expensive exact
computations can be avoided by the use of filters. The predicate is first
evaluated using rounding computations, and an error estimation gives a
certificate of the validity of the result. In this note, we studies the
statistical efficiency of filters for cosphericity predicate with an assumption
of regular distribution of the points. We prove that the expected value of the
polynomial corresponding to the in sphere test is greater than epsilon with
probability O(epsilon log 1/epsilon) improving the results of a previous paper
by the same contributors.

<id>
cs/9907029v2
<category>
cs.CG
<abstract>
The assumption of real-number arithmetic, which is at the basis of
conventional geometric algorithms, has been seriously challenged in recent
years, since digital computers do not exhibit such capability.
  A geometric predicate usually consists of evaluating the sign of some
algebraic expression. In most cases, rounded computations yield a reliable
result, but sometimes rounded arithmetic introduces errors which may invalidate
the algorithms. The rounded arithmetic may produce an incorrect result only if
the exact absolute value of the algebraic expression is smaller than some
(small) varepsilon, which represents the largest error that may arise in the
evaluation of the expression. The threshold varepsilon depends on the structure
of the expression and on the adopted computer arithmetic, assuming that the
input operands are error-free.
  A pair (arithmetic engine,threshold) is an "arithmetic filter". In this paper
we develop a general technique for assessing the efficacy of an arithmetic
filter. The analysis consists of evaluating both the threshold and the
probability of failure of the filter.
  To exemplify the approach, under the assumption that the input points be
chosen randomly in a unit ball or unit cube with uniform density, we analyze
the two important predicates "which-side" and "insphere". We show that the
probability that the absolute values of the corresponding determinants be no
larger than some positive value V, with emphasis on small V, is Theta(V) for
the which-side predicate, while for the insphere predicate it is Theta(V^(2/3))
in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higher
dimensions. Constants are small, and are given in the paper.

<id>
cs/9907030v1
<category>
cs.CG
<abstract>
We describe simple linear time algorithms for coloring the squares of
balanced and unbalanced quadtrees so that no two adjacent squares are given the
same color. If squares sharing sides are defined as adjacent, we color balanced
quadtrees with three colors, and unbalanced quadtrees with four colors; these
results are both tight, as some quadtrees require this many colors. If squares
sharing corners are defined as adjacent, we color balanced or unbalanced
quadtrees with six colors; for some quadtrees, at least five colors are
required.

<id>
cs/9908006v2
<category>
cs.CG
<abstract>
Two results in "computational origami" are illustrated.

<id>
cs/9908007v1
<category>
cs.CG
<abstract>
Open problems from the 15th Annual ACM Symposium on Computational Geometry.

<id>
cs/9908016v1
<category>
cs.CG
<abstract>
We use circle-packing methods to generate quadrilateral meshes for polygonal
domains, with guaranteed bounds both on the quality and the number of elements.
We show that these methods can generate meshes of several types: (1) the
elements form the cells of a Voronoi diagram, (2) all elements have two
opposite right angles, (3) all elements are kites, or (4) all angles are at
most 120 degrees. In each case the total number of elements is O(n), where n is
the number of input vertices.

<id>
cs/9909004v1
<category>
cs.CG
<abstract>
We consider the motion planning problem for a point constrained to move along
a smooth closed convex path of bounded curvature. The workspace of the moving
point is bounded by a convex polygon with m vertices, containing an obstacle in
a form of a simple polygon with $n$ vertices. We present an O(m+n) time
algorithm finding the path, going around the obstacle, whose curvature is the
smallest possible.

<id>
cs/9909005v1
<category>
cs.CG
<abstract>
A circle $C$ separates two planar sets if it encloses one of the sets and its
open interior disk does not meet the other set. A separating circle is a
largest one if it cannot be locally increased while still separating the two
given sets. An Theta(n log n) optimal algorithm is proposed to find all largest
circles separating two given sets of line segments when line segments are
allowed to meet only at their endpoints. In the general case, when line
segments may intersect $\Omega(n^2)$ times, our algorithm can be adapted to
work in O(n alpha(n) log n) time and O(n \alpha(n)) space, where alpha(n)
represents the extremely slowly growing inverse of the Ackermann function.

<id>
cs/9909006v1
<category>
cs.CG
<abstract>
We study the problem of computing the free space F of a simple legged robot
called the spider robot. The body of this robot is a single point and the legs
are attached to the body. The robot is subject to two constraints: each leg has
a maximal extension R (accessibility constraint) and the body of the robot must
lie above the convex hull of its feet (stability constraint). Moreover, the
robot can only put its feet on some regions, called the foothold regions. The
free space F is the set of positions of the body of the robot such that there
exists a set of accessible footholds for which the robot is stable. We present
an efficient algorithm that computes F in O(n2 log n) time using O(n2 alpha(n))
space for n discrete point footholds where alpha(n) is an extremely slowly
growing function (alpha(n) <= 3 for any practical value of n). We also present
an algorithm for computing F when the foothold regions are pairwise disjoint
polygons with n edges in total. This algorithm computes F in O(n2 alpha8(n) log
n) time using O(n2 alpha8(n)) space (alpha8(n) is also an extremely slowly
growing function). These results are close to optimal since Omega(n2) is a
lower bound for the size of F.

<id>
cs/9909007v1
<category>
cs.CG
<abstract>
Two planar sets are circularly separable if there exists a circle enclosing
one of the sets and whose open interior disk does not intersect the other set.
  This paper studies two problems related to circular separability. A
linear-time algorithm is proposed to decide if two polygons are circularly
separable. The algorithm outputs the smallest separating circle. The second
problem asks for the largest circle included in a preprocessed, convex polygon,
under some point and/or line constraints. The resulting circle must contain the
query points and it must lie in the halfplanes delimited by the query lines.

<id>
cs/9909017v1
<category>
cs.CG
<abstract>
Given a finite set of non-collinear points in the plane, there exists a line
that passes through exactly two points. Such a line is called an ordinary line.
An efficient algorithm for computing such a line was proposed by Mukhopadhyay
et al. In this note we extend this result in two directions. We first show how
to use this algorithm to compute an ordinary conic, that is, a conic passing
through exactly five points, assuming that all the points do not lie on the
same conic. Both our proofs of existence and the consequent algorithms are
simpler than previous ones. We next show how to compute an ordinary hyperplane
in three and higher dimensions.

<id>
cs/0009013v2
<category>
cs.CG
<abstract>
In this paper we present algorithms for a number of problems in geometric
pattern matching where the input consist of a collections of segments in the
plane. Our work consists of two main parts. In the first, we address problems
and measures that relate to collections of orthogonal line segments in the
plane. Such collections arise naturally from problems in mapping buildings and
robot exploration.
  We propose a new measure of segment similarity called a \emph{coverage
measure}, and present efficient algorithms for maximising this measure between
sets of axis-parallel segments under translations. Our algorithms run in time
$O(n^3\polylog n)$ in the general case, and run in time $O(n^2\polylog n)$ for
the case when all segments are horizontal. In addition, we show that when
restricted to translations that are only vertical, the Hausdorff distance
between two sets of horizontal segments can be computed in time roughly
$O(n^{3/2}{\sl polylog}n)$. These algorithms form significant improvements over
the general algorithm of Chew et al. that takes time $O(n^4 \log^2 n)$. In the
second part of this paper we address the problem of matching polygonal chains.
We study the well known \Frd, and present the first algorithm for computing the
\Frd under general translations. Our methods also yield algorithms for
computing a generalization of the \Fr distance, and we also present a simple
approximation algorithm for the \Frd that runs in time $O(n^2\polylog n)$.

<id>
cs/0009024v1
<category>
cs.CG
<abstract>
We give algorithms for computing the regression depth of a k-flat for a set
of n points in R^d. The running time is O(n^(d-2) + n log n) when 0 < k < d-1,
faster than the best time bound for hyperplane regression or for data depth.

<id>
cs/0010039v1
<category>
cs.CG
<abstract>
It has recently been established by Below, De Loera, and Richter-Gebert that
finding a minimum size (or even just a small) triangulation of a convex
polyhedron is NP-complete. Their 3SAT-reduction proof is discussed.

<id>
cs/0101006v2
<category>
cs.CG
<abstract>
We give linear-time quasiconvex programming algorithms for finding a Moebius
transformation of a set of spheres in a unit ball or on the surface of a unit
sphere that maximizes the minimum size of a transformed sphere. We can also use
similar methods to maximize the minimum distance among a set of pairs of input
points. We apply these results to vertex separation and symmetry display in
spherical graph drawing, viewpoint selection in hyperbolic browsing, element
size control in conformal structured mesh generation, and brain flat mapping.

<id>
cs/0103017v1
<category>
cs.CG
<abstract>
We consider the complexity of Delaunay triangulations of sets of points in
R^3 under certain practical geometric constraints. The spread of a set of
points is the ratio between the longest and shortest pairwise distances. We
show that in the worst case, the Delaunay triangulation of n points in R^3 with
spread D has complexity Omega(min{D^3, nD, n^2}) and O(min{D^4, n^2}). For the
case D = Theta(sqrt{n}), our lower bound construction consists of a uniform
sample of a smooth convex surface with bounded curvature. We also construct a
family of smooth connected surfaces such that the Delaunay triangulation of any
good point sample has near-quadratic complexity.

<id>
cs/0108020v3
<category>
cs.CG
<abstract>
We define and examine flip operations for quadrilateral and hexahedral
meshes, similar to the flipping transformations previously used in triangular
and tetrahedral mesh generation.

<id>
cs/0202011v1
<category>
cs.CG
<abstract>
In this paper, we give upper and lower bounds on the number of Steiner points
required to construct a strictly convex quadrilateral mesh for a planar point
set. In particular, we show that $3{\lfloor\frac{n}{2}\rfloor}$ internal
Steiner points are always sufficient for a convex quadrilateral mesh of $n$
points in the plane. Furthermore, for any given $n\geq 4$, there are point sets
for which $\lceil\frac{n-3}{2}\rceil-1$ Steiner points are necessary for a
convex quadrilateral mesh.

<id>
cs/0203025v1
<category>
cs.CG
<abstract>
In this note we consider the problem of manufacturing a convex polyhedral
object via casting. We consider a generalization of the sand casting process
where the object is manufactured by gluing together two identical faces of
parts cast with a single piece mold. In this model we show that the class of
convex polyhedra which can be enclosed between two concentric spheres of the
ratio of their radii less than 1.07 cannot be manufactured using only two cast
parts.

<id>
cs/0204042v1
<category>
cs.CG
<abstract>
We examine a computational geometric problem concerning the structure of
polymers. We model a polymer as a polygonal chain in three dimensions. Each
edge splits the polymer into two subchains, and a dihedral rotation rotates one
of these chains rigidly about this edge. The problem is to determine, given a
chain, an edge, and an angle of rotation, if the motion can be performed
without causing the chain to self-intersect. An Omega(n log n) lower bound on
the time complexity of this problem is known.
  We prove that preprocessing a chain of n edges and answering n dihedral
rotation queries is 3SUM-hard, giving strong evidence that solving n queries
requires Omega(n^2) time in the worst case. For dynamic queries, which also
modify the chain if the requested dihedral rotation is feasible, we show that
answering n queries is by itself 3SUM-hard, suggesting that sublinear query
time is impossible after any amount of preprocessing.

<id>
cs/0201010v1
<category>
cs.GT
<abstract>
This paper analyzes individually-rational ex post equilibrium in the VC
(Vickrey-Clarke) combinatorial auctions. If $\Sigma$ is a family of bundles of
goods, the organizer may restrict the participants by requiring them to submit
their bids only for bundles in $\Sigma$. The $\Sigma$-VC combinatorial auctions
(multi-good auctions) obtained in this way are known to be
individually-rational truth-telling mechanisms. In contrast, this paper deals
with non-restricted VC auctions, in which the buyers restrict themselves to
bids on bundles in $\Sigma$, because it is rational for them to do so. That is,
it may be that when the buyers report their valuation of the bundles in
$\Sigma$, they are in an equilibrium. We fully characterize those $\Sigma$ that
induce individually rational equilibrium in every VC auction, and we refer to
the associated equilibrium as a bundling equilibrium. The number of bundles in
$\Sigma$ represents the communication complexity of the equilibrium. A special
case of bundling equilibrium is partition-based equilibrium, in which $\Sigma$
is a field, that is, it is generated by a partition. We analyze the tradeoff
between communication complexity and economic efficiency of bundling
equilibrium, focusing in particular on partition-based equilibrium.

<id>
cs/0202015v2
<category>
cs.GT
<abstract>
In most of microeconomic theory, consumers are assumed to exhibit decreasing
marginal utilities. This paper considers combinatorial auctions among such
submodular buyers. The valuations of such buyers are placed within a hierarchy
of valuations that exhibit no complementarities, a hierarchy that includes also
OR and XOR combinations of singleton valuations, and valuations satisfying the
gross substitutes property. Those last valuations are shown to form a
zero-measure subset of the submodular valuations that have positive measure.
While we show that the allocation problem among submodular valuations is
NP-hard, we present an efficient greedy 2-approximation algorithm for this case
and generalize it to the case of limited complementarities. No such
approximation algorithm exists in a setting allowing for arbitrary
complementarities. Some results about strategic aspects of combinatorial
auctions among players with decreasing marginal utilities are also presented.

<id>
cs/0202017v1
<category>
cs.GT
<abstract>
Some important classical mechanisms considered in Microeconomics and Game
Theory require the solution of a difficult optimization problem. This is true
of mechanisms for combinatorial auctions, which have in recent years assumed
practical importance, and in particular of the gold standard for combinatorial
auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these
mechanisms - in particular, their truth revelation properties - assumes that
the optimization problems are solved precisely. In reality, these optimization
problems can usually be solved only in an approximate fashion. We investigate
the impact on such mechanisms of replacing exact solutions by approximate ones.
Specifically, we look at a particular greedy optimization method. We show that
the GVA payment scheme does not provide for a truth revealing mechanism. We
introduce another scheme that does guarantee truthfulness for a restricted
class of players. We demonstrate the latter property by identifying natural
properties for combinatorial auctions and showing that, for our restricted
class of players, they imply that truthful strategies are dominant. Those
properties have applicability beyond the specific auction studied.

<id>
cs/0202023v2
<category>
cs.GT
<abstract>
A model for decision making that generalizes Expected Utility Maximization is
presented. This model, Expected Qualitative Utility Maximization, encompasses
the Maximin criterion. It relaxes both the Independence and the Continuity
postulates. Its main ingredient is the definition of a qualitative order on
nonstandard models of the real numbers and the consideration of nonstandard
utilities. Expected Qualitative Utility Maximization is characterized by an
original weakening of von Neumann-Morgenstern's postulates. Subjective
probabilities may be defined from those weakened postulates, as Anscombe and
Aumann did from the original postulates. Subjective probabilities are numbers,
not matrices as in the Subjective Expected Lexicographic Utility approach. JEL
no.: D81 Keywords: Utility Theory, Non-Standard Utilities, Qualitative Decision
Theory

<id>
cs/0202028v1
<category>
cs.GT
<abstract>
Certain services may be provided in a continuous, one-dimensional, ordered
range of different qualities and a customer requiring a service of quality q
can only be offered a quality superior or equal to q. Only a discrete set of
different qualities will be offered, and a service provider will provide the
same service (of fixed quality b) to all customers requesting qualities of
service inferior or equal to b. Assuming all services (of quality b) are priced
identically, a monopolist will choose the qualities of service and the prices
that maximize profit but, under perfect competition, a service provider will
choose the (inferior) quality of service that can be priced at the lowest
price. Assuming significant economies of scale, two fundamentally different
regimes are possible: either a number of different classes of service are
offered (DC regime), or a unique class of service offers an unbounded quality
of service (UC regime). The DC regime appears in one of two sub-regimes: one,
BDC, in which a finite number of classes is offered, the qualities of service
offered are bounded and requests for high-quality services are not met, or UDC
in which an infinite number of classes of service are offered and every request
is met. The types of the demand curve and of the economies of scale, not the
pace of technological change, determine the regime and the class boundaries.
The price structure in the DC regime obeys very general laws.

<id>
cs/0202029v1
<category>
cs.GT
<abstract>
The consideration of nonstandard models of the real numbers and the
definition of a qualitative ordering on those models provides a generalization
of the principle of maximization of expected utility. It enables the decider to
assign probabilities of different orders of magnitude to different events or to
assign utilities of different orders of magnitude to different outcomes. The
properties of this generalized notion of rationality are studied in the
frameworks proposed by von Neumann and Morgenstern and later by Anscombe and
Aumann. It is characterized by an original weakening of their postulates in two
different situations: nonstandard probabilities and standard utilities on one
hand and standard probabilities and nonstandard utilities on the other hand.
This weakening concerns both Independence and Continuity. It is orthogonal with
the weakening proposed by lexicographic orderings.

<id>
cs/0205076v1
<category>
cs.GT
<abstract>
In multiagent settings where the agents have different preferences,
preference aggregation is a central issue. Voting is a general method for
preference aggregation, but seminal results have shown that all general voting
protocols are manipulable. One could try to avoid manipulation by using voting
protocols where determining a beneficial manipulation is hard. Especially among
computational agents, it is reasonable to measure this hardness by
computational complexity. Some earlier work has been done in this area, but it
was assumed that the number of voters and candidates is unbounded. We derive
hardness results for practical multiagent settings where the number of
candidates is small but the number of voters can be large. We show that with
complete information about the others' votes, individual manipulation is easy,
and coalitional manipulation is easy with unweighted voters. However,
constructive coalitional manipulation with weighted voters is intractable for
all of the voting protocols under study, except for the nonrandomized Cup.
Destructive manipulation tends to be easier. Randomizing over instantiations of
the protocols (such as schedules of the Cup protocol) can be used to make
manipulation hard. Finally, we show that under weak assumptions, if weighted
coalitional manipulation with complete information about the others' votes is
hard in some voting protocol, then individual and unweighted manipulation is
hard when there is uncertainty about the others' votes.

<id>
cs/0306128v3
<category>
cs.GT
<abstract>
The 2 x 2 games, in particular the Prisoner's Dilemma, have been extensively
used in studies into reciprocal cooperation and, to a lesser extent, kin
selection. This paper examines the suitability of the 2 x 2 games for modelling
the evolution of cooperation through reciprocation and kin selection. This
examination is not restricted to the Prisoner's Dilemma, but includes the other
non-trivial symmetric 2 x 2 games. We show that the popularity of the
Prisoner's Dilemma for modelling social and biotic interaction is justified by
its superiority according to these criteria. Indeed, the Prisoner's Dilemma is
unique in providing the simplest support for reciprocal cooperation, and
additive kin-selected altruism. However, care is still required in choosing the
particular Prisoner's Dilemma payoff matrix to use. This paper reviews the
impact of non-linear payoffs for the application of Hamilton's rule to typical
altruistic interactions, and derives new results for cases in which the roles
of potential altruist and beneficiary are separated. In doing so we find the
same equilibrium condition holds in continuous games between relatives, and in
discrete games with roles.

<id>
cs/0310039v1
<category>
cs.GT
<abstract>
Peer-To-Peer (P2P) networks are self-organizing, distributed systems, with no
centralized contributority or infrastructure. Because of the voluntary
participation, the availability of resources in a P2P system can be highly
variable and unpredictable. In this paper, we use ideas from Game Theory to
study the interaction of strategic and rational peers, and propose a
differential service-based incentive scheme to improve the system's
performance.

<id>
cs/0312005v2
<category>
cs.GT
<abstract>
A bidimensional representation of the space of 2x2 Symmetric Games in the
strategic representation is proposed. This representation provides a tool for
the classification of 2x2 symmetric games, quantification of the fraction of
them having a certain feature, and predictions of changes in the
characteristics of a game when a change in done on the payoff matrix that
defines it.

<id>
cs/0404016v1
<category>
cs.GT
<abstract>
This paper investigates the different effects of chaotic switching on
Parrondo's games, as compared to random and periodic switching. The rate of
winning of Parrondo's games with chaotic switching depends on coefficient(s)
defining the chaotic generator, initial conditions of the chaotic sequence and
the proportion of Game A played. Maximum rate of winning can be obtained with
all the above mentioned factors properly set, and this occurs when chaotic
switching approaches periodic behavior.

<id>
cs/0406051v1
<category>
cs.GT
<abstract>
We show, that a simple generalization of the Deferred Acceptance Procedure
with firms proposing due to Gale and Shapley(1962), yeild outcomes for a
two-sided contract choice problem, which necessarily belong to the core and are
Weakly Pareto Optimal for firms. Under additional assumptions: (a) given any
two distinct workers, the set of yields acheivable by a firm with the first
worker is disjoint from the set of yields acheivable by it with the second, and
(b) the contract choice problem is pair-wise efficient, we prove that there is
no stable outcome at which a firm can get more than what it gets at the unique
outcome of our procedure.

<id>
cs/0408065v6
<category>
cs.GT
<abstract>
This paper proves the existence of non-empty cores for directed network
problems with quotas and for those combinatorial allocation problems which
permit only exclusive allocations.

<id>
cs/0503035v1
<category>
cs.GT
<abstract>
In this note we consider a society that partitions itself into disjoint
jurisdictions, each choosing a location of its public project and a taxation
scheme to finance it. The set of public project is multi-dimensional, and their
costs could vary from jurisdiction to jurisdiction. We impose two principles,
egalitarianism, that requires the equalization of the total cost for all agents
in the same jurisdiction, and efficiency, that implies the minimization of the
aggregate total cost within jurisdiction. We show that these two principles
always yield a core-stable partition but a Nash stable partition may fail to
exist.

<id>
cs/0506038v1
<category>
cs.GT
<abstract>
On information security outsourcing market, an important reason that firms do
not want to let outside firms(usually called MSSPs-Managed Security Service
Providers) to take care of their security need is that they worry about service
quality MSSPs provide because they cannot monitor effort of the MSSPs. Since
MSSPs action is unobservable to buyers, MSSPs can lower cost by working less
hard than required in the contract and get higher profit. In the asymmetric
information literature, this possible secret shirking behavior is termed as
moral hazard problem. This paper considers a game theoretic economic framework
to show that under information asymmetry, an optimal contract can be designed
so that MSSPs will stick to their promised effort level. We also show that the
optimal contract should be performance-based, i.e., payment to MSSP should base
on performance of MSSP's security service period by period. For comparison, we
also showed that if the moral hazard problem does not exist, the optimal
contract does not depend on MSSP's performance. A contract that specifies
constant payment to MSSP will be optimal. Besides these, we show that for no
matter under perfect information scenario or imperfect information scenario,
the higher the transaction cost is, the lower payment to MSSPs will be.

<id>
cs/0506054v1
<category>
cs.GT
<abstract>
We consider a resource allocation problem where individual users wish to send
data across a network to maximize their utility, and a cost is incurred at each
link that depends on the total rate sent through the link. It is known that as
long as users do not anticipate the effect of their actions on prices, a simple
proportional pricing mechanism can maximize the sum of users' utilities minus
the cost (called aggregate surplus). Continuing previous efforts to quantify
the effects of selfish behavior in network pricing mechanisms, we consider the
possibility that users anticipate the effect of their actions on link prices.
Under the assumption that the links' marginal cost functions are convex, we
establish existence of a Nash equilibrium. We show that the aggregate surplus
at a Nash equilibrium is no worse than a factor of 4*sqrt{2} - 5 times the
optimal aggregate surplus; thus, the efficiency loss when users are selfish is
no more than approximately 34%.

<id>
cs/0507007v4
<category>
cs.GT
<abstract>
We consider the untyped lambda calculus with constructors and recursively
defined constants. We construct a domain-theoretic model such that any term not
denoting bottom is strongly normalising provided all its `stratified
approximations' are. From this we derive a general normalisation theorem for
applied typed lambda-calculi: If all constants have a total value, then all
typeable terms are strongly normalising. We apply this result to extensions of
G\"odel's system T and system F extended by various forms of bar recursion for
which strong normalisation was hitherto unknown.

<id>
cs/0509063v1
<category>
cs.GT
<abstract>
Two natural strategy elimination procedures have been studied for strategic
games. The first one involves the notion of (strict, weak, etc) dominance and
the second the notion of rationalizability. In the case of dominance the
criterion of order independence allowed us to clarify which notions and under
what circumstances are robust. In the case of rationalizability this criterion
has not been considered. In this paper we investigate the problem of order
independence for rationalizability by focusing on three naturally entailed
reduction relations on games. These reduction relations are distinguished by
the adopted reference point for the notion of a better response. Additionally,
they are parametrized by the adopted system of beliefs. We show that for one
reduction relation the outcome of its (possibly transfinite) iterations does
not depend on the order of elimination of the strategies. This result does not
hold for the other two reduction relations. However, under a natural assumption
the iterations of all three reduction relations yield the same outcome. The
obtained order independence results apply to the frameworks considered in
Bernheim 84 and Pearce 84. For finite games the iterations of all three
reduction relations coincide and the order independence holds for three natural
systems of beliefs considered in the literature.

<id>
cs/0510031v1
<category>
cs.GT
<abstract>
In this paper we present a novel generic mapping between Graphical Games and
Markov Random Fields so that pure Nash equilibria in the former can be found by
statistical inference on the latter. Thus, the problem of deciding whether a
graphical game has a pure Nash equilibrium, a well-known intractable problem,
can be attacked by well-established algorithms such as Belief Propagation,
Junction Trees, Markov Chain Monte Carlo and Simulated Annealing. Large classes
of graphical games become thus tractable, including all classes already known,
but also new classes such as the games with O(log n) treewidth.

<id>
cs/0512083v2
<category>
cs.GT
<abstract>
Mechanism design uses the tools of economics and game theory to design rules
of interaction for economic transactions that will,in principle, yield some de-
sired outcome. In the last few years this field has received much interest of
researchers in computer science, especially with the Internet developing as a
platform for communications and connections among enormous numbers of computers
and humans. Arguably the most positive result in mechanism de- sign is truthful
and there are only one general truthful mechanisms so far : the generalized
Vickrey-Clarke-Groves (VCG) mechanism. But VCG mecha- nism has one shortage:
The implementation of truthfulness is on the cost of decreasing the revenue of
the mechanism. (e.g., Ning Chen and Hong Zhu. [1999]). We introduce three new
characters of mechanism:partly truthful, criti- cal, consistent, and introduce
a new mechanism: X mechanism that satisfy the above three characters. Like VCG
mechanism, X mechanism also generalizes from Vickery Auction and is consistent
with Vickery auction in many ways, but the extended methods used in X mechanism
is different from that in VCG mechanism . This paper will demonstrate that X
mechanism better than VCG mechanism in optimizing utility of mechanism, which
is the original intention of mechanism design. So partly truthful,critical and
consistent are at least as important as truthful in mechanism design, and they
beyond truthful in many situations.As a result, we conclude that partly
truthful,critical and consistent are three new directions in mechanism design.

<id>
cs/0602019v1
<category>
cs.GT
<abstract>
In this work, we propose a game theoretic framework to analyze the behavior
of cognitive radios for distributed adaptive channel allocation. We define two
different objective functions for the spectrum sharing games, which capture the
utility of selfish users and cooperative users, respectively. Based on the
utility definition for cooperative users, we show that the channel allocation
problem can be formulated as a potential game, and thus converges to a
deterministic channel allocation Nash equilibrium point. Alternatively, a
no-regret learning implementation is proposed for both scenarios and it is
shown to have similar performance with the potential game when cooperation is
enforced, but with a higher variability across users. The no-regret learning
formulation is particularly useful to accommodate selfish users.
Non-cooperative learning games have the advantage of a very low overhead for
information exchange in the network.
  We show that cooperation based spectrum sharing etiquette improves the
overall network performance at the expense of an increased overhead required
for information exchange.

<id>
cs/0603032v3
<category>
cs.GT
<abstract>
We discuss bundle auctions within the framework of an integer allocation
problem. We show that for multi-unit auctions, of which bundle auctions are a
special case, market equilibrium and constrained market equilibrium are
equivalent concepts. This equivalence, allows us to obtain a computable
necessary and sufficient condition for the existence of constrained market
equilibrium for bundle auctions. We use this result to obtain a necessary and
sufficient condition for the existence of market equilibrium for multi-unit
auctions. After obtaining the induced bundle auction of a nonnegative TU game,
we show that the existence of market equilibrium implies the existence of a
possibly different market equilibrium as well, which corresponds very naturally
to an outcome in the matching core of the TU game. Consequently we show that
the matching core of the nonnegative TU game is non-empty if and only if the
induced market game has a market equilibrium.

<id>
cs/0606044v4
<category>
cs.GT
<abstract>
In {\em set-system auctions}, there are several overlapping teams of agents,
and a task that can be completed by any of these teams. The buyer's goal is to
hire a team and pay as little as possible. Recently, Karlin, Kempe and Tamir
introduced a new definition of {\em frugality ratio} for this setting.
Informally, the frugality ratio is the ratio of the total payment of a
mechanism to perceived fair cost. In this paper, we study this together with
alternative notions of fair cost, and how the resulting frugality ratios relate
to each other for various kinds of set systems.
  We propose a new truthful polynomial-time auction for the vertex cover
problem (where the feasible sets correspond to the vertex covers of a given
graph), based on the {\em local ratio} algorithm of Bar-Yehuda and Even. The
mechanism guarantees to find a winning set whose cost is at most twice the
optimal. In this situation, even though it is NP-hard to find a lowest-cost
feasible set, we show that {\em local optimality} of a solution can be used to
derive frugality bounds that are within a constant factor of best possible. To
prove this result, we use our alternative notions of frugality via a
bootstrapping technique, which may be of independent interest.

<id>
cs/0606127v1
<category>
cs.GT
<abstract>
We make three different types of contributions to cost-sharing: First, we
identify several new classes of combinatorial cost functions that admit
incentive-compatible mechanisms achieving both a constant-factor approximation
of budget-balance and a polylogarithmic approximation of the social cost
formulation of efficiency. Second, we prove a new, optimal lower bound on the
approximate efficiency of every budget-balanced Moulin mechanism for Steiner
tree or SSRoB cost functions. This lower bound exposes a latent approximation
hierarchy among different cost-sharing problems. Third, we show that weakening
the definition of incentive-compatibility to strategyproofness can permit
exponentially more efficient approximately budget-balanced mechanisms, in
particular for set cover cost-sharing problems.

<id>
cs/0607117v1
<category>
cs.GT
<abstract>
Many popular search engines run an auction to determine the placement of
advertisements next to search results. Current auctions at Google and Yahoo!
let advertisers specify a single amount as their bid in the auction. This bid
is interpreted as the maximum amount the advertiser is willing to pay per click
on its ad. When search queries arrive, the bids are used to rank the ads
linearly on the search result page. The advertisers pay for each user who
clicks on their ad, and the amount charged depends on the bids of all the
advertisers participating in the auction. In order to be effective, advertisers
seek to be as high on the list as their budget permits, subject to the market.
  We study the problem of ranking ads and associated pricing mechanisms when
the advertisers not only specify a bid, but additionally express their
preference for positions in the list of ads. In particular, we study "prefix
position auctions" where advertiser $i$ can specify that she is interested only
in the top $b_i$ positions.
  We present a simple allocation and pricing mechanism that generalizes the
desirable properties of current auctions that do not have position constraints.
In addition, we show that our auction has an "envy-free" or "symmetric" Nash
equilibrium with the same outcome in allocation and pricing as the well-known
truthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that this
equilibrium is the best such equilibrium for the advertisers in terms of the
profit made by each advertiser. We also discuss other position-based auctions.

<id>
cs/0608011v3
<category>
cs.GT
<abstract>
The rationalizability concept was introduced in \cite{Ber84} and
  \cite{Pea84} to assess what can be inferred by rational players in a
non-cooperative game in the presence of common knowledge. However, this notion
can be defined in a number of ways that differ in seemingly unimportant minor
details. We shed light on these differences, explain their impact, and clarify
for which games these definitions coincide. Then we apply the same analysis to
explain the differences and similarities between various ways the iterated
elimination of strictly dominated strategies was defined in the literature.
This allows us to clarify the results of \cite{DS02} and \cite{CLL05} and
improve upon them. We also consider the extension of these results to strict
dominance by a mixed strategy. Our approach is based on a general study of the
operators on complete lattices. We allow transfinite iterations of the
considered operators and clarify the need for them. The advantage of such a
general approach is that a number of results, including order independence for
some of the notions of rationalizability and strict dominance, come for free.

<id>
cs/0608120v2
<category>
cs.GT
<abstract>
Ordinal automata are used to model physical systems with Zeno behavior. Using
automata and games techniques we solve a control problem formulated and left
open by Demri and Nowak in 2005. It involves partial observability and a new
synchronization between the controller and the environment.

<id>
cs/0609017v1
<category>
cs.GT
<abstract>
We submitted two kinds of strategies to the iterated prisoner's dilemma (IPD)
competitions organized by Graham Kendall, Paul Darwen and Xin Yao in 2004 and
2005. Our strategies performed exceedingly well in both years. One type is an
intelligent and optimistic enhanced version of the well known TitForTat
strategy which we named OmegaTitForTat. It recognizes common behaviour patterns
and detects and recovers from repairable mutual defect deadlock situations,
otherwise behaving much like TitForTat. The second type consists of a set of
strategies working together as a team. These group strategies have one
distinguished individual Godfather strategy that plays OmegaTitForTat against
non-members while heavily profiting from the behaviour of the other members of
his group, the Hitman. The Hitman willingly let themselves being abused by
their Godfather while themselves lowering the scores of all other players as
much as possible, thus further maximizing the performance of their Godfather in
relation to other participants. The study of collusion in the simplified
framework of the iterated prisoner's dilemma allows us to draw parallels to
many common aspects of reality both in Nature as well as Human Society, and
therefore further extends the scope of the iterated prisoner's dilemma as a
metaphor for the study of cooperative behaviour in a new and natural direction.
We further provide evidence that it will be unavoidable that such group
strategies will dominate all future iterated prisoner's dilemma competitions as
they can be stealthy camouflaged as non-group strategies with arbitrary
subtlety. Moreover, we show that the general problem of recognizing stealth
colluding strategies is undecidable in the theoretical sense.

<id>
cs/0610026v1
<category>
cs.GT
<abstract>
We consider the machine covering problem for selfish related machines. For a
constant number of machines, m, we show a monotone polynomial time
approximation scheme (PTAS) with running time that is linear in the number of
jobs. It uses a new technique for reducing the number of jobs while remaining
close to the optimal solution. We also present an FPTAS for the classical
machine covering problem (the previous best result was a PTAS) and use this to
give a monotone FPTAS.
  Additionally, we give a monotone approximation algorithm with approximation
ratio \min(m,(2+\eps)s_1/s_m) where \eps>0 can be chosen arbitrarily small and
s_i is the (real) speed of machine i. Finally we give improved results for two
machines.
  Our paper presents the first results for this problem in the context of
selfish machines.

<id>
cs/0610166v1
<category>
cs.GT
<abstract>
We give a new simple proof of the decidability of the First Order Theory of
(omega^omega^i,+) and the Monadic Second Order Theory of (omega^i,<), improving
the complexity in both cases. Our algorithm is based on tree automata and a new
representation of (sets of) ordinals by (infinite) trees.

<id>
cs/9810003v1
<category>
cs.CV
<abstract>
This paper presents a multiscale decomposition algorithm. Unlike standard
wavelet transforms, the proposed operator is both linear and shift invariant.
The central idea is to obtain shift invariance by averaging the aligned wavelet
transform projections over all circular shifts of the signal. It is shown how
the same transform can be obtained by a linear filter bank.

<id>
cs/9810017v1
<category>
cs.CV
<abstract>
We give a systematic, abstract formulation of the image normalization method
as applied to a general group of image transformations, and then illustrate the
abstract analysis by applying it to the hierarchy of viewing transformations of
a planar object.

<id>
cs/9908017v1
<category>
cs.CV
<abstract>
This paper presents an invariant under scaling and linear brightness change.
The invariant is based on differentials and therefore is a local feature.
Rotationally invariant 2-d differential Gaussian operators up to third order
are proposed for the implementation of the invariant. The performance is
analyzed by simulating a camera zoom-out.

<id>
cs/0001024v1
<category>
cs.CV
<abstract>
We describe a simple, but efficient algorithm for the generation of dilated
contours from bilevel images. The initial part of the contour extraction is
explained to be a good candidate for parallel computer code generation. The
remainder of the algorithm is of linear nature.

<id>
cs/0003065v1
<category>
cs.CV
<abstract>
Fractal image compression, Culik's image compression and zerotree prediction
coding of wavelet image decomposition coefficients succeed only because typical
images being compressed possess a significant degree of self-similarity.
Besides the common concept, these methods turn out to be even more tightly
related, to the point of algorithmical reducibility of one technique to
another. The goal of the present paper is to demonstrate these relations.
  The paper offers a plain-term interpretation of Culik's image compression, in
regular image processing terms, without resorting to finite state machines and
similar lofty language. The interpretation is shown to be algorithmically
related to an IFS fractal image compression method: an IFS can be exactly
transformed into Culik's image code. Using this transformation, we will prove
that in a self-similar (part of an) image any zero wavelet coefficient is the
root of a zerotree, or its branch.
  The paper discusses the zerotree coding of (wavelet/projection) coefficients
as a common predictor/corrector, applied vertically through different layers of
a multiresolutional decomposition, rather than within the same view. This
interpretation leads to an insight into the evolution of image compression
techniques: from a causal single-layer prediction, to non-causal same-view
predictions (wavelet decomposition among others) and to a causal cross-layer
prediction (zero-trees, Culik's method).

<id>
cs/0003079v1
<category>
cs.CV
<abstract>
This paper presents invariants under gamma correction and similarity
transformations. The invariants are local features based on differentials which
are implemented using derivatives of the Gaussian. The use of the proposed
invariant representation is shown to yield improved correlation results in a
template matching scenario.

<id>
cs/0004012v1
<category>
cs.CV
<abstract>
This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences.

<id>
cs/0005001v1
<category>
cs.CV
<abstract>
The paper has established and verified the theory prevailing widely among
image and pattern recognition specialists that the bottom-up indirect regional
matching process is the more stable and the more robust than the global
matching process against concentrated types of noise represented by clutter,
outlier or occlusion in the imagery. We have demonstrated this by analyzing the
effect of concentrated noise on a typical decision making process of a
simplified two candidate voting model where our theorem establishes the lower
bounds to a critical breakdown point of election (or decision) result by the
bottom-up matching process are greater than the exact bound of the global
matching process implying that the former regional process is capable of
accommodating a higher level of noise than the latter global process before the
result of decision overturns. We present a convincing experimental verification
supporting not only the theory by a white-black flag recognition problem in the
presence of localized noise but also the validity of the conjecture by a facial
recognition problem that the theorem remains valid for other decision making
processes involving an important dimension-reducing transform such as principal
component analysis or a Gabor transform.

<id>
cs/0006001v1
<category>
cs.CV
<abstract>
A Bayesian classifier that up-weights the differences in the attribute values
is discussed. Using four popular datasets from the UCI repository, some
interesting features of the network are illustrated. The network is suitable
for classification problems.

<id>
cs/0006002v1
<category>
cs.CV
<abstract>
The difference-boosting algorithm is used on letters dataset from the UCI
repository to classify distorted raster images of English alphabets. In
contrast to rather complex networks, the difference-boosting is found to
produce comparable or better classification efficiency on this complex problem.

<id>
cs/0006047v1
<category>
cs.CV
<abstract>
We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials.

<id>
cs/0208005v1
<category>
cs.CV
<abstract>
The problem of searching for a model-based scene interpretation is analyzed
within a probabilistic framework. Object models are formulated as generative
models for range data of the scene. A new statistical criterion, the truncated
object probability, is introduced to infer an optimal sequence of object
hypotheses to be evaluated for their match to the data. The truncated
probability is partly determined by prior knowledge of the objects and partly
learned from data. Some experiments on sequence quality and object segmentation
and recognition from stereo data are presented. The article recovers classic
concepts from object recognition (grouping, geometric hashing, alignment) from
the probabilistic perspective and adds insight into the optimal ordering of
object hypotheses for evaluation. Moreover, it introduces point-relation
densities, a key component of the truncated probability, as statistical models
of local surface shape.

<id>
cs/0301001v1
<category>
cs.CV
<abstract>
We study theoretical and computational aspects of the least squares fit (LSF)
of circles and circular arcs. First we discuss the existence and uniqueness of
LSF and various parametrization schemes. Then we evaluate several popular
circle fitting algorithms and propose a new one that surpasses the existing
methods in reliability. We also discuss and compare direct (algebraic) circle
fits.

<id>
cs/0303015v1
<category>
cs.CV
<abstract>
We study the problem of fitting parametrized curves to noisy data. Under
certain assumptions (known as Cartesian and radial functional models), we
derive asymptotic expressions for the bias and the covariance matrix of the
parameter estimates. We also extend Kanatani's version of the Cramer-Rao lower
bound, which he proved for unbiased estimates only, to more general estimates
that include many popular algorithms (most notably, the orthogonal least
squares and algebraic fits). We then show that the gradient-weighted algebraic
fit is statistically efficient and describe all other statistically efficient
algebraic fits.

<id>
cs/0307045v1
<category>
cs.CV
<abstract>
Most algorithms in 3D computer vision rely on the pinhole camera model
because of its simplicity, whereas virtually all imaging devices introduce
certain amount of nonlinear distortion, where the radial distortion is the most
severe part. Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved. An application of the new radial distortion model is non-iterative
yellow line alignment with a calibrated camera on ODIS, a robot built in our
CSOIS.

<id>
cs/0307046v1
<category>
cs.CV
<abstract>
Common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
radial distortion model with an easy analytical undistortion formula, which
also belongs to the polynomial approximation category. Experimental results are
presented to show that with this radial distortion model, satisfactory accuracy
is achieved.

<id>
cs/0307047v1
<category>
cs.CV
<abstract>
The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
class of rational radial distortion models with easy analytical undistortion
formulae. Experimental results are presented to show that with this class of
rational radial distortion models, satisfactory and comparable accuracy is
achieved.

<id>
cs/0307051v1
<category>
cs.CV
<abstract>
The common approach to radial distortion is by the means of polynomial
approximation, which introduces distortion-specific parameters into the camera
model and requires estimation of these distortion parameters. The task of
estimating radial distortion is to find a radial distortion model that allows
easy undistortion as well as satisfactory accuracy. This paper presents a new
piecewise radial distortion model with easy analytical undistortion formula.
The motivation for seeking a piecewise radial distortion model is that, when a
camera is resulted in a low quality during manufacturing, the nonlinear radial
distortion can be complex. Using low order polynomials to approximate the
radial distortion might not be precise enough. On the other hand, higher order
polynomials suffer from the inverse problem. With the new piecewise radial
distortion function, more flexibility is obtained and the radial undistortion
can be performed analytically. Experimental results are presented to show that
with this new piecewise radial distortion model, better performance is achieved
than that using the single function. Furthermore, a comparable performance with
the conventional polynomial model using 2 coefficients can also be
accomplished.

<id>
cs/0307072v1
<category>
cs.CV
<abstract>
The task of camera calibration is to estimate the intrinsic and extrinsic
parameters of a camera model. Though there are some restricted techniques to
infer the 3-D information about the scene from uncalibrated cameras, effective
camera calibration procedures will open up the possibility of using a wide
range of existing algorithms for 3-D reconstruction and recognition.
  The applications of camera calibration include vision-based metrology, robust
visual platooning and visual docking of mobile robots where the depth
information is important.

<id>
cs/0308003v1
<category>
cs.CV
<abstract>
The commonly used radial distortion model for camera calibration is in fact
an assumption or a restriction. In practice, camera distortion could happen in
a general geometrical manner that is not limited to the radial sense. This
paper proposes a simplified geometrical distortion modeling method by using two
different radial distortion functions in the two image axes. A family of
simplified geometric distortion models is proposed, which are either simple
polynomials or the rational functions of polynomials. Analytical geometric
undistortion is possible using two of the distortion functions discussed in
this paper and their performance can be improved by applying a piecewise
fitting idea. Our experimental results show that the geometrical distortion
models always perform better than their radial distortion counterparts.
Furthermore, the proposed geometric modeling method is more appropriate for
cameras whose distortion is not perfectly radially symmetric around the center
of distortion.

<id>
cs/0308034v1
<category>
cs.CV
<abstract>
In the paper will be presented a safety and security system based on
fingerprint technology. The results suggest a new scenario where the new cars
can use a fingerprint sensor integrated in car handle to allow access and in
the dashboard as starter button.

<id>
cs/0308035v1
<category>
cs.CV
<abstract>
In the paper will be presented a safety system based on iridology. The
results suggest a new scenario where the security problem in supervised and
unsupervised areas can be treat with the present system and the iris image
recognition.

<id>
cs/0401017v2
<category>
cs.CV
<abstract>
For many tracking and surveillance applications, background subtraction
provides an effective means of segmenting objects moving in front of a static
background. Researchers have traditionally used combinations of morphological
operations to remove the noise inherent in the background-subtracted result.
Such techniques can effectively isolate foreground objects, but tend to lose
fidelity around the borders of the segmentation, especially for noisy input.
This paper explores the use of a minimum graph cut algorithm to segment the
foreground, resulting in qualitatively and quantitiatively cleaner
segmentations. Experiments on both artificial and real data show that the
graph-based method reduces the error around segmented foreground objects. A
MATLAB code implementation is available at
http://www.cs.smith.edu/~nhowe/research/code/#fgseg

<id>
cs/0401018v1
<category>
cs.CV
<abstract>
A method of temporal factor prognosis of TE (tick-borne encephalitis)
infection has been developed. The high precision of the prognosis results for a
number of geographical regions of Primorsky Krai has been achieved. The method
can be applied not only to epidemiological research but also to others.

<id>
cs/0402020v1
<category>
cs.CV
<abstract>
Despite encouraging recent progresses in ensemble approaches, classification
methods seem to have reached a plateau in development. Further advances depend
on a better understanding of geometrical and topological characteristics of
point sets in high-dimensional spaces, the preservation of such characteristics
under feature transformations and sampling processes, and their interaction
with geometrical models used in classifiers. We discuss an attempt to measure
such properties from data sets and relate them to classifier accuracies.

<id>
cs/0405093v2
<category>
cs.CV
<abstract>
This publication presents methods for face detection, analysis and
recognition: fast normalized cross-correlation (fast correlation coefficient)
between multiple templates based face pre-detection method, method for
detection of exact face contour based on snakes and Generalized Gradient Vector
Flow field, method for combining recognition algorithms based on Cumulative
Match Characteristics in order to increase recognition speed and accuracy, and
face recognition method based on Principal Component Analysis of the Wavelet
Packet Decomposition allowing to use PCA - based recognition method with large
number of training images. For all the methods are presented experimental
results and comparisons of speed and accuracy with large face databases.

<id>
cs/0405095v1
<category>
cs.CV
<abstract>
This paper presents a blind detection and compensation technique for camera
lens geometric distortions. The lens distortion introduces higher-order
correlations in the frequency domain and in turn it can be detected using
higher-order spectral analysis tools without assuming any specific calibration
target. The existing blind lens distortion removal method only considered a
single-coefficient radial distortion model. In this paper, two coefficients are
considered to model approximately the geometric distortion. All the models
considered have analytical closed-form inverse formulae.

<id>
cs/0406008v1
<category>
cs.CV
<abstract>
We study image compression by a separable wavelet basis
$\big\{\psi(2^{k_1}x-i)\psi(2^{k_2}y-j),$ $\phi(x-i)\psi(2^{k_2}y-j),$
$\psi(2^{k_1}(x-i)\phi(y-j),$ $\phi(x-i)\phi(y-i)\big\},$ where $k_1, k_2 \in
\mathbb{Z}_+$; $i,j\in\mathbb{Z}$; and $\phi,\psi$ are elements of a standard
biorthogonal wavelet basis in $L_2(\mathbb{R})$. Because $k_1\ne k_2$, the
supports of the basis elements are rectangles, and the corresponding transform
is known as the {\em rectangular wavelet transform}. We prove that if
one-dimensional wavelet basis has $M$ dual vanishing moments then the rate of
approximation by $N$ coefficients of rectangular wavelet transform is
$\mathcal{O}(N^{-M}\log^C N)$ for functions with mixed derivative of order $M$
in each direction.
  The square wavelet transform yields the approximation rate is
$\mathcal{O}(N^{-M/2})$ for functions with all derivatives of the total order
$M$. Thus, the rectangular wavelet transform can outperform the square one if
an image has a mixed derivative. We provide experimental comparison of image
compression which shows that rectangular wavelet transform outperform the
square one.

<id>
cs/0502095v2
<category>
cs.CV
<abstract>
The Gradient Vector Flow (GVF) is a vector diffusion approach based on
Partial Differential Equations (PDEs). This method has been applied together
with snake models for boundary extraction medical images segmentation. The key
idea is to use a diffusion-reaction PDE to generate a new external force field
that makes snake models less sensitivity to initialization as well as improves
the snake's ability to move into boundary concavities. In this paper, we
firstly review basic results about convergence and numerical analysis of usual
GVF schemes. We point out that GVF presents numerical problems due to
discontinuities image intensity. This point is considered from a practical
viewpoint from which the GVF parameters must follow a relationship in order to
improve numerical convergence. Besides, we present an analytical analysis of
the GVF dependency from the parameters values. Also, we observe that the method
can be used for multiply connected domains by just imposing the suitable
boundary condition. In the experimental results we verify these theoretical
points and demonstrate the utility of GVF on a segmentation approach that we
have developed based on snakes.

<id>
cs/0505006v1
<category>
cs.CV
<abstract>
Image information content is known to be a complicated and controvercial
problem. This paper posits a new image information content definition.
Following the theory of Solomonoff-Kolmogorov-Chaitin's complexity, we define
image information content as a set of descriptions of imafe data structures.
Three levels of such description can be generally distinguished: 1)the global
level, where the coarse structure of the entire scene is initially outlined; 2)
the intermediate level, where structures of separate, non-overlapping image
regions usually associated with individual scene objects are deliniated; and 3)
the low-level description, where local image structures observed in a limited
and restricted field of view are resolved. A technique for creating such image
information content descriptors is developed. Its algorithm is presented and
elucidated with some examples, which demonstrate the effectiveness of the
proposed approach.

<id>
cs/9809018v1
<category>
cs.CY
<abstract>
Technology designers often strive to design systems that are flexible enough
to be used in a wide range of situations. Software engineers, in particular,
are trained to seek general solutions to problems. General solutions can be
used not only to address the problem at hand, but also to address a wide range
of problems that the designers may not have even anticipated. Sometimes
designers wish to provide general solutions, while encouraging certain uses of
their technology and discouraging or precluding others. They may attempt to
influence the use of technology by ``hard-wiring'' it so that it only can be
used in certain ways, licensing it so that those who use it are legally
obligated to use it in certain ways, issuing guidelines for how it should be
used, or providing resources that make it easier to use the technology as the
designers intended than to use it in any other way.
  This paper examines several cases where designers have attempted to influence
the use of technology through one of these mechanisms. Such cases include key
recovery encryption, Pegasus Mail, Platform for Internet Content Selection
(PICS) Guidelines, Java, Platform for Privacy Preferences Project (P3P)
Implementation Guide, Apple's style guidelines, and Microsoft Foundation
Classes. In some of these cases, the designers sought to influence the use of
technology for competitive reasons or in order to promote standardization or
interoperability. However, in other cases designers were motivated by
policy-related goals such as protecting privacy or free speech. As new
technologies are introduced with the express purpose of advancing
policy-related goals (for example, PICS and P3P), it is especially important to
understand the roles designers might play in influencing the use of technology.

<id>
cs/9903013v1
<category>
cs.CY
<abstract>
In this work the impact of the Internet culture on standard mainstream
societies has been analyzed. After analytically establishing the fact that the
Net can be viewed as a pan-societal superstructure which supports its own
distinct culture, an ethnographic analysis is provided to find out the key
aspects of this culture. The elements of this culture which have an empowering
impacts on the standard mainstream societies, as well as the elements in it
which can cause discouraging social effects are then discussed by a global
investigation of the present status of various fundamental aspects (e,g,
education, economics, politics, entertainment etc) of the mainstream societies
as well as their links with the Net culture. Though immensely potential for
providing various prominent positive impacts, the key findings of this work
indicate that misuse of Internet can create tremendous harm to the members of
the mainstream societies by generating a set of morally crippled people as well
as a future generation completely void of principles and ethics. This
structured diagnostic approach to the social problems caused by the manhandling
of Internet leads to a concrete effort of providing the measures that can be
taken to enhance or to overcome the supporting and limiting effects of the Net
culture respectively with the intent to benefit our society and to protect the
teratoidation of certain ethical values.

<id>
cs/9909008v2
<category>
cs.CY
<abstract>
The shift towards the use of electronic media in scholarly communication
appears to be an inescapable imperative. However, these shifts are uneven, both
with respect to field and with respect to the form of communication. Different
scientific fields have developed and use distinctly different communicative
forums, both in the paper and electronic arenas, and these forums play
different communicative roles within the field. One common claim is that we are
in the early stages of an electronic revolution, that it is only a matter of
time before other fields catch up with the early adopters, and that all fields
converge on a stable set of electronic forums. A social shaping of technology
(SST) perspective helps us to identify important social forces centered around
disciplinary constructions of trust and of legitimate communication that pull
against convergence. This analysis concludes that communicative plurality and
communicative heterogeneity are durable features of the scholarly landscape,
and that we are likely to see field differences in the use of and meaning
ascribed to communications forums persist, even as overall use of electronic
communications technologies both in science and in society as a whole
increases.

<id>
cs/0001011v1
<category>
cs.CY
<abstract>
A variety of tools have been introduced recently that are designed to help
people protect their privacy on the Internet. These tools perform many
different functions in-cluding encrypting and/or anonymizing communications,
preventing the use of persistent identifiers such as cookies, automatically
fetching and analyzing web site privacy policies, and displaying
privacy-related information to users. This paper discusses the set of privacy
tools that aim specifically at facilitating notice and choice about Web site
data practices. While these tools may also have components that perform other
functions such as encryption, or they may be able to work in conjunction with
other privacy tools, the primary pur-pose of these tools is to help make users
aware of web site privacy practices and to make it easier for users to make
informed choices about when to provide data to web sites. Examples of such
tools include the Platform for Privacy Preferences (P3P) and various
infomediary services.

<id>
cs/0005007v1
<category>
cs.CY
<abstract>
Collaboratories refer to laboratories where scientists can work together
while they are in distant locations from each other and from key equipment.
They have captured the interest both of CSCW researchers and of science funders
who wish to optimize the use of rare scientific equipment and expertise. We
examine the kind of CSCW conceptions that help us best understand the character
of working relationships in these scientific collaboratories. Our model,
inspired by actor-network theory, considers technologies as Socio-technical
Interaction Networks (STINs). This model provides a rich understanding of the
scientific collaboratories, and also a more complete understanding of the
conditions and activities that support collaborative work in them. We
illustrate the significance of STIN models with several cases drawn from the
fields of high energy physics and materials science.

<id>
cs/0108014v1
<category>
cs.CY
<abstract>
This paper examines the effect of ownership concentration on product
position, product variety and readership in markets for daily newspapers. US
antitrust policy presumes that mergers reduce the amount and diversity of
content available to consumers. However, the effects of consolidation in
differentiated product markets cannot be determined solely from theory. Because
multi-product firms internalize business stealing, mergers may encourage firms
to reposition products, leading to more, not less, variety. Using data on
reporter assignments from 1993-1999, results show that differentiation and
variety increase with concentration. Moreover, there is evidence that
additional variety increases readership, suggesting that concentration benefits
consumers.

<id>
cs/0108015v1
<category>
cs.CY
<abstract>
Recent trends reveal the search by companies for a legal hook to prevent the
undesired and uncontributorized copying of information posted on websites. In the
center of this controversy are metasites, websites that display prices for a
variety of vendors. Metasites function by implementing shopbots, which extract
pricing data from other vendors' websites. Technological mechanisms have proved
unsuccessful in blocking shopbots, and in response, websites have asserted a
variety of legal claims. Two recent cases, which rely on the troublesome
trespass to chattels doctrine, suggest that contract law may provide a less
demanding legal method of preventing the search of websites by data robots. If
blocking collection of pricing data is as simple as posting an online contract,
the question arises whether this end result is desirable and legally viable.

<id>
cs/0109007v2
<category>
cs.CY
<abstract>
The popular conception is that data traffic nearly, if not already, exceeds
voice traffic on backbone networks. However, the results of research reported
in this paper imply that voice traffic greatly exceeds data traffic when real
users are asked to estimate their usage of a wide variety of media. Media usage
was surveyed for students in New York City and in Los Angeles. Other than
significant differences in radio listening, e-mails, and downloads, the usage
was quite similar. Telephone usage (wired and wireless) was nearly an hour per
day. When converted to bits, the telephone traffic was much greater than the
data traffic over the Internet. This paper reports on the details of the two
user studies. The traffic implications of the results are estimated. The
finding that voice exceeds data will then be reconciled with the popular
opposite conception.

<id>
cs/0109008v1
<category>
cs.CY
<abstract>
While the 1996 Telecommunications Act requires all incumbent local telephone
companies to cooperate with local entrants, section 271 of the Act provides the
Bell companies (but not GTE) additional incentives to cooperate. Using an
original data set, I compare the negotiations of AT&T, as a local entrant, with
GTE and with the Bell companies in states where both operate. My results
suggest that the differential incentives matter: The Bells accommodate entry
more than does GTE, as evidenced in quicker agreements, less litigation, and
more favorable prices offered for network access. Consistent with this, there
is more entry into Bell territories

<id>
cs/0109009v1
<category>
cs.CY
<abstract>
Our goal is to distinguish between the following two hypotheses: (A) The
Internet will remain disproportionately in English and will, over time, cause
more people to learn English as second language and thus solidify the role of
English as a global language. This outcome will prevail even though there are
more native Chinese and Spanish speakers than there are native English
speakers. (B) As the Internet matures, it will more accurately reflect the
native languages spoken around the world (perhaps weighted by purchasing power)
and will not promote English as a global language.
  English's "early lead" on the web is more likely to persist if those who are
not native English speakers frequently access the large number of English
language web sites that are currently available. In that case, many existing
web sites will have little incentive to develop non-English versions of their
sites, and new sites will tend to gravitate towards English. The key empirical
question, therefore, is whether individuals whose native language is not
English use the Web, or certain types of Web sites, less than do native English
speakers. In order to examine this issue empirically, we employ a unique data
set on Internet use at the individual level in Canada from Media Metrix. Canada
provides an ideal setting to examine this issue because English is one of the
two official languages.
  Our preliminary results suggest that English web sites are not a barrier to
Internet use for French-speaking Quebecois. These preliminary results are
consistent with the scenario in which the Internet will promote English as a
global language.

<id>
cs/0109012v1
<category>
cs.CY
<abstract>
The unique challenge presented by the Internet is that compliance with local
laws is rarely sufficient to assure a business that it has limited its exposure
to legal risk. The paper identifies why the challenge of adequately accounting
for the legal risk arising from Internet jurisdiction has been aggravated in
recent years by the adoption of the Zippo legal framework, commonly referred to
as the passive versus active test. The test provides parties with only limited
guidance and often results in detrimental judicial decisions from a policy
perspective. Given the inadequacies of the Zippo passive versus active test,
the paper argues that it is now fitting to identify a more effective standard
for determining when it is appropriate to assert jurisdiction in cases
involving predominantly Internet-based contacts. The solution submitted in the
paper is to move toward a targeting-based analysis. Unlike the Zippo approach,
a targeting analysis would seek to identify the intentions of the parties and
to assess the steps taken to either enter or avoid a particular jurisdiction.
Targeting would also lessen the reliance on effects-based analysis, the source
of considerable uncertainty since Internet-based activity can ordinarily be
said to create some effects in most jurisdictions. To identify the appropriate
criteria for a targeting test, the paper recommends returning to the core
jurisdictional principle -- foreseeability. Foreseeability in the targeting
context depends on three factors -- contracts, technology, and actual or
implied knowledge.

<id>
cs/0109016v1
<category>
cs.CY
<abstract>
As far as many consumers and businessmen and women are concerned,
increasingly wireline and wireless services, including those provided by
terrestrial and satellite systems, are considered to be substitutes and
sometimes complements, regardless of the laws and regulations applicable to
them. At the same time, many writers and even government agencies (such as the
FCC) have suggested that users of the spectrum should be given more
property-like rights in the use of the spectrum and at a minimum should be
given much more flexibility in how they may use the spectrum. Two recent
developments have important implications with respect to "convergence,"
spectrum property rights and flexible use of the spectrum. The first
development involves several proposals to provide terrestrial wireless services
within spectrum in use or planned to be used to provide satellite services. The
second development is the passage of the 2000 ORBIT Act which specifically
forbids the use of license auctions to select among mutually exclusive
applicants to provide international or global satellite communications service.
The purpose of this paper is to discuss some of the questions raised by these
two events, but not necessarily to provide definitive answers or solutions.

<id>
cs/0109021v2
<category>
cs.CY
<abstract>
The Internet Domain Name System (DNS) is a hierarchical name space that
enables the assignment of unique, mnemonic identifiers to Internet hosts and
the consistent mapping of these names to IP addresses. The root of the domain
name system is the top of the hierarchy and is currently managed by a
quasi-private centralized regulatory contributority, the Internet Corporation for
Assigned Names and Numbers (ICANN). This paper identifies and discusses the
economic and policy issues raised by competing DNS roots. The paper provides a
precise definition of root-competition and shows that multiple roots are a
species of standards competition, in which network externalities play a major
role. The paper performs a structural analysis of the different forms that
competing DNS roots can take and their effects on end-user compatibility. It
then explores the policy implications of the various forms of competition.
  The thesis of the paper is that root competition is caused by a severe
disjunction between the demand for and supply of top-level domain names. ICANN
has contributorized a tiny number of new top-level domains (7) and subjected their
operators to excruciatingly slow and expensive contractual negotiations. The
growth of alternate DNS roots is an attempt to bypass that bottleneck. The
paper arrives at the policy conclusion that competition among DNS roots should
be permitted and is a healthy outlet for inefficiency or abuses of power by the
dominant root administrator.

<id>
cs/0109026v1
<category>
cs.CY
<abstract>
Drawing on perspectives from telecommunications policy and neo-Gramscian
understandings of international political economy, this paper offers an
explanation and analysis of the shifting patterns of regulation which have been
evident in the telecommunications sector in recent years. It aims to illustrate
explain and explore the implications of the movement of regulatory sovereignty
away from the nation-state, through regional conduits, to global organisations
in the crystallisation of a world system of telecommunications governance.
  Our central argument is that telecommunications governance has evolved from a
regulatory arena characterised, in large part, by national diversity, to one
wherein a more convergent global multilayered system is emerging. We suggest
that the epicentre of this regulatory system is the relatively new World Trade
Organisation (WTO). Working in concert with the WTO are existing
well-established nodes regulation. In further complement, we see regional
regulatory projects, notably the European Union (EU), as important conduits and
nodes of regulation in the consolidation of a global regulatory regime.
  By way of procedure, we first explore the utility of a neo-Gramscian approach
for understanding the development of global regulatory frameworks. Second, we
survey something of the recent history - and, in extension, conventional wisdom
- of telecommunications regulation at national and regional levels. Third, we
demonstrate how a multilayered system of global telecommunications regulation
has emerged centred around the regulatory contributority of the WTO. Finally, we
offer our concluding comments.

<id>
cs/0109032v1
<category>
cs.CY
<abstract>
Our research, which began fielding surveys in 1995, and which have been
repeated with variation in 1996, 1997 and 2000, was apparently the first to use
national random telephone survey methods to track social and community aspects
of Internet use, and to compare users and non-users. It also seems to be among
the first that used these methods to compare users with non-users in regards to
communication, social and community issues. The work has been largely supported
by grants from the Markle Foundation of New York City as well as the Robert
Wood Johnson Foundation.
  Abridged, see full text for complete abstract.

<id>
cs/0109035v1
<category>
cs.CY
<abstract>
A dichotomy in regulatory treatment and corporate cultures exists between
Internet Service Providers (ISPs) and telecommunication carriers. Telephone
company executives (Bell Heads) may resent regulation, but they accept their
fate and work creatively to exploit anomalies and opportunities to secure a
regulation-conferred competitive advantage. Most ISP executives (Net Heads)
appear to embrace a libertarian attitude, strongly opposing any government
involvement. Despite the clash of cultures, the telecommunications and Internet
worlds have merged. Such convergence jeopardizes the ability of Net Heads to
avoid some degree of regulation, particularly when they offer services
functionally equivalent to what their Bell Head counterparts offer.
  This paper will assess the regulatory consequences when telecommunication and
Internet services converge in the marketplace and in terms of operating
technologies. The paper identifies commercial developments in the Internet to
support the view that the Internet has become more hierarchical and more like
telecommunication networks. The paper concludes that telecommunication carriers
will display superior skill in working the regulatory process to their
advantage. The paper suggests that Bell Heads will outmaneuver Net Heads
particularly when the revenue siphoning effect of Internet-mediated services
offsets the revenues generated from ISP leases of telecommunication
transmission capacity.

<id>
cs/0109036v2
<category>
cs.CY
<abstract>
This paper examines the relationship between changes in telecommunications
provider concentration on international long distance routes and changes in
prices on those routes. Overall, decreased concentration is associated with
significantly lower prices to consumers of long distance services. However, the
relationship between concentration and price varies according to the type of
long distance plan considered. For the international flagship plans frequently
selected by more price-conscious consumers of international long distance,
increased competition on a route is associated with lower prices. In contrast,
for the basic international plans that are the default selection for consumers,
increased competition on a route is actually associated with higher prices.
Thus, somewhat surprisingly, price dispersion appears to increase as
competition increases.

<id>
cs/0109037v1
<category>
cs.CY
<abstract>
Standard-setting organizations (SSOs) regularly encounter situations in which
one or more companies claim to own proprietary rights that cover a proposed
industry standard. The industry cannot adopt the standard without the
permission of the intellectual property owner (or owners).
  How SSOs respond to those who assert intellectual property rights is
critically important. Whether or not private companies retain intellectual
property rights in group standards will determine whether a standard is "open"
or "closed." It will determine who can sell compliant products, and it may well
influence whether the standard adopted in the market is one chosen by a group
or one offered by a single company. SSO rules governing intellectual property
rights will also affect how standards change as technology improves.
  Given the importance of SSO rules governing intellectual property rights,
there has been surprisingly little treatment of SSOs or their intellectual
property rules in the legal literature. My aim in this article is to fill that
void. To do so, I have surveyed the intellectual property policies of dozens of
SSOs, primarily but not exclusively in the computer networking and
telecommunications industries.

<id>
cs/0109038v1
<category>
cs.CY
<abstract>
Successful achievement of public policies requires satisfaction of conditions
affecting political feasibility for policy adoption and maintenance as well as
economic viability of the desired activity or enterprise. This paper discusses
the difficulties of satisfying these joint constraints given the legacy of the
common law doctrines of "just price" and "businesses affected with a public
interest." In this regard, it is helpful to view traditional public utility
regulation as a form of welfare state regulation, as it suffers from similar
political problems from policy retrenchment. The retrenchment problems are
examined in the context of the electricity crisis in California as well as the
passage and implementation of the Telecommunications Act of 1996. As expected,
retrenchment from low residential retail rates - the most universalistic
benefit for customers - faces the greatest political resistance. The societal
trade-offs between monopoly and competition must be reexamined in light of the
greater instability and political difficulties under a deregulatory regime.

<id>
cs/0109043v1
<category>
cs.CY
<abstract>
In the pre-divestiture era, the regulatory environment in the U.S. was fairly
uniform and harmonious with the FCC setting the course and the accommodative
state PUCs making corresponding changes in their own policies. The divestiture
fractured this monolithic system as it forced the PUCs to respond to new forces
unleashed in their own backyards. Soon there was great diversity in the overall
regulatory landscape. Within this new environment, there is considerable
disparity among the PUCs in terms of their ability to implement new ideas. This
paper seeks to understand the structural factors that influence the latitude of
regulatory action by PUCs via a comparative study of local telephone
competition policy making in Arkansas and New York. The analysis suggests that
the presence or absence of countervailing forces determines the relative
autonomy the PUCs enjoy and thereby their ability to introduce new ideas into
their states.

<id>
cs/0109044v1
<category>
cs.CY
<abstract>
ENUM creates many new market opportunities and raises several important
policy issues related to the implementation and administration of the ENUM
database and services. Recent World Telecommunications Policy Forum 2001 dealt
with the emergence of ENUM as an important numbering issue of IP telephony.
This paper prepares some important emerging issues of ENUM administration and
policy by taking an empirical research approach from the bottom up.
  We will identify potential key ENUM services, and estimating the size of the
service market opportunities created by the availability of PSTN-IP addressing
and mapping mechanisms, particularly in the context of IP telephony. Also, we
analyze the possible administrative models and relationship scenarios among
different ENUM players such as Registry(ies), Registrars, Telephone Service
Providers, ENUM Application Service Providers, etc. Then, we will assess the
effects of various administrative model architectures of ENUM service by
looking at the market opportunities and motivations of the players. From the
empirical findings, we will draw the implications on transactions among
different kinds of ENUM service providers. Finally, the results of the model
analysis will be used for the discussion of policy related issues around the
ENUM and IP telephony services.
  Keywords: IP Telephony, ENUM, Internet Policy, Numbering and Addressing
System, Service and Market Study, Administration Model, Empirical Market Study.

<id>
cs/0109045v1
<category>
cs.CY
<abstract>
Focusing on the telecom manufacturing industry in China, this paper contends
that the existing literature needs to be expanded in order to explain the
Chinese case. First, product cycle theory could be applied to explain
multinational corporations' strategies of importing and localizing their
products in China in order to take advantage of lower labor costs and often
more significantly to break barriers to the Chinese market. Second, there are
no significant indicators pointing to local multinational subsidiaries and
indigenous manufacturers serving as a substantial part of the cross-national
production networks in the global telecom industry yet, although there are some
signs of potential development. Third, the success of "Wintelism" and the
maturity of cross-national production networks in the global market have had
significant impacts on China's indigenous industry.

<id>
cs/0109046v1
<category>
cs.CY
<abstract>
While traditional radio stations are subject to extensive government
regulations, Internet radio stations remain largely unregulated. As Internet
radio usage has increased certain stakeholders have begun to argue that these
Internet radio broadcasters are providing significant and diverse programming
to American audiences and that government regulation of spectrum-using radio
station ownership may be further relaxed.
  One of the primary justifications for regulation of ownership has been to
protect diversity in broadcasting. This study hypothesizes that Internet radio
broadcasting does add diversity to the radio broadcasting industry and that it
should be considered as relevant by regulators.
  This study evaluates the role of Internet radio broadcasters according to
five criteria intended to gauge the level of diversity being delivered to
listeners online. By measuring the levels of format, channel, ownership,
location and language diversity among Internet radio stations, it is possible
to draw benchmark lessons about the new medium's ability to provide Americans
with diverse broadcasting options.
  The study finds that Internet radio broadcasters are in fact adding
measurable diversity to the radio broadcasting industry. Internet broadcasters
are providing audiences with access to an increasing number of stations,
owners, formats, and language choices, and it is likely that technologies
aiding in the mobility of access as well as broadband evolution will reinforce
these findings.

<id>
cs/0109047v1
<category>
cs.CY
<abstract>
South Africa adopted the GATS Basic Agreement on Telecommunications and the
regulatory principles in 1998. Obligations undertaken by South Africa mirrored
the framework for the gradual telecommunications reform process that was begun
in 1996. In the light of two threatened actions for anti-competitive practices
in violation of the Agreement, this paper reviews the nature of the commitments
undertaken by South Africa and assesses the country's compliance to date. This
paper also seeks to explore the tension that arises between domestic policy
reforms and international trade aspirations. It is argued that the dynamic
produced through this tension affords domestic governments a mechanism with
which to balance the seemingly opposing goals of competition and development.
It is further argued that the broad regulatory principles, adopted by all
signatories and often criticized for lack of precision, facilitate this fine
balancing and affords domestic governments an opportunity to advance sovereign
concerns while pursuing international trade ideals.

<id>
cs/0109048v1
<category>
cs.CY
<abstract>
In asserting a competitive market environment as a justification for
regulatory forbearance, the Telecommunications Act of 1996 finally articulated
a clear standard for the FCC's public interest standard, one of the most
protean concepts in communications. This seeming clarity has not, however,
inhibited intense political conflict over the term. This paper examines public
and regulatory debate over the AOL Time Warner merger as an example of the way
in which the linkage between competitions and commons policy becomes relevant
to communications policy, particularly in relation to mass media, and discusses
interpretations of the public interest in the current FCC. The paper proposes
that the Telecom Act's goal of fostering economic competition among information
service providers, and the democratic ideal of nurturing public relationships
and behaviors can be linked. Competition policy that creates the opportunity
for untrammeled interactivity also provides a sine qua non to nurture the
social phenomenon of the commons. The linked concepts of competition and
commons could also provide useful ways to interpret the public interest in
policy arenas as spectrum allocation and intellectual property.

<id>
cs/0109049v1
<category>
cs.CY
<abstract>
Many people expect the Internet to change American politics, most likely in
the direction of increasing direct citizen participation and forcing government
officials to respond more quickly to voter concerns. A recent California
initiative with these objectives would contributorize use of encrypted digital
signatures over the Internet to qualify candidates, initiatives, and other
ballot measures. Proponents of Internet signature gathering say it will
significantly lower the cost of qualifying initiatives and thereby reduce the
influence of organized, well-financed interest groups. They also believe it
will increase both public participation in the political process and public
understanding about specific measures. However, opponents question whether
Internet security is adequate to prevent widespread abuse and argue that the
measure would create disadvantages for those who lack access to the Internet.
Beyond issues of security, cost, and access lie larger questions about the
effects of Internet signature gathering on direct democracy. Would it encourage
greater and more informed public participation in the political process? Or
would it flood voters with ballot measures and generally worsen current
problems with the initiative process itself? Because we lack good data on these
questions, answers to them today are largely conjectural. We can be fairly
sure, however, that Internet petition signing, like Internet voting, will have
unintended consequences.

<id>
cs/0109050v1
<category>
cs.CY
<abstract>
A critical element of most national telecom policy objectives is advancing
universal service. In a multi-operator context, this is usually operationalized
through Universal Service Obligations (USO), by which various operators are
mandated to provide a part of their services to rural areas or to high cost to
serve customers at "affordable" prices.
  This paper highlights the various issues in USO from a developing country
perspective. The first part of this paper gives an overview of USO practices
and issues. The second part reviews the Telecom Regulatory Authority of India's
recommendations on USO cost estimation. In the third part, the paper analyzes
characteristics of rural exchanges with a view to evolve a framework for
assessing USO. This framework is applicable to developing countries as the
study carried out in this paper is in the context of a developing country
characterized by low telecom penetration and non-availability of data with
regulators.

<id>
cs/0109051v1
<category>
cs.CY
<abstract>
Internet technology should eventually provide important improvements over
established media not only in the efficiency of broadband delivery, and of
particularimportance, in the efficiency of business models that can be used to
collect money for that programming. I identify five economic characteristics of
Internet technology that should lead to these greater efficiencies: (1) lower
delivery costs and reduced capacity constraints, (2) more efficient
interactivity, (3) more efficient advertising and sponsorship, (4) more
efficient direct pricing and bundling, and (5) lower costs of copying and
sharing.
  The most successful Internet TV business models are likely to involve
syndication to or from other media, and also international distribution. In the
broader context, Internet TV is another syndication outlet by which program
suppliers can segment their overall markets and thus support higher production
investments. Many innovative and more sharply focused programs will surely
prosper on Internet TV, but the attractiveness to audiences of high production
value programming will tend to advantage broad appeal programming, such as
Hollywood movies. Historical evidence about the performance of cable television
and videocassettes is presented to support these points.

<id>
cs/0109052v1
<category>
cs.CY
<abstract>
This paper develops a theoretical perspective on globalization and the
Information Society and combines it with a critical usage of international
regime theory as a heuristic for understanding the current historical period of
transition from an international telecommunicaitons regime (Cowhey, 1990; 1994)
to a new and complex regime aimed at providing governance for the Global
Information Infrastructure and Global Information Society (GII/GIS). In
analyzing the principles, values, norms, rules, collective decision-making
prdocedures, and enforcement mechanisms of the emergent GII/GIS regime, this
paper differentiates between three regime levels: (1) Macro-Regime--global; (2)
Mezzo-Regime--regional and sub-regional; and (3) Micro-Regime--national. The
paper employs a case-study approach to explore some of the specific national
responses (i.e. South Africa) to this regime transition, with an analysis of
potential best practices and lessons learned for other emerging economies. Key
findings in this paper are: (1) that a range of social, political, economic,
and technological factors are eroding the existing international
telecommunications regime (e.g., VOIP;, call-back, VSATs, accounting rate
restructuring, pressure for applicaitons development, and SMMEs); (2) a new
regime for global information and communicaitons policy is emerging, but is
being driven not by the broad possibilities of the Information Society, but by
the more specific interests of global and multi-national corporations related
to global e-commerce; (3) numerous strategic responses have been developed at
national, subregional, and regional levels to the challenges of this transition
in both developed and developing regions; and (4) without a collaborative
response, the developing world will be further marginalized by this new regime.

<id>
cs/0109053v1
<category>
cs.CY
<abstract>
Consumers value keeping some information about them private from potential
marketers. E-commerce dramatically increases the potential for marketers to
accumulate otherwise private information about potential customers. Online
marketers claim that this information enables them to better market their
products. Policy makers are currently drafting rules to regulate the way in
which these marketers can collect, store, and share this information. However,
there is little evidence yet either of consumers' valuation of their privacy or
of the benefits they might reap through better target marketing. We provide a
framework for measuring a portion of the benefits from allowing marketers to
make better use of consumer information. Target marketing is likely to reduce
consumer search costs, improve consumer product selection decisions, and lower
the marketing costs of goods sold. Our model allows us to estimate the value to
consumers of only the latter, price reductions from more efficient marketing.

<id>
cs/9809031v1
<category>
cs.CR
<abstract>
We investigate, in the Shannon model, the security of constructions
corresponding to double and (two-key) triple DES. That is, we consider
F_{k1}(F_{k2}(.)) and F_{k1}(F_{k2}^{-1}(F_{k1}(.))) with the component
functions being ideal ciphers. This models the resistance of these
constructions to ``generic'' attacks like meet in the middle attacks. We obtain
the first proof that composition actually increases the security of these
constructions in some meaningful sense. We compute a bound on the probability
of breaking the double cipher as a function of the number of computations of
the base cipher made, and the number of examples of the composed cipher seen,
and show that the success probability is the square of that for a single key
cipher. The same bound holds for the two-key triple cipher. The first bound is
tight and shows that meet in the middle is the best possible generic attack
against the double cipher.

<id>
cs/9809124v1
<category>
cs.CR
<abstract>
A security policy states the acceptable actions of an information system, as
the actions bear on security. There is a pressing need for organizations to
declare their security policies, even informal statements would be better than
the current practice. But, formal policy statements are preferable to support
(1) reasoning about policies, e.g., for consistency and completeness, (2)
automated enforcement of the policy, e.g., using wrappers around legacy systems
or after the fact with an intrusion detection system, and (3) other formal
manipulation of policies, e.g., the composition of policies. We present LaSCO,
the Language for Security Constraints on Objects, in which a policy consists of
two parts: the domain (assumptions about the system) and the requirement (what
is allowed assuming the domain is satisfied). Thus policies defined in LaSCO
have the appearance of conditional access control statements. LaSCO policies
are specified as expressions in logic and as directed graphs, giving a visual
view of policy. LaSCO has a simple semantics in first order logic (which we
provide), thus permitting policies we write, even for complex policies, to be
very perspicuous. LaSCO has syntax to express many of the situations we have
found to be useful on policies or, more interesting, the composition of
policies. LaSCO has an object-oriented structure, permitting it to be useful to
describe policies on the objects and methods of an application written in an
object-oriented language, in addition to the traditional policies on operating
system objects. A LaSCO specification can be automatically translated into
executable code that checks an invocation of a program with respect to a
policy. The implementation of LaSCO is in Java, and generates wrappers to check
Java programs with respect to a policy.

<id>
cs/9902010v1
<category>
cs.CR
<abstract>
We present here a generalization of the work done by Rabin and Ben-Or. We
give a protocol for multiparty computation which tolerates any Q^2 active
adversary structure based on the existence of a broadcast channel, secure
communication between each pair of participants, and a monotone span program
with multiplication tolerating the structure. The secrecy achieved is
unconditional although we allow an exponentially small probability of error.
This is possible due to a protocol for computing the product of two values
already shared by means of a homomorphic commitment scheme which appeared
originally in a paper of Chaum, Evertse and van de Graaf.

<id>
cs/9903001v1
<category>
cs.CR
<abstract>
These notes are a brief introduction to the RSA algorithm and modular
arithmetic. They are intended for an undergraduate audience.

<id>
cs/9904005v1
<category>
cs.CR
<abstract>
This paper provides a proof of the proposed Internet standard Transport Level
Security protocol using the Gong-Needham-Yahalom logic. It is intended as a
teaching aid and hopes to show to students: the potency of a formal method for
protocol design; some of the subtleties of authenticating parties on a network
where all messages can be intercepted; the design of what should be a widely
accepted standard.

<id>
cs/9909012v1
<category>
cs.CR
<abstract>
Research in the field of electronic signature confirmation has been active
for some 20 years now. Unfortunately present certificate-based solutions also
come from that age when no-one knew about online data transmission. The
official standardized X.509 framework also depends heavily on offline
operations, one of the most complicated ones being certificate revocation
handling. This is done via huge Certificate Revocation Lists which are both
inconvenient and expencive. Several improvements to these lists are proposed
and in this report we try to analyze them briefly. We conclude that although it
is possible to do better than in the original X.509 setting, none of the
solutions presented this far is good enough.

<id>
cs/9912019v2
<category>
cs.CR
<abstract>
The paper was retracted.

<id>
cs/0003066v1
<category>
cs.CR
<abstract>
In this dissertation, we present LaSCO, the Language for Security Constraints
on Objects, a new approach to expressing security policies using policy graphs
and present a method for enforcing policies so expressed. Other approaches for
stating security policies fall short of what is desirable with respect to
either policy clarity, executability, or the precision with which a policy may
be expressed. However, LaSCO is designed to have those three desirable
properties of a security policy language as well as: relevance for many
different systems, statement of policies at an appropriate level of detail,
user friendliness for both casual and expert users, and amenability to formal
reasoning. In LaSCO, the constraints of a policy are stated as directed graphs
annotated with expressions describing the situation under which the policy
applies and what the requirement is. LaSCO may be used for such diverse
applications as executing programs, file systems, operating systems,
distributed systems, and networks.
  Formal operational semantics have been defined for LaSCO. An architecture for
implementing LaSCO on any system, is presented along with an implementation of
the system-independent portion in Perl. Using this, we have implemented LaSCO
for Java programs, preventing Java programs from violating policy. A GUI to
facilitate writing policies is provided. We have studied applying LaSCO to a
network as viewed by GrIDS, a distributed intrusion detection system for large
networks, and propose a design. We conclude that LaSCO has characteristics that
enable its use on different types of systems throughout the process of
precisely expressing a policy, understanding the implications of a policy, and
implementing it on a system.

<id>
cs/0010019v1
<category>
cs.CR
<abstract>
We take a critical look at the relationship between the security of
cryptographic schemes in the Random Oracle Model, and the security of the
schemes that result from implementing the random oracle by so called
"cryptographic hash functions". The main result of this paper is a negative
one: There exist signature and encryption schemes that are secure in the Random
Oracle Model, but for which any implementation of the random oracle results in
insecure schemes.
  In the process of devising the above schemes, we consider possible
definitions for the notion of a "good implementation" of a random oracle,
pointing out limitations and challenges.

<id>
cs/0011004v2
<category>
cs.CR
<abstract>
In this short note we want to introduce {\em anonymous oblivious transfer} a
new cryptographic primitive which can be proven to be strictly more powerful
than oblivious transfer. We show that all functions can be robustly realized by
multi party protocols with {\em anonymous oblivious transfer}. No assumption
about possible collusions of cheaters or disruptors have to be made.
Furthermore we shortly discuss how to realize anonymous oblivious transfer with
oblivious broadcast or by quantum cryptography. The protocol of anonymous
oblivious transfer was inspired by a quantum protocol: the anonymous quantum
channel.

<id>
cs/0101020v2
<category>
cs.CR
<abstract>
With oblivious transfer multiparty protocols become possible even in the
presence of a faulty majority. But all known protocols can be aborted by just
one disruptor.
  This paper presents more robust solutions for multiparty protocols with
oblivious transfer. This additional robustness against disruptors weakens the
security of the protocol and the guarantee that the result is correct. We can
observe a trade off between robustness against disruption and security and
correctness.
  We give an application to quantum multiparty protocols. These allow the
implementation of oblivious transfer and the protocols of this paper relative
to temporary assumptions, i.e., the security increases after the termination of
the protocol.

<id>
cs/0102012v1
<category>
cs.CR
<abstract>
This paper discusses mixing of chaotic systems as a dependable method for
secure communication. Distribution of the entropy function for steady state as
well as plaintext input sequences are analyzed. It is shown that the mixing of
chaotic sequences results in a sequence that does not have any state dependence
on the information encrypted by them. The generated output states of such a
cipher approach the theoretical maximum for both complexity measures and cycle
length. These features are then compared with some popular ciphers.

<id>
cs/0104004v1
<category>
cs.CR
<abstract>
Suppose there is a group of N people some of whom possess a specific
property. For example, their wealth is above or below a threshold, they voted
for a particular candidate, they have a certain disease, etc. The group wants
to find out how many of its members posses the property -- without revealing
the identities. Unless of course it turns out that all members do or do not
have the attribute of interest. However, in all other cases the counting
algorithm should guarantee that nobody can find out if a particular individual
possesses the property unless all the other N-1 members of the group collude.
  The present article describes a method to solve the confidential counting
problem with only 3*N-2 pairwise communications, or 2*N broadcasts (the last
N-1 pairwise communications are merely to announce the result). The counting
algorithm does not require any trusted third parties. All communications
between parties involved can be conducted in public without compromising the
security of counting.

<id>
cs/0107003v2
<category>
cs.CR
<abstract>
We consider zero knowledge interactive proofs in a richer, more realistic
communication environment. In this setting, one may simultaneously engage in
many interactive proofs, and these proofs may take place in an asynchronous
fashion. It is known that zero-knowledge is not necessarily preserved in such
an environment; we show that for a large class of protocols, it cannot be
preserved. Any 4 round (computational) zero-knowledge interactive proof (or
argument) for a non-trivial language L is not black-box simulatable in the
asynchronous setting.

<id>
cs/0107004v1
<category>
cs.CR
<abstract>
A proof is concurrent zero-knowledge if it remains zero-knowledge when many
copies of the proof are run in an asynchronous environment, such as the
Internet. It is known that zero-knowledge is not necessarily preserved in such
an environment. Designing concurrent zero-knowledge proofs is a fundamental
issue in the study of zero-knowledge since known zero-knowledge protocols
cannot be run in a realistic modern computing environment. In this paper we
present a concurrent zero-knowledge proof systems for all languages in NP.
Currently, the proof system we present is the only known proof system that
retains the zero-knowledge property when copies of the proof are allowed to run
in an asynchronous environment. Our proof system has $\tilde{O}(\log^2 k)$
rounds (for a security parameter $k$), which is almost optimal, as it is shown
by Canetti Kilian Petrank and Rosen that black-box concurrent zero-knowledge
requires $\tilde{\Omega}(\log k)$ rounds.
  Canetti, Goldreich, Goldwasser and Micali introduced the notion of {\em
resettable} zero-knowledge, and modified an earlier version of our proof system
to obtain the first resettable zero-knowledge proof system. This protocol
requires $k^{\theta(1)}$ rounds. We note that their technique also applies to
our current proof system, yielding a resettable zero-knowledge proof for NP
with $\tilde{O}(\log^2 k)$ rounds.

<id>
cs/0108017v1
<category>
cs.CR
<abstract>
This paper discusses the security considerations for remote electronic voting
in public elections. In particular, we examine the feasibility of running
national federal elections over the Internet. The focus of this paper is on the
limitations of the current deployed infrastructure in terms of the security of
the hosts and the Internet itself. We conclude that at present, our
infrastructure is inadequate for remote Internet voting.

<id>
cs/0110009v1
<category>
cs.CR
<abstract>
The early promises of DNA computing to deliver a massively parallel
architecture well-suited to computationally hard problems have so far been
largely unkept. Indeed, it is probably fair to say that only toy problems have
been addressed experimentally. Recent experimental development on algorithmic
self-assembly using DNA tiles seem to offer the most promising path toward a
potentially useful application of the DNA computing concept. In this paper, we
explore new geometries for algorithmic self-assembly, departing from those
previously described in the literature. This enables us to carry out
mathematical operations like binary multiplication or cyclic convolution
product. We then show how to use the latter operation to implement an attack
against the well-known public-key crypto system NTRU.

<id>
cs/0110019v1
<category>
cs.CR
<abstract>
The approach for a network behavior description in terms of numerical
time-dependant functions of the protocol parameters is suggested. This provides
a basis for application of methods of mathematical and theoretical physics for
information flow analysis on network and for extraction of patterns of typical
network behavior. The information traffic can be described as a trajectory in
multi-dimensional parameter-time space with dimension about 10-12. Based on
this study some algorithms for the proposed intrusion detection system are
discussed.

<id>
cs/0110024v1
<category>
cs.CR
<abstract>
We propose pretty simple password-authenticated key-exchange protocol which
is based on the difficulty of solving DDH problem. It has the following
advantages: (1) Both $y_1$ and $y_2$ in our protocol are independent and thus
they can be pre-computed and can be sent independently. This speeds up the
protocol. (2) Clients and servers can use almost the same algorithm. This
reduces the implementation costs without accepting replay attacks and abuse of
entities as oracles.

<id>
cs/0201003v1
<category>
cs.CR
<abstract>
Random beacons-information sources that broadcast a stream of random digits
unknown by anyone beforehand-are useful for various cryptographic purposes. But
such beacons can be easily and undetectably sabotaged, so that their output is
known beforehand by a dishonest party, who can use this information to defeat
the cryptographic protocols supposedly protected by the beacon. We explore a
strategy to reduce this hazard by combining the outputs from several
noninteracting (eg spacelike-separated) beacons by XORing them together to
produce a single digit stream which is more trustworthy than any individual
beacon, being random and unpredictable if at least one of the contributing
beacons is honest. If the contributing beacons are not spacelike separated, so
that a dishonest beacon can overhear and adapt to earlier outputs of other
beacons, the beacons' trustworthiness can still be enhanced to a lesser extent
by a time sharing strategy. We point out some disadvantages of alternative
trust amplification methods based on one-way hash functions.

<id>
cs/0206020v1
<category>
cs.CR
<abstract>
An approach for real-time network monitoring in terms of numerical
time-dependant functions of protocol parameters is suggested. Applying complex
systems theory for information f{l}ow analysis of networks, the information
traffic is described as a trajectory in multi-dimensional parameter-time space
with about 10-12 dimensions. The network traffic description is synthesized by
applying methods of theoretical physics and complex systems theory, to provide
a robust approach for network monitoring that detects known intrusions, and
supports developing real systems for detection of unknown intrusions. The
methods of data analysis and pattern recognition presented are the basis of a
technology study for an automatic intrusion detection system that detects the
attack in the reconnaissance stage.

<id>
cs/0207079v4
<category>
cs.CR
<abstract>
An important problem of modern cryptography concerns secret public-key
computations in algebraic structures. We construct homomorphic cryptosystems
being (secret) epimorphisms f:G --> H, where G, H are (publically known) groups
and H is finite. A letter of a message to be encrypted is an element h element
of H, while its encryption g element of G is such that f(g)=h. A homomorphic
cryptosystem allows one to perform computations (operating in a group G) with
encrypted information (without knowing the original message over H).
  In this paper certain homomorphic cryptosystems are constructed for the first
time for non-abelian groups H (earlier, homomorphic cryptosystems were known
only in the Abelian case). In fact, we present such a system for any solvable
(fixed) group H.

<id>
cs/0207080v1
<category>
cs.CR
<abstract>
Public-key cryptosystems are suggested based on invariants of groups. We give
also an overview of the known cryptosystems which involve groups.

<id>
cs/0208002v1
<category>
cs.CR
<abstract>
The pit recording of file, the coefficient of compression are introduced. The
theoretical limit of the information compression as minimal coefficient of
compression for the given length of alphabet are found.

<id>
cs/0208003v2
<category>
cs.CR
<abstract>
The clones of MV2 algorithm for any radix are discussed. The three various
examples of ones are represented.

<id>
cs/0301020v1
<category>
cs.CR
<abstract>
We explain how a differential fault analysis (DFA) works on AES 128, 192 or
256 bits.

<id>
cs/0301022v4
<category>
cs.CR
<abstract>
In this paper homomorphic cryptosystems are designed for the first time over
any finite group. Applying Barrington's construction we produce for any boolean
circuit of the logarithmic depth its encrypted simulation of a polynomial size
over an appropriate finitely generated group.

<id>
cs/0306032v1
<category>
cs.CR
<abstract>
In this note, we describe a probabilistic attack on public key cryptosystems
based on the word/conjugacy problems for finitely presented groups of the type
proposed recently by Anshel, Anshel and Goldfeld. In such a scheme, one makes
use of the property that in the given group the word problem has a polynomial
time solution, while the conjugacy problem has no known polynomial solution. An
example is the braid group from topology in which the word problem is solvable
in polynomial time while the only known solutions to the conjugacy problem are
exponential. The attack in this paper is based on having a canonical
representative of each string relative to which a length function may be
computed. Hence the term length attack. Such canonical representatives are
known to exist for the braid group.

<id>
cs/0306107v1
<category>
cs.CR
<abstract>
Strand spaces are a popular framework for the analysis of security protocols.
Strand spaces have some similarities to a formalism used successfully to model
protocols for distributed systems, namely multi-agent systems. We explore the
exact relationship between these two frameworks here. It turns out that a key
difference is the handling of agents, which are unspecified in strand spaces
and explicit in multi-agent systems. We provide a family of translations from
strand spaces to multi-agent systems parameterized by the choice of agents in
the strand space. We also show that not every multi-agent system of interest
can be expressed as a strand space. This reveals a lack of expressiveness in
the strand-space framework that can be characterized by our translation. To
highlight this lack of expressiveness, we show one simple way in which strand
spaces can be extended to model more systems.

<id>
cs/0307029v1
<category>
cs.CR
<abstract>
The basic properties of RSA cryptosystems and some classical attacks on them
are described. Derived from geometric properties of the Euler functions, the
Euler function rays, a new ansatz to attack RSA cryptosystems is presented. A
resulting, albeit inefficient, algorithm is given. It essentially consists of a
loop with starting value determined by the Euler function ray and with step
width given by a function $\omega_e(n)$ being a multiple of the order
$\mathrm{ord}_n(e)$, where $e$ denotes the public key exponent and $n$ the RSA
modulus. For $n=pq$ and an estimate $r<\sqrt{pq}$ for the smaller prime factor
$p$, the running time is given by $T(e,n,r) = O((r-p)\ln e \ln n \ln r).$

<id>
cs/9301115v1
<category>
cs.DS
<abstract>
This article is a sketch of ideas that were once intended to appear in the
contributor's famous series, "The Art of Computer Programming". He generalizes the
notion of a context-free language from a set to a multiset of words over an
alphabet. The idea is to keep track of the number of ways to parse a string.
For example, "fruit flies like a banana" can famously be parsed in two ways;
analogous examples in the setting of programming languages may yet be important
in the future.
  The treatment is informal but essentially rigorous.

<id>
cs/9608105v1
<category>
cs.DS
<abstract>
A perturbation technique can be used to simplify and sharpen A. C. Yao's
theorems about the behavior of shellsort with increments $(h,g,1)$. In
particular, when $h=\Theta(n^{7/15})$ and $g=\Theta(h^{1/5})$, the average
running time is $O(n^{23/15})$. The proof involves interesting properties of
the inversions in random permutations that have been $h$-sorted and $g$-sorted.

<id>
cs/9801103v1
<category>
cs.DS
<abstract>
Mallows and Riordan showed in 1968 that labeled trees with a small number of
inversions are related to labeled graphs that are connected and sparse. Wright
enumerated sparse connected graphs in 1977, and Kreweras related the inversions
of trees to the so-called ``parking problem'' in 1980. A~combination of these
three results leads to a surprisingly simple analysis of the behavior of
hashing by linear probing, including higher moments of the cost of successful
search.

<id>
cs/9809012v1
<category>
cs.DS
<abstract>
The classic all-terminal network reliability problem posits a graph, each of
whose edges fails independently with some given probability.

<id>
cs/9812007v1
<category>
cs.DS
<abstract>
We significantly improve known time bounds for solving the minimum cut
problem on undirected graphs. We use a ``semi-duality'' between minimum cuts
and maximum spanning tree packings combined with our previously developed
random sampling techniques. We give a randomized algorithm that finds a minimum
cut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We
also give a simpler randomized algorithm that finds all minimum cuts with high
probability in O(n^2 log n) time. This variant has an optimal RNC
parallelization. Both variants improve on the previous best time bound of O(n^2
log^3 n). Other applications of the tree-packing approach are new, nearly tight
bounds on the number of near minimum cuts a graph may have and a new data
structure for representing them in a space-efficient manner.

<id>
cs/9812008v1
<category>
cs.DS
<abstract>
We consider the problem of coloring k-colorable graphs with the fewest
possible colors. We present a randomized polynomial time algorithm that colors
a 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log
n), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any
vertex. Besides giving the best known approximation ratio in terms of n, this
marks the first non-trivial approximation result as a function of the maximum
degree Delta. This result can be generalized to k-colorable graphs to obtain a
coloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}
log^{1/2} n) colors. Our results are inspired by the recent work of Goemans and
Williamson who used an algorithm for semidefinite optimization problems, which
generalize linear programs, to obtain improved approximations for the MAX CUT
and MAX 2-SAT problems. An intriguing outcome of our work is a duality
relationship established between the value of the optimum solution to our
semidefinite program and the Lovasz theta-function. We show lower bounds on the
gap between the optimum solution of our semidefinite program and the actual
chromatic number; by duality this also demonstrates interesting new facts about
the theta-function.

<id>
cs/9903010v1
<category>
cs.DS
<abstract>
We examine possibility to design an efficient solving algorithm for problems
of the class \np. It is introduced a classification of \np problems by the
property that a partial solution of size $k$ can be extended into a partial
solution of size $k+1$ in polynomial time. It is defined an unique class
problems to be worth to search an efficient solving algorithm. The problems,
which are outside of this class, are inherently exponential. We show that the
Hamiltonian cycle problem is inherently exponential.

<id>
cs/9906021v1
<category>
cs.DS
<abstract>
Tomography is the area of reconstructing objects from projections. Here we
wish to reconstruct a set of cells in a two dimensional grid, given the number
of cells in every row and column. The set is required to be an hv-convex
polyomino, that is all its cells must be connected and the cells in every row
and column must be consecutive. A simple, polynomial algorithm for
reconstructing hv-convex polyominoes is provided, which is several orders of
magnitudes faster than the best previously known algorithm from Barcucci et al.
In addition, the problem of reconstructing a special class of centered
hv-convex polyominoes is addressed. (An object is centered if it contains a row
whose length equals the total width of the object). It is shown that in this
case the reconstruction problem can be solved in linear time.

<id>
cs/9911003v1
<category>
cs.DS
<abstract>
We solve the subgraph isomorphism problem in planar graphs in linear time,
for any pattern of constant size. Our results are based on a technique of
partitioning the planar graph into pieces of small tree-width, and applying
dynamic programming within each piece. The same methods can be used to solve
other planar graph problems including connectivity, diameter, girth, induced
subgraph isomorphism, and shortest paths.

<id>
cs/9912014v1
<category>
cs.DS
<abstract>
We develop data structures for dynamic closest pair problems with arbitrary
distance functions, that do not necessarily come from any geometric structure
on the objects. Based on a technique previously used by the contributor for
Euclidean closest pairs, we show how to insert and delete objects from an
n-object set, maintaining the closest pair, in O(n log^2 n) time per update and
O(n) space. With quadratic space, we can instead use a quadtree-like structure
to achieve an optimal time bound, O(n) per update. We apply these data
structures to hierarchical clustering, greedy matching, and TSP heuristics, and
discuss other potential applications in machine learning, Groebner bases, and
local improvement algorithms for partition and placement problems. Experiments
show our new methods to be faster in practice than previously used heuristics.

<id>
cs/9912020v2
<category>
cs.DS
<abstract>
We discuss some aspects of approximating functions on high-dimensional data
sets with additive functions or ANOVA decompositions, that is, sums of
functions depending on fewer variables each. It is seen that under appropriate
smoothness conditions, the errors of the ANOVA decompositions are of order
$O(n^{m/2})$ for approximations using sums of functions of up to $m$ variables
under some mild restrictions on the (possibly dependent) predictor variables.
Several simulated examples illustrate this behaviour.

<id>
cs/0003078v1
<category>
cs.DS
<abstract>
We examine the Maximum Independent Set Problem in an undirected graph. The
main result is that this problem can be considered as the solving the same
problem in a subclass of the weighted normal twin-orthogonal graphs. The
problem is formulated which is dual to the problem above. It is shown that, for
trivial twin-orthogonal graphs, any of its maximal independent set is also
maximum one.

<id>
cs/0006046v1
<category>
cs.DS
<abstract>
We consider worst case time bounds for NP-complete problems including 3-SAT,
3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a
constraint satisfaction (CSP) formulation of these problems. 3-SAT is
equivalent to (2,3)-CSP while the other problems above are special cases of
(3,2)-CSP; there is also a natural duality transformation from (a,b)-CSP to
(b,a)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the
time bounds for solving the other problems listed above. Our techniques involve
a mixture of Davis-Putnam-style backtracking with more sophisticated matching
and network flow based ideas.

<id>
cs/0007029v1
<category>
cs.DS
<abstract>
We determine the asymptotical satisfiability probability of a random
at-most-k-Horn formula, via a probabilistic analysis of a simple version,
called PUR, of positive unit resolution. We show that for k=k(n)->oo the
problem can be ``reduced'' to the case k(n)=n, that was solved in
cs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR
is modeled by a simple queuing chain, leading to a closed-form solution when
k=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case.
Under a rescaled parameter, the graphs of satisfaction probability
corresponding to finite values of k converge to the one for the uniform case, a
``dimension-dependent behavior'' similar to the one found experimentally by
Kirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively
explained by a threshold property for the number of iterations of PUR makes on
random satisfiable Horn formulas.

<id>
cs/0007043v1
<category>
cs.DS
<abstract>
In this paper we present a new data structure for double ended priority
queue, called min-max fine heap, which combines the techniques used in fine
heap and traditional min-max heap. The standard operations on this proposed
structure are also presented, and their analysis indicates that the new
structure outperforms the traditional one.

<id>
cs/0008011v1
<category>
cs.DS
<abstract>
We present two new algorithms for solving the {\em All Pairs Shortest Paths}
(APSP) problem for weighted directed graphs. Both algorithms use fast matrix
multiplication algorithms.
  The first algorithm solves the APSP problem for weighted directed graphs in
which the edge weights are integers of small absolute value in $\Ot(n^{2+\mu})$
time, where $\mu$ satisfies the equation $\omega(1,\mu,1)=1+2\mu$ and
$\omega(1,\mu,1)$ is the exponent of the multiplication of an $n\times n^\mu$
matrix by an $n^\mu \times n$ matrix. Currently, the best available bounds on
$\omega(1,\mu,1)$, obtained by Coppersmith, imply that $\mu<0.575$. The running
time of our algorithm is therefore $O(n^{2.575})$. Our algorithm improves on
the $\Ot(n^{(3+\omega)/2})$ time algorithm, where $\omega=\omega(1,1,1)<2.376$
is the usual exponent of matrix multiplication, obtained by Alon, Galil and
Margalit, whose running time is only known to be $O(n^{2.688})$.
  The second algorithm solves the APSP problem {\em almost} exactly for
directed graphs with {\em arbitrary} non-negative real weights. The algorithm
runs in $\Ot((n^\omega/\eps)\log (W/\eps))$ time, where $\eps>0$ is an error
parameter and W is the largest edge weight in the graph, after the edge weights
are scaled so that the smallest non-zero edge weight in the graph is 1. It
returns estimates of all the distances in the graph with a stretch of at most
$1+\eps$. Corresponding paths can also be found efficiently.

<id>
cs/0009006v1
<category>
cs.DS
<abstract>
We consider worst case time bounds for NP-complete problems including 3-SAT,
3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a
constraint satisfaction (CSP) formulation of these problems; 3-SAT is
equivalent to (2,3)-CSP while the other problems above are special cases of
(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the
time bounds for solving the other problems listed above. Our techniques involve
a mixture of Davis-Putnam-style backtracking with more sophisticated matching
and network flow based ideas.

<id>
cs/0011047v1
<category>
cs.DS
<abstract>
The contributor presents two tricks to accelerate depth-first search algorithms
for a class of combinatorial puzzle problems, such as tiling a tray by a fixed
set of polyominoes. The first trick is to implement each assumption of the
search with reversible local operations on doubly linked lists. By this trick,
every step of the search affects the data incrementally.
  The second trick is to add a ghost square that represents the identity of
each polyomino. Thus puts the rule that each polyomino be used once on the same
footing as the rule that each square be covered once. The coding simplifies to
a more abstract form which is equivalent to 0-1 integer programming. More
significantly for the total computation time, the search can naturally switch
between placing a fixed polyomino or covering a fixed square at different
stages, according to a combined heuristic.
  Finally the contributor reports excellent performance for his algorithm for some
familiar puzzles. These include tiling a hexagon by 19 hexiamonds and the N
queens problem for N up to 18.

<id>
cs/0012002v1
<category>
cs.DS
<abstract>
In this paper we present a random shuffling scheme to apply with adaptive
sorting algorithms. Adaptive sorting algorithms utilize the presortedness
present in a given sequence. We have probabilistically increased the amount of
presortedness present in a sequence by using a random shuffling technique that
requires little computation. Theoretical analysis suggests that the proposed
scheme can improve the performance of adaptive sorting. Experimental results
show that it significantly reduces the amount of disorder present in a given
sequence and improves the execution time of adaptive sorting algorithm as well.

<id>
cs/0111050v7
<category>
cs.DS
<abstract>
We introduce the smoothed analysis of algorithms, which is a hybrid of the
worst-case and average-case analysis of algorithms. In smoothed analysis, we
measure the maximum over inputs of the expected performance of an algorithm
under small random perturbations of that input. We measure this performance in
terms of both the input size and the magnitude of the perturbations. We show
that the simplex algorithm has polynomial smoothed complexity.

<id>
cs/0112022v2
<category>
cs.DS
<abstract>
In many applications, it is necessary to determine the string similarity.
Edit distance[WF74] approach is a classic method to determine Field Similarity.
A well known dynamic programming algorithm [GUS97] is used to calculate edit
distance with the time complexity O(nm). (for worst case, average case and even
best case) Instead of continuing with improving the edit distance approach,
[LL+99] adopted a brand new approach-token-based approach. Its new concept of
token-base-retain the original semantic information, good time complex-O(nm)
(for worst, average and best case) and good experimental performance make it a
milestone paper in this area. Further study indicates that there is still room
for improvement of its Field Similarity algorithm. Our paper is to introduce a
package of substring-based new algorithms to determine Field Similarity.
Combined together, our new algorithms not only achieve higher accuracy but also
gain the time complexity O(knm) (k<0.75) for worst case, O(*n) where <6 for
average case and O(1) for best case. Throughout the paper, we use the approach
of comparative examples to show higher accuracy of our algorithms compared to
the one proposed in [LL+99]. Theoretical analysis, concrete examples and
experimental result show that our algorithms can significantly improve the
accuracy and time complexity of the calculation of Field Similarity. [US97] D.
Guseld. Algorithms on Strings, Trees and Sequences, in Computer Science and
Computational Biology. [LL+99] Mong Li Lee, Cleansing data for mining and
warehousing, In Proceedings of the 10th International Conference on Database
and Expert Systems Applications (DEXA99), pages 751-760,August 1999. [WF74] R.
Wagner and M. Fisher, The String to String Correction Problem, JACM 21 pages
168-173, 1974.

<id>
cs/0203018v1
<category>
cs.DS
<abstract>
We study the problem of compressing massive tables within the
partition-training paradigm introduced by Buchsbaum et al. [SODA'00], in which
a table is partitioned by an off-line training procedure into disjoint
intervals of columns, each of which is compressed separately by a standard,
on-line compressor like gzip. We provide a new theory that unifies previous
experimental observations on partitioning and heuristic observations on column
permutation, all of which are used to improve compression rates. Based on the
theory, we devise the first on-line training algorithms for table compression,
which can be applied to individual files, not just continuously operating
sources; and also a new, off-line training algorithm, based on a link to the
asymmetric traveling salesman problem, which improves on prior work by
rearranging columns prior to partitioning. We demonstrate these results
experimentally. On various test files, the on-line algorithms provide 35-55%
improvement over gzip with negligible slowdown; the off-line reordering
provides up to 20% further improvement over partitioning alone. We also show
that a variation of the table compression problem is MAX-SNP hard.

<id>
cs/0204033v1
<category>
cs.DS
<abstract>
We show that several versions of Floyd and Rivest's algorithm Select for
finding the $k$th smallest of $n$ elements require at most
$n+\min\{k,n-k\}+o(n)$ comparisons on average and with high probability. This
rectifies the analysis of Floyd and Rivest, and extends it to the case of
nondistinct elements. Our computational results confirm that Select may be the
best algorithm in practice.

<id>
cs/0205029v1
<category>
cs.DS
<abstract>
Pattern-matching-based document-compression systems (e.g. for faxing) rely on
finding a small set of patterns that can be used to represent all of the ink in
the document. Finding an optimal set of patterns is NP-hard; previous
compression schemes have resorted to heuristics. This paper describes an
extension of the cross-entropy approach, used previously for measuring pattern
similarity, to this problem. This approach reduces the problem to a k-medians
problem, for which the paper gives a new algorithm with a provably good
performance guarantee. In comparison to previous heuristics (First Fit, with
and without generalized Lloyd's/k-means postprocessing steps), the new
algorithm generates a better codebook, resulting in an overall improvement in
compression performance of almost 17%.

<id>
cs/0205039v1
<category>
cs.DS
<abstract>
Mixed packing and covering problems are problems that can be formulated as
linear programs using only non-negative coefficients. Examples include
multicommodity network flow, the Held-Karp lower bound on TSP, fractional
relaxations of set cover, bin-packing, knapsack, scheduling problems,
minimum-weight triangulation, etc. This paper gives approximation algorithms
for the general class of problems. The sequential algorithm is a simple greedy
algorithm that can be implemented to find an epsilon-approximate solution in
O(epsilon^-2 log m) linear-time iterations. The parallel algorithm does
comparable work but finishes in polylogarithmic time.
  The results generalize previous work on pure packing and covering (the
special case when the constraints are all "less-than" or all "greater-than") by
Michael Luby and Noam Nisan (1993) and Naveen Garg and Jochen Konemann (1998).

<id>
cs/0205048v2
<category>
cs.DS
<abstract>
We give a polynomial-time approximation scheme for the generalization of
Huffman Coding in which codeword letters have non-uniform costs (as in Morse
code, where the dash is twice as long as the dot). The algorithm computes a
(1+epsilon)-approximate solution in time O(n + f(epsilon) log^3 n), where n is
the input size.

<id>
cs/0205049v1
<category>
cs.DS
<abstract>
Describes a near-linear-time algorithm for a variant of Huffman coding, in
which the letters may have non-uniform lengths (as in Morse code), but with the
restriction that each word to be encoded has equal probability. [See also
``Huffman Coding with Unequal Letter Costs'' (2002).]

<id>
cs/0206033v1
<category>
cs.DS
<abstract>
Falmagne recently introduced the concept of a medium, a combinatorial object
encompassing hyperplane arrangements, topological orderings, acyclic
orientations, and many other familiar structures. We find efficient solutions
for several algorithmic problems on media: finding short reset sequences,
shortest paths, testing whether a medium has a closed orientation, and listing
the states of a medium given a black-box description.

<id>
cs/0207061v2
<category>
cs.DS
<abstract>
We present algorithms that run in linear time on pointer machines for a
collection of problems, each of which either directly or indirectly requires
the evaluation of a function defined on paths in a tree. These problems
previously had linear-time algorithms but only for random-access machines
(RAMs); the best pointer-machine algorithms were super-linear by an
inverse-Ackermann-function factor. Our algorithms are also simpler, in some
cases substantially, than the previous linear-time RAM algorithms. Our
improvements come primarily from three new ideas: a refined analysis of path
compression that gives a linear bound if the compressions favor certain nodes,
a pointer-based radix sort as a replacement for table-based methods, and a more
careful partitioning of a tree into easily managed parts. Our algorithms
compute nearest common ancestors off-line, verify and construct minimum
spanning trees, do interval analysis on a flowgraph, find the dominators of a
flowgraph, and build the component tree of a weighted tree.

<id>
cs/0207066v1
<category>
cs.DS
<abstract>
Dealing with the NP-complete Dominating Set problem on undirected graphs, we
demonstrate the power of data reduction by preprocessing from a theoretical as
well as a practical side. In particular, we prove that Dominating Set
restricted to planar graphs has a so-called problem kernel of linear size,
achieved by two simple and easy to implement reduction rules. Moreover, having
implemented our reduction rules, first experiments indicate the impressive
practical potential of these rules. Thus, this work seems to open up a new and
prospective way how to cope with one of the most important problems in graph
theory and combinatorial optimization.

<id>
cs/9809005v1
<category>
cs.DB
<abstract>
Simple economic and performance arguments suggest appropriate lifetimes for
main memory pages and suggest optimal page sizes. The fundamental tradeoffs are
the prices and bandwidths of RAMs and disks. The analysis indicates that with
today's technology, five minutes is a good lifetime for randomly accessed
pages, one minute is a good lifetime for two-pass sequentially accessed pages,
and 16 KB is a good size for index pages. These rules-of-thumb change in
predictable ways as technology ratios change. They also motivate the importance
of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.

<id>
cs/9809023v2
<category>
cs.DB
<abstract>
We study a set of linear transformations on the Fourier series representation
of a sequence that can be used as the basis for similarity queries on
time-series data. We show that our set of transformations is rich enough to
formulate operations such as moving average and time warping. We present a
query processing algorithm that uses the underlying R-tree index of a
multidimensional data set to answer similarity queries efficiently. Our
experiments show that the performance of this algorithm is competitive to that
of processing ordinary (exact match) queries using the index, and much faster
than sequential scanning. We relate our transformations to the general
framework for similarity queries of Jagadish et al.

<id>
cs/9809033v2
<category>
cs.DB
<abstract>
We propose an improvement of the known DFT-based indexing technique for fast
retrieval of similar time sequences. We use the last few Fourier coefficients
in the distance computation without storing them in the index since every
coefficient at the end is the complex conjugate of a coefficient at the
beginning and as strong as its counterpart. We show analytically that this
observation can accelerate the search time of the index by more than a factor
of two. This result was confirmed by our experiments, which were carried out on
real stock prices and synthetic data.

<id>
cs/9909016v1
<category>
cs.DB
<abstract>
We identify two unreasonable, though standard, assumptions made by database
query optimizers that can adversely affect the quality of the chosen evaluation
plans. One assumption is that it is enough to optimize for the expected
case---that is, the case where various parameters (like available memory) take
on their expected value. The other assumption is that the parameters are
constant throughout the execution of the query. We present an algorithm based
on the ``System R''-style query optimization algorithm that does not rely on
these assumptions. The algorithm we present chooses the plan of the least
expected cost instead of the plan of least cost given some fixed value of the
parameters. In execution environments that exhibit a high degree of
variability, our techniques should result in better performance.

<id>
cs/9910021v1
<category>
cs.DB
<abstract>
Complex queries are becoming commonplace, with the growing use of decision
support systems. These complex queries often have a lot of common
sub-expressions, either within a single query, or across multiple such queries
run as a batch. Multi-query optimization aims at exploiting common
sub-expressions to reduce evaluation cost. Multi-query optimization has
hither-to been viewed as impractical, since earlier algorithms were exhaustive,
and explore a doubly exponential search space.
  In this paper we demonstrate that multi-query optimization using heuristics
is practical, and provides significant benefits. We propose three cost-based
heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple
modifications to the Volcano search strategy, and a greedy heuristic. Our
greedy heuristic incorporates novel optimizations that improve efficiency
greatly. Our algorithms are designed to be easily added to existing optimizers.
We present a performance study comparing the algorithms, using workloads
consisting of queries from the TPC-D benchmark. The study shows that our
algorithms provide significant benefits over traditional optimization, at a
very acceptable overhead in optimization time.

<id>
cs/9912015v1
<category>
cs.DB
<abstract>
XML is becoming the most relevant new standard for data representation and
exchange on the WWW. Novel languages for extracting and restructuring the XML
content have been proposed, some in the tradition of database query languages
(i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query
language has yet been decided, but the discussion is ongoing within the World
Wide Web Consortium and within many academic institutions and Internet-related
major companies. We present a comparison of five, representative query
languages for XML, highlighting their common features and differences.

<id>
cs/0003005v1
<category>
cs.DB
<abstract>
In data warehouse and data mart systems, queries often take a long time to
execute due to their complex nature. Query response times can be greatly
improved by caching final/intermediate results of previous queries, and using
them to answer later queries. In this paper we describe a caching system called
Exchequer which incorporates several novel features including optimization
aware cache maintenance and the use of a cache aware optimizer. In contrast, in
existing work, the module that makes cost-benefit decisions is part of the
cache manager and works independent of the optimizer which essentially
reconsiders these decisions while finding the best plan for a query. In our
work, the optimizer takes the decisions for the cache manager. Furthermore,
existing approaches are either restricted to cube (slice/point) queries, or
cache just the query results. On the other hand, our work is extens ible and in
fact presents a data-model independent framework and algorithm. Our
experimental results attest to the efficacy of our cache management techniques
and show that over a wide range of parameters (a) Exchequer's query response
times are lower by more than 30% compared to the best performing competitor,
and (b) Exchequer can deliver the same response time as its competitor with
just one tenth of the cache size.

<id>
cs/0003006v1
<category>
cs.DB
<abstract>
Because the presence of views enhances query performance, materialized views
are increasingly being supported by commercial database/data warehouse systems.
Whenever the data warehouse is updated, the materialized views must also be
updated. However, whereas the amount of data entering a warehouse, the query
loads, and the need to obtain up-to-date responses are all increasing, the time
window available for making the warehouse up-to-date is shrinking. These trends
necessitate efficient techniques for the maintenance of materialized views.
  In this paper, we show how to find an efficient plan for maintenance of a
{\em set} of views, by exploiting common subexpressions between different view
maintenance expressions. These common subexpressions may be materialized
temporarily during view maintenance. Our algorithms also choose
subexpressions/indices to be materialized permanently (and maintained along
with other materialized views), to speed up view maintenance. While there has
been much work on view maintenance in the past, our novel contributions lie in
exploiting a recently developed framework for multiquery optimization to
efficiently find good view maintenance plans as above. In addition to faster
view maintenance, our algorithms can also be used to efficiently select
materialized views to speed up workloads containing queries.

<id>
cs/0007044v2
<category>
cs.DB
<abstract>
Recent trends in information management involve the periodic transcription of
data onto secondary devices in a networked environment, and the proper
scheduling of these transcriptions is critical for efficient data management.
To assist in the scheduling process, we are interested in modeling the
reduction of consistency over time between a relation and its replica, termed
obsolescence of data. The modeling is based on techniques from the field of
stochastic processes, and provides several stochastic models for content
evolution in the base relations of a database, taking referential integrity
constraints into account. These models are general enough to accommodate most
of the common scenarios in databases, including batch insertions and life spans
both with and without memory. As an initial "proof of concept" of the
applicability of our approach, we validate the insertion portion of our model
framework via experiments with real data feeds. We also discuss a set of
transcription protocols which make use of the proposed stochastic model.

<id>
cs/0011024v1
<category>
cs.DB
<abstract>
Queries involving aggregation are typical in database applications. One of
the main ideas to optimize the execution of an aggregate query is to reuse
results of previously answered queries. This leads to the problem of rewriting
aggregate queries using views. Due to a lack of theory, algorithms for this
problem were rather ad-hoc. They were sound, but were not proven to be
complete.
  Recently we have given syntactic characterizations for the equivalence of
aggregate queries and applied them to decide when there exist rewritings.
However, these decision procedures do not lend themselves immediately to an
implementation. In this paper, we present practical algorithms for rewriting
queries with $\COUNT$ and $\SUM$. Our algorithms are sound. They are also
complete for important cases. Our techniques can be used to improve well-known
procedures for rewriting non-aggregate queries. These procedures can then be
adapted to obtain algorithms for rewriting queries with $\MIN$ and $\MAX$. The
algorithms presented are a basis for realizing optimizers that rewrite queries
using views.

<id>
cs/0011041v1
<category>
cs.DB
<abstract>
EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graphical abstract
syntax and a formal concrete syntax are presented for EquiX queries. In
addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.

<id>
cs/0103004v1
<category>
cs.DB
<abstract>
The Harland document management system implements a data model in which
document (object) structure can be altered by mixin-style multiple inheritance
at any time. This kind of structural fluidity has long been supported by
knowledge-base management systems, but its use has primarily been in support of
reasoning and inference. In this paper, we report our experiences building and
supporting several non-trivial applications on top of this data model. Based on
these experiences, we argue that structural fluidity is convenient for
data-intensive applications other than knowledge-base management. Specifically,
we suggest that this flexible data model is a natural fit for the decoupled
programming methodology that arises naturally when using enterprise component
frameworks.

<id>
cs/0106055v1
<category>
cs.DB
<abstract>
The need for Knowledge and Data Discovery Management Systems (KDDMS) that
support ad hoc data mining queries has been long recognized. A significant
amount of research has gone into building tightly coupled systems that
integrate association rule mining with database systems. In this paper, we
describe a seamless integration scheme for database queries and association
rule discovery using a common query optimizer for both. Query trees of
expressions in an extended algebra are used for internal representation in the
optimizer. The algebraic representation is flexible enough to deal with
constrained association rule queries and other variations of association rule
specifications. We propose modularization to simplify the query tree for
complex tasks in data mining. It paves the way for making use of existing
algorithms for constructing query plans in the optimization process. How the
integration scheme we present will facilitate greater user control over the
data mining process is also discussed. The work described in this paper forms
part of a larger project for fully integrating data mining with database
management.

<id>
cs/0109084v1
<category>
cs.DB
<abstract>
This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland,
and Washington, DC) and explores strategies being employed by community
activists and local governments to create and sustain community networking
projects. In some cities, community networking initiatives are relatively
mature, while in others they are in early or intermediate stages. The paper
looks at several factors that help explain the evolution of community networks
in cities:
  1) Local government support; 2) Federal support 3) Degree of community
activism, often reflected by public-private partnerships that help support
community networks.
  In addition to these (more or less) measurable elements of local support, the
case studies enable description of the different objectives of community
networks in different cities. Several community networking projects aim to
improve the delivery of government services (e.g., Portland and Cleveland),
some have a job-training focus (e.g., Austin, Washington, DC), others are
oriented very explicitly toward community building (Nashville, DC), and others
toward neighborhood entrepreneurship (Portland and Cleveland).
  The paper ties the case studies together by asking whether community
technology initiatives contribute to social capital in the cities studied.

<id>
cs/0110020v1
<category>
cs.DB
<abstract>
Large organizations today are being served by different types of data
processing and informations systems, ranging from the operational (OLTP)
systems, data warehouse systems, to data mining and business intelligence
applications. It is important to create an integrated repository of what these
systems contain and do in order to use them collectively and effectively. The
repository contains metadata of source systems, data warehouse, and also the
business metadata. Decision support and business analysis require extensive and
in-depth understanding of business entities, tasks, rules and the environment.
The purpose of business metadata is to provide this understanding. Realizing
the importance of metadata, many standardization efforts has been initiated to
define metadata models. In trying to define an integrated metadata and
information systems for a banking application, we discover some important
limitations or inadequacies of the business metadata proposals. They relate to
providing an integrated and flexible inter-operability and navigation between
metadata and data, and to the important issue of systematically handling
temporal characteristics and evolution of the metadata itself.
  In this paper, we study the issue of structuring business metadata so that it
can provide a context for business management and decision support when
integrated with data warehousing. We define temporal object-oriented business
metadata model, and relate it both to the technical metadata and the data
warehouse. We also define ways of accessing and navigating metadata in
conjunction with data.

<id>
cs/0110044v1
<category>
cs.DB
<abstract>
EquiX is a search language for XML that combines the power of querying with
the simplicity of searching. Requirements for such languages are discussed and
it is shown that EquiX meets the necessary criteria. Both a graph-based
abstract syntax and a formal concrete syntax are presented for EquiX queries.
In addition, the semantics is defined and an evaluation algorithm is presented.
The evaluation algorithm is polynomial under combined complexity.
  EquiX combines pattern matching, quantification and logical expressions to
query both the data and meta-data of XML documents. The result of a query in
EquiX is a set of XML documents. A DTD describing the result documents is
derived automatically from the query.

<id>
cs/0110052v1
<category>
cs.DB
<abstract>
The web, through many search engine sites, has popularized the keyword-based
search paradigm, where a user can specify a string of keywords and expect to
retrieve relevant documents, possibly ranked by their relevance to the query.
Since a lot of information is stored in databases (and not as HTML documents),
it is important to provide a similar search paradigm for databases, where users
can query a database without knowing the database schema and database query
languages such as SQL. In this paper, we propose such a database search system,
which accepts a free-form query as a collection of keywords, translates it into
queries on the database using the database metadata, and presents query results
in a well-structured and browsable form. Th eysytem maps keywords onto the
database schema and uses inter-relationships (i.e., data semantics) among the
referred tables to generate meaningful query results. We also describe our
prototype for database search, called Mragyati. Th eapproach proposed here is
scalable, as it does not build an in-memory graph of the entire database for
searching for relationships among the objects selected by the user's query.

<id>
cs/0111004v1
<category>
cs.DB
<abstract>
The Relational Database Aspects of Argonnes ATLAS Control System Argonnes
ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two
separate database concepts. The first is the distributed real-time database
structure provided by the commercial product Vsystem [1]. The second is a more
static relational database archiving system designed by ATLAS personnel using
Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS
facility has presented a unique opportunity to construct a control system
relational database that is capable of storing and retrieving complete archived
tune-up configurations for the entire accelerator. This capability has been a
major factor in allowing the facility to adhere to a rigorous operating
schedule. Most recently, a Web-based operator interface to the control systems
Oracle Rdb database has been installed. This paper explains the history of the
ATLAS database systems, how they interact with each other, the design of the
new Web-based operator interface, and future plans.

<id>
cs/0111006v1
<category>
cs.DB
<abstract>
Since Self-Describing Data Sets (SDDS) were first introduced, the source code
has been ported to many different operating systems and various languages. SDDS
is now available in C, Tcl, Java, Fortran, and Python. All of these versions
are supported on Solaris, Linux, and Windows. The C version of SDDS is also
supported on VxWorks. With the recent addition of the Java port, SDDS can now
be deployed on virtually any operating system. Due to this proliferation, SDDS
files serve to link not only a collection of C programs, but programs and
scripts in many languages on different operating systems. The platform
independent binary feature of SDDS also facilitates portability among operating
systems. This paper presents an overview of various benefits of SDDS platform
interoperability.

<id>
cs/0202035v1
<category>
cs.DB
<abstract>
In optimizing queries, solutions based on AND/OR DAG can generate all
possible join orderings and select placements before searching for optimal
query execution strategy. But as the number of joins and selection conditions
increase, the space and time complexity to generate optimal query plan
increases exponentially. In this paper, we use join graph for a relational
database schema to either pre-compute all possible join orderings that can be
executed and store it as a join DAG or, extract joins in the queries to
incrementally build a history join DAG as and when the queries are executed.
The select conditions in the queries are appropriately placed in the retrieved
join DAG (or, history join DAG) to generate optimal query execution strategy.
We experimentally evaluate our query optimization technique on TPC-D/H query
sets to show their effectiveness over AND/OR DAG query optimization strategy.
Finally, we illustrate how our technique can be used for efficient multiple
query optimization and selection of materialized views in data warehousing
environments.

<id>
cs/0202037v2
<category>
cs.DB
<abstract>
We describe a meta-querying system for databases containing queries in
addition to ordinary data. In the context of such databases, a meta-query is a
query about queries. Representing stored queries in XML, and using the standard
XML manipulation language XSLT as a sublanguage, we show that just a few
features need to be added to SQL to turn it into a fully-fledged meta-query
language. The good news is that these features can be directly supported by
extensible database technology.

<id>
cs/0204010v1
<category>
cs.DB
<abstract>
We consider here the problem of obtaining reliable, consistent information
from inconsistent databases -- databases that do not have to satisfy given
integrity constraints. We use the notion of consistent query answer -- a query
answer which is true in every (minimal) repair of the database. We provide a
complete classification of the computational complexity of consistent answers
to first-order queries w.r.t. functional dependencies and denial constraints.
We show how the complexity depends on the {\em type} of the constraints
considered, their {\em number}, and the {\em size} of the query. We obtain
several new PTIME cases, using new algorithms.

<id>
cs/0205060v1
<category>
cs.DB
<abstract>
Graph simulation (using graph schemata or data guides) has been successfully
proposed as a technique for adding structure to semistructured data. Design
patterns for description (such as meta-classes and homomorphisms between schema
layers), which are prominent in the object-oriented programming community,
constitute a generalization of this graph simulation approach.
  In this paper, we show description applicable to a wide range of data models
that have some notion of object (-identity), and propose to turn it into a data
model primitive much like, say, inheritance. We argue that such an extension
fills a practical need in contemporary data management. Then, we present
algebraic techniques for query optimization (using the notions of described and
description queries). Finally, in the semistructured setting, we discuss the
pruning of regular path queries (with nested conditions) using description
meta-data. In this context, our notion of meta-data extends graph schemata and
data guides by meta-level values, allowing to boost query performance and to
reduce the redundancy of data.

<id>
cs/0207093v1
<category>
cs.DB
<abstract>
The handling of user preferences is becoming an increasingly important issue
in present-day information systems. Among others, preferences are used for
information filtering and extraction to reduce the volume of data presented to
the user. They are also used to keep track of user profiles and formulate
policies to improve and automate decision making.
  We propose here a simple, logical framework for formulating preferences as
preference formulas. The framework does not impose any restrictions on the
preference relations and allows arbitrary operation and predicate signatures in
preference formulas. It also makes the composition of preference relations
straightforward. We propose a simple, natural embedding of preference formulas
into relational algebra (and SQL) through a single winnow operator
parameterized by a preference formula. The embedding makes possible the
formulation of complex preference queries, e.g., involving aggregation, by
piggybacking on existing SQL constructs. It also leads in a natural way to the
definition of further, preference-related concepts like ranking. Finally, we
present general algebraic laws governing the winnow operator and its
interaction with other relational algebra operators. The preconditions on the
applicability of the laws are captured by logical formulas. The laws provide a
formal foundation for the algebraic optimization of preference queries. We
demonstrate the usefulness of our approach through numerous examples.

<id>
cs/0207094v1
<category>
cs.DB
<abstract>
A relational database is inconsistent if it does not satisfy a given set of
integrity constraints. Nevertheless, it is likely that most of the data in it
is consistent with the constraints. In this paper we apply logic programming
based on answer sets to the problem of retrieving consistent information from a
possibly inconsistent database. Since consistent information persists from the
original database to every of its minimal repairs, the approach is based on a
specification of database repairs using disjunctive logic programs with
exceptions, whose answer set semantics can be represented and computed by
systems that implement stable model semantics. These programs allow us to
declare persistence by defaults and repairing changes by exceptions. We
concentrate mainly on logic programs for binary integrity constraints, among
which we find most of the integrity constraints found in practice.

<id>
cs/0211020v2
<category>
cs.DB
<abstract>
Research on information extraction from Web pages (wrapping) has seen much
activity recently (particularly systems implementations), but little work has
been done on formally studying the expressiveness of the formalisms proposed or
on the theoretical foundations of wrapping. In this paper, we first study
monadic datalog over trees as a wrapping language. We show that this simple
language is equivalent to monadic second order logic (MSO) in its ability to
specify wrappers. We believe that MSO has the right expressiveness required for
Web information extraction and propose MSO as a yardstick for evaluating and
comparing wrappers. Along the way, several other results on the complexity of
query evaluation and query containment for monadic datalog over trees are
established, and a simple normal form for this language is presented. Using the
above results, we subsequently study the kernel fragment Elog$^-$ of the Elog
wrapping language used in the Lixto system (a visual wrapper generator).
Curiously, Elog$^-$ exactly captures MSO, yet is easier to use. Indeed,
programs in this language can be entirely visually specified.

<id>
cs/0212004v1
<category>
cs.DB
<abstract>
We address the problem of minimal-change integrity maintenance in the context
of integrity constraints in relational databases. We assume that
integrity-restoration actions are limited to tuple deletions. We identify two
basic computational issues: repair checking (is a database instance a repair of
a given database?) and consistent query answers (is a tuple an answer to a
given query in every repair of a given database?). We study the computational
complexity of both problems, delineating the boundary between the tractable and
the intractable. We consider denial constraints, general functional and
inclusion dependencies, as well as key and foreign key constraints. Our results
shed light on the computational feasibility of minimal-change integrity
maintenance. The tractable cases should lead to practical implementations. The
intractability results highlight the inherent limitations of any integrity
enforcement mechanism, e.g., triggers or referential constraint actions, as a
way of performing minimal-change integrity maintenance.

<id>
cs/0212017v1
<category>
cs.DB
<abstract>
We present a data model for spatio-temporal databases. In this model
spatio-temporal data is represented as a finite union of objects described by
means of a spatial reference object, a temporal object and a geometric
transformation function that determines the change or movement of the reference
object in time.
  We define a number of practically relevant classes of spatio-temporal
objects, and give complete results concerning closure under Boolean set
operators for these classes. Since only few classes are closed under all set
operators, we suggest an extension of the model, which leads to better closure
properties, and therefore increased practical applicability. We also discuss a
normal form for this extended data model.

<id>
cs/0212051v1
<category>
cs.DB
<abstract>
Comprehensive semantic descriptions of Web services are essential to exploit
them in their full potential, that is, discovering them dynamically, and
enabling automated service negotiation, composition and monitoring. The
semantic mechanisms currently available in service registries which are based
on taxonomies fail to provide the means to achieve this. Although the terms
taxonomy and ontology are sometimes used interchangably there is a critical
difference. A taxonomy indicates only class/subclass relationship whereas an
ontology describes a domain completely. The essential mechanisms that ontology
languages provide include their formal specification (which allows them to be
queried) and their ability to define properties of classes. Through properties
very accurate descriptions of services can be defined and services can be
related to other services or resources. In this paper, we discuss the
advantages of describing service semantics through ontology languages and
describe how to relate the semantics defined with the services advertised in
service registries like UDDI and ebXML.

<id>
cs/0212052v1
<category>
cs.DB
<abstract>
In this paper we describe a framework for exploiting the semantics of Web
services through UDDI registries. As a part of this framework, we extend the
DAML-S upper ontology to describe the functionality we find essential for
e-businesses. This functionality includes relating the services with electronic
catalogs, describing the complementary services and finding services according
to the properties of products or services. Once the semantics is defined, there
is a need for a mechanism in the service registry to relate it with the service
advertised. The ontology model developed is general enough to be used with any
service registry. However when it comes to relating the semantics with services
advertised, the capabilities provided by the registry effects how this is
achieved. We demonstrate how to integrate the described service semantics to
UDDI registries.

<id>
cs/9811020v1
<category>
cs.DL
<abstract>
This paper addresses the issue of making legacy information (that material
held in paper format only) electronically searchable and retrievable. We used
proprietary software and commercial hardware to create a process for scanning,
cataloging, archiving and electronically disseminating full-text documents.
This process is relatively easy to implement and reasonably affordable.

<id>
cs/9812009v1
<category>
cs.DL
<abstract>
This paper presents the design and the current prototype implementation of an
interactive vocal Information Retrieval system that can be used to access
articles of a large newspaper archive using a telephone. The results of
preliminary investigation into the feasibility of such a system are also
presented.

<id>
cs/9812016v1
<category>
cs.DL
<abstract>
As most electronic journals available today have been derived from print
originals, print journals have become a vital element in the broad development
of electronic journals publishing. Further dependence on the print publishing
model, however, will be a constraint on the continuing development of
e-journals, and a series of conflicts are likely to arise. Making the most of
e-journals requires that a distinctive new publishing model is developed. We
consider some of the issues that will be fundamental in this new model,
starting with user motivations and some reported publisher experiences, both of
which suggest a broadening desire for comprehensive linked archives. This leads
in turn to questions about the impact of rights assignment by contributors, in
particular the common practice of giving exlusive rights to publishers for
individual works. Some non-prescriptive solutions are suggested, and four steps
towards optimum e-journals are proposed.

<id>
cs/9812020v1
<category>
cs.DL
<abstract>
We describe the Computing Research Repository (CoRR), a new electronic
archive for rapid dissemination and archiving of computer science research
results. CoRR was initiated in September 1998 through the cooperation of ACM,
LANL (Los Alamos National Laboratory) e-Print archive, and NCSTRL (Networked
Computer Science Technical Research Library. Through its implementation of the
Dienst protocol, CoRR combines the open and extensible architecture of NCSTRL
with the reliable access and well-established management practices of the LANL
XXX e-Print repository. This architecture will allow integration with other
e-Print archives and provides a foundation for a future broad-based scholarly
digital library. We describe the decisions that were made in creating CoRR, the
architecture of the CoRR/NCSTRL interoperation, and issues that have arisen
during the operation of CoRR.

<id>
cs/9901009v1
<category>
cs.DL
<abstract>
The conversion of scholarly journals to digital format is proceeding rapidly,
especially for those from large commercial and learned society publishers. This
conversion offers the best hope for survival for such publishers. The infamous
"journal crisis" is more of a library cost crisis than a publisher pricing
problem, with internal library costs much higher than the amount spent on
purchasing books and journals. Therefore publishers may be able to retain or
even increase their revenues and profits, while at the same time providing a
superior service. To do this, they will have to take over many of the function
of libraries, and they can do that only in the digital domain. This paper
examines publishers' strategies, how they are likely to evolve, and how they
will affect libraries.

<id>
cs/9902003v1
<category>
cs.DL
<abstract>
The paper describes an extensible model for implementing a user-centered,
customizable interface to a library's collection of information resources. This
model, called MyLibrary, integrates the principles of librarianship
(collection, organization, dissemination, and evaluation) with globally
networked computing resources creating a dynamic, customer-driven front-end to
any library's set of materials. The model supports a framework for libraries to
provide enhanced access to local and remote sets of data, information, and
knowledge. At the same, the model does not overwhelm its users with too much
information because the users control exactly how much information is displayed
to them at any given time. The model is active and not passive; direct human
interaction, computer mediated guidance and communication technologies, as well
as current awareness services all play indispensable roles in this system.

<id>
cs/9902004v1
<category>
cs.DL
<abstract>
This paper describes the Alex Catalogue of Electronic Texts, the only
Internet-accessible collection of digital documents allowing the user to 1)
dynamically create customized, typographically readable documents on demand, 2)
search the content of one or more documents from the collection simultaneously,
3) create sets of documents from the collection for review and annotation, and
4) publish these sets of annotated documents in turn fostering a sense of
community around the Catalogue. More than a just a collection of links that
will break over time, Alex is an archive of electronic texts providing
unprecedented access to its content and features allowing it to meet the needs
of a wide variety of users and settings. Furthermore, the process of
maintaining the Catalogue is streamlined with tools for automatic acquisition
and cataloging making it possible to sustain the service with a minimum of
personnel.

<id>
cs/9902007v1
<category>
cs.DL
<abstract>
Keyphrases provide semantic metadata that summarize and characterize
documents. This paper describes Kea, an algorithm for automatically extracting
keyphrases from text. Kea identifies candidate keyphrases using lexical
methods, calculates feature values for each candidate, and uses a
machine-learning algorithm to predict which candidates are good keyphrases. The
machine learning scheme first builds a prediction model using training
documents with known keyphrases, and then uses the model to find keyphrases in
new documents. We use a large test corpus to evaluate Kea's effectiveness in
terms of how many contributor-assigned keyphrases are correctly identified. The
system is simple, robust, and publicly available.

<id>
cs/9902009v1
<category>
cs.DL
<abstract>
Commercial OCR packages work best with high-quality scanned images. They
often produce poor results when the image is degraded, either because the
original itself was poor quality, or because of excessive photocopying. The
ability to predict the word failure rate of OCR from a statistical analysis of
the image can help in making decisions in the trade-off between the success
rate of OCR and the cost of human correction of errors. This paper describes an
investigation of OCR of degraded text images using a standard OCR engine (Adobe
Capture). The documents were selected from those in the archive at Los Alamos
National Laboratory. By introducing noise in a controlled manner into perfect
documents, we show how the quality of OCR can be predicted from the nature of
the noise. The preliminary results show that a simple noise model can give good
prediction of the number of OCR errors.

<id>
cs/9902011v1
<category>
cs.DL
<abstract>
Recommender systems improve access to relevant products and information by
making personalized suggestions based on previous examples of a user's likes
and dislikes. Most existing recommender systems use social filtering methods
that base recommendations on other users' preferences. By contrast,
content-based methods use information about an item itself to make suggestions.
This approach has the advantage of being able to recommended previously unrated
items to users with unique interests and to provide explanations for its
recommendations. We describe a content-based book recommending system that
utilizes information extraction and a machine-learning algorithm for text
categorization. Initial experimental results demonstrate that this approach can
produce accurate recommendations.

<id>
cs/9902012v1
<category>
cs.DL
<abstract>
In this paper we describe our efforts to bring scientific data into the
digital library. This has required extension of the standard WWW, and also the
extension of metadata standards far beyond the Dublin Core. Our system
demonstrates this technology for real scientific data from astronomy.

<id>
cs/9902013v1
<category>
cs.DL
<abstract>
Digital libraries must reach out to users from all walks of life, serving
information needs at all levels. To do this, they must attain high standards of
usability over an extremely broad audience. This paper details the evolution of
one important digital library component as it has grown in functionality and
usefulness over several years of use by a live, unrestricted community. Central
to its evolution have been user studies, analysis of use patterns, and
formative usability evaluation. We extrapolate that all three components are
necessary in the production of successful digital library systems.

<id>
cs/9902016v1
<category>
cs.DL
<abstract>
MPEG is undertaking a new initiative to standardize content description of
audio and video data/documents. When it is finalized in 2001, MPEG-7 is
expected to provide standardized description schemes for concise and
unambiguous content description of data/documents of complex media types.
Meanwhile, other meta-data or description schemes, such as Dublin Core, XML,
etc., are becoming popular in different application domains. In this paper, we
propose the Multimedia Description Framework (MDF), which is designated to
accommodate multiple description (meta-data) schemes, both MPEG-7 and
non-MPEG-7, into integrated architecture. We will use examples to show how MDF
description makes use of combined strength of different description schemes to
enhance its expression power and flexibility. We conclude the paper with
discussion of using MDF description of a movie video to search/retrieve
required scene clips from the movie, on the MDF prototype system we have
implemented.

<id>
cs/9902020v1
<category>
cs.DL
<abstract>
We describe an architecture and investigate the characteristics of
distributed searching in federated digital libraries. We introduce the notion
of a query mediator as a digital library service responsible for selecting
among available search engines, routing queries to those search engines, and
aggregating results. We examine operational data from the NCSTRL distributed
digital library that reveals a number of characteristics of distributed
resource discovery. These include availability and response time of indexers
and the distinction between the query mediator view of these characteristics
and the indexer view.

<id>
cs/9902022v1
<category>
cs.DL
<abstract>
With the growing significance of digital libraries and the Internet, more and
more electronic texts become accessible to a wide and geographically disperse
public. This requires adequate tools to facilitate indexing, storage, and
retrieval of documents written in different languages. We present a method for
semi-automatic indexing of electronic documents and construction of a
multilingual thesaurus, which can be used for query formulation and information
retrieval. We use special dictionaries and user interaction in order to solve
ambiguities and find adequate canonical terms in the language and adequate
abstract language-independent terms. The abstract thesaurus is updated
incrementally by new indexed documents and is used to search document
concerning terms in a query to the document base.

<id>
cs/9902023v1
<category>
cs.DL
<abstract>
A theoretic framework for multimedia information retrieval is introduced
which guarantees optimal retrieval effectiveness. In particular, a Ranking
Principle for Distributed Multimedia-Documents (RPDM) is described together
with an algorithm that satisfies this principle. Finally, the RPDM is shown to
be a generalization of the Probability Ranking principle (PRP) which guarantees
optimal retrieval effectiveness in the case of text document retrieval. The PRP
justifies theoretically the relevance ranking adopted by modern search engines.
In contrast to the classical PRP, the new RPDM takes into account transmission
and inspection time, and most importantly, aspectual recall rather than simple
recall.

<id>
cs/0009004v1
<category>
cs.DL
<abstract>
Based on an empirical analysis of contributor usage of CoRR, and of its
predecessor in the Los Alamos eprint archives, it is shown that CoRR has not
yet been able to match the early growth of the Los Alamos physics archives.
Some of the reasons are implicit in Halpern's paper, and we explore them
further here. In particular we refer to the need to promote CoRR more
effectively for its intended community - computer scientists in universities,
industrial research labs and in government. We take up some points of detail on
this new world of open archiving concerning central versus distributed
self-archiving, publication, the restructuring of the journal publishers'
niche, peer review and copyright.

<id>
cs/0101027v1
<category>
cs.DL
<abstract>
I outline the involvement of the Los Alamos e-print archive (arXiv) within
the Open Archives Initiative (OAI) and describe the implementation of the data
provider side of the OAI protocol v1.0. I highlight the ways in which we map
the existing structure of arXiv onto elements of the protocol.

<id>
cs/0106057v1
<category>
cs.DL
<abstract>
In this article I outline the ideas behind the Open Archives Initiative
metadata harvesting protocol (OAIMH), and attempt to clarify some common
misconceptions. I then consider how the OAIMH protocol can be used to expose
and harvest metadata. Perl code examples are given as practical illustration.

<id>
cs/0112017v1
<category>
cs.DL
<abstract>
With the increasing technical sophistication of both information consumers
and providers, there is increasing demand for more meaningful experiences of
digital information. We present a framework that separates digital object
experience, or rendering, from digital object storage and manipulation, so the
rendering can be tailored to particular communities of users. Our framework
also accommodates extensible digital object behaviors and interoperability. The
two key components of our approach are 1) exposing structural metadata
associated with digital objects -- metadata about the labeled access points
within a digital object and 2) information intermediaries called context
brokers that match structural characteristics of digital objects with
mechanisms that produce behaviors. These context brokers allow for localized
rendering of digital information stored externally.

<id>
cs/0201025v1
<category>
cs.DL
<abstract>
We describe the core components of the architecture for the (NSDL) National
Science, Mathematics, Engineering, and Technology Education Digital Library.
Over time the NSDL will include heterogeneous users, content, and services. To
accommodate this, a design for a technical and organization infrastructure has
been formulated based on the notion of a spectrum of interoperability. This
paper describes the first phase of the interoperability infrastructure
including the metadata repository, search and discovery services, rights
management services, and user interface portal facilities.

<id>
cs/0201027v1
<category>
cs.DL
<abstract>
We describe work leading toward specification of a technical architecture for
the National Science, Mathematics, Engineering, and Technology Education
Digital Library (NSDL). This includes a technical scope and a functional model,
with some elaboration on the particularly rich set of library services that
NSDL is expected eventually to encompass.

<id>
cs/0208012v1
<category>
cs.DL
<abstract>
Science projects are data publishers. The scale and complexity of current and
future science data changes the nature of the publication process. Publication
is becoming a major project component. At a minimum, a project must preserve
the ephemeral data it gathers. Derived data can be reconstructed from metadata,
but metadata is ephemeral. Longer term, a project should expect some archive to
preserve the data. We observe that pub-lished scientific data needs to be
available forever ? this gives rise to the data pyramid of versions and to data
inflation where the derived data volumes explode. As an example, this article
describes the Sloan Digital Sky Survey (SDSS) strategies for data publication,
data access, curation, and preservation.

<id>
cs/0208039v1
<category>
cs.DL
<abstract>
Through a collaborative effort, the Fermilab Information Resources Department
and Computing Division have created a "virtual library" of technical
publications that provides public access to electronic full-text documents.
This paper will discuss the vision, planning and milestones of the project, as
well as the hardware, software and interdepartmental cooperation components.

<id>
cs/0210029v1
<category>
cs.DL
<abstract>
This paper describes technological and methodological options to achieve
interoperability in accessing electronic information resources, available in
Internet, in the scope of Brazilian Digital Library in Science and Technology
Project - BDL, developed by Brazilian Institute for Scientific and Technical
Information - IBICT. It stresses the impact of the Web in the publishing and
communication processes in science and technology and also in the information
systems and libraries. The work points out the two major objectives of the BDL
Project: facilitates electronic publishing of different full text materials
such as theses, journal articles, conference papers,grey literature - by
Brazilian scientific community, so amplifying their nationally and
internationally visibility; and achieving, through a unified gateway, thus
avoiding a user to navigate and query across different information resources
individually. The work explains technological options and standards that will
assure interoperability in this context.

<id>
cs/0307008v1
<category>
cs.DL
<abstract>
The Open Archives Initiative (OAI) was created as a practical way to promote
interoperability between eprint repositories. Although the scope of the OAI has
been broadened, eprint repositories still represent a significant fraction of
OAI data providers. In this article I present a brief survey of OAI eprint
repositories, and of services using metadata harvested from eprint repositories
using the OAI protocol for metadata harvesting (OAI-PMH). I then discuss
several situations where metadata harvesting may be used to further improve the
utility of eprint archives as a component of the scholarly communication
infrastructure.

<id>
cs/0307049v1
<category>
cs.DL
<abstract>
We give a simple proof of the finite presentation of Sela's limit groups by
using free actions on $\bbR^n$-trees. We first prove that Sela's limit groups
do have a free action on an $\bbR^n$-tree. We then prove that a finitely
generated group having a free action on an $\bbR^n$-tree can be obtained from
free abelian groups and surface groups by a finite sequence of free products
and amalgamations over cyclic groups. As a corollary, such a group is finitely
presented, has a finite classifying space, its abelian subgroups are finitely
generated and contains only finitely many conjugacy classes of non-cyclic
maximal abelian subgroups.

<id>
cs/0401028v1
<category>
cs.DL
<abstract>
We describe a system used by the NASA Astrophysics Data System to identify
bibliographic references obtained from scanned article pages by OCR methods
with records in a bibliographic database. We analyze the process generating the
noisy references and conclude that the three-step procedure of correcting the
OCR results, parsing the corrected string and matching it against the database
provides unsatisfactory results. Instead, we propose a method that allows a
controlled merging of correction, parsing and matching, inspired by dependency
grammars. We also report on the effectiveness of various heuristics that we
have employed to improve recall.

<id>
cs/0401029v1
<category>
cs.DL
<abstract>
We discuss a methodology to dynamically generate links among digital objects
by means of an unsupervised learning mechanism which analyzes user link
traversal patterns. We performed an experiment with a test bed of 150 complex
data objects, referred to as buckets. Each bucket manages its own content,
provides methods to interact with users and individually maintains a set of
links to other buckets. We demonstrate that buckets were capable of dynamically
adjusting their links to other buckets according to user link selections,
thereby generating a meaningful network of bucket relations. Our results
indicate such adaptive networks of linked buckets approximate the collective
link preferences of a community of user

<id>
cs/0411077v1
<category>
cs.DL
<abstract>
The LOCKSS digital preservation system collects content by crawling the web
and preserves it in the format supplied by the publisher. Eventually, browsers
will no longer understand that format. A process called format migration
converts it to a newer format that the browsers do understand. The LOCKSS
program has designed and tested an initial implementation of format migration
for Web content that is transparent to readers, building on the content
negotiation capabilities of HTTP.

<id>
cs/9907002v1
<category>
cs.DM
<abstract>
This paper analyzes the distribution of cycle lengths in turbo decoding and
low-density parity check (LDPC) graphs. The properties of such cycles are of
significant interest in the context of iterative decoding algorithms which are
based on belief propagation or message passing. We estimate the probability
that there exist no simple cycles of length less than or equal to k at a
randomly chosen node in a turbo decoding graph using a combination of counting
arguments and independence assumptions. For large block lengths n, this
probability is approximately e^{-{2^{k-1}-4}/n}, k>=4. Simulation results
validate the accuracy of the various approximations. For example, for turbo
codes with a block length of 64000, a randomly chosen node has a less than 1%
chance of being on a cycle of length less than or equal to 10, but has a
greater than 99.9% chance of being on a cycle of length less than or equal to
20. The effect of the "S-random" permutation is also analyzed and it is shown
that while it eliminates short cycles of length k<8, it does not significantly
affect the overall distribution of cycle lengths. Similar analyses and
simulations are also presented for graphs for LDPC codes. The paper concludes
by commenting briefly on how these results may provide insight into the
practical success of iterative decoding methods.

<id>
cs/0003002v3
<category>
cs.DM
<abstract>
This paper has been withdrawn by the contributor(s),

<id>
cs/0003063v2
<category>
cs.DM
<abstract>
This paper has been temporarily withdrawn by the contributor(s),

<id>
cs/0010036v1
<category>
cs.DM
<abstract>
This paper is devoted to the study of the dynamics of a discrete system
related to some self stabilizing protocol on a ring of processors.

<id>
cs/0011031v1
<category>
cs.DM
<abstract>
The aim of this paper is to present and describe SimLab 1.1 (Simulation
Laboratory for Uncertainty and Sensitivity Analysis) software designed for
Monte Carlo analysis that is based on performing multiple model evaluations
with probabilistically selected model input. The results of these evaluations
are used to determine both the uncertainty in model predictions and the input
variables that drive this uncertainty. This methodology is essential in
situations where a decision has to be taken based on the model results; typical
examples include risk and emergency management systems, financial analysis and
many others. It is also highly recommended as part of model validation, even
where the models are used for diagnostic purposes, as an element of sound model
building. SimLab allows an exploration of the space of possible alternative
model assumptions and structure on the prediction of the model, thereby testing
both the quality of the model and the robustness of the model based inference.

<id>
cs/0106001v1
<category>
cs.DM
<abstract>
In this paper we study random linear systems with $k$ variables per equation
over the finite field GF(2), or equivalently $k$-XOR-CNF formulas. In a
previous paper Creignou and Daud\'e proved that the phase transition for the
consistency (satisfiability) of such systems (formulas) exhibits a sharp
threshold. Here we prove that the phase transition occurs as the number of
equations (clauses) is proportional to the number of variables. For any $k\ge
3$ we establish first estimates for the critical ratio. For $k=3$ we get 0.93
as an upper bound, 0.89 as a lower bound, whereas experiments suggest that the
critical ratio is approximately 0.92.

<id>
cs/0106002v2
<category>
cs.DM
<abstract>
Assembly line balancing problems consist in partitioning the work necessary
to assemble a number of products among different stations of an assembly line.
We present a hybrid approach for solving such problems, which combines
constraint programming and integer programming.

<id>
cs/0111061v2
<category>
cs.DM
<abstract>
For a given finite class of finite graphs H, a graph G is called a
realization of H if the neighbourhood of its any vertex induces the subgraph
isomorphic to a graph of H. We consider the following problem known as the
Generalized Neighbourhood Problem (GNP): given a finite class of finite graphs
H, does there exist a non-empty graph G that is a realization of H? In fact,
there are two modifications of that problem, namely the finite (the existence
of a finite realization is required) and infinite one (the realization is
required to be infinite). In this paper we show that GNP and its modifications
for all finite classes H of finite graphs are reduced to the same problems with
an additional restriction on H. Namely, the orders of any two graphs of H are
equal and every graph of H has exactly s dominating vertices.

<id>
cs/0209005v1
<category>
cs.DM
<abstract>
We know that the marginals in a multinomial distribution are binomial
variates exhibiting a negative correlation. But we can construct two linear
combinations of such marginals in such a way to obtain a positive correlation.
We discuss the restrictions that are to be imposed on the parameters of the
given marginals to accomplish such a result. Next we discuss the regression
function, showing that it is a linear function but not homoscedastic.

<id>
cs/0301025v1
<category>
cs.DM
<abstract>
In this paper we propose a simple and efficient strategy to obtain a data
structure generator to accomplish a perfect hash of quite general order
restricted multidimensional arrays named {\em phormas}. The constructor of such
objects gets two parameters as input: an n-vector a of non negative integers
and a boolean function B on the types of order restrictions on the coordinates
of the valid n-vectors bounded by a. At compiler time, the phorma constructor
builds, from the pair a,B, a digraph G(a,B) with a single source s and a single
sink t such that the st-paths are in 1-1 correspondence with the members of the
B-restricted a-bounded array A(a,B). Besides perfectly hashing A(a,B), G(a,B)
is an instance of an NW-family. This permits other useful computational tasks
on it.

<id>
cs/0305006v1
<category>
cs.DM
<abstract>
A coloring of a complete bipartite graph is shuffle-preserved if it is the
case that assigning a color $c$ to edges $(u, v)$ and $(u', v')$ enforces the
same color assignment for edges $(u, v')$ and $(u',v)$. (In words, the induced
subgraph with respect to color $c$ is complete.) In this paper, we investigate
a variant of the Ramsey problem for the class of complete bipartite
multigraphs. (By a multigraph we mean a graph in which multiple edges, but no
loops, are allowed.) Unlike the conventional m-coloring scheme in Ramsey theory
which imposes a constraint (i.e., $m$) on the total number of colors allowed in
a graph, we introduce a relaxed version called m-local coloring which only
requires that, for every vertex $v$, the number of colors associated with $v$'s
incident edges is bounded by $m$. Note that the number of colors found in a
graph under $m$-local coloring may exceed m. We prove that given any $n \times
n$ complete bipartite multigraph $G$, every shuffle-preserved $m$-local
coloring displays a monochromatic copy of $K_{p,p}$ provided that $2(p-1)(m-1)
< n$. Moreover, the above bound is tight when (i) $m=2$, or (ii) $n=2^k$ and
$m=3\cdot 2^{k-2}$ for every integer $k\geq 2$. As for the lower bound of $p$,
we show that the existence of a monochromatic $K_{p,p}$ is not guaranteed if
$p> \lceil \frac{n}{m} \rceil$. Finally, we give a generalization for
$k$-partite graphs and a method applicable to general graphs. Many conclusions
found in $m$-local coloring can be inferred to similar results of $m$-coloring.

<id>
cs/0305039v2
<category>
cs.DM
<abstract>
The relationship between the length of a word and the maximum length of its
unbordered factors is investigated in this paper. Consider a finite word w of
length n. We call a word bordered, if it has a proper prefix which is also a
suffix of that word. Let f(w) denote the maximum length of all unbordered
factors of w, and let p(w) denote the period of w. Clearly, f(w) < p(w)+1.
  We establish that f(w) = p(w), if w has an unbordered prefix of length f(w)
and n > 2f(w)-2. This bound is tight and solves the stronger version of a 21
years old conjecture by Duval. It follows from this result that, in general, n
> 3f(w)-3 implies f(w) = p(w) which gives an improved bound for the question
asked by Ehrenfeucht and Silberger in 1979.

<id>
cs/0305051v1
<category>
cs.DM
<abstract>
The bandwidth of a graph is the labeling of vertices with minimum maximum
edge difference. For many graph families this is NP-complete. A classic result
computes the bandwidth for the hypercube. We generalize this result to give
sharp lower bounds for products of cliques. This problem turns out to be
equivalent to one in communication over multiple channels in which channels can
fail and the information sent over those channels is lost. The goal is to
create an encoding that minimizes the difference between the received and the
original information while having as little redundancy as possible. Berger-Wolf
and Reingold [2] have considered the problem for the equal size cliques (or
equal capacity channels). This paper presents a tight lower bound and an
algorithm for constructing the labeling for the product of any number of
arbitrary size cliques.

<id>
cs/0309017v1
<category>
cs.DM
<abstract>
We characterize the set of planar locally finite Cayley graphs, and give a
finite representation of these graphs by a special kind of finite state
automata called labeling schemes. As a result, we are able to enumerate and
describe all planar locally finite Cayley graphs of a given degree. This
analysis allows us to solve the problem of decision of the locally finite
planarity for a word-problem-decidable presentation.
  Keywords: vertex-transitive, Cayley graph, planar graph, tiling, labeling
scheme

<id>
cs/0404031v1
<category>
cs.DM
<abstract>
Characterisations of interval graphs, comparability graphs, co-comparability
graphs, permutation graphs, and split graphs in terms of linear orderings of
the vertex set are presented. As an application, it is proved that interval
graphs, co-comparability graphs, AT-free graphs, and split graphs have
bandwidth bounded by their maximum degree.

<id>
cs/0406010v1
<category>
cs.DM
<abstract>
Based on Jensen formulae and the second kind of Chebyshev polynomials,
another proof is presented for an extension of a curious binomial identity due
to Z. W. Sun and K. J. Wu.

<id>
cs/0408067v1
<category>
cs.DM
<abstract>
Rule k is a localized approximation algorithm that finds a small connected
dominating set in a graph. We estimate the expected size of the Rule k
dominating set for the model of random unit disk graphs constructed from n
random points in an s_n by s_n square region of the plane.

<id>
cs/0408068v1
<category>
cs.DM
<abstract>
Li and Wu proposed Rule 2, a localized approximation algorithm that attempts
to find a small connected dominating set in a graph. Here we study the
asymptotic performance of Rule 2 on random unit disk graphs formed from n
random points in an s_n by s_n square region of the plane. If s_n is below the
threshold for connectivity, then Rule 2 produces a dominating set whose
expected size is O(n/(loglog n)^{3/2}). We conjecture that this bound is not
optimal.

<id>
cs/0409023v2
<category>
cs.DM
<abstract>
We consider functions mapping non-negative integers to non-negative real
numbers such that a and a+n are mapped to values at least 1/n apart. In this
paper we use a novel method to construct such a function. We conjecture that
the supremum of the generated function is optimal and pose some unsolved
problems.

<id>
cs/0410031v2
<category>
cs.DM
<abstract>
Due to the appearance of uncontrollable events in discrete event systems, one
may wish to replace the behavior leading to the uncontrollability of
pre-specified language by some quite similar one. To capture this similarity,
we introduce metric to traditional supervisory control theory and generalize
the concept of original controllability to $\ld$-controllability, where $\ld$
indicates the similarity degree of two languages. A necessary and sufficient
condition for a language to be $\ld$-controllable is provided. We then examine
some properties of $\ld$-controllable languages and present an approach to
optimizing a realization.

<id>
cs/0412078v1
<category>
cs.DM
<abstract>
We consider the class of the topologically locally finite (in short TLF)
planar vertex-transitive graphs, a class containing in particular all the
one-ended planar Cayley graphs and the normal transitive tilings. We
characterize these graphs with a finite local representation and a special kind
of finite state automaton named labeling scheme. As a result, we are able to
enumerate and describe all TLF-planar vertex-transitive graphs of any given
degree. Also, we are able decide to whether any TLF-planar transitive graph is
Cayley or not.

<id>
cs/0502038v1
<category>
cs.DM
<abstract>
In this paper we examine the classes of graphs whose $K_n$-complements are
trees and quasi-threshold graphs and derive formulas for their number of
spanning trees; for a subgraph $H$ of $K_n$, the $K_n$-complement of $H$ is the
graph $K_n-H$ which is obtained from $K_n$ by removing the edges of $H$. Our
proofs are based on the complement spanning-tree matrix theorem, which
expresses the number of spanning trees of a graph as a function of the
determinant of a matrix that can be easily constructed from the adjacency
relation of the graph. Our results generalize previous results and extend the
family of graphs of the form $K_n-H$ admitting formulas for the number of their
spanning trees.

<id>
cs/0503009v1
<category>
cs.DM
<abstract>
A sense of direction is an edge labeling on graphs that follows a globally
consistent scheme and is known to considerably reduce the complexity of several
distributed problems. In this paper, we study a particular instance of sense of
direction, called a chordal sense of direction (CSD). In special, we identify
the class of k-regular graphs that admit a CSD with exactly k labels (a minimal
CSD). We prove that connected graphs in this class are Hamiltonian and that the
class is equivalent to that of circulant graphs, presenting an efficient
(polynomial-time) way of recognizing it when the graphs' degree k is fixed.

<id>
cs/0503039v12
<category>
cs.DM
<abstract>
Here I share a few notes I used in various course lectures, talks, etc. Some
may be just calculations that in the textbooks are more complicated, scattered,
or less specific; others may be simple observations I found useful or curious.

<id>
cs/0504082v1
<category>
cs.DM
<abstract>
We consider the class A of graphs that contain no odd hole, no antihole, and
no ``prism'' (a graph consisting of two disjoint triangles with three disjoint
paths between them). We show that the coloring algorithm found by the second
and fourth contributor can be implemented in time O(n^2m) for any graph in A with n
vertices and m edges, thereby improving on the complexity proposed in the
original paper.

<id>
cs/0505036v1
<category>
cs.DM
<abstract>
Let $G$ be an Eulerian directed graph with an arc-labeling such that arcs
going out from the same vertex have different labels. In this work, we present
an algorithm to construct the Eulerian trail starting at an arbitrary vertex
$v$ of minimum lexicographical label among labels of all Eulerian trails
starting at this vertex.
  We also show an application of this algorithm to construct the minimal de
Bruijn sequence of a language.

<id>
cs/0505088v4
<category>
cs.DM
<abstract>
A cycle double cover (CDC) of an undirected graph is a collection of the
graph's cycles such that every edge of the graph belongs to exactly two cycles.
We describe a constructive method for generating all the cubic graphs that have
a 6-CDC (a CDC in which every cycle has length 6). As an application of the
method, we prove that all such graphs have a Hamiltonian cycle. A sense of
direction is an edge labeling on graphs that follows a globally consistent
scheme and is known to considerably reduce the complexity of several
distributed problems. In [9], a particular instance of sense of direction,
called a chordal sense of direction (CSD), is studied and the class of
k-regular graphs that admit a CSD with exactly k labels (a minimal CSD) is
analyzed. We now show that nearly all the cubic graphs in this class have a
6-CDC, the only exception being K4.

<id>
cs/0507016v1
<category>
cs.DM
<abstract>
We consider the problem of minimizing the makespan in a flowshop involving
maximal and minimal time lags. Time lag constraints generalize the classical
precedence constraints between operations. We assume that such constraints are
only defined between operations of the same job. We propose a solution method
and present several extensions.

<id>
cs/0507017v2
<category>
cs.DM
<abstract>
The following optimization problem was introduced in \cite{gutinDAM}, where
it was motivated by a real-world problem in defence logistics. Suppose we are
given a pair of digraphs $D,H$ and a positive cost $c_i(u)$ for each $u\in
V(D)$ and $i\in V(H)$. The cost of a homomorphism $f$ of $D$ to $H$ is
$\sum_{u\in V(D)}c_{f(u)}(u)$. For a fixed digraph $H$, the minimum cost
homomorphism problem for $H$, MinHOMP($H$), is stated as follows: For an input
digraph $D$ and costs $c_i(u)$ for each $u\in V(D)$ and $i\in V(H)$, verify
whether there is a homomorphism of $D$ to $H$ and, if it exists, find such a
homomorphism of minimum cost.
  We obtain dichotomy classifications of the computational complexity of the
list homomorphism problem and MinHOMP($H$), when $H$ is a semicomplete digraph
(a digraph in which every two vertices have at least one arc between them). Our
dichotomy for the list homomorphism problem coincides with the one obtained by
Bang-Jensen, Hell and MacGillivray in 1988 for the homomorphism problem when
$H$ is a semicomplete digraph: both problems are polynomial solvable if $H$ has
at most one cycle; otherwise, both problems are NP-complete. The dichotomy for
\MiP is different: the problem is polynomial time solvable if $H$ is acyclic or
$H$ is a cycle of length 2 or 3; otherwise, the problem is NP-hard.

<id>
cs/0509004v1
<category>
cs.DM
<abstract>
The pre-coloring extension problem consists, given a graph $G$ and a subset
of nodes to which some colors are already assigned, in finding a coloring of
$G$ with the minimum number of colors which respects the pre-coloring
assignment. This can be reduced to the usual coloring problem on a certain
contracted graph. We prove that pre-coloring extension is polynomial for
complements of Meyniel graphs. We answer a question of Hujter and Tuza by
showing that ``PrExt perfect'' graphs are exactly the co-Meyniel graphs, which
also generalizes results of Hujter and Tuza and of Hertz. Moreover we show
that, given a co-Meyniel graph, the corresponding contracted graph belongs to a
restricted class of perfect graphs (``co-Artemis'' graphs, which are
``co-perfectly contractile'' graphs), whose perfectness is easier to establish
than the strong perfect graph theorem. However, the polynomiality of our
algorithm still depends on the ellipsoid method for coloring perfect graphs.

<id>
cs/9809125v1
<category>
cs.DC
<abstract>
This paper presents a synchronization-based, multi-process computational
model of anticipatory systems called the Phase Web. It describes a
self-organizing paradigm that explicitly recognizes and exploits the existence
of a boundary between inside and outside, accepts and exploits intentionality,
and uses explicit self-reference to describe eg. auto-poiesis. The model
explicitly connects computation to a discrete Clifford algebraic formalization
that is in turn extended into homology and co-homology, wherein the recursive
nature of objects and boundaries becomes apparent and itself subject to
hierarchical recursion. Topsy, a computer program embodying the Phase Web, is
available at www.cs.auc.dk/topsy.

<id>
cs/9810019v1
<category>
cs.DC
<abstract>
Gryphon is a distributed computing paradigm for message brokering, which is
the transferring of information in the form of streams of events from
information providers to information consumers. This extended abstract outlines
the major problems in message brokering and Gryphon's approach to solving them.

<id>
cs/9909013v1
<category>
cs.DC
<abstract>
We show that, contrary to common belief, Dijkstra's self-stabilizing mutual
exclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number of
states per node is one less than the number of nodes on the ring.

<id>
cs/9909015v1
<category>
cs.DC
<abstract>
We argue that the tools of decision theory need to be taken more seriously in
the specification and analysis of systems. We illustrate this by considering a
simple problem involving reliable communication, showing how considerations of
utility and probability can be used to decide when it is worth sending
heartbeat messages and, if they are sent, how often they should be sent.

<id>
cs/0002008v1
<category>
cs.DC
<abstract>
We present a theory of automata with boundary for designing, modelling and
analysing distributed systems. Notions of behaviour, design and simulation
appropriate to the theory are defined. The problem of model checking for
deadlock detection is discussed, and an algorithm for state space reduction in
exhaustive search, based on the theory presented here, is described. Three
examples of the application of the theory are given, one in the course of the
development of the ideas and two as illustrative examples of the use of the
theory.

<id>
cs/0003054v1
<category>
cs.DC
<abstract>
The idle computers on a local area, campus area, or even wide area network
represent a significant computational resource---one that is, however, also
unreliable, heterogeneous, and opportunistic. This type of resource has been
used effectively for embarrassingly parallel problems but not for more tightly
coupled problems. We describe an algorithm that allows branch-and-bound
problems to be solved in such environments. In designing this algorithm, we
faced two challenges: (1) scalability, to effectively exploit the variably
sized pools of resources available, and (2) fault tolerance, to ensure the
reliability of services. We achieve scalability through a fully decentralized
algorithm, by using a membership protocol for managing dynamically available
resources. However, this fully decentralized design makes achieving reliability
even more challenging. We guarantee fault tolerance in the sense that the loss
of up to all but one resource will not affect the quality of the solution. For
propagating information efficiently, we use epidemic communication for both the
membership protocol and the fault-tolerance mechanism. We have developed a
simulation framework that allows us to evaluate design alternatives. Results
obtained in this framework suggest that our techniques can execute scalably and
reliably.

<id>
cs/0004013v1
<category>
cs.DC
<abstract>
Sorting is one of the classic problems of computer science. Whilst well
understood on sequential machines, the diversity of architectures amongst
parallel systems means that algorithms do not perform uniformly on all
platforms. This document describes the implementation of a radix based
algorithm for sorting positive integers on a Fujitsu AP1000 Supercomputer,
which was constructed as an entry in the Joint Symposium on Parallel Processing
(JSPP) 1994 Parallel Software Contest (PSC94). Brief consideration is also
given to a full radix sort conducted in parallel across the machine.

<id>
cs/0006004v1
<category>
cs.DC
<abstract>
The problem of minimizing mean response time of generic jobs submitted to a
heterogenous distributed computer systems is considered in this paper. A static
load balancing strategy, in which decision of redistribution of loads does not
depend on the state of the system, is used for this purpose. The article is
closely related to a previous article on the same topic. The present article
points out number of inconsistencies in the previous article, provides a new
formulation, and discusses the impact of new findings, based on the improved
formulation, on the results of the previous article.

<id>
cs/0006008v1
<category>
cs.DC
<abstract>
We consider a system of t synchronous processes that communicate only by
sending messages to one another, and that together must perform $n$ independent
units of work. Processes may fail by crashing; we want to guarantee that in
every execution of the protocol in which at least one process survives, all n
units of work will be performed. We consider three parameters: the number of
messages sent, the total number of units of work performed (including
multiplicities), and time. We present three protocols for solving the problem.
All three are work-optimal, doing O(n+t) work. The first has moderate costs in
the remaining two parameters, sending O(t\sqrt{t}) messages, and taking O(n+t)
time. This protocol can be easily modified to run in any completely
asynchronous system equipped with a failure detection mechanism. The second
sends only O(t log{t}) messages, but its running time is large (exponential in
n and t). The third is essentially time-optimal in the (usual) case in which
there are no failures, and its time complexity degrades gracefully as the
number of failures increases.

<id>
cs/0007015v1
<category>
cs.DC
<abstract>
Phase clocks are synchronization tools that implement a form of logical time
in distributed systems. For systems tolerating transient faults by self-repair
of damaged data, phase clocks can enable reasoning about the progress of
distributed repair procedures. This paper presents a phase clock algorithm
suited to the model of transient memory faults in asynchronous systems with
read/write registers. The algorithm is self-stabilizing and guarantees accuracy
of phase clocks within O(k) time following an initial state that is k-faulty.
Composition theorems show how the algorithm can be used for the timing of
distributed procedures that repair system outputs.

<id>
cs/0009021v1
<category>
cs.DC
<abstract>
The availability of powerful microprocessors and high-speed networks as
commodity components has enabled high performance computing on distributed
systems (wide-area cluster computing). In this environment, as the resources
are usually distributed geographically at various levels (department,
enterprise, or worldwide) there is a great challenge in integrating,
coordinating and presenting them as a single resource to the user; thus forming
a computational grid. Another challenge comes from the distributed ownership of
resources with each resource having its own access policy, cost, and mechanism.
  The proposed Nimrod/G grid-enabled resource management and scheduling system
builds on our earlier work on Nimrod and follows a modular and component-based
architecture enabling extensibility, portability, ease of development, and
interoperability of independently developed components. It uses the Globus
toolkit services and can be easily extended to operate with any other emerging
grid middleware services. It focuses on the management and scheduling of
computations over dynamic resources scattered geographically across the
Internet at department, enterprise, or global level with particular emphasis on
developing scheduling schemes based on the concept of computational economy for
a real test bed, namely, the Globus testbed (GUSTO).

<id>
cs/0012024v4
<category>
cs.DC
<abstract>
Byzantine Agreement introduced in [Pease, Shostak, Lamport, 80] is a widely
used building block of reliable distributed protocols. It simulates broadcast
despite the presence of faulty parties within the network, traditionally using
only private unicast links. Under such conditions, Byzantine Agreement requires
more than 2/3 of the parties to be compliant. [Fitzi, Maurer, 00], constructed
a Byzantine Agreement protocol for any compliant majority based on an
additional primitive allowing transmission to any two parties simultaneously.
They proposed a problem of generalizing these results to wider channels and
fewer compliant parties. We prove that 2f < kh condition is necessary and
sufficient for implementing broadcast with h compliant and f faulty parties
using k-cast channels.

<id>
cs/0102016v1
<category>
cs.DC
<abstract>
Many scientific applications are I/O intensive and generate or access large
data sets, spanning hundreds or thousands of "files." Management, storage,
efficient access, and analysis of this data present an extremely challenging
task. We have developed a software system, called Scientific Data Manager
(SDM), that uses a combination of parallel file I/O and database support for
high-performance scientific data management. SDM provides a high-level API to
the user and internally, uses a parallel file system to store real data and a
database to store application-related metadata. In this paper, we describe how
we designed and implemented SDM to support irregular applications. SDM can
efficiently handle the reading and writing of data in an irregular mesh as well
as the distribution of index values. We describe the SDM user interface and how
we implemented it to achieve high performance. SDM makes extensive use of
MPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a
history file to optimize the cost of the index distribution using the metadata
stored in the database. We present performance results with two irregular
applications, a CFD code called FUN3D and a Rayleigh-Taylor instability code,
on the SGI Origin2000 at Argonne National Laboratory.

<id>
cs/0102017v1
<category>
cs.DC
<abstract>
Parallel jobs are different from sequential jobs and require a different type
of process management. We present here a process management system for parallel
programs such as those written using MPI. A primary goal of the system, which
we call MPD (for multipurpose daemon), is to be scalable. By this we mean that
startup of interactive parallel jobs comprising thousands of processes is
quick, that signals can be quickly delivered to processes, and that stdin,
stdout, and stderr are managed intuitively. Our primary target is parallel
machines made up of clusters of SMPs, but the system is also useful in more
tightly integrated environments. We describe how MPD enables much faster
startup and better runtime management of parallel jobs. We show how close
control of stdio can support the easy implementation of a number of convenient
system utilities, even a parallel debugger. We describe a simple but general
interface that can be used to separate any process manager from a parallel
library, which we use to keep MPD separate from MPICH.

<id>
cs/0104002v1
<category>
cs.DC
<abstract>
The Globus Data Grid architecture provides a scalable infrastructure for the
management of storage resources and data that are distributed across Grid
environments. These services are designed to support a variety of scientific
applications, ranging from high-energy physics to computational genomics, that
require access to large amounts of data (terabytes or even petabytes) with
varied quality of service requirements. By layering on a set of core services,
such as data transport, security, and replica cataloging, one can construct
various higher-level services. In this paper, we discuss the design and
implementation of a high-level replica selection service that uses information
regarding replica location and user preferences to guide selection from among
storage replica alternatives. We first present a basic replica selection
service design, then show how dynamic information collected using Globus
information service capabilities concerning storage system properties can help
improve and optimize the selection process. We demonstrate the use of Condor's
ClassAds resource description and matchmaking mechanism as an efficient tool
for representing and matching storage resource capabilities and policies
against application requirements.

<id>
cs/0105013v1
<category>
cs.DC
<abstract>
The first self-stabilizing algorithm [Dij73] assumed the existence of a
central daemon, that activates one processor at time to change state as a
function of its own state and the state of a neighbor. Subsequent research has
reconsidered this algorithm without the assumption of a central daemon, and
under different forms of communication, such as the model of link registers. In
all of these investigations, one common feature is the atomicity of
communication, whether by shared variables or read/write registers. This paper
weakens the atomicity assumptions for the communication model, proposing
versions of [Dij73] that tolerate various weaker forms of atomicity. First, a
solution for the case of regular registers is presented. Then the case of safe
registers is considered, with both negative and positive results presented. The
paper also presents an implementation of [Dij73] based on registers that have
probabilistically correct behavior, which requires a notion of weak
stabilization.

<id>
cs/0105034v1
<category>
cs.DC
<abstract>
This paper precisely analyzes the wire density and required area in standard
layout styles for the hypercube. The most natural, regular layout of a
hypercube of N^2 nodes in the plane, in a N x N grid arrangement, uses
floor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number of
tracks per row can be reduced by 1 with a less regular design.) This paper also
gives a simple formula for the wire density at any cut position and a full
characterization of all places where the wire density is maximized (which does
not occur at the bisection).

<id>
cs/0106020v1
<category>
cs.DC
<abstract>
The accelerated development in Grid and peer-to-peer computing has positioned
them as promising next generation computing platforms. They enable the creation
of Virtual Enterprises (VE) for sharing resources distributed across the world.
However, resource management, application development and usage models in these
environments is a complex undertaking. This is due to the geographic
distribution of resources that are owned by different organizations. The
resource owners of each of these resources have different usage or access
policies and cost models, and varying loads and availability. In order to
address complex resource management issues, we have proposed a computational
economy framework for resource allocation and for regulating supply and demand
in Grid computing environments. The framework provides mechanisms for
optimizing resource provider and consumer objective functions through trading
and brokering services. In a real world market, there exist various economic
models for setting the price for goods based on supply-and-demand and their
value to the user. They include commodity market, posted price, tenders and
auctions. In this paper, we discuss the use of these models for interaction
between Grid components in deciding resource value and the necessary
infrastructure to realize them. In addition to normal services offered by Grid
computing systems, we need an infrastructure to support interaction protocols,
allocation mechanisms, currency, secure banking, and enforcement services.
Furthermore, we demonstrate the usage of some of these economic models in
resource brokering through Nimrod/G deadline and cost-based scheduling for two
different optimization strategies on the World Wide Grid (WWG) testbed.

<id>
cs/0106038v1
<category>
cs.DC
<abstract>
High-throughput computing projects require the solution of large numbers of
problems. In many cases, these problems can be solved on desktop PCs, or can be
broken down into independent "PC-solvable" sub-problems. In such cases, the
projects are high-performance computing projects, but only because of the sheer
number of the needed calculations. We briefly describe our efforts to increase
the throughput of one such project. We then explain how to easily set up a
distributed computing facility composed of standard networked PCs running
Windows 95, 98, 2000, or NT. The facility requires no special software or
hardware, involves little or no re-coding of application software, and operates
almost invisibly to the owners of the PCs. Depending on the number and quality
of PCs recruited, performance can rival that of supercomputers.

<id>
cs/0106056v2
<category>
cs.DC
<abstract>
We present the first explicit, and currently simplest, randomized algorithm
for 2-process wait-free test-and-set. It is implemented with two 4-valued
single writer single reader atomic variables. A test-and-set takes at most 11
expected elementary steps, while a reset takes exactly 1 elementary step. Based
on a finite-state analysis, the proofs of correctness and expected length are
compressed into one table.

<id>
cs/0107034v1
<category>
cs.DC
<abstract>
The NEOS Server 4.0 provides a general Internet-based client/server as a link
between users and software applications. The administrative guide covers the
fundamental principals behind the operation of the NEOS Server, installation
and trouble-shooting of the Server software, and implementation details of
potential interest to a NEOS Server administrator. The guide also discusses
making new software applications available through the Server, including areas
of concern to remote solver administrators such as maintaining security,
providing usage instructions, and enforcing reasonable restrictions on jobs.
The administrative guide is intended both as an introduction to the NEOS Server
and as a reference for use when running the Server.

<id>
cs/0108001v1
<category>
cs.DC
<abstract>
The ability to harness heterogeneous, dynamically available "Grid" resources
is attractive to typically resource-starved computational scientists and
engineers, as in principle it can increase, by significant factors, the number
of cycles that can be delivered to applications. However, new adaptive
application structures and dynamic runtime system mechanisms are required if we
are to operate effectively in Grid environments. In order to explore some of
these issues in a practical setting, we are developing an experimental
framework, called Cactus, that incorporates both adaptive application
structures for dealing with changing resource characteristics and adaptive
resource selection mechanisms that allow applications to change their resource
allocations (e.g., via migration) when performance falls outside specified
limits. We describe here the adaptive resource selection mechanisms and
describe how they are used to achieve automatic application migration to
"better" resources following performance degradation. Our results provide
insights into the architectural structures required to support adaptive
resource selection. In addition, we suggest that this "Cactus Worm" is an
interesting challenge problem for Grid computing.

<id>
cs/0108002v1
<category>
cs.DC
<abstract>
Shared registers are basic objects used as communication mediums in
asynchronous concurrent computation. A concurrent timestamp system is a higher
typed communication object, and has been shown to be a powerful tool to solve
many concurrency control problems. It has turned out to be possible to
construct such higher typed objects from primitive lower typed ones. The next
step is to find efficient constructions. We propose a very efficient wait-free
construction of bounded concurrent timestamp systems from 1-writer multireader
registers. This finalizes, corrects, and extends, a preliminary bounded
multiwriter construction proposed by the second contributor in 1986. That work
partially initiated the current interest in wait-free concurrent objects, and
introduced a notion of discrete vector clocks in distributed algorithms.

<id>
cs/0108019v1
<category>
cs.DC
<abstract>
We describe a family of MPI applications we call the Parallel Unix Commands.
These commands are natural parallel versions of common Unix user commands such
as ls, ps, and find, together with a few similar commands particular to the
parallel environment. We describe the design and implementation of these
programs and present some performance results on a 256-node Linux cluster. The
Parallel Unix Commands are open source and freely available.

<id>
cs/0109017v1
<category>
cs.DC
<abstract>
The Message Passing Interface (MPI) has been extremely successful as a
portable way to program high-performance parallel computers. This success has
occurred in spite of the view of many that message passing is difficult and
that other approaches, including automatic parallelization and directive-based
parallelism, are easier to use. This paper argues that MPI has succeeded
because it addresses all of the important issues in providing a parallel
programming model.

<id>
cs/0110058v1
<category>
cs.DC
<abstract>
We discuss the use of both MPI and OpenMP in the teaching of senior
undergraduate and junior graduate classes in parallel programming. We briefly
introduce the OpenMP standard and discuss why we have chosen to use it in
parallel programming classes. Advantages of using OpenMP over message passing
methods are discussed. We also include a brief enumeration of some of the
drawbacks of using OpenMP and how these drawbacks are being addressed by
supplementing OpenMP with additional MPI codes and projects. Several projects
given in my class are also described in this paper.

<id>
cs/0111028v1
<category>
cs.DC
<abstract>
TANGO is an object oriented control system toolkit based on CORBA presently
under development at the ESRF. IN this paper, the TANGO philosophy is briefly
presented. All the existing tools developed around TANGO will also be
presented. This include a code genrator, a WEB interface to TANGO objects, an
administration tool and an interface to LabView. Finally, an xample of a TANGO
device server for OPC device is given.

<id>
cs/0111031v1
<category>
cs.DC
<abstract>
The Integrated Computer Control System (ICCS) is based on a scalable software
framework that is distributed over some 325 computers throughout the NIF
facility. The framework provides templates and services at multiple levels of
abstraction for the construction of software applications that communicate via
CORBA (Common Object Request Broker Architecture). Various forms of
object-oriented software design patterns are implemented as templates to be
extended by application software. Developers extend the framework base classes
to model the numerous physical control points, thereby sharing the
functionality defined by the base classes. About 56,000 software objects each
individually addressed through CORBA are to be created in the complete ICCS.
Most objects have a persistent state that is initialized at system start-up and
stored in a database. Additional framework services are provided by centralized
server programs that implement events, alerts, reservations, message logging,
database/file persistence, name services, and process management. The ICCS
software framework approach allows for efficient construction of a software
system that supports a large number of distributed control points representing
a complex control application.

<id>
cs/0111033v1
<category>
cs.DC
<abstract>
he ESRF control system is in the process of being modernised. The present
contrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC,
Motif and C. The new control system will be based on compact PCI, 100 MHz
Ethernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main
frontend operating system will be GNU/Linux running on Intel/x86 and
Motorola/68k. Linux will also be used on handheld devices for mobile control.
This poster describes how GNU/Linux is being used to modernise the control
system and what problems have been encountered so far

<id>
cs/0111047v1
<category>
cs.DC
<abstract>
Computational Grids are emerging as a popular paradigm for solving
large-scale compute and data intensive problems in science, engineering, and
commerce. However, application composition, resource management and scheduling
in these environments is a complex undertaking. In this paper, we illustrate
the creation of a virtual laboratory environment by leveraging existing Grid
technologies to enable molecular modeling for drug design on distributed
resources. It involves screening millions of molecules of chemical compounds
against a protein target, chemical database (CDB) to identify those with
potential use for drug design. We have grid-enabled the molecular docking
process by composing it as a parameter sweep application using the Nimrod-G
tools. We then developed new tools for remote access to molecules in CDB small
molecule database. The Nimrod-G resource broker along with molecule CDB data
broker is used for scheduling and on-demand processing of jobs on distributed
grid resources. The results demonstrate the ease of use and suitability of the
Nimrod-G and virtual laboratory tools.

<id>
cs/9301114v1
<category>
cs.GL
<abstract>
The contributor argues to Silicon Valley that the most important and powerful part
of computer science is work that is simultaneously theoretical and practical.
He particularly considers the intersection of the theory of algorithms and
practical software development. He combines examples from the development of
the TeX typesetting system with clever jokes, criticisms, and encouragements.

<id>
cs/9809010v1
<category>
cs.GL
<abstract>
All information about physical objects including humans, buildings,
processes, and organizations will be online. This trend is both desirable and
inevitable. Cyberspace will provide the basis for wonderful new ways to inform,
entertain, and educate people. The information and the corresponding systems
will streamline commerce, but will also provide new levels of personal service,
health care, and automation. The most significant benefit will be a
breakthrough in our ability to remotely communicate with one another using all
our senses.
  The ACM and the transistor were born in 1947. At that time the stored program
computer was a revolutionary idea and the transistor was just a curiosity. Both
ideas evolved rapidly. By the mid 1960s integrated circuits appeared --
allowing mass fabrication of transistors on silicon substrates. This allowed
low-cost mass-produced computers. These technologies enabled extraordinary
increases in processing speed and memory coupled with extraordinary price
declines.
  The only form of processing and memory more easily, cheaply, and rapidly
fabricated is the human brain. Peter Cohrane (1996) estimates the brain to have
a processing power of around 1000 million-million operations per second, (one
Petaops) and a memory of 10 Terabytes. If current trends continue, computers
could have these capabilities by 2047. Such computers could be 'on body'
personal assistants able to recall everything one reads, hears, and sees.

<id>
cs/9911005v1
<category>
cs.GL
<abstract>
Charles Babbage's vision of computing has largely been realized. We are on
the verge of realizing Vannevar Bush's Memex. But, we are some distance from
passing the Turing Test. These three visions and their associated problems have
provided long-range research goals for many of us. For example, the scalability
problem has motivated me for several decades. This talk defines a set of
fundamental research problems that broaden the Babbage, Bush, and Turing
visions. They extend Babbage's computational goal to include highly-secure,
highly-available, self-programming, self-managing, and self-replicating
systems. They extend Bush's Memex vision to include a system that automatically
organizes, indexes, digests, evaluates, and summarizes information (as well as
a human might). Another group of problems extends Turing's vision of
intelligent machines to include prosthetic vision, speech, hearing, and other
senses. Each problem is simply stated and each is orthogonal from the others,
though they share some common core technologies

<id>
cs/0012003v1
<category>
cs.GL
<abstract>
Issues related to a materialist philosophy are explored as concerns the
implied equivalence of computers running software and human observers. One
issue explored concerns the measurement process in quantum mechanics. Another
issue explored concerns the nature of experience as revealed by the existence
of dreams. Some difficulties stemming from a materialist philosophy as regards
these issues are pointed out. For example, a gedankenexperiment involving what
has been called "negative" observation is discussed that illustrates the
difficulty with a materialist assumption in quantum mechanics. Based on an
exploration of these difficulties, specifications are outlined briefly that
would provide a means to demonstrate the equivalence of of computers running
software and human experience given a materialist assumption.

<id>
cs/0106022v1
<category>
cs.GL
<abstract>
Computer scientists are in the position to create new, free high-quality
journals. So what would it take?

<id>
cs/0110018v2
<category>
cs.GL
<abstract>
ENUM marks either the convergence or collision of the public telephone
network with the Internet. ENUM is an innovation in the domain name system
(DNS). It starts with numerical domain names that are used to query DNS name
servers. The servers respond with address information found in DNS records.
This can be telephone numbers, email addresses, fax numbers, SIP addresses, or
other information. The concept is to use a single number in order to obtain a
plethora of contact information.
  By convention, the Internet Engineering Task Force (IETF) ENUM Working Group
determined that an ENUM number would be the same numerical string as a
telephone number. In addition, the assignee of an ENUM number would be the
assignee of that telephone number. But ENUM could work with any numerical
string or, in fact, any domain name. The IETF is already working on using E.212
numbers with ENUM. [Abridged]

<id>
cs/0210001v1
<category>
cs.GL
<abstract>
We discuss the scientific contributions of Edsger Wybe Dijkstra, his opinions
and his legacy.

<id>
cs/0306132v1
<category>
cs.GL
<abstract>
In this work we firstly review some results in Classical Information Theory.
Next, we try to generalize these results by using the Tsallis entropy. We
present a preliminary result and discuss our aims in this field.

<id>
cs/0402037v2
<category>
cs.GL
<abstract>
The main ideas behind developments in the theory and technology of quantum
computation were formulated in the late 1970s and early 1980s by two physicists
in the West and a mathematician in the former Soviet Union. It is not generally
known in the West that the subject has roots in the Russian technical
literature. The contributor hopes to present as impartial a synthesis as possible of
the early history of thought on this subject. The role of reversible and
irreversible computational processes is examined briefly as it relates to the
origins of quantum computing and the so-called Information Paradox in physics.

<id>
cs/0410075v1
<category>
cs.GL
<abstract>
The (non-initialized, non-deterministic) asynchronous systems (in the
input-output sense) are multi-valued functions from m-dimensional signals to
sets of n-dimensional signals, the concept being inspired by the modeling of
the asynchronous circuits. Our purpose is to state the problem of the their
stability.

<id>
cs/0411009v2
<category>
cs.GL
<abstract>
The latches are simple circuits with feedback from the digital electrical
engineering. We have included in our work the C element of Muller, the RS
latch, the clocked RS latch, the D latch and also circuits containing two
interconnected latches: the edge triggered RS flip-flop, the D flip-flop, the
JK flip-flop, the T flip-flop. The purpose of this study is to model with
equations the previous circuits, considered to be ideal, i.e. non-inertial. The
technique of analysis is the pseudoboolean differential calculus.

<id>
cs/0412090v1
<category>
cs.GL
<abstract>
The chapter from the book introduces the delay theory, whose purpose is the
modeling of the asynchronous circuits from digital electrical engineering with
ordinary and differential pseudo-boolean equations.

<id>
cs/0602070v1
<category>
cs.GL
<abstract>
The technical challenges of scaling websites with large and growing member
bases, like social networking sites, are numerous. One of these challenges is
how to evenly distribute the growing member base across all available
resources. This paper will explore various methods that address this issue. The
techniques used in this paper can be generalized and applied to various other
problems that need to distribute data evenly amongst a finite amount of
resources.

<id>
cs/0607022v1
<category>
cs.GL
<abstract>
This is a rough draft synopsis of a book presently in preparation. This book
provides a systematic critique of the software industry. This critique is
accomplished using classical methods in practical design science.

<id>
cs/0610127v1
<category>
cs.GL
<abstract>
The asynchronous systems $f$ are the models of the asynchronous circuits from
digital electrical engineering. They are multi-valued functions that associate
to each input $u:\mathbf{R}\to \{0,1\}^{m}$ a set of states $x\in f(u),$ where
$x:\mathbf{R}\to \{0,1\}^{n}.$ The intersection of the systems allows adding
supplementary conditions in modeling and the union of the systems allows
considering the validity of one of two systems in modeling, for example when
testing the asynchronous circuits and the circuit is supposed to be 'good' or
'bad'. The purpose of the paper is that of analyzing the intersection and the
union against the initial/final states, initial/final time, initial/final state
functions, subsystems, dual systems, inverse systems, Cartesian product of
systems, parallel connection and serial connection of systems.

<id>
cs/0702141v1
<category>
cs.GL
<abstract>
Computer science is seeing a decline in enrollment at all levels of
education, including undergraduate and graduate study. This paper reports on
the results of a study conducted at the University of Illinois at
Urbana-Champaign which evaluated students attitudes regarding three areas which
can contribute to improved enrollment in the Department of Computer Science:
Recruitment, preparation and retention. The results of our study saw two
themes. First, the department's tight research focus appears to draw
significant attention from other activities -- such as teaching, service, and
other community-building activities -- that are necessary for a department's
excellence. Yet, as demonstrated by our second theme, one partial solution is
to better promote such activities already employed by the department to its
students and faculty. Based on our results, we make recommendations for
improvements and enhancements based on the current state of practice at peer
institutions.

<id>
0705.0742v1
<category>
cs.GL
<abstract>
We propose a soft-output detection scheme for Multiple-Input-Multiple-Output
(MIMO) systems. The detector employs Markov Chain Monte Carlo method to compute
bit reliabilities from the signals received and is thus suited for coded MIMO
systems. It offers a good trade-off between achievable performance and
algorithmic complexity.

<id>
0712.2594v1
<category>
cs.GL
<abstract>
This paper documents the formation of the European Spreadsheet Risks Interest
Group (EuSpRIG www.eusprig.org) and outlines some of the research undertaken
and reported upon by interested parties in EuSpRIG publications

<id>
0804.0879v1
<category>
cs.GL
<abstract>
The latches are simple circuits with feedback from the digital electrical
engineering. We have included in our work the C element of Muller, the RS
latch, the clocked RS latch, the D latch and also circuits containing two
interconnected latches: the edge triggered RS flip-flop, the D flip-flop, the
JK flip-flop, the T flip-flop. The purpose of this study is to model with
equations the previous circuits, considered to be ideal, i.e. non-inertial. The
technique of analysis is the pseudoboolean differential calculus.

<id>
0804.2035v1
<category>
cs.GL
<abstract>
The asynchronous systems are the models of the asynchronous circuits from the
digital electrical engineering and non-anticipation is one of the most
important properties in systems theory. Our present purpose is to introduce
several concepts of non-anticipation of the asynchronous systems.

<id>
0804.2621v1
<category>
cs.GL
<abstract>
This Master of Science in Computer and Information Sciences (MICS) is an
international accredited master program that has been initiated in 2004 and
started in September 2005. MICS is a research-oriented academic study of 4
semesters and a continuation of the Bachelor towards the PhD. It is completely
taught in English, supported by lecturers coming from more than ten different
countries. This report compass a description of its underlying architecture,
describes some implementation details and gives a presentation of diverse
experiences and results. As the program has been designed and implemented right
after the creation of the University, the significance of the program is
moreover a self-discovery of the computer science department, which has finally
led to the creation of the today's research institutes and research axes.

<id>
0807.4132v3
<category>
cs.GL
<abstract>
The increasing relevance of areas such as real-time and embedded systems,
pervasive computing, hybrid systems control, and biological and social systems
modeling is bringing a growing attention to the temporal aspects of computing,
not only in the computer science domain, but also in more traditional fields of
engineering.
  This article surveys various approaches to the formal modeling and analysis
of the temporal features of computer-based systems, with a level of detail that
is suitable also for non-specialists. In doing so, it provides a unifying
framework, rather than just a comprehensive list of formalisms.
  The paper first lays out some key dimensions along which the various
formalisms can be evaluated and compared. Then, a significant sample of
formalisms for time modeling in computing are presented and discussed according
to these dimensions. The adopted perspective is, to some extent, historical,
going from "traditional" models and formalisms to more modern ones.

<id>
0808.3717v1
<category>
cs.GL
<abstract>
Development organizations and International Non-Governmental Organizations
have been emphasizing the high potential of Free and Open Source Software for
the Less Developed Countries. Cost reduction, less vendor dependency and
increased potential for local capacity development have been their main
arguments. In spite of its advantages, Free and Open Source Software is not
widely adopted at the African continent. In this book the contributors will explore
the grounds on with these expectations are based. Where do they come from and
is there evidence to support these expectations? Over the past years several
projects have been initiated and some good results have been achieved, but at
the same time many challenges were encountered. What lessons can be drawn from
these experiences and do these experiences contain enough evidence to support
the high expectations? Several projects and their achievements will be
considered. In the final part of the book the future of Free and Open Source
Software for Development will be explored. Special attention is given to the
African continent since here challenges are highest. What is the role of Free
and open Source Software for Development and how do we need to position and
explore the potential? What are the threats? The book aims at professionals
that are engaged in the design and implementation of ICT for Development
(ICT4D) projects and want to improve their understanding of the role Free and
Open Source Software can play.

<id>
1012.4170v2
<category>
cs.GL
<abstract>
A significant amount of high-impact contemporary scientific research occurs
where biology, computer science, engineering and chemistry converge. Although
programmes have been put in place to support such work, the complex dynamics of
interdisciplinarity are still poorly understood. In this paper we interrogate
the nature of interdisciplinary research and how we might measure its
"success", identify potential barriers to its implementation, and suggest
possible mechanisms for removing these impediments.

<id>
1206.4708v1
<category>
cs.GL
<abstract>
The asynchronous systems f are multi-valued functions, representing the
non-deterministic models of the asynchronous circuits from the digital
electrical engineering. In real time, they map an 'admissible input' function
u:R\rightarrow{0,1}^{m} to a set f(u) of 'possible states' x\inf(u), where
x:R\rightarrow{0,1}^{m}. When f is defined by making use of a 'generator
function' {\Phi}:{0,1}^{n}\times{0,1}^{m}\rightarrow{0,1}^{n}, the system is
called regular. The usual definition of the serial connection of systems as
composition of multi-valued functions does not bring the regular systems into
regular systems, thus the first issue in this study is to modify in an
acceptable manner the definition of the serial connection in a way that matches
regularity. This intention was expressed for the first time, without proving
the regularity of the serial connection of systems, in a previous work. Our
present purpose is to restate with certain corrections and prove that result.

<id>
0910.5001v1
<category>
cs.GL
<abstract>
The dialogue develops arguments for and against adopting a new world system,
info-computationalist naturalism, that is poised to replace the traditional
mechanistic world system. We try to figure out what the info-computational
paradigm would mean, in particular its pancomputationalism. We make some steps
towards developing the notion of computing that is necessary here, especially
in relation to traditional notions. We investigate whether pancomputationalism
can possibly provide the basic causal structure to the world, whether the
overall research programme appears productive and whether it can revigorate
computationalism in the philosophy of mind.

<id>
1211.5508v1
<category>
cs.GL
<abstract>
A significant amount of high-impact contemporary scientific research occurs
where biology, computer science, engineering and chemistry converge. Although
programmes have been put in place to support such work, the complex dynamics of
interdisciplinarity are still poorly understood. In this paper we highlight
potential barriers to effective research across disciplines, and suggest, using
a case study, possible mechanisms for removing these impediments.

<id>
1002.1936v1
<category>
cs.GL
<abstract>
We introduce a new visual analytic approach to the study of scientific
discoveries and knowledge diffusion. Our approach enhances contemporary
co-citation network analysis by enabling analysts to identify co-citation
clusters of cited references intuitively, synthesize thematic contexts in which
these clusters are cited, and trace how research focus evolves over time. The
new approach integrates and streamlines a few previously isolated techniques
such as spectral clustering and feature selection algorithms. The integrative
procedure is expected to empower and strengthen analytical and sense making
capabilities of scientists, learners, and researchers to understand the
dynamics of the evolution of scientific domains in a wide range of scientific
fields, science studies, and science policy evaluation and planning. We
demonstrate the potential of our approach through a visual analysis of the
evolution of astronomical research associated with the Sloan Digital Sky Survey
(SDSS) using bibliographic data between 1994 and 2008. In addition, we also
demonstrate that the approach can be consistently applied to a set of
heterogeneous data sources such as e-prints on arXiv, publications on ADS, and
NSF awards related to the same topic of SDSS.

<id>
1210.3597v1
<category>
cs.GL
<abstract>
Although the history of informatics is recent, this field poses unusual
problems with respect to its preservation. These problems are amplified by
legal issues, digital law being in itself a subject matter whose history is
also worth presenting in a computer science museum. The purpose of this paper
is to present a quick overview of the evolution of law regarding digital
matters, from an historical perspective as well as with respect to the
preservation and presentation of the works.

<id>
1210.7784v1
<category>
cs.GL
<abstract>
This text presents the research field of natural/unconventional computing as
it appears in the book COMPUTING NATURE. The articles discussed consist a
selection of works from the Symposium on Natural Computing at AISB-IACAP
(British Society for the Study of Artificial Intelligence and the Simulation of
Behaviour and The International Association for Computing and Philosophy) World
Congress 2012, held at the University of Birmingham, celebrating Turing
centenary. The COMPUTING NATURE is about nature considered as the totality of
physical existence, the universe. By physical we mean all phenomena, objects
and processes, that are possible to detect either directly by our senses or via
instruments. Historically, there have been many ways of describing the universe
(cosmic egg, cosmic tree, theistic universe, mechanistic universe) while a
particularly prominent contemporary approach is computational universe, as
discussed in this article.

<id>
cs/9301112v1
<category>
cs.GR
<abstract>
We study the configurations of pixels that occur when two digitized straight
lines meet each other.

<id>
cs/0206029v1
<category>
cs.GR
<abstract>
This paper presents an efficient method for generating and rendering
photorealistic hair in two dimensional pictures. The method consists of three
major steps. Simulating an artist drawing is used to design the rough hair
shape. A convolution based filter is then used to generate photorealistic hair
patches. A refine procedure is finally used to blend the boundaries of the
patches with surrounding areas. This method can be used to create all types of
photorealistic human hair (head hair, facial hair and body hair). It is also
suitable for fur and grass generation. Applications of this method include:
hairstyle designing/editing, damaged hair image restoration, human hair
animation, virtual makeover of a human, and landscape creation.

<id>
cs/0304011v1
<category>
cs.GR
<abstract>
Environment maps are used to simulate reflections off curved objects. We
present a technique to reflect a user, or a group of users, in a real
environment, onto a virtual object, in a virtual reality application, using the
live video feeds from a set of cameras, in real-time. Our setup can be used in
a variety of environments ranging from outdoor or indoor scenes.

<id>
cs/0305057v1
<category>
cs.GR
<abstract>
The Persint program is designed for the three-dimensional representation of
objects and for the interfacing and access to a variety of independent
applications, in a fully interactive way. Facilities are provided for the
spatial navigation and the definition of the visualization properties, in order
to interactively set the viewing and viewed points, and to obtain the desired
perspective. In parallel, applications may be launched through the use of
dedicated interfaces, such as the interactive reconstruction and display of
physics events. Recent developments have focalized on the interfacing to the
XML ATLAS General Detector Description AGDD, making it a widely used tool for
XML developers. The graphics capabilities of this program were exploited in the
context of the ATLAS 2002 Muon Testbeam where it was used as an online event
display, integrated in the online software framework and participating in the
commissioning and debug of the detector system.

<id>
cs/0306012v1
<category>
cs.GR
<abstract>
Many entities managed by HEP Software Frameworks represent spatial
(3-dimensional) real objects. Effective definition, manipulation and
visualization of such objects is an indispensable functionality.
  GraXML is a modular Geometric Modeling toolkit capable of processing
geometric data of various kinds (detector geometry, event geometry) from
different sources and delivering them in ways suitable for further use.
Geometric data are first modeled in one of the Generic Models. Those Models are
then used to populate powerful Geometric Model based on the Java3D technology.
While Java3D has been originally created just to provide visualization of 3D
objects, its light weight and high functionality allow an effective reuse as a
general geometric component. This is possible also thanks to a large overlap
between graphical and general geometric functionality and modular design of
Java3D itself. Its graphical functionalities also allow a natural visualization
of all manipulated elements.
  All these techniques have been developed primarily (or only) for the Java
environment. It is, however, possible to interface them transparently to
Frameworks built in other languages, like for example C++.
  The GraXML toolkit has been tested with data from several sources, as for
example ATLAS and ALICE detector description and ATLAS event data. Prototypes
for other sources, like Geometry Description Markup Language (GDML) exist too
and interface to any other source is easy to add.

<id>
cs/0306031v1
<category>
cs.GR
<abstract>
A new graphics client prototype for the HepRep protocol is presented. Based
on modern toolkits and high level languages (C++ and Ruby), Fred is an
experiment to test applicability of scripting facilities to the high energy
physics event display domain. Its flexible structure, extensibility and the use
of the HepRep protocol are key features for its use in the astroparticle
experiment GLAST.

<id>
cs/0306059v1
<category>
cs.GR
<abstract>
HepRep is a generic, hierarchical format for description of graphics
representables that can be augmented by physics information and relational
properties. It was developed for high energy physics event display applications
and is especially suited to client/server or component frameworks. The GLAST
experiment, an international effort led by NASA for a gamma-ray telescope to
launch in 2006, chose HepRep to provide a flexible, extensible and maintainable
framework for their event display without tying their users to any one graphics
application. To support HepRep in their GUADI infrastructure, GLAST developed a
HepRep filler and builder architecture. The architecture hides the details of
XML and CORBA in a set of base and helper classes allowing physics experts to
focus on what data they want to represent. GLAST has two GAUDI services:
HepRepSvc, which registers HepRep fillers in a global registry and allows the
HepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be
published through a CORBA interface and which allows the client application to
feed commands back to GAUDI (such as start next event, or run some GAUDI
algorithm). GLAST's HepRep solution gives users a choice of client
applications, WIRED (written in Java) or FRED (written in C++ and Ruby), and
leaves them free to move to any future HepRep-compliant event display.

<id>
cs/0307065v1
<category>
cs.GR
<abstract>
We present an efficient and inexpensive to develop application for
interactive high-performance parallel visualization. We extend popular APIs
such as Open Inventor and VTK to support commodity-based cluster visualization.
Our implementation follows a standard master/slave concept: the general idea is
to have a ``Master'' node, which will intercept a sequential graphical user
interface (GUI) and broadcast it to the ``Slave'' nodes. The interactions
between the nodes are implemented using MPI. The parallel remote rendering uses
Chromium. This paper is mainly the report of our implementation experiences. We
present in detail the proposed model and key aspects of its implementation.
Also, we present performance measurements, we benchmark and quantitatively
demonstrate the dependence of the visualization speed on the data size and the
network bandwidth, and we identify the singularities and draw conclusions on
Chromium's sort-first rendering architecture. The most original part of this
work is the combined use of Open Inventor and Chromium.

<id>
cs/0311034v1
<category>
cs.GR
<abstract>
Conventional visualization media such as MRI prints and computer screens are
inherently two dimensional, making them incapable of displaying true 3D volume
data sets. By applying only transparency or intensity projection, and ignoring
light-matter interaction, results will likely fail to give optimal results.
Little research has been done on using reflectance functions to visually
separate the various segments of a MRI volume. We will explore if applying
specific reflectance functions to individual anatomical structures can help in
building an intuitive 2D image from a 3D dataset. We will test our hypothesis
by visualizing a statistical analysis of the genetic influences on variations
in human brain morphology because it inherently contains complex and many
different types of data making it a good candidate for our approach

<id>
cs/0404022v1
<category>
cs.GR
<abstract>
This paper presents an algorithm that transforms color visual images, like
photographs or paintings, into tactile graphics. In the algorithm, the edges of
objects are detected and colors of the objects are estimated. Then, the edges
and the colors are encoded into lines and textures in the output tactile image.
Design of the method is substantiated by various qualities of haptic
recognizing of images. Also, means of presentation of the tactile images in
printouts are discussed. Example translated images are shown.

<id>
cs/0405048v1
<category>
cs.GR
<abstract>
We develop multiple view visualization of higher dimensional data. Our work
was chiefly motivated by the need to extract insight from four dimensional
Quantum Chromodynamic (QCD) data. We develop visualization where multiple
views, generally views of 3D projections or slices of a higher dimensional
data, are tightly coupled not only by their specific order but also by a view
synchronizing interaction style, and an internally defined interaction
language. The tight coupling of the different views allows a fast and
well-coordinated exploration of the data. In particular, the visualization
allowed us to easily make consistency checks of the 4D QCD data and to infer
the correctness of particle properties calculations. The software developed was
also successfully applied in material studies, in particular studies of
meteorite properties. Our implementation uses the VTK API. To handle a large
number of views (slices/projections) and to still maintain good resolution, we
use IBM T221 display (3840 X 2400 pixels).

<id>
cs/0507012v1
<category>
cs.GR
<abstract>
The past two decades showed a rapid growing of physically-based modeling of
fluids for computer graphics applications. In this area, a common top down
approach is to model the fluid dynamics by Navier-Stokes equations and apply a
numerical techniques such as Finite Differences or Finite Elements for the
simulation. In this paper we focus on fluid modeling through Lattice Gas
Cellular Automata (LGCA) for computer graphics applications. LGCA are discrete
models based on point particles that move on a lattice, according to suitable
and simple rules in order to mimic a fully molecular dynamics. By
Chapman-Enskog expansion, a known multiscale technique in this area, it can be
demonstrated that the Navier-Stokes model can be reproduced by the LGCA
technique. Thus, with LGCA we get a fluid model that does not require solution
of complicated equations. Therefore, we combine the advantage of the low
computational cost of LGCA and its ability to mimic the realistic fluid
dynamics to develop a new animating framework for computer graphics
applications. In this work, we discuss the theoretical elements of our proposal
and show experimental results.

<id>
cs/0508002v1
<category>
cs.GR
<abstract>
Von Neuman's work on universal machines and the hardware development have
allowed the simulation of dynamical systems through a large set of interacting
agents. This is a bottom-up approach which tries to derive global properties of
a complex system through local interaction rules and agent behaviour.
Traditionally, such systems are modeled and simulated through top-down methods
based on differential equations. Agent-Based Modeling has the advantage of
simplicity and low computational cost. However, unlike differential equations,
there is no standard way to express agent behaviour. Besides, it is not clear
how to analytically predict the results obtained by the simulation. In this
paper we survey some of these methods. For expressing agent behaviour formal
methods, like Stochastic Process Algebras have been used. Such approach is
useful if the global properties of interest can be expressed as a function of
stochastic time series. However, if space variables must be considered, we
shall change the focus. In this case, multiscale techniques, based on
Chapman-Enskog expansion, was used to establish the connection between the
microscopic dynamics and the macroscopic observables. Also, we use data mining
techniques,like Principal Component Analysis (PCA), to study agent systems like
Cellular Automata. With the help of these tools we will discuss a simple
society model, a Lattice Gas Automaton for fluid modeling, and knowledge
discovery in CA databases. Besides, we show the capabilities of the NetLogo, a
software for agent simulation of complex system and show our experience about.

<id>
cs/0510087v1
<category>
cs.GR
<abstract>
This article introduces a Mathematica package providing a graphics export
function that automatically replaces Mathematica expressions in a graphic by
the corresponding LaTeX constructs and positions them correctly. It thus
facilitates the creation of publication-quality Enscapulated PostScript (EPS)
graphics.

<id>
cs/0603132v1
<category>
cs.GR
<abstract>
We define a Graphics Turing Test to measure graphics performance in a similar
manner to the definition of the traditional Turing Test. To pass the test one
needs to reach a computational scale, the Graphics Turing Scale, for which
Computer Generated Imagery becomes comparatively indistinguishable from real
images while also being interactive. We derive an estimate for this
computational scale which, although large, is within reach of todays
supercomputers. We consider advantages and disadvantages of various computer
systems designed to pass the Graphics Turing Test. Finally we discuss
commercial applications from the creation of such a system, in particular
Interactive Cinema.

<id>
cs/0607050v2
<category>
cs.GR
<abstract>
We describe a system that lets a designer interactively draw patterns of
strokes in the picture plane, then guide the synthesis of similar patterns over
new picture regions. Synthesis is based on an initial user-assisted analysis
phase in which the system recognizes distinct types of strokes (hatching and
stippling) and organizes them according to perceptual grouping criteria. The
synthesized strokes are produced by combining properties (eg. length,
orientation, parallelism, proximity) of the stroke groups extracted from the
input examples. We illustrate our technique with a drawing application that
allows the control of attributes and scale-dependent reproduction of the
synthesized patterns.

<id>
cs/0609084v1
<category>
cs.GR
<abstract>
The paper describes a new image processing for a non-photorealistic
rendering. The algorithm is based on a random generation of gray tones and
competing statistical requirements. The gray tone value of each pixel in the
starting image is replaced selecting among randomly generated tone values,
according to the statistics of nearest-neighbor and next-nearest-neighbor
pixels. Two competing conditions for replacing the tone values - one position
on the local mean value the other on the local variance - produce a peculiar
pattern on the image. This pattern has a labyrinthine tiling aspect. For
certain subjects, the pattern enhances the look of the image.

<id>
cs/0610088v1
<category>
cs.GR
<abstract>
We have recently developed an algorithm for vector field visualization with
oriented streamlines, able to depict the flow directions everywhere in a dense
vector field and the sense of the local orientations. The algorithm has useful
applications in the visualization of the director field in nematic liquid
crystals. Here we propose an improvement of the algorithm able to enhance the
visualization of the local magnitude of the field. This new approach of the
algorithm is compared with the same procedure applied to the Line Integral
Convolution (LIC) visualization.

<id>
cs/0702026v1
<category>
cs.GR
<abstract>
Shape preservation behavior of a spline consists of criterial conditions for
preserving convexity, inflection, collinearity, torsion and coplanarity shapes
of data polgonal arc. We present our results which acts as an improvement in
the definitions of and provide geometrical insight into each of the above shape
preservation criteria. We also investigate the effect of various results from
the literature on various shape preservation criteria. These results have not
been earlier refered in the context of shape preservation behaviour of splines.
We point out that each curve segment need to satisfy more than one shape
preservation criteria. We investigate the conflict between different shape
preservation criteria 1)on each curve segment and 2)of adjacent curve segments.
We derive simplified formula for shape preservation criteria for cubic curve
segments. We study the shape preservation behavior of cubic Catmull-Rom splines
and see that, though being very simple spline curve, it indeed satisfy all the
shape preservation criteria.

<id>
0712.0121v1
<category>
cs.GR
<abstract>
This paper describes the implementation and evaluation of an open source
library for mathematical morphology based on packed binary and run-length
compressed images for document imaging applications. Abstractions and patterns
useful in the implementation of the interval operations are described. A number
of benchmarks and comparisons to bit-blit based implementations on standard
document images are provided.

<id>
0801.1500v1
<category>
cs.GR
<abstract>
We investigate raytracing performance that can be achieved on a class of Blue
Gene supercomputers. We measure a 822 times speedup over a Pentium IV on a 6144
processor Blue Gene/L. We measure the computational performance as a function
of number of processors and problem size to determine the scaling performance
of the raytracing calculation on the Blue Gene. We find nontrivial scaling
behavior at large number of processors. We discuss applications of this
technology to scientific visualization with advanced lighting and high
resolution. We utilize three racks of a Blue Gene/L in our calculations which
is less than three percent of the the capacity of the worlds largest Blue Gene
computer.

<id>
0801.2175v1
<category>
cs.GR
<abstract>
This article introduces the next version of MathPSfrag. MathPSfrag is a
Mathematica package that during export automatically replaces all expressions
in a plot by corresponding LaTeX commands. The new version can also produce
LaTeX independent images; e.g., PDF files for inclusion in pdfLaTeX. Moreover
from these files a preview is generated and shown within Mathematica.

<id>
0804.0561v1
<category>
cs.GR
<abstract>
A new computer haptics algorithm to be used in general interactive
manipulations of deformable virtual objects is presented. In multimodal
interactive simulations, haptic feedback computation often comes from contact
forces. Subsequently, the fidelity of haptic rendering depends significantly on
contact space modeling. Contact and friction laws between deformable models are
often simplified in up to date methods. They do not allow a "realistic"
rendering of the subtleties of contact space physical phenomena (such as slip
and stick effects due to friction or mechanical coupling between contacts). In
this paper, we use Signorini's contact law and Coulomb's friction law as a
computer haptics basis. Real-time performance is made possible thanks to a
linearization of the behavior in the contact space, formulated as the so-called
Delassus operator, and iteratively solved by a Gauss-Seidel type algorithm.
Dynamic deformation uses corotational global formulation to obtain the Delassus
operator in which the mass and stiffness ratio are dissociated from the
simulation time step. This last point is crucial to keep stable haptic
feedback. This global approach has been packaged, implemented, and tested.
Stable and realistic 6D haptic feedback is demonstrated through a clipping task
experiment.

<id>
0807.1667v1
<category>
cs.GR
<abstract>
We consider perturbations of the complex quadratic map $ z \to z^2 +c$ and
corresponding changes in their quasi-Mandelbrot sets. Depending on particular
perturbation, visual forms of quasi-Mandelbrot set changes either sharply (when
the perturbation reaches some critical value) or continuously. In the latter
case we have a smooth transition from the classical form of the set to some
forms, constructed from mostly linear structures, as it is typical for
two-dimensional real number dynamics. Two examples of continuous evolution of
the quasi-Mandelbrot set are described.

<id>
0812.1647v1
<category>
cs.GR
<abstract>
In this work, we present a new method for generating a threshold structure.
This kind of structure can be advantageously used in various halftoning
algorithms such as clustered-dot or dispersed-dot dithering, error diffusion
with threshold modulation, etc. The proposed method is based on rectifiable
polyominoes -- a non-periodic hierarchical structure, which tiles the Euclidean
plane with no gaps. Each polyomino contains a fixed number of discrete
threshold values. Thanks to its inherent non-periodic nature combined with
off-line optimization of threshold values, our polyomino-based threshold
structure shows blue-noise spectral properties. The halftone images produced
with this threshold structure have high visual quality. Although the proposed
method is general, and can be applied on any polyomino tiling, we consider one
particular case: tiling with G-hexominoes. We compare our polyomino-based
threshold structure with the best known state-of-the-art methods for generation
threshold matrices, and conclude considerable improvement achieved with our
method.

<id>
0907.4364v2
<category>
cs.GR
<abstract>
This thesis presents a two-layer uniform facet elastic object for real-time
simulation based on physics modeling method. It describes the elastic object
procedural modeling algorithm with particle system from the simplest
one-dimensional object, to more complex two-dimensional and three-dimensional
objects.
  The double-layered elastic object consists of inner and outer elastic mass
spring surfaces and compressible internal pressure. The density of the inner
layer can be set different from the density of the outer layer; the motion of
the inner layer can be opposite to the motion of the outer layer. These special
features, which cannot be achieved by a single layered object, result in
improved imitation of a soft body, such as tissue's liquidity non-uniform
deformation. The construction of the double-layered elastic object is closer to
the real tissue's physical structure.
  The inertial behavior of the elastic object is well illustrated in
environments with gravity and collisions with walls, ceiling, and floor. The
collision detection is defined by elastic collision penalty method and the
motion of the object is guided by the Ordinary Differential Equation
computation.
  Users can interact with the modeled objects, deform them, and observe the
response to their action in real time.

<id>
0911.0902v1
<category>
cs.GR
<abstract>
Many image watermarking schemes have been proposed in recent years, but they
usually involve embedding a watermark to the entire image without considering
only a particular object in the image, which the image owner may be interested
in. This paper proposes a watermarking scheme that can embed a watermark to an
arbitrarily shaped object in an image. Before embedding, the image owner
specifies an object of arbitrary shape that is of a concern to him. Then the
object is transformed into the wavelet domain using in place lifting shape
adaptive DWT(SADWT) and a watermark is embedded by modifying the wavelet
coefficients. In order to make the watermark robust and transparent, the
watermark is embedded in the average of wavelet blocks using the visual model
based on the human visual system. Wavelet coefficients n least significant bits
(LSBs) are adjusted in concert with the average. Simulation results shows that
the proposed watermarking scheme is perceptually invisible and robust against
many attacks such as lossy compression (e.g.JPEG, JPEG2000), scaling, adding
noise, filtering, etc.

<id>
0912.3923v1
<category>
cs.GR
<abstract>
In this paper a novel spatial domain LSB based watermarking scheme for color
Images is proposed. The proposed scheme is of type blind and invisible
watermarking. Our scheme introduces the concept of storing variable number of
bits in each pixel based on the actual color value of pixel. Equal or higher
the color value of channels with respect to intensity of pixel stores higher
number of watermark bits. The Red, Green and Blue channel of the color image
has been used for watermark embedding. The watermark is embedded into selected
channels of pixel. The proposed method supports high watermark embedding
capacity, which is equivalent to the size of cover image. The security of
watermark is preserved by permuting the watermark bits using secret key. The
proposed scheme is found robust to various image processing operations such as
image compression, blurring, salt and pepper noise, filtering and cropping.

<id>
1001.3481v2
<category>
cs.GR
<abstract>
Removed by arXiv administration. This article was plagiarised from
http://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other
locations.

<id>
1001.3496v1
<category>
cs.GR
<abstract>
In this paper a new watermarking scheme is presented based on log-average
luminance. A colored-image is divided into blocks after converting the RGB
colored image to YCbCr color space. A monochrome image of 1024 bytes is used as
the watermark. To embed the watermark, 16 blocks of size 8X8 are selected and
used to embed the watermark image into the original image. The selected blocks
are chosen spirally (beginning form the center of the image) among the blocks
that have log-average luminance higher than or equal the log-average luminance
of the entire image. Each byte of the monochrome watermark is added by updating
a luminance value of a pixel of the image. If the byte of the watermark image
represented white color (255) a value <alpha> is added to the image pixel
luminance value, if it is black (0) the <alpha> is subtracted from the
luminance value. To extract the watermark, the selected blocks are chosen as
the above, if the difference between the luminance value of the watermarked
image pixel and the original image pixel is greater than 0, the watermark pixel
is supposed to be white, otherwise it supposed to be black. Experimental
results show that the proposed scheme is efficient against changing the
watermarked image to grayscale, image cropping, and JPEG compression.

<id>
cs/9811026v2
<category>
cs.HC
<abstract>
The electronic presentation of text in small display windows is mushrooming.
In the present paper, four ways of presenting text in a small display window
were examined and compared to a Normal Page condition: rapid serial visual
presentation (RSVP), RSVP with a Completion Meter, Sentence-by-Sentence
presentation, and Sentence-by-Sentence presentation with a Completion Meter.
Dependent measures were reading efficiency - speed and comprehension - and
preference. For designers of hardware or software with small display windows,
the results suggest the following: (1) Though RSVP is disliked by readers, the
present methods of allowing self-pacing and regressions in RSVP, unlike earlier
tested methods, are efficient and feasible. (2) Slower reading in RSVP should
be achieved by increasing pauses between sentences or by repeating sentences,
not by decreasing the within-sentence rate. (3) Completion meters do not
interfere with performance, and are usually preferred. (4) The space-saving
Sentence-by-Sentence format is as efficient and as preferred as the Normal Page
format.

<id>
cs/9812023v1
<category>
cs.HC
<abstract>
Training in motor skills such as athletics, dance, or gymnastics is not
possible today except in the direct presence of the coach/instructor. This
paper describes a computer vision based gesture recognition system which is
used to metamorphose the user into a Virtual person, e.g. as a Kathakali
dancer, which is graphically recreated at a near or diatant location. Thus this
can be seen by an off-site coach using low-bandwidth joint-motion data which
permits real time animation. The metamorphosis involves altering the appearance
and identity of the user and also creating a specific environment possibly in
interaction with other virtual creatures.
  A robust vision module is used to identify the user, based on very simple
binary image processing in real time which also manages to resolve
self-occlusion, correct for clothing/colour and other variations among users.
Gestures are identified by locating key points at the shoulder, elbow and wrist
joint, which are then recreated in an articulated humanoid model, which in this
instance, representes a Kathakali dancer in elaborate traditional dress. Unlike
glove based or other and movement tracking systems, this application requires
the user to wear no hardwire devices and is aimed at making gesture tracking
simpler, cheaper, and more user friendly.

<id>
cs/9903019v2
<category>
cs.HC
<abstract>
The paper presents an introductory overview of the workflow automation area,
outlining the main types, basic technologies, the essential features of
workflow applications. Two sorts of process models for the definition of
workflows (according to the conversation-based and activity-based
methodologies) are sketched. Later on, the nature of Lotus Notes and its
capabilities (as an environment for workflow management systems development)
are indicated. Concluding, the experience of automating administrative
workflows (developing a Subsystem of Inter-institutional Document Management of
the VADIS project) is briefly outlined.

<id>
cs/0101029v1
<category>
cs.HC
<abstract>
We describe tap tips, a technique for providing touch-screen target location
hints. Tap tips are lightweight in that they are non-modal, appear only when
needed, require a minimal number of user gestures, and do not add to the
standard touchscreen gesture vocabulary. We discuss our implementation of tap
tips in an electronic guidebook system and some usability test results.

<id>
cs/0101035v2
<category>
cs.HC
<abstract>
In this paper, we describe an electronic guidebook prototype and report on a
study of its use in a historic house. Supported by mechanisms in the guidebook,
visitors constructed experiences that had a high degree of interaction with
three entities: the guidebook, their companions, and the house and its
contents. For example, we found that most visitors played audio descriptions
played through speakers (rather than using headphones or reading textual
descriptions) to facilitate communication with their companions.

<id>
cs/0110006v1
<category>
cs.HC
<abstract>
This paper explains four things in a unified way. First, how e-commerce can
generate price equilibria where physical shops either compete with virtual
shops for consumers with Internet access, or alternatively, sell only to
consumers with no Internet access. Second, how these price equilibria might
involve price dispersion on-line. Third, why prices may be higher on-line.
Fourth, why established firms can, but need not, be more reluctant than newly
created firm to adopt e-commerce. For this purpose we develop a model where
e-commerce reduces consumers' search costs, involves trade-offs for consumers,
and reduces retailing costs.

<id>
cs/0111024v1
<category>
cs.HC
<abstract>
There has been a widespread emergence of computing devices in the past few
years that go beyond the capabilities of traditional desktop computers.
However, users want to use the same kinds of applications and access the same
data and information on these appliances that they can access on their desktop
computers. The user interfaces for these platforms go beyond the traditional
interaction metaphors. It is a challenge to build User Interfaces (UIs) for
these devices of differing capabilities that allow the end users to perform the
same kinds of tasks. The User Interface Markup Language (UIML) is an XML-based
language that allows the canonical description of UIs for different platforms.
We describe the language features of UIML that facilitate the development of
multi-platform UIs. We also describe the key aspects of our approach that makes
UIML succeed where previous approaches failed, namely the division in the
representation of a UI, the use of a generic vocabulary, and an integrated
development environment specifically designed for transformation-based UI
development. Finally we describe the initial details of a multi-step usability
engineering process for building multi-platform UI using UIML.

<id>
cs/0111025v1
<category>
cs.HC
<abstract>
There has been a widespread emergence of computing devices in the past few
years that go beyond the capabilities of traditional desktop computers. These
devices have varying input/output characteristics, modalities and interaction
mechanisms. However, users want to use the same kinds of applications and
access the same data and information on these appliances that they can access
on their desktop computers. The user interfaces for these devices and platforms
go beyond the traditional interaction metaphors. It is a challenge to build
User Interfaces (UIs) for these devices of differing capabilities that allow
the end users to perform the same kinds of tasks. The User Interface Markup
Language (UIML) is an XML-based language that allows the canonical description
of UIs for different platforms. We present a multi-step transformation-based
framework for building Multi-Platform User Interfaces using UIML. We describe
the language features of UIML that facilitate the development of multi-platform
UIs, the multi-step process involved in our framework and the transformations
needed to build the UIs.

<id>
cs/0205054v1
<category>
cs.HC
<abstract>
We describe an electronic guidebook, Sotto Voce, that enables visitors to
share audio information by eavesdropping on each other's guidebook activity. We
have conducted three studies of visitors using electronic guidebooks in a
historic house: one study with open air audio played through speakers and two
studies with eavesdropped audio. An analysis of visitor interaction in these
studies suggests that eavesdropped audio provides more social and interactive
learning resources than open air audio played through speakers.

<id>
cs/0205055v1
<category>
cs.HC
<abstract>
We present a case study of an iterative design process that includes a
conversation analyst. We discuss potential benefits of conversation analysis
for design, and we describe our strategies for integrating the conversation
analyst in the design process. Since the analyst on our team had no previous
exposure to design or engineering, and none of the other members of our team
had any experience with conversation analysis, we needed to build a foundation
for our interaction. One of our key strategies was to pair the conversation
analyst with a designer in a highly interactive collaboration. Our tactics have
been effective on our project, leading to valuable results that we believe we
could not have obtained using another method. We hope that this paper can serve
as a practical guide to those interested in establishing a productive and
efficient working relationship between a conversation analyst and the other
members of a design team.

<id>
cs/0210014v1
<category>
cs.HC
<abstract>
The Sonix is the main control software for the IBR-2 instruments. This is a
modular configurable and flexible system created using the Varman (real time
database) and the X11/OS9 graphical package in the OS-9 environment. In the
last few years we were mostly focused on making this system more reliable and
user friendly. Because the VME hardware and software upgrade is rather
expensive we would like to replace existing VME + OS9 control computers with
the PC+Windows XP ones in the future. This could be done with the help of
VME-PCI adapters.

<id>
cs/0211037v1
<category>
cs.HC
<abstract>
Web pages contain a large variety of information, but are largely designed
for use by graphical web browsers. Mobile access to web-based information often
requires presenting HTML web pages using channels that are limited in their
graphical capabilities such as small-screens or audio-only interfaces. Content
transcoding and annotations have been explored as methods for intelligently
presenting HTML documents. Much of this work has focused on transcoding for
small-screen devices such as are found on PDAs and cell phones. Here, we focus
on the use of annotations and transcoding for presenting HTML content through a
voice user interface instantiated in VoiceXML. This transcoded voice interface
is designed with an assumption that it will not be used for extended web
browsing by voice, but rather to quickly gain directed access to information on
web pages. We have found repeated structures that are common in the
presentation of data on web pages that are well suited for voice presentation
and navigation. In this paper, we describe these structures and their use in an
annotation system we have implemented that produces a VoiceXML interface to
information originally embedded in HTML documents. We describe the transcoding
process used to translate HTML into VoiceXML, including transcoding features we
have designed to lead to highly usable VoiceXML code.

<id>
cs/0304001v1
<category>
cs.HC
<abstract>
Unfamiliar, large-scale virtual environments are difficult to navigate. This
paper presents design guidelines to ease navigation in such virtual
environments. The guidelines presented here focus on the design and placement
of landmarks in virtual environments. Moreover, the guidelines are based
primarily on the extensive empirical literature on navigation in the real
world. A rationale for this approach is provided by the similarities between
navigational behavior in real and virtual environments.

<id>
cs/0305003v1
<category>
cs.HC
<abstract>
The Ubiquitous Interactor (UBI) addresses the problems of design and
development that arise around services that need to be accessed from many
different devices. In UBI, the same service can present itself with different
user interfaces on different devices. This is done by separating interaction
between users and services from presentation. The interaction is kept the same
for all devices, and different presentation information is provided for
different devices. This way, tailored user interfaces for many different
devices can be created without multiplying development and maintenance work. In
this paper we describe the system design of UBI, the system implementation, and
two services implemented for the system: a calendar service and a stockbroker
service.

<id>
cs/0309001v2
<category>
cs.HC
<abstract>
This paper presents an exploratory study of college-age students using
two-way, push-to-talk cellular radios. We describe the observed and reported
use of cellular radio by the participants, the activities and purposes for
which they adopted it, and their responses. We then examine these empirical
results using mediated communication theory. Cellular radios have a unique
combination of affordances relative to other media used by this age group,
including instant messaging (IM) and mobile phones; the results of our analysis
do suggest explanations for some observed phenomena but also highlight the
counter-intuitive nature of other phenomena. For example, although the radios
have many important dissimilarities with IM from the viewpoint of mediated
communication theory, the observed use patterns resembled those of IM to a
surprising degree.

<id>
cs/0311006v1
<category>
cs.HC
<abstract>
This paper presents an exploratory study of college-age students using
two-way, push-to-talk cellular radios. We describe the observed and reported
use of cellular radio by the participants. We discuss how the half-duplex,
lightweight cellular radio communication was associated with reduced
interactional commitment, which meant the cellular radios could be used for a
wide range of conversation styles. One such style, intermittent conversation,
is characterized by response delays. Intermittent conversation is surprising in
an audio medium, since it is typically associated with textual media such as
instant messaging. We present design implications of our findings.

<id>
cs/0312017v1
<category>
cs.HC
<abstract>
This paper describes some preliminary results from a 20-week study on the use
of Compaq iPAQ Personal Digital Assistants (PDAs) by 10 senior developers,
analysts, technical managers, and senior organisational managers. The goal of
the study was to identify what applications were used, how and where they were
used, the problems and issues that arose, and how use of the iPAQs changed over
the study period. The paper highlights some interesting uses of the iPAQs, and
identifies some of the characteristics of successful mobile applications.

<id>
cs/0402036v1
<category>
cs.HC
<abstract>
In this position paper we propose a process model that provides a development
infrastructure in which the usability engineering and software engineering life
cycles co-exist in complementary roles. We describe the motivation, hurdles,
rationale, arguments, and implementation plan for the need, specification, and
the usefulness of such a model.

<id>
cs/0405078v1
<category>
cs.HC
<abstract>
Generative Programming (GP) is a computing paradigm allowing automatic
creation of entire software families utilizing the configuration of elementary
and reusable components. GP can be projected on different technologies, e.g.
C++-templates, Java-Beans, Aspect-Oriented Programming (AOP), or Frame
technology. This paper focuses on Frame Technology, which aids the possible
implementation and completion of software components. The purpose of this paper
is to introduce the GP paradigm in the area of GUI application generation. It
demonstrates how automatically customized executable applications with GUI
parts can be generated from an abstract specification.

<id>
cs/0405108v1
<category>
cs.HC
<abstract>
In current presence or availability systems, the method of presenting a
user's state often supposes an instantaneous notion of that state - for
example, a visualization is rendered or an inference is made about the
potential actions that might be consistent with a user's state. Drawing on
observational research on the use of existing communication technology, we
argue (as have others in the past) that determination of availability is often
a joint process, and often one that takes the form of a negotiation (whether
implicit or explicit). We briefly describe our current research on applying
machine learning to infer degrees of conversational engagement from observed
conversational behavior. Such inferences can be applied to facilitate the
implicit negotiation of conversational engagement - in effect, helping users to
weave together the act of contact with the act of determining availability.

<id>
cs/0405109v1
<category>
cs.HC
<abstract>
We provide two case studies in the application of ideas drawn from
conversation analysis to the design of technologies that enhance the experience
of human conversation. We first present a case study of the design of an
electronic guidebook, focusing on how conversation analytic principles played a
role in the design process. We then discuss how the guidebook project has
inspired our continuing work in social, mobile audio spaces. In particular, we
describe some as yet unrealized concepts for adaptive audio spaces.

<id>
cs/0409041v1
<category>
cs.HC
<abstract>
A Survey of the principal literature on Human Centred Design reveals the four
most referenced principles. These are discussed with reference to the
application of a particular website, and a user survey is constructed based
upon the four principles.

<id>
cs/0505011v1
<category>
cs.HC
<abstract>
As computers become more ubiquitous, traditional two-dimensional interfaces
must be replaced with interfaces based on a three-dimensional metaphor.
However, these interfaces must still be as simple and functional as their
two-dimensional predecessors. This paper introduces SWiM, a new interface for
moving application windows between various screens, such as wall displays,
laptop monitors, and desktop displays, in a three-dimensional physical
environment. SWiM was designed based on the results of initial "paper and
pencil" user tests of three possible interfaces. The results of these tests led
to a map-like interface where users select the destination display for their
application from various icons. If the destination is a mobile display it is
not displayed on the map. Instead users can select the screen's name from a
list of all possible destination displays. User testing of SWiM was conducted
to discover whether it is easy to learn and use. Users that were asked to use
SWiM without any instructions found the interface as intuitive to use as users
who were given a demonstration. The results show that SWiM combines simplicity
and functionality to create an interface that is easy to learn and easy to use.

<id>
cs/0507019v1
<category>
cs.HC
<abstract>
Pervasive personal communication technologies offer the potential for
important social benefits for individual users, but also the potential for
significant social difficulties and costs. In research on face-to-face social
interaction, ambiguity is often identified as an important resource for
resolving social difficulties. In this paper, we discuss two design cases of
personal communication systems, one based on fieldwork of a commercial system
and another based on an unrealized design concept. The cases illustrate how
user behavior concerning a particular social difficulty, unexplained
unresponsiveness, can be influenced by technological issues that result in
interactional ambiguity. The cases also highlight the need to balance the
utility of ambiguity against the utility of usability and communicative
clarity.

<id>
cs/0508041v2
<category>
cs.HC
<abstract>
Input method (IM) is a sine qua non for text entry of many Asian languages,
but its potential applications on other languages remain under-explored. This
paper proposes a philosophy of input method design by seeing it as a
nonintrusive plug-in text service framework. Such design allows new
functionalities of text processing to be attached onto a running application
without any tweaking of code. We also introduce OpenVanilla, a cross-platform
framework that is designed with the above-mentioned model in mind. Frameworks
like OpenVanilla have shown that an input method can be more than just a text
entry tool: it offers a convenient way for developing various text service and
language tools.

<id>
cs/0508042v2
<category>
cs.HC
<abstract>
This paper has been withdrawn by the contributor, because it was merged into
cs.HC/0508041

<id>
cs/0601021v1
<category>
cs.HC
<abstract>
We introduce a novel approach to control physical lighting parameters by
means of a pressure-sensitive touchpad. The two-dimensional area of the
touchpad is subdivided into 5 virtual sliders, each controlling the intensity
of a color (red, green, blue, yellow, and white). The physical interaction
methodology is modeled directly after ubiquitous mechanical sliders and dimmers
which tend to be used for intensity/volume control. Our abstraction to a
pressure-sensitive touchpad provides advantages and introduces additional
benefits over such existing devices.

<id>
cs/0601025v1
<category>
cs.HC
<abstract>
Most research on 3D user interfaces aims at providing only a single sensory
modality. One challenge is to integrate several sensory modalities into a
seamless system while preserving each modality's immersion and performance
factors. This paper concerns manipulation tasks and proposes a visuo-haptic
system integrating immersive visualization, tactile force and tactile feedback
with co-location. An industrial application is presented.

<id>
cs/0604103v1
<category>
cs.HC
<abstract>
Under consideration are the general set of Human computer Interaction (HCI)
and Educational principles from prominent contributors in the field and the
construction of a system for evaluating Virtual Learning Environments (VLEs)
with respect to the application of these HCI and Educational Principles. A
frequency analysis of principles is used to obtain the most significant set.
Metrics are devised to provide objective measures of these principles and a
consistent testing regime is introduced. These principles are used to analyse
the University VLE Blackboard. An open source VLE is also constructed with
similar content to Blackboard courses so that a systematic comparison can be
made. HCI and Educational metrics are determined for each VLE.

<id>
cs/0606107v1
<category>
cs.HC
<abstract>
In this report, we describe the work done in a project that explored the
human information processing aspects of a personal memex (a memex to organize
personal information). In the project, we considered the use of the personal
memex, focusing on information recall, by three populations: people with Mild
Cognitive Impairment, those diagnosed with Macular Degeneration, and a
high-functioning population. The outcomes of the project included human
information processing-centered design guidelines for the memex interface, a
low-fidelity prototype, and an annotated bibliography for human information
processing, usability and design literature relating to the memex and the
populations we explored.

<id>
cs/0002010v1
<category>
cs.IR
<abstract>
We discuss how distributed designs that draw from biological network
metaphors can largely improve the current state of information retrieval and
knowledge management of distributed information systems. In particular, two
adaptive recommendation systems named TalkMine and @ApWeb are discussed in more
detail. TalkMine operates at the semantic level of keywords. It leads different
databases to learn new and adapt existing keywords to the categories recognized
by its communities of users using distributed algorithms. @ApWeb operates at
the structural level of information resources, namely citation or hyperlink
structure. It relies on collective behavior to adapt such structure to the
expectations of users. TalkMine and @ApWeb are currently being implemented for
the research library of the Los Alamos National Laboratory under the Active
Recommendation Project. Together they define a biologically motivated
information retrieval system, recommending simultaneously at the level of user
knowledge categories expressed in keywords, and at the level of individual
documents and their associations to other documents. Rather than passive
information retrieval, with this system, users obtain an active, evolving
interaction with information resources.

<id>
cs/0003001v1
<category>
cs.IR
<abstract>
Computers and devices are largely unaware of events taking place in the
world. This could be changed if news were made available in a
computer-understandable form. In this paper we present XML documents called
NewsForms that represent the key points of 17 types of news events. We discuss
the benefits of computer-understandable news and present the NewsExtract
program for converting text news stories into NewsForms.

<id>
cs/0007017v1
<category>
cs.IR
<abstract>
Data modeling is one of the most difficult tasks in application engineering.
The engineer must be aware of the use cases and the required application
services and at a certain point of time he has to fix the data model which
forms the base for the application services. However, once the data model has
been fixed it is difficult to consider changing needs. This might be a problem
in specific domains, which are as dynamic as the healthcare domain. With fuzzy
data we address all those data that are difficult to organize in a single
database. In this paper we discuss a gradual and pragmatic approach that uses
the XML technology to conquer more model flexibility. XML may provide the clue
between unstructured text data and structured database solutions and shift the
paradigm from "organizing the data along a given model" towards "organizing the
data along user requirements".

<id>
cs/0102002v1
<category>
cs.IR
<abstract>
In this paper we discuss several issues related to automated text
classification of web sites. We analyze the nature of web content and metadata
in relation to requirements for text features. We find that HTML metatags are a
good source of text features, but are not in wide use despite their role in
search engine rankings. We present an approach for targeted spidering including
metadata extraction and opportunistic crawling of specific semantic hyperlinks.
We describe a system for automatically classifying web sites into industry
categories and present performance results based on different combinations of
text features and training data. This system can serve as the basis for a
generalized framework for automated metadata creation.

<id>
cs/0209021v1
<category>
cs.IR
<abstract>
Context and context-awareness provides computing environments with the
ability to usefully adapt the services or information they provide. It is the
ability to implicitly sense and automatically derive the user needs that
separates context-aware applications from traditionally designed applications,
and this makes them more attentive, responsive, and aware of their user's
identity, and their user's environment. This paper argues that context-aware
applications capable of supporting complex, cognitive activities can be built
from a model of context called Activity-Centric Context. A conceptual model of
Activity-Centric context is presented. The model is illustrated via a detailed
example.

<id>
cs/0306021v2
<category>
cs.IR
<abstract>
We present a new visualization method to summarize and present periodic
population movement between distinct locations, such as floors, buildings,
cities, or the like. In the specific case of this paper, we have chosen to
focus on student movement between college dormitories on the Columbia
University campus. The visual information is presented to the information
analyst in the form of an interactive geographical map, in which specific
temporal periods as well as individual buildings can be singled out for
detailed data exploration. The navigational interface has been designed to
specifically meet a geographical setting.

<id>
cs/0306026v1
<category>
cs.IR
<abstract>
The adoption of Grid technology has the potential to greatly aid the BaBar
experiment. BdbServer was originally designed to extract copies of data from
the Objectivity/DB database at SLAC and IN2P3. With data now stored in multiple
locations in a variety of data formats, we are enhancing this tool. This will
enable users to extract selected deep copies of event collections and ship them
to the requested site using the facilities offered by the existing Grid
infrastructure. By building on the work done by various groups in BaBar, and
the European DataGrid, we have successfully expanded the capabilities of the
BdbServer software. This should provide a framework for future work in data
distribution.

<id>
cs/0306094v1
<category>
cs.IR
<abstract>
The BABAR Web site was established in 1993 at the Stanford Linear Accelerator
Center (SLAC) to support the BABAR experiment, to report its results, and to
facilitate communication among its scientific and engineering collaborators,
currently numbering about 600 individuals from 75 collaborating institutions in
10 countries. The BABAR Web site is, therefore, a community Web site. At the
same time it is hosted at SLAC and funded by agencies that demand adherence to
policies decided under different priorities. Additionally, the BABAR Web
administrators deal with the problems that arise during the course of managing
users, content, policies, standards, and changing technologies. Desired
solutions to some of these problems may be incompatible with the overall
administration of the SLAC Web sites and/or the SLAC policies and concerns.
There are thus different perspectives of the same Web site and differing
expectations in segments of the SLAC population which act as constraints and
challenges in any review or re-engineering activities. Web Engineering, which
post-dates the BABAR Web, has aimed to provide a comprehensive understanding of
all aspects of Web development. This paper reports on the first part of a
recent review of application of Web Engineering methods to the BABAR Web site,
which has led to explicit user and information models of the BABAR community
and how SLAC and the BABAR community relate and react to each other. The paper
identifies the issues of a community Web site in a hierarchical,
semi-governmental sector and formulates a strategy for periodic reviews of
BABAR and similar sites.

<id>
cs/0308042v1
<category>
cs.IR
<abstract>
WWW has a scale-free structure where novel information is often difficult to
locate. Moreover, Intelligent agents easily get trapped in this structure. Here
a novel method is put forth, which turns these traps into information
repositories, supplies: We populated an Internet environment with intelligent
news foragers. Foraging has its associated cost whereas foragers are rewarded
if they detect not yet discovered novel information. The intelligent news
foragers crawl by using the estimated long-term cumulated reward, and also have
a finite sized memory: the list of most promising supplies. Foragers form an
artificial life community: the most successful ones are allowed to multiply,
while unsuccessful ones die out. The specific property of this community is
that there is no direct communication amongst foragers but the centralized
rewarding system. Still, fast division of work is achieved.

<id>
cs/0402061v1
<category>
cs.IR
<abstract>
In this short technical report, we define on the sample space R^D a distance
between data points which depends on their correlation. We also derive an
expression for the center of mass of a set of points with respect to this
distance.

<id>
cs/0410055v1
<category>
cs.IR
<abstract>
In this lecture I discuss some aspects of MKM, Mathematical Knowledge
Management, with particuar emphasis on information storage and information
retrieval.

<id>
cs/0411026v1
<category>
cs.IR
<abstract>
The article presents an online relevancy tuning method using explicit user
feedback. The contributor developed and tested a method of words' weights
modification based on search result evaluation by user. User decides whether
the result is useful or not after inspecting the full result content. The
experiment proved that the constantly accumulated words weights base leads to
better search quality in a specified data domain. The contributor also suggested
future improvements of the method.

<id>
cs/0503011v1
<category>
cs.IR
<abstract>
In-degree, PageRank, number of visits and other measures of Web page
popularity significantly influence the ranking of search results by modern
search engines. The assumption is that popularity is closely correlated with
quality, a more elusive concept that is difficult to measure directly.
Unfortunately, the correlation between popularity and quality is very weak for
newly-created pages that have yet to receive many visits and/or in-links.
Worse, since discovery of new content is largely done by querying search
engines, and because users usually focus their attention on the top few
results, newly-created but high-quality pages are effectively ``shut out,'' and
it can take a very long time before they become popular.
  We propose a simple and elegant solution to this problem: the introduction of
a controlled amount of randomness into search result ranking methods. Doing so
offers new pages a chance to prove their worth, although clearly using too much
randomness will degrade result quality and annul any benefits achieved. Hence
there is a tradeoff between exploration to estimate the quality of new pages
and exploitation of pages already known to be of high quality. We study this
tradeoff both analytically and via simulation, in the context of an economic
objective function based on aggregate result quality amortized over time. We
show that a modest amount of randomness leads to improved search results.

<id>
cs/0503020v1
<category>
cs.IR
<abstract>
The use of citation counts to assess the impact of research articles is well
established. However, the citation impact of an article can only be measured
several years after it has been published. As research articles are
increasingly accessed through the Web, the number of times an article is
downloaded can be instantly recorded and counted. One would expect the number
of times an article is read to be related both to the number of times it is
cited and to how old the article is. This paper analyses how short-term Web
usage impact predicts medium-term citation impact. The physics e-print archive
(arXiv.org) is used to test this.

<id>
cs/0503021v1
<category>
cs.IR
<abstract>
This article is a critique of: "The 'Green' and 'Gold' Roads to Open Access:
The Case for Mixing and Matching" by Jean-Claude Guedon (in Serials Review
30(4) 2004).
  Open Access (OA) means: free online access to all peer-reviewed journal
articles. Jean-Claude Guedon argues against the efficacy of contributor
self-archiving of peer-reviewed journal articles (the "Green" road to OA). He
suggests instead that we should convert to Open Access Publishing (the "Golden"
road to OA) by "mixing and matching" Green and Gold as follows: o First,
self-archive dissertations (not published, peer-reviewed journal articles). o
Second, identify and tag how those dissertations have been evaluated and
reviewed. o Third, self-archive unrefereed preprints (not published,
peer-reviewed journal articles). o Fourth, develop new mechanisms for
evaluating and reviewing those unrefereed preprints, at multiple levels. The
result will be OA Publishing (Gold). I argue that rather than yet another 10
years of speculation like this, what is actually needed (and imminent) is for
OA self-archiving to be mandated by research funders and institutions so that
the self-archiving of published, peer-reviewed journal articles (Green) can be
fast-forwarded to 100% OA.

<id>
cs/0505039v1
<category>
cs.IR
<abstract>
In this paper we present a number of measures that compare rankings of search
engine results. We apply these measures to five queries that were monitored
daily for two periods of about 21 days each. Rankings of the different search
engines (Google, Yahoo and Teoma for text searches and Google, Yahoo and
Picsearch for image searches) are compared on a daily basis, in addition to
longitudinal comparisons of the same engine for the same query over time. The
results and rankings of the two periods are compared as well.

<id>
cs/0506047v1
<category>
cs.IR
<abstract>
This paper presents an original methodology to consider question answering.
We noticed that query expansion is often incorrect because of a bad
understanding of the question. But the automatic good understanding of an
utterance is linked to the context length, and the question are often short.
This methodology proposes to analyse the documents and to construct an
informative structure from the results of the analysis and from a semantic text
expansion. The linguistic analysis identifies words (tokenization and
morphological analysis), links between words (syntactic analysis) and word
sense (semantic disambiguation). The text expansion adds to each word the
synonyms matching its sense and replaces the words in the utterances by
derivatives, modifying the syntactic schema if necessary. In this way, whatever
enrichment may be, the text keeps the same meaning, but each piece of
information matches many realisations. The questioning method consists in
constructing a local informative structure without enrichment, and matches it
with the documentary structure. If a sentence in the informative structure
matches the question structure, this sentence is the answer to the question.

<id>
cs/0506048v1
<category>
cs.IR
<abstract>
External linguistic resources have been used for a very long time in
information extraction. These methods enrich a document with data that are
semantically equivalent, in order to improve recall. For instance, some of
these methods use synonym dictionaries. These dictionaries enrich a sentence
with words that have a similar meaning. However, these methods present some
serious drawbacks, since words are usually synonyms only in restricted
contexts. The method we propose here consists of using word sense
disambiguation rules (WSD) to restrict the selection of synonyms to only these
that match a specific syntactico-semantic context. We show how WSD rules are
built and how information extraction techniques can benefit from the
application of these rules.

<id>
cs/0507024v2
<category>
cs.IR
<abstract>
This paper presents some experiments in clustering homogeneous XMLdocuments
to validate an existing classification or more generally anorganisational
structure. Our approach integrates techniques for extracting knowledge from
documents with unsupervised classification (clustering) of documents. We focus
on the feature selection used for representing documents and its impact on the
emerging classification. We mix the selection of structured features with fine
textual selection based on syntactic characteristics.We illustrate and evaluate
this approach with a collection of Inria activity reports for the year 2003.
The objective is to cluster projects into larger groups (Themes), based on the
keywords or different chapters of these activity reports. We then compare the
results of clustering using different feature selections, with the official
theme structure used by Inria.

<id>
cs/0507069v1
<category>
cs.IR
<abstract>
The main aspects of XML retrieval are identified by analysing and comparing
the following two behaviours: the behaviour of the assessor when judging the
relevance of returned document components; and the behaviour of users when
interacting with components of XML documents. We argue that the two INEX
relevance dimensions, Exhaustivity and Specificity, are not orthogonal
dimensions; indeed, an empirical analysis of each dimension reveals that the
grades of the two dimensions are correlated to each other. By analysing the
level of agreement between the assessor and the users, we aim at identifying
the best units of retrieval. The results of our analysis show that the highest
level of agreement is on highly relevant and on non-relevant document
components, suggesting that only the end points of the INEX 10-point relevance
scale are perceived in the same way by both the assessor and the users. We
propose a new definition of relevance for XML retrieval and argue that its
corresponding relevance scale would be a better choice for INEX.

<id>
cs/0507070v1
<category>
cs.IR
<abstract>
This paper investigates the impact of three approaches to XML retrieval:
using Zettair, a full-text information retrieval system; using eXist, a native
XML database; and using a hybrid system that takes full article answers from
Zettair and uses eXist to extract elements from those articles. For the
content-only topics, we undertake a preliminary analysis of the INEX 2003
relevance assessments in order to identify the types of highly relevant
document components. Further analysis identifies two complementary sub-cases of
relevance assessments ("General" and "Specific") and two categories of topics
("Broad" and "Narrow"). We develop a novel retrieval module that for a
content-only topic utilises the information from the resulting answer list of a
native XML database and dynamically determines the preferable units of
retrieval, which we call "Coherent Retrieval Elements". The results of our
experiments show that -- when each of the three systems is evaluated against
different retrieval scenarios (such as different cases of relevance
assessments, different topic categories and different choices of evaluation
metrics) -- the XML retrieval systems exhibit varying behaviour and the best
performance can be reached for different values of the retrieval parameters. In
the case of INEX 2003 relevance assessments for the content-only topics, our
newly developed hybrid XML retrieval system is substantially more effective
than either Zettair or eXist, and yields a robust and a very effective XML
retrieval.

<id>
cs/0508017v1
<category>
cs.IR
<abstract>
Three approaches to content-and-structure XML retrieval are analysed in this
paper: first by using Zettair, a full-text information retrieval system; second
by using eXist, a native XML database, and third by using a hybrid XML
retrieval system that uses eXist to produce the final answers from likely
relevant articles retrieved by Zettair. INEX 2003 content-and-structure topics
can be classified in two categories: the first retrieving full articles as
final answers, and the second retrieving more specific elements within articles
as final answers. We show that for both topic categories our initial hybrid
system improves the retrieval effectiveness of a native XML database. For
ranking the final answer elements, we propose and evaluate a novel retrieval
model that utilises the structural relationships between the answer elements of
a native XML database and retrieves Coherent Retrieval Elements. The final
results of our experiments show that when the XML retrieval task focusses on
highly relevant elements our hybrid XML retrieval system with the Coherent
Retrieval Elements module is 1.8 times more effective than Zettair and 3 times
more effective than eXist, and yields an effective content-and-structure XML
retrieval.

<id>
cs/0508036v2
<category>
cs.IR
<abstract>
This paper presents some experiments in clustering homogeneous XMLdocuments
to validate an existing classification or more generally anorganisational
structure. Our approach integrates techniques for extracting knowledge from
documents with unsupervised classification (clustering) of documents. We focus
on the feature selection used for representing documents and its impact on the
emerging classification. We mix the selection of structured features with fine
textual selection based on syntactic characteristics.We illustrate and evaluate
this approach with a collection of Inria activity reports for the year 2003.
The objective is to cluster projects into larger groups (Themes), based on the
keywords or different chapters of these activity reports. We then compare the
results of clustering using different feature selections, with the official
theme structure used by Inria.

<id>
cs/0509005v1
<category>
cs.IR
<abstract>
In this paper, we present an algorithm for automatically building expertise
evidence for finding experts within an organization by combining structured
corporate information with different content. We also describe our test data
collection and our evaluation method. Evaluation of the algorithm shows that
using organizational structure leads to a significant improvement in the
precision of finding an expert. Furthermore we evaluate the impact of using
different data sources on the quality of the results and conclude that Expert
Finding is not a "one engine fits all" solution. It requires an analysis of the
information space into which a solution will be placed and the appropriate
selection and weighting scheme of the data sources.

<id>
cs/0510025v1
<category>
cs.IR
<abstract>
As Web sites are now ordinary products, it is necessary to explicit the
notion of quality of a Web site. The quality of a site may be linked to the
easiness of accessibility and also to other criteria such as the fact that the
site is up to date and coherent. This last quality is difficult to insure
because sites may be updated very frequently, may have many contributors, may be
partially generated and in this context proof-reading is very difficult. The
same piece of information may be found in different occurrences, but also in
data or meta-data, leading to the need for consistency checking. In this paper
we make a parallel between programs and Web sites. We present some examples of
semantic constraints that one would like to specify (constraints between the
meaning of categories and sub-categories in a thematic directory, consistency
between the organization chart and the rest of the site in an academic site).
We present quickly the Natural Semantics, a way to specify the semantics of
programming languages that inspires our works. Then we propose a specification
language for semantic constraints in Web sites that, in conjunction with the
well known ``make'' program, permits to generate some site verification tools
by compiling the specification into Prolog code. We apply our method to a large
XML document which is the scientific part of our institute activity report,
tracking errors or inconsistencies and also constructing some indicators that
can be used by the management of the institute.

<id>
cs/0512032v1
<category>
cs.IR
<abstract>
A growing category of vehicle-infrastructure cooperative (VIC) applications
requires telematics software components distributed between an
infrastructure-based management center and a number of vehicles. This article
presents an approach based on a software framework, focusing on a Telematic
Management System (TMS), a component suite aimed to run inside an
infrastructure-based operations center, in some cases interacting with legacy
systems like Advanced Traffic Management Systems or Vehicle Relationship
Management. The TMS framework provides support for modular, flexible,
prototyping and implementation of VIC applications. This work has received the
support of the European Commission in the context of the projects REACT and
CyberCars.

<id>
cs/0512085v1
<category>
cs.IR
<abstract>
This paper presents a novel analysis and visualization of English Wikipedia
data. Our specific interest is the analysis of basic statistics, the
identification of the semantic structure and age of the categories in this free
online encyclopedia, and the content coverage of its highly productive contributors.
The paper starts with an introduction of Wikipedia and a review of related
work. We then introduce a suite of measures and approaches to analyze and map
the semantic structure of Wikipedia. The results show that co-occurrences of
categories within individual articles have a power-law distribution, and when
mapped reveal the nicely clustered semantic structure of Wikipedia. The results
also reveal the content coverage of the article's contributors, although the roles
these contributors play are as varied as the contributors themselves. We conclude with a
discussion of major results and planned future work.

<id>
cs/0601103v1
<category>
cs.IR
<abstract>
This paper introduces Google Web APIs (Google APIs) as an instrument and
playground for webometric studies. Several examples of Google APIs
implementations are given. Our examples show that this Google Web Service can
be used successfully for informetric Internet based studies albeit with some
restrictions.

<id>
cs/0606105v1
<category>
cs.IR
<abstract>
The continuous improvement in TQM is considered as the core value by which
organisation could maintain a competitive edge. Several techniques and tools
are known to support this core value but most of the time these techniques are
informal and without modelling the interdependence between the core value and
tools. Thus, technique formalisation is one of TQM challenges for increasing
efficiency of quality process implementation. In that way, the paper proposes
and experiments an advanced quality modelling approach based on meta-modelling
the "process approach" as advocated by the standard ISO9000:2000. This
meta-model allows formalising the interdependence between technique, tools and
core value

<id>
cs/0607012v1
<category>
cs.IR
<abstract>
This paper reports on the INRIA group's approach to XML mining while
participating in the INEX XML Mining track 2005. We use a flexible
representation of XML documents that allows taking into account the structure
only or both the structure and content. Our approach consists of representing
XML documents by a set of their sub-paths, defined according to some criteria
(length, root beginning, leaf ending). By considering those sub-paths as words,
we can use standard methods for vocabulary reduction, and simple clustering
methods such as K-means that scale well. We actually use an implementation of
the clustering algorithm known as "dynamic clouds" that can work with distinct
groups of independent variables put in separate variables. This is useful in
our model since embedded sub-paths are not independent: we split potentially
dependant paths into separate variables, resulting in each of them containing
independant paths. Experiments with the INEX collections show good results for
the structure-only collections, but our approach could not scale well for large
structure-and-content collections.

<id>
cs/9906001v1
<category>
cs.IT
<abstract>
This paper computationally obtains optimal bounded-weight, binary,
error-correcting codes for a variety of distance bounds and dimensions. We
compare the sizes of our codes to the sizes of optimal constant-weight, binary,
error-correcting codes, and evaluate the differences.

<id>
cs/0406039v3
<category>
cs.IT
<abstract>
Let A(q,n,d) denote the maximum size of a q-ary code of length n and distance
d. We study the minimum asymptotic redundancy \rho(q,n,d)=n-log_q A(q,n,d) as n
grows while q and d are fixed. For any d and q<=d-1, long algebraic codes are
designed that improve on the BCH codes and have the lowest asymptotic
redundancy \rho(q,n,d) <= ((d-3)+1/(d-2)) log_q n known to date. Prior to this
work, codes of fixed distance that asymptotically surpass BCH codes and the
Gilbert-Varshamov bound were designed only for distances 4,5 and 6.

<id>
cs/0406048v1
<category>
cs.IT
<abstract>
We give a new lower bound on the expansion coefficient of an edge-vertex
graph of a $d$-regular graph. As a consequence, we obtain an improvement on the
lower bound on relative minimum distance of the expander codes constructed by
Sipser and Spielman. We also derive some improved results on the vertex
expansion of graphs that help us in improving the parameters of the expander
codes of Alon, Bruck, Naor, Naor, and Roth.

<id>
cs/0407010v1
<category>
cs.IT
<abstract>
We derive improved bounds on the error and erasure rate for spherical codes
and for binary linear codes under Forney's erasure/list decoding scheme and
prove some related results.

<id>
cs/0407011v3
<category>
cs.IT
<abstract>
We address the problem of bounding below the probability of error under
maximum likelihood decoding of a binary code with a known distance distribution
used on a binary symmetric channel. An improved upper bound is given for the
maximum attainable exponent of this probability (the reliability function of
the channel). In particular, we prove that the ``random coding exponent'' is
the true value of the channel reliability for code rate $R$ in some interval
immediately below the critical rate of the channel. An analogous result is
obtained for the Gaussian channel.

<id>
cs/0408008v1
<category>
cs.IT
<abstract>
We study codes on graphs combined with an iterative message passing algorithm
for quantization. Specifically, we consider the binary erasure quantization
(BEQ) problem which is the dual of the binary erasure channel (BEC) coding
problem. We show that duals of capacity achieving codes for the BEC yield codes
which approach the minimum possible rate for the BEQ. In contrast, low density
parity check codes cannot achieve the minimum rate unless their density grows
at least logarithmically with block length. Furthermore, we show that duals of
efficient iterative decoding algorithms for the BEC yield efficient encoding
algorithms for the BEQ. Hence our results suggest that graphical models may
yield near optimal codes in source coding as well as in channel coding and that
duality plays a key role in such constructions.

<id>
cs/0408017v1
<category>
cs.IT
<abstract>
A variable-length code is a fix-free code if no codeword is a prefix or a
suffix of any other codeword. In a fix-free code any finite sequence of
codewords can be decoded in both directions, which can improve the robustness
to channel noise and speed up the decoding process. In this paper we prove a
new sufficient condition of the existence of fix-free codes and improve the
upper bound on the redundancy of optimal fix-free codes.

<id>
cs/0408038v1
<category>
cs.IT
<abstract>
Fundamental results concerning the dynamics of abelian group codes
(behaviors) and their duals are developed. Duals of sequence spaces over
locally compact abelian groups may be defined via Pontryagin duality; dual
group codes are orthogonal subgroups of dual sequence spaces. The dual of a
complete code or system is finite, and the dual of a Laurent code or system is
(anti-)Laurent. If C and C^\perp are dual codes, then the state spaces of C act
as the character groups of the state spaces of C^\perp. The controllability
properties of C are the observability properties of C^\perp. In particular, C
is (strongly) controllable if and only if C^\perp is (strongly) observable, and
the controller memory of C is the observer memory of C^\perp. The controller
granules of C act as the character groups of the observer granules of C^\perp.
Examples of minimal observer-form encoder and syndrome-former constructions are
given. Finally, every observer granule of C is an "end-around" controller
granule of C.

<id>
cs/0408062v1
<category>
cs.IT
<abstract>
We consider lossy source coding when side information affecting the
distortion measure may be available at the encoder, decoder, both, or neither.
For example, such distortion side information can model reliabilities for noisy
measurements, sensor calibration information, or perceptual effects like
masking and sensitivity to context. When the distortion side information is
statistically independent of the source, we show that in many cases (e.g, for
additive or multiplicative distortion side information) there is no penalty for
knowing the side information only at the encoder, and there is no advantage to
knowing it at the decoder. Furthermore, for quadratic distortion measures
scaled by the distortion side information, we evaluate the penalty for lack of
encoder knowledge and show that it can be arbitrarily large. In this scenario,
we also sketch transform based quantizers constructions which efficiently
exploit encoder side information in the high-resolution limit.

<id>
cs/0409011v2
<category>
cs.IT
<abstract>
We continue to discuss why MMSE estimation arises in coding schemes that
approach the capacity of linear Gaussian channels. Here we consider schemes
that involve successive decoding, such as decision-feedback equalization or
successive cancellation.

<id>
cs/0409026v1
<category>
cs.IT
<abstract>
We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel and is refined for the BEC.

<id>
cs/0409027v1
<category>
cs.IT
<abstract>
We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel, and is refined for the BEC.

<id>
cs/0409053v1
<category>
cs.IT
<abstract>
We discuss why MMSE estimation arises in lattice-based schemes for
approaching the capacity of linear Gaussian channels, and comment on its
properties.

<id>
cs/0410002v1
<category>
cs.IT
<abstract>
We compare the elementary theories of Shannon information and Kolmogorov
complexity, the extent to which they have a common purpose, and where they are
fundamentally different. We discuss and relate the basic notions of both
theories: Shannon entropy versus Kolmogorov complexity, the relation of both to
universal coding, Shannon mutual information versus Kolmogorov (`algorithmic')
mutual information, probabilistic sufficient statistic versus algorithmic
sufficient statistic (related to lossy compression in the Shannon theory versus
meaningful information in the Kolmogorov theory), and rate distortion theory
versus Kolmogorov's structure function. Part of the material has appeared in
print before, scattered through various publications, but this is the first
comprehensive systematic comparison. The last mentioned relations are new.

<id>
cs/0410003v2
<category>
cs.IT
<abstract>
Capacity formulas and random-coding exponents are derived for a generalized
family of Gel'fand-Pinsker coding problems. These exponents yield asymptotic
upper bounds on the achievable log probability of error. In our model,
information is to be reliably transmitted through a noisy channel with finite
input and output alphabets and random state sequence, and the channel is
selected by a hypothetical adversary. Partial information about the state
sequence is available to the encoder, adversary, and decoder. The design of the
transmitter is subject to a cost constraint. Two families of channels are
considered: 1) compound discrete memoryless channels (CDMC), and 2) channels
with arbitrary memory, subject to an additive cost constraint, or more
generally to a hard constraint on the conditional type of the channel output
given the input. Both problems are closely connected. The random-coding
exponent is achieved using a stacked binning scheme and a maximum penalized
mutual information decoder, which may be thought of as an empirical generalized
Maximum a Posteriori decoder. For channels with arbitrary memory, the
random-coding exponents are larger than their CDMC counterparts. Applications
of this study include watermarking, data hiding, communication in presence of
partially known interferers, and problems such as broadcast channels, all of
which involve the fundamental idea of binning.

<id>
cs/0410008v1
<category>
cs.IT
<abstract>
We consider source coding with fixed lag side information at the decoder. We
focus on the special case of perfect side information with unit lag
corresponding to source coding with feedforward (the dual of channel coding
with feedback) introduced by Pradhan. We use this duality to develop a linear
complexity algorithm which achieves the rate-distortion bound for any
memoryless finite alphabet source and distortion measure.

<id>
cs/0410040v1
<category>
cs.IT
<abstract>
We propose use of QR factorization with sort and Dijkstra's algorithm for
decreasing the computational complexity of the sphere decoder that is used for
ML detection of signals on the multi-antenna fading channel. QR factorization
with sort decreases the complexity of searching part of the decoder with small
increase in the complexity required for preprocessing part of the decoder.
Dijkstra's algorithm decreases the complexity of searching part of the decoder
with increase in the storage complexity. The computer simulation demonstrates
that the complexity of the decoder is reduced by the proposed methods
significantly.

<id>
cs/0410041v1
<category>
cs.IT
<abstract>
In this paper, we analyze the performance of space-time block codes which
enable symbolwise maximum likelihood decoding. We derive an upper bound of
maximum mutual information (MMI) on space-time block codes that enable
symbolwise maximum likelihood decoding for a frequency non-selective
quasi-static fading channel. MMI is an upper bound on how much one can send
information with vanishing error probability by using the target code.

<id>
cs/0411006v1
<category>
cs.IT
<abstract>
In this paper, we present two low complexity algorithms that achieve capacity
for the noiseless (d,k) constrained channel when k=2d+1, or when k-d+1 is not
prime. The first algorithm, called symbol sliding, is a generalized version of
the bit flipping algorithm introduced by Aviran et al. [1]. In addition to
achieving capacity for (d,2d+1) constraints, it comes close to capacity in
other cases. The second algorithm is based on interleaving, and is a
generalized version of the bit stuffing algorithm introduced by Bender and Wolf
[2]. This method uses fewer than k-d biased bit streams to achieve capacity for
(d,k) constraints with k-d+1 not prime. In particular, the encoder for
(d,d+2^m-1) constraints, 1\le m<\infty, requires only m biased bit streams.

<id>
cs/0411011v1
<category>
cs.IT
<abstract>
Capacity analysis for channels with side information at the receiver has been
an active area of interest. This problem is well investigated for the case of
finite alphabet channels. However, the results are not easily generalizable to
the case of continuous alphabet channels due to analytic difficulties inherent
with continuous alphabets. In the first part of this two-part paper, we address
an analytical framework for capacity analysis of continuous alphabet channels
with side information at the receiver. For this purpose, we establish novel
necessary and sufficient conditions for weak* continuity and strict concavity
of the mutual information. These conditions are used in investigating the
existence and uniqueness of the capacity-achieving measures. Furthermore, we
derive necessary and sufficient conditions that characterize the capacity value
and the capacity-achieving measure for continuous alphabet channels with side
information at the receiver.

<id>
cs/0411012v1
<category>
cs.IT
<abstract>
In this part, we consider the capacity analysis for wireless mobile systems
with multiple antenna architectures. We apply the results of the first part to
a commonly known baseband, discrete-time multiple antenna system where both the
transmitter and receiver know the channel's statistical law. We analyze the
capacity for additive white Gaussian noise (AWGN) channels, fading channels
with full channel state information (CSI) at the receiver, fading channels with
no CSI, and fading channels with partial CSI at the receiver. For each type of
channels, we study the capacity value as well as issues such as the existence,
uniqueness, and characterization of the capacity-achieving measures for
different types of moment constraints. The results are applicable to both
Rayleigh and Rician fading channels in the presence of arbitrary line-of-sight
and correlation profiles.

<id>
cs/0411014v4
<category>
cs.IT
<abstract>
We examine the structure of families of distortion balls from the perspective
of Kolmogorov complexity. Special attention is paid to the canonical
rate-distortion function of a source word which returns the minimal Kolmogorov
complexity of all distortion balls containing that word subject to a bound on
their cardinality. This canonical rate-distortion function is related to the
more standard algorithmic rate-distortion function for the given distortion
measure. Examples are given of list distortion, Hamming distortion, and
Euclidean distortion. The algorithmic rate-distortion function can behave
differently from Shannon's rate-distortion function. To this end, we show that
the canonical rate-distortion function can and does assume a wide class of
shapes (unlike Shannon's); we relate low algorithmic mutual information to low
Kolmogorov complexity (and consequently suggest that certain aspects of the
mutual information formulation of Shannon's rate-distortion function behave
differently than would an analogous formulation using algorithmic mutual
information); we explore the notion that low Kolmogorov complexity distortion
balls containing a given word capture the interesting properties of that word
(which is hard to formalize in Shannon's theory) and this suggests an approach
to denoising; and, finally, we show that the different behavior of the
rate-distortion curves of individual source words to some extent disappears
after averaging over the source words.

<id>
cs/0411036v2
<category>
cs.IT
<abstract>
The feedback capacity of the stationary Gaussian additive noise channel has
been open, except for the case where the noise is white. Here we find the
feedback capacity of the stationary first-order moving average additive
Gaussian noise channel in closed form. Specifically, the channel is given by
$Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\{X_i\}$ satisfies a power
constraint and the noise $\{Z_i\}$ is a first-order moving average Gaussian
process defined by $Z_i = \alpha U_{i-1} + U_i,$ $|\alpha| \le 1,$ with white
Gaussian innovations $U_i,$ $i = 0,1,....$
  We show that the feedback capacity of this channel is $-\log x_0,$ where
$x_0$ is the unique positive root of the equation $ \rho x^2 = (1-x^2) (1 -
|\alpha|x)^2,$ and $\rho$ is the ratio of the average input power per
transmission to the variance of the noise innovation $U_i$. The optimal coding
scheme parallels the simple linear signalling scheme by Schalkwijk and Kailath
for the additive white Gaussian noise channel -- the transmitter sends a
real-valued information-bearing signal at the beginning of communication and
subsequently refines the receiver's error by processing the feedback noise
signal through a linear stationary first-order autoregressive filter. The
resulting error probability of the maximum likelihood decoding decays
doubly-exponentially in the duration of the communication. This feedback
capacity of the first-order moving average Gaussian channel is very similar in
form to the best known achievable rate for the first-order
\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, and
Tiernan, although the optimality of the latter is yet to be established.

<id>
cs/0411073v1
<category>
cs.IT
<abstract>
Geographic routing with greedy relaying strategies have been widely studied
as a routing scheme in sensor networks. These schemes assume that the nodes
have perfect information about the location of the destination. When the
distance between the source and destination is normalized to unity, the
asymptotic routing delays in these schemes are $\Theta(\frac{1}{M(n)}),$ where
M(n) is the maximum distance traveled in a single hop (transmission range of a
radio). In this paper, we consider routing scenarios where nodes have location
errors (imprecise GPS), or where only coarse geographic information about the
destination is available, and only a fraction of the nodes have routing
information. We show that even with such imprecise or limited
destination-location information, the routing delays are
$\Theta(\frac{1}{M(n)})$. We also consider the throughput-capacity of networks
with progressive routing strategies that take packets closer to the destination
in every step, but not necessarily along a straight-line. We show that the
throughput-capacity with progressive routing is order-wise the same as the
maximum achievable throughput-capacity.

<id>
cs/0411098v1
<category>
cs.IT
<abstract>
We obtain the first term in the high signal-to-noise ratio (SNR) expansion of
the capacity of fading networks where the transmitters and receivers--while
fully cognizant of the fading \emph{law}--have no access to the fading
\emph{realization}. This term is an integer multiple of $\log \log
\textnormal{SNR}$ with the coefficient having a simple combinatorial
characterization.

<id>
cs/0412060v2
<category>
cs.IT
<abstract>
The dependence of the Gaussian input information rate on the line-of-sight
(LOS) matrix in multiple-input multiple-output coherent Rician fading channels
is explored. It is proved that the outage probability and the mutual
information induced by a multivariate circularly symmetric Gaussian input with
any covariance matrix are monotonic in the LOS matrix D, or more precisely,
monotonic in D'D in the sense of the Loewner partial order. Conversely, it is
also demonstrated that this ordering on the LOS matrices is a necessary
condition for the uniform monotonicity over all input covariance matrices. This
result is subsequently applied to prove the monotonicity of the isotropic
Gaussian input information rate and channel capacity in the singular values of
the LOS matrix. Extensions to multiple-access channels are also discussed.

<id>
cs/0412067v1
<category>
cs.IT
<abstract>
Recently, a quasi-orthogonal space-time block code (QSTBC) capable of
achieving a significant fraction of the outage mutual information of a
multiple-input-multiple output (MIMO) wireless communication system for the
case of four transmit and one receive antennas was proposed. We generalize
these results to $n_T=2^n$ transmit and an arbitrary number of receive antennas
$n_R$. Furthermore, we completely characterize the structure of the equivalent
channel for the general case and show that for all $n_T=2^n$ and $n_R$ the
eigenvectors of the equivalent channel are fixed and independent from the
channel realization. Furthermore, the eigenvalues of the equivalent channel are
independent identically distributed random variables each following a
noncentral chi-square distribution with $4n_R$ degrees of freedom.
  Based on these important insights into the structure of the QSTBC, we derive
an analytical lower bound for the fraction of outage probability achieved with
QSTBC and show that this bound is tight for low signal-to-noise-ratios (SNR)
values and also for increasing number of receive antennas. We also present an
upper bound, which is tight for high SNR values and derive analytical
expressions for the case of four transmit antennas. Finally, by utilizing the
special structure of the QSTBC we propose a new transmit strategy, which
decouples the signals transmitted from different antennas in order to detect
the symbols separately with a linear ML-detector rather than joint detection,
an up to now only known advantage of orthogonal space-time block codes (OSTBC).

<id>
cs/0412108v1
<category>
cs.IT
<abstract>
This paper deals with arbitrarily distributed finite-power input signals
observed through an additive Gaussian noise channel. It shows a new formula
that connects the input-output mutual information and the minimum mean-square
error (MMSE) achievable by optimal estimation of the input given the output.
That is, the derivative of the mutual information (nats) with respect to the
signal-to-noise ratio (SNR) is equal to half the MMSE, regardless of the input
statistics. This relationship holds for both scalar and vector signals, as well
as for discrete-time and continuous-time noncausal MMSE estimation. This
fundamental information-theoretic result has an unexpected consequence in
continuous-time nonlinear estimation: For any input signal with finite power,
the causal filtering MMSE achieved at SNR is equal to the average value of the
noncausal smoothing MMSE achieved with a channel whose signal-to-noise ratio is
chosen uniformly distributed between 0 and SNR.

<id>
cs/0412111v2
<category>
cs.IT
<abstract>
A new lower bound on the error probability of maximum likelihood decoding of
a binary code on a binary symmetric channel was proved in Barg and McGregor
(2004, cs.IT/0407011). It was observed in that paper that this bound leads to a
new region of code rates in which the random coding exponent is asymptotically
tight, giving a new region in which the reliability of the BSC is known
exactly. The present paper explains the relation of these results to the union
bound on the error probability.

<id>
cs/0412112v1
<category>
cs.IT
<abstract>
We introduce the idea of distortion side information, which does not directly
depend on the source but instead affects the distortion measure. We show that
such distortion side information is not only useful at the encoder, but that
under certain conditions, knowing it at only the encoder is as good as knowing
it at both encoder and decoder, and knowing it at only the decoder is useless.
Thus distortion side information is a natural complement to the signal side
information studied by Wyner and Ziv, which depends on the source but does not
involve the distortion measure. Furthermore, when both types of side
information are present, we characterize the penalty for deviating from the
configuration of encoder-only distortion side information and decoder-only
signal side information, which in many cases is as good as full side
information knowledge.

<id>
cs/9905014v1
<category>
cs.LG
<abstract>
This paper presents the MAXQ approach to hierarchical reinforcement learning
based on decomposing the target Markov decision process (MDP) into a hierarchy
of smaller MDPs and decomposing the value function of the target MDP into an
additive combination of the value functions of the smaller MDPs. The paper
defines the MAXQ hierarchy, proves formal results on its representational
power, and establishes five conditions for the safe use of state abstractions.
The paper presents an online model-free learning algorithm, MAXQ-Q, and proves
that it converges wih probability 1 to a kind of locally-optimal policy known
as a recursively optimal policy, even in the presence of the five kinds of
state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q
through a series of experiments in three domains and shows experimentally that
MAXQ-Q (with state abstractions) converges to a recursively optimal policy much
faster than flat Q learning. The fact that MAXQ learns a representation of the
value function has an important benefit: it makes it possible to compute and
execute an improved, non-hierarchical policy via a procedure similar to the
policy improvement step of policy iteration. The paper demonstrates the
effectiveness of this non-hierarchical execution experimentally. Finally, the
paper concludes with a comparison to related work and a discussion of the
design tradeoffs in hierarchical reinforcement learning.

<id>
cs/9905015v1
<category>
cs.LG
<abstract>
Many researchers have explored methods for hierarchical reinforcement
learning (RL) with temporal abstractions, in which abstract actions are defined
that can perform many primitive actions before terminating. However, little is
known about learning with state abstractions, in which aspects of the state
space are ignored. In previous work, we developed the MAXQ method for
hierarchical RL. In this paper, we define five conditions under which state
abstraction can be combined with the MAXQ value function decomposition. We
prove that the MAXQ-Q learning algorithm converges under these conditions and
show experimentally that state abstraction is important for the successful
application of MAXQ-Q learning.

<id>
cs/0001004v1
<category>
cs.LG
<abstract>
The multiplicative Newton-like method developed by the contributor et al. is
extended to the situation where the dynamics is restricted to the orthogonal
group. A general framework is constructed without specifying the cost function.
Though the restriction to the orthogonal groups makes the problem somewhat
complicated, an explicit expression for the amount of individual jumps is
obtained. This algorithm is exactly second-order-convergent. The global
instability inherent in the Newton method is remedied by a
Levenberg-Marquardt-type variation. The method thus constructed can readily be
applied to the independent component analysis. Its remarkable performance is
illustrated by a numerical simulation.

<id>
cs/0002006v1
<category>
cs.LG
<abstract>
We construct new algorithms from scratch, which use the fourth order cumulant
of stochastic variables for the cost function. The multiplicative updating rule
here constructed is natural from the homogeneous nature of the Lie group and
has numerous merits for the rigorous treatment of the dynamics. As one
consequence, the second order convergence is shown. For the cost function,
functions invariant under the componentwise scaling are choosen. By identifying
points which can be transformed to each other by the scaling, we assume that
the dynamics is in a coset space. In our method, a point can move toward any
direction in this coset. Thus, no prewhitening is required.

<id>
cs/0009001v3
<category>
cs.LG
<abstract>
Given a reference computer, Kolmogorov complexity is a well defined function
on all binary strings. In the standard approach, however, only the asymptotic
properties of such functions are considered because they do not depend on the
reference computer. We argue that this approach can be more useful if it is
refined to include an important practical case of simple binary strings.
Kolmogorov complexity calculus may be developed for this case if we restrict
the class of available reference computers. The interesting problem is to
define a class of computers which is restricted in a {\it natural} way modeling
the real-life situation where only a limited class of computers is physically
available to us. We give an example of what such a natural restriction might
look like mathematically, and show that under such restrictions some error
terms, even logarithmic in complexity, can disappear from the standard
complexity calculus.
  Keywords: Kolmogorov complexity; Algorithmic information theory.

<id>
cs/0009007v1
<category>
cs.LG
<abstract>
In real-world environments it usually is difficult to specify target
operating conditions precisely, for example, target misclassification costs.
This uncertainty makes building robust classification systems problematic. We
show that it is possible to build a hybrid classifier that will perform at
least as well as the best available classifier for any target conditions. In
some cases, the performance of the hybrid actually can surpass that of the best
known classifier. This robust performance extends across a wide variety of
comparison frameworks, including the optimization of metrics such as accuracy,
expected cost, lift, precision, recall, and workforce utilization. The hybrid
also is efficient to build, to store, and to update. The hybrid is based on a
method for the comparison of classifier performance that is robust to imprecise
class distributions and misclassification costs. The ROC convex hull (ROCCH)
method combines techniques from ROC analysis, decision analysis and
computational geometry, and adapts them to the particulars of analyzing learned
classifiers. The method is efficient and incremental, minimizes the management
of classifier performance data, and allows for clear visual comparisons and
sensitivity analyses. Finally, we point to empirical evidence that a robust
hybrid classifier indeed is needed for many real-world problems.

<id>
cs/0011032v1
<category>
cs.LG
<abstract>
An approach to clustering is presented that adapts the basic top-down
induction of decision trees method towards clustering. To this aim, it employs
the principles of instance based learning. The resulting methodology is
implemented in the TIC (Top down Induction of Clustering trees) system for
first order clustering. The TIC system employs the first order logical decision
tree representation of the inductive logic programming system Tilde. Various
experiments with TIC are presented, in both propositional and relational
domains.

<id>
cs/0011044v1
<category>
cs.LG
<abstract>
When comparing inductive logic programming (ILP) and attribute-value learning
techniques, there is a trade-off between expressive power and efficiency.
Inductive logic programming techniques are typically more expressive but also
less efficient. Therefore, the data sets handled by current inductive logic
programming systems are small according to general standards within the data
mining community. The main source of inefficiency lies in the assumption that
several examples may be related to each other, so they cannot be handled
independently.
  Within the learning from interpretations framework for inductive logic
programming this assumption is unnecessary, which allows to scale up existing
ILP algorithms. In this paper we explain this learning setting in the context
of relational databases. We relate the setting to propositional data mining and
to the classical ILP setting, and show that learning from interpretations
corresponds to learning from multiple relations and thus extends the
expressiveness of propositional learning, while maintaining its efficiency to a
large extent (which is not the case in the classical ILP setting).
  As a case study, we present two alternative implementations of the ILP system
Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which
loads all data in main memory, and Tilde-LDS, which loads the examples one by
one. We experimentally compare the implementations, showing Tilde-LDS can
handle large data sets (in the order of 100,000 examples or 100 MB) and indeed
scales up linearly in the number of examples.

<id>
cs/0103003v1
<category>
cs.LG
<abstract>
In order for an agent to perform well in partially observable domains, it is
usually necessary for actions to depend on the history of observations. In this
paper, we explore a {\it stigmergic} approach, in which the agent's actions
include the ability to set and clear bits in an external memory, and the
external memory is included as part of the input to the agent. In this case, we
need to learn a reactive policy in a highly non-Markovian domain. We explore
two algorithms: SARSA(\lambda), which has had empirical success in partially
observable domains, and VAPS, a new algorithm due to Baird and Moore, with
convergence guarantees in partially observable domains. We compare the
performance of these two algorithms on benchmark problems.

<id>
cs/0110036v1
<category>
cs.LG
<abstract>
Cross-validation is a useful and generally applicable technique often
employed in machine learning, including decision tree induction. An important
disadvantage of straightforward implementation of the technique is its
computational overhead. In this paper we show that, for decision trees, the
computational overhead of cross-validation can be reduced significantly by
integrating the cross-validation with the normal decision tree induction
process. We discuss how existing decision tree algorithms can be adapted to
this aim, and provide an analysis of the speedups these adaptations may yield.
The analysis is supported by experimental results.

<id>
cs/0211003v1
<category>
cs.LG
<abstract>
The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for
construction of probabilistic classifiers. This paper presents an empirical
comparison of the MBBC algorithm with three other Bayesian classifiers: Naive
Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these
are implemented using the K2 framework of Cooper and Herskovits. The
classifiers are compared in terms of their performance (using simple accuracy
measures and ROC curves) and speed, on a range of standard benchmark data sets.
It is concluded that MBBC is competitive in terms of speed and accuracy with
the other algorithms considered.

<id>
cs/0211007v1
<category>
cs.LG
<abstract>
In biological data, it is often the case that observed data are available
only for a subset of samples. When a kernel matrix is derived from such data,
we have to leave the entries for unavailable samples as missing. In this paper,
we make use of a parametric model of kernel matrices, and estimate missing
entries by fitting the model to existing entries. The parametric model is
created as a set of spectral variants of a complete kernel matrix derived from
another information source. For model fitting, we adopt the em algorithm based
on the information geometry of positive definite matrices. We will report
promising results on bacteria clustering experiments using two marker
sequences: 16S and gyrB.

<id>
cs/0309015v1
<category>
cs.LG
<abstract>
To learn (statistical) dependencies among random variables requires
exponentially large sample size in the number of observed random variables if
any arbitrary joint probability distribution can occur.
  We consider the case that sparse data strongly suggest that the probabilities
can be described by a simple Bayesian network, i.e., by a graph with small
in-degree \Delta. Then this simple law will also explain further data with high
confidence. This is shown by calculating bounds on the VC dimension of the set
of those probability measures that correspond to simple graphs. This allows to
select networks by structural risk minimization and gives reliability bounds on
the error of the estimated joint measure without (in contrast to a previous
paper) any prior assumptions on the set of possible joint measures.
  The complexity for searching the optimal Bayesian networks of in-degree
\Delta increases only polynomially in the number of random varibales for
constant \Delta and the optimal joint measure associated with a given graph can
be found by convex optimization.

<id>
cs/0311042v1
<category>
cs.LG
<abstract>
We make progress on two important problems regarding attribute efficient
learnability.
  First, we give an algorithm for learning decision lists of length $k$ over
$n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time
$n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision
lists that has both subexponential sample complexity and subexponential running
time in the relevant parameters. Our approach establishes a relationship
between attribute efficient learning and polynomial threshold functions and is
based on a new construction of low degree, low weight polynomial threshold
functions for decision lists. For a wide range of parameters our construction
matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives
an essentially optimal tradeoff between polynomial threshold function degree
and weight.
  Second, we give an algorithm for learning an unknown parity function on $k$
out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.
For $k=o(\log n)$ this yields a polynomial time algorithm with sample
complexity $o(n)$. This is the first polynomial time algorithm for learning
parity on a superconstant number of variables with sublinear sample complexity.

<id>
cs/0312004v1
<category>
cs.LG
<abstract>
Using naive Bayes for email classification has become very popular within the
last few months. They are quite easy to implement and very efficient. In this
paper we want to present empirical results of email classification using a
combination of naive Bayes and k-nearest neighbor searches. Using this
technique we show that the accuracy of a Bayes filter can be improved slightly
for a high number of features and significantly for a small number of features.

<id>
cs/0401005v1
<category>
cs.LG
<abstract>
It is offered to pool test points of different subjects and different aspects
of the same subject together in order to get the unitary rating score, by the
way of nonlinear transformation of indicator points in accordance with Zipf's
distribution. It is proposed to use the well-studied distribution of
Intellectuality Quotient IQ as the reference distribution for latent variable
"progress in studies".

<id>
cs/0412003v1
<category>
cs.LG
<abstract>
For the last years, time-series mining has become a challenging issue for
researchers. An important application lies in most monitoring purposes, which
require analyzing large sets of time-series for learning usual patterns. Any
deviation from this learned profile is then considered as an unexpected
situation. Moreover, complex applications may involve the temporal study of
several heterogeneous parameters. In that paper, we propose a method for mining
heterogeneous multivariate time-series for learning meaningful patterns. The
proposed approach allows for mixed time-series -- containing both pattern and
non-pattern data -- such as for imprecise matches, outliers, stretching and
global translating of patterns instances in time. We present the early results
of our approach in the context of monitoring the health status of a person at
home. The purpose is to build a behavioral profile of a person by analyzing the
time variations of several quantitative or qualitative parameters recorded
through a provision of sensors installed in the home.

<id>
cs/0502016v1
<category>
cs.LG
<abstract>
We discuss stability for a class of learning algorithms with respect to noisy
labels. The algorithms we consider are for regression, and they involve the
minimization of regularized risk functionals, such as L(f) := 1/N sum_i
(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when
y_i is a noisy version of f*(x_i) for some function f* in H, the output of the
algorithm converges to f* as the regularization term and noise simultaneously
vanish. We consider two flavors of this problem, one where a data set of N
points remains fixed, and the other where N -> infinity. For the case where N
-> infinity, we give conditions for convergence to f_E (the function which is
the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we
describe the limiting 'non-noisy', 'non-regularized' function f*, and give
conditions for convergence. In the process, we develop a set of tools for
dealing with functionals such as L(f), which are applicable to many other
problems in learning theory.

<id>
cs/0504001v1
<category>
cs.LG
<abstract>
We consider the probability hierarchy for Popperian FINite learning and study
the general properties of this hierarchy. We prove that the probability
hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and
p_2 and answers whether PFIN-type learning with the probability of success p_1
is equivalent to PFIN-type learning with the probability of success p_2.
  To prove our result, we analyze the topological structure of the probability
hierarchy. We prove that it is well-ordered in descending ordering and
order-equivalent to ordinal epsilon_0. This shows that the structure of the
hierarchy is very complicated.
  Using similar methods, we also prove that, for PFIN-type learning, team
learning and probabilistic learning are of the same power.

<id>
cs/0506004v4
<category>
cs.LG
<abstract>
We analyze a new algorithm for probability forecasting of binary observations
on the basis of the available data, without making any assumptions about the
way the observations are generated. The algorithm is shown to be well
calibrated and to have good resolution for long enough sequences of
observations and for a suitable choice of its parameter, a kernel on the
Cartesian product of the forecast space $[0,1]$ and the data space. Our main
results are non-asymptotic: we establish explicit inequalities, shown to be
tight, for the performance of the algorithm.

<id>
cs/0506007v2
<category>
cs.LG
<abstract>
We consider a general class of forecasting protocols, called "linear
protocols", and discuss several important special cases, including multi-class
forecasting. Forecasting is formalized as a game between three players:
Reality, whose role is to generate observations; Forecaster, whose goal is to
predict the observations; and Skeptic, who tries to make money on any lack of
agreement between Forecaster's predictions and the actual observations. Our
main mathematical result is that for any continuous strategy for Skeptic in a
linear protocol there exists a strategy for Forecaster that does not allow
Skeptic's capital to grow. This result is a meta-theorem that allows one to
transform any continuous law of probability in a linear protocol into a
forecasting strategy whose predictions are guaranteed to satisfy this law. We
apply this meta-theorem to a weak law of large numbers in Hilbert spaces to
obtain a version of the K29 prediction algorithm for linear protocols and show
that this version also satisfies the attractive properties of proper
calibration and resolution under a suitable choice of its kernel parameter,
with no assumptions about the way the data is generated.

<id>
cs/0506057v2
<category>
cs.LG
<abstract>
This article offers a 3-parameter model of testing, with 1) the difference
between the ability level of the examinee and item difficulty; 2) the examinee
discrimination and 3) the item discrimination as model parameters.

<id>
cs/0506085v1
<category>
cs.LG
<abstract>
We propose a new framework for building and evaluating machine learning
algorithms. We argue that many real-world problems require an agent which must
quickly learn to respond to demands, yet can continue to perform and respond to
new training throughout its useful life. We give a framework for how such
agents can be built, describe several metrics for evaluating them, and show
that subtle changes in system construction can significantly affect agent
performance.

<id>
cs/0507033v2
<category>
cs.LG
<abstract>
We present in this work a new methodology to design kernels on data which is
structured with smaller components, such as text, images or sequences. This
methodology is a template procedure which can be applied on most kernels on
measures and takes advantage of a more detailed "bag of components"
representation of the objects. To obtain such a detailed description, we
consider possible decompositions of the original bag into a collection of
nested bags, following a prior knowledge on the objects' structure. We then
consider these smaller bags to compare two objects both in a detailed
perspective, stressing local matches between the smaller bags, and in a global
or coarse perspective, by considering the entire bag. This multiresolution
approach is likely to be best suited for tasks where the coarse approach is not
precise enough, and where a more subtle mixture of both local and global
similarities is necessary to compare objects. The approach presented here would
not be computationally tractable without a factorization trick that we
introduce before presenting promising results on an image retrieval task.

<id>
cs/0507044v1
<category>
cs.LG
<abstract>
This paper shows how universal learning can be achieved with expert advice.
To this aim, we specify an experts algorithm with the following
characteristics: (a) it uses only feedback from the actions actually chosen
(bandit setup), (b) it can be applied with countably infinite expert classes,
and (c) it copes with losses that may grow in time appropriately slowly. We
prove loss bounds against an adaptive adversary. From this, we obtain a master
algorithm for "reactive" experts problems, which means that the master's
actions may influence the behavior of the adversary. Our algorithm can
significantly outperform standard experts algorithms on such problems. Finally,
we combine it with a universal expert class. The resulting universal learner
performs -- in a certain sense -- almost as well as any computable strategy,
for any online decision problem. We also specify the (worst-case) convergence
speed, which is very slow.

<id>
cs/0507062v1
<category>
cs.LG
<abstract>
A main problem of "Follow the Perturbed Leader" strategies for online
decision problems is that regret bounds are typically proven against oblivious
adversary. In partial observation cases, it was not clear how to obtain
performance guarantees against adaptive adversary, without worsening the
bounds. We propose a conceptually simple argument to resolve this problem.
Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed
bandit problem is shown. This bound holds for the common FPL variant using only
the observations from designated exploration rounds. Using all observations
allows for the stronger bound of O(t^(1/2)), matching the best bound known so
far (and essentially the known lower bound) for adversarial bandits.
Surprisingly, this variant does not even need explicit exploration, it is
self-stabilizing. However the sampling probabilities have to be either
externally provided or approximated to sufficient accuracy, using O(t^2 log t)
samples in each step.

<id>
cs/0509055v1
<category>
cs.LG
<abstract>
Naive Bayes is a simple Bayesian classifier with strong independence
assumptions among the attributes. This classifier, desipte its strong
independence assumptions, often performs well in practice. It is believed that
relaxing the independence assumptions of a naive Bayes classifier may improve
the classification accuracy of the resulting structure. While finding an
optimal unconstrained Bayesian Network (for most any reasonable scoring
measure) is an NP-hard problem, it is possible to learn in polynomial time
optimal networks obeying various structural restrictions. Several contributors have
examined the possibilities of adding augmenting arcs between attributes of a
Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN
structure in which the augmenting arcs form a tree on the attributes, and
present a polynomial time algorithm that learns an optimal TAN with respect to
MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the
augmenting arcs form a forest on the attributes (a collection of trees, hence a
relaxation of the stuctural restriction of TAN), and present heuristic search
methods for learning good, though not optimal, augmenting arc sets. The
contributors, however, evaluate the learned structure only in terms of observed
misclassification error and not against a scoring metric, such as MDL. In this
paper, we present a simple, polynomial time greedy algorithm for learning an
optimal Augmented Bayes Network with respect to MDL score.

<id>
cs/0510038v4
<category>
cs.LG
<abstract>
We consider the problem of learning unions of rectangles over the domain
$[b]^n$, in the uniform distribution membership query learning setting, where
both b and n are "large". We obtain poly$(n, \log b)$-time algorithms for the
following classes:
  - poly$(n \log b)$-way Majority of $O(\frac{\log(n \log b)} {\log \log(n \log
b)})$-dimensional rectangles.
  - Union of poly$(\log(n \log b))$ many $O(\frac{\log^2 (n \log b)} {(\log
\log(n \log b) \log \log \log (n \log b))^2})$-dimensional rectangles.
  - poly$(n \log b)$-way Majority of poly$(n \log b)$-Or of disjoint
$O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.
  Our main algorithmic tool is an extension of Jackson's boosting- and
Fourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$,
building on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to
obtain the results stated above are techniques from exact learning [Beimel,
Kushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$
circuits [Jackson, Klivans, Servedio 2002] and on representing Boolean
functions as thresholds of parities [Klivans, Servedio 2001].

<id>
cs/0511058v2
<category>
cs.LG
<abstract>
We consider the problem of on-line prediction of real-valued labels, assumed
bounded in absolute value by a known constant, of new objects from known
labeled objects. The prediction algorithm's performance is measured by the
squared deviation of the predictions from the actual labels. No stochastic
assumptions are made about the way the labels and objects are generated.
Instead, we are given a benchmark class of prediction rules some of which are
hoped to produce good predictions. We show that for a wide range of
infinite-dimensional benchmark classes one can construct a prediction algorithm
whose cumulative loss over the first N examples does not exceed the cumulative
loss of any prediction rule in the class plus O(sqrt(N)); the main differences
from the known results are that we do not impose any upper bound on the norm of
the considered prediction rules and that we achieve an optimal leading term in
the excess loss of our algorithm. If the benchmark class is "universal" (dense
in the class of continuous functions on each compact set), this provides an
on-line non-stochastic analogue of universally consistent prediction in
non-parametric statistics. We use two proof techniques: one is based on the
Aggregating Algorithm and the other on the recently developed method of
defensive forecasting.

<id>
cs/0511088v1
<category>
cs.LG
<abstract>
The problem of finding an optimum using noisy evaluations of a smooth cost
function arises in many contexts, including economics, business, medicine,
experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t -
x*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1,
>...) generated by an unbiased feedback process observing noisy evaluations of
an unknown quadratic function maximised at x*. The bound is tight, as the proof
leads to a simple algorithm which meets it. We further establish a bound on the
total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may
impose practical limitations on an agent's performance, as O(eps^-4) queries
are made before the queries converge to x* with eps accuracy.

<id>
cs/9301101v1
<category>
cs.LO
<abstract>
Manna and Waldinger's theory of substitutions and unification has been
verified using the Cambridge LCF theorem prover. A proof of the monotonicity of
substitution is presented in detail, as an example of interaction with LCF.
Translating the theory into LCF's domain-theoretic logic is largely
straightforward. Well-founded induction on a complex ordering is translated
into nested structural inductions. Correctness of unification is expressed
using predicates for such properties as idempotence and most-generality. The
verification is presented as a series of lemmas. The LCF proofs are compared
with the original ones, and with other approaches. It appears difficult to find
a logic that is both simple and flexible, especially for proving termination.

<id>
cs/9301102v1
<category>
cs.LO
<abstract>
Martin-L\"of's Intuitionistic Theory of Types is becoming popular for formal
reasoning about computer programs. To handle recursion schemes other than
primitive recursion, a theory of well-founded relations is presented. Using
primitive recursion over higher types, induction and recursion are formally
derived for a large class of well-founded relations. Included are < on natural
numbers, and relations formed by inverse images, addition, multiplication, and
exponentiation of other relations. The constructions are given in full detail
to allow their use in theorem provers for Type Theory, such as Nuprl. The
theory is compared with work in the field of ordinal recursion over higher
types.

<id>
cs/9301103v1
<category>
cs.LO
<abstract>
Boyer and Moore have discussed a recursive function that puts conditional
expressions into normal form [1]. It is difficult to prove that this function
terminates on all inputs. Three termination proofs are compared: (1) using a
measure function, (2) in domain theory using LCF, (3) showing that its
recursion relation, defined by the pattern of recursive calls, is well-founded.
The last two proofs are essentially the same though conducted in markedly
different logical frameworks. An obviously total variant of the normalize
function is presented as the `computational meaning' of those two proofs. A
related function makes nested recursive calls. The three termination proofs
become more complex: termination and correctness must be proved simultaneously.
The recursion relation approach seems flexible enough to handle subtle
termination proofs where previously domain theory seemed essential.

<id>
cs/9301104v1
<category>
cs.LO
<abstract>
An interactive theorem prover, Isabelle, is under development. In LCF, each
inference rule is represented by one function for forwards proof and another (a
tactic) for backwards proof. In Isabelle, each inference rule is represented by
a Horn clause. Resolution gives both forwards and backwards proof, supporting a
large class of logics. Isabelle has been used to prove theorems in
Martin-L\"of's Constructive Type Theory. Quantifiers pose several difficulties:
substitution, bound variables, Skolemization. Isabelle's representation of
logical syntax is the typed lambda-calculus, requiring higher- order
unification. It may have potential for logic programming. Depth-first
subgoaling along inference rules constitutes a higher-order Prolog.

<id>
cs/9301105v1
<category>
cs.LO
<abstract>
Isabelle is an interactive theorem prover that supports a variety of logics.
It represents rules as propositions (not as functions) and builds proofs by
combining rules. These operations constitute a meta-logic (or `logical
framework') in which the object-logics are formalized. Isabelle is now based on
higher-order logic -- a precise and well-understood foundation. Examples
illustrate use of this meta-logic to formalize logics and proofs. Axioms for
first-order logic are shown sound and complete. Backwards proof is formalized
by meta-reasoning about object-level entailment. Higher-order logic has several
practical advantages over other meta-logics. Many proof techniques are known,
such as Huet's higher-order unification procedure.

<id>
cs/9301106v1
<category>
cs.LO
<abstract>
Isabelle is a generic theorem prover, designed for interactive reasoning in a
variety of formal theories. At present it provides useful proof procedures for
Constructive Type Theory, various first-order logics, Zermelo-Fraenkel set
theory, and higher-order logic. This survey of Isabelle serves as an
introduction to the literature. It explains why generic theorem proving is
beneficial. It gives a thorough history of Isabelle, beginning with its origins
in the LCF system. It presents an account of how logics are represented,
illustrated using classical logic. The approach is compared with the Edinburgh
Logical Framework. Several of the Isabelle object-logics are presented.

<id>
cs/9301107v1
<category>
cs.LO
<abstract>
Simple type theory is formulated for use with the generic theorem prover
Isabelle. This requires explicit type inference rules. There are function,
product, and subset types, which may be empty. Descriptions (the eta-operator)
introduce the Axiom of Choice. Higher-order logic is obtained through
reflection between formulae and terms of type bool. Recursive types and
functions can be formally constructed. Isabelle proof procedures are described.
The logic appears suitable for general mathematics as well as computational
problems.

<id>
cs/9301108v1
<category>
cs.LO
<abstract>
Many automatic theorem-provers rely on rewriting. Using theorems as rewrite
rules helps to simplify the subgoals that arise during a proof.
  LCF is an interactive theorem-prover intended for reasoning about
computation. Its implementation of rewriting is presented in detail. LCF
provides a family of rewriting functions, and operators to combine them. A
succession of functions is described, from pattern matching primitives to the
rewriting tool that performs most inferences in LCF proofs.
  The design is highly modular. Each function performs a basic, specific task,
such as recognizing a certain form of tautology. Each operator implements one
method of building a rewriting function from simpler ones. These pieces can be
put together in numerous ways, yielding a variety of rewrit- ing strategies.
  The approach involves programming with higher-order functions. Rewriting
functions are data values, produced by computation on other rewriting
functions. The code is in daily use at Cambridge, demonstrating the practical
use of functional programming.

<id>
cs/9301109v1
<category>
cs.LO
<abstract>
An attempt at unifying logic and functional programming is reported. As a
starting point, we take the view that "logic programs" are not about logic but
constitute inductive definitions of sets and relations. A skeletal language
design based on these considerations is sketched and a prototype implementation
discussed.

<id>
cs/9301110v1
<category>
cs.LO
<abstract>
A step-by-step presentation of the code for a small theorem prover introduces
theorem-proving techniques. The programming language used is Standard ML. The
prover operates on a sequent calculus formulation of first-order logic, which
is briefly explained. The implementation of unification and logical inference
is shown. The prover is demonstrated on several small examples, including one
that shows its limitations. The final part of the paper is a survey of
contemporary research on interactive theorem proving.

<id>
cs/9311103v1
<category>
cs.LO
<abstract>
A logic for specification and verification is derived from the axioms of
Zermelo-Fraenkel set theory. The proofs are performed using the proof assistant
Isabelle. Isabelle is generic, supporting several different logics. Isabelle
has the flexibility to adapt to variants of set theory. Its higher-order syntax
supports the definition of new binding operators. Unknowns in subgoals can be
instantiated incrementally. The paper describes the derivation of rules for
descriptions, relations and functions, and discusses interactive proofs of
Cantor's Theorem, the Composition of Homomorphisms challenge [9], and Ramsey's
Theorem [5]. A generic proof assistant can stand up against provers dedicated
to particular logics.

<id>
cs/9511102v1
<category>
cs.LO
<abstract>
A theory of recursive definitions has been mechanized in Isabelle's
Zermelo-Fraenkel (ZF) set theory. The objective is to support the formalization
of particular recursive definitions for use in verification, semantics proofs
and other computational reasoning. Inductively defined sets are expressed as
least fixedpoints, applying the Knaster-Tarski Theorem over a suitable set.
Recursive functions are defined by well-founded recursion and its derivatives,
such as transfinite recursion. Recursive data structures are expressed by
applying the Knaster-Tarski Theorem to a set, such as V[omega], that is closed
under Cartesian product and disjoint sum. Worked examples include the
transitive closure of a relation, lists, variable-branching trees and mutually
recursive trees and forests. The Schr\"oder-Bernstein Theorem and the soundness
of propositional logic are proved in Isabelle sessions.

<id>
cs/9511103v1
<category>
cs.LO
<abstract>
A special final coalgebra theorem, in the style of Aczel's, is proved within
standard Zermelo-Fraenkel set theory. Aczel's Anti-Foundation Axiom is replaced
by a variant definition of function that admits non-well-founded constructions.
Variant ordered pairs and tuples, of possibly infinite length, are special
cases of variant functions. Analogues of Aczel's Solution and Substitution
Lemmas are proved in the style of Rutten and Turi. The approach is less general
than Aczel's, but the treatment of non-well-founded objects is simple and
concrete. The final coalgebra of a functor is its greatest fixedpoint. The
theory is intended for machine implementation and a simple case of it is
already implemented using the theorem prover Isabelle.

<id>
cs/9612104v1
<category>
cs.LO
<abstract>
Fairly deep results of Zermelo-Frenkel (ZF) set theory have been mechanized
using the proof assistant Isabelle. The results concern cardinal arithmetic and
the Axiom of Choice (AC). A key result about cardinal multiplication is K*K =
K, where K is any infinite cardinal. Proving this result required developing
theories of orders, order-isomorphisms, order types, ordinal arithmetic,
cardinals, etc.; this covers most of Kunen, Set Theory, Chapter I. Furthermore,
we have proved the equivalence of 7 formulations of the Well-ordering Theorem
and 20 formulations of AC; this covers the first two chapters of Rubin and
Rubin, Equivalents of the Axiom of Choice, and involves highly technical
material. The definitions used in the proofs are largely faithful in style to
the original mathematics.

<id>
cs/9711105v1
<category>
cs.LO
<abstract>
A theory of recursive and corecursive definitions has been developed in
higher-order logic (HOL) and mechanized using Isabelle. Least fixedpoints
express inductive data types such as strict lists; greatest fixedpoints express
coinductive data types, such as lazy lists. Well-founded recursion expresses
recursive functions over inductive data types; corecursion expresses functions
that yield elements of coinductive data types. The theory rests on a
traditional formalization of infinite trees. The theory is intended for use in
specification and verification. It supports reasoning about a wide range of
computable functions, but it does not formalize their operational semantics and
can express noncomputable functions also. The theory is illustrated using
finite and infinite lists. Corecursion expresses functions over infinite lists;
coinduction reasons about such functions.

<id>
cs/9711106v1
<category>
cs.LO
<abstract>
This book chapter establishes connections between the interactive proof tool
Isabelle and classical tableau and resolution technology. Isabelle's classical
reasoner is described and demonstrated by an extended case study: the
Church-Rosser theorem for combinators. Compared with other interactive theorem
provers, Isabelle's classical reasoner achieves a high degree of automation.

<id>
cs/9809014v1
<category>
cs.LO
<abstract>
Uniform proofs are sequent calculus proofs with the following characteristic:
the last step in the derivation of a complex formula at any stage in the proof
is always the introduction of the top-level logical symbol of that formula. We
investigate the relevance of this uniform proof notion to structuring proof
search in classical logic. A logical language in whose context provability is
equivalent to uniform provability admits of a goal-directed proof procedure
that interprets logical symbols as search directives whose meanings are given
by the corresponding inference rules. While this uniform provability property
does not hold directly of classical logic, we show that it holds of a fragment
of it that only excludes essentially positive occurrences of universal
quantifiers under a modest, sound, modification to the set of assumptions: the
addition to them of the negation of the formula being proved. We further note
that all uses of the added formula can be factored into certain derived rules.
The resulting proof system and the uniform provability property that holds of
it are used to outline a proof procedure for classical logic. An interesting
aspect of this proof procedure is that it incorporates within it previously
proposed mechanisms for dealing with disjunctive information in assumptions and
for handling hypotheticals. Our analysis sheds light on the relationship
between these mechanisms and the notion of uniform proofs.

<id>
cs/9809015v1
<category>
cs.LO
<abstract>
Based on an analysis of the inference rules used, we provide a
characterization of the situations in which classical provability entails
intuitionistic provability. We then examine the relationship of these
derivability notions to uniform provability, a restriction of intuitionistic
provability that embodies a special form of goal-directedness. We determine,
first, the circumstances in which the former relations imply the latter. Using
this result, we identify the richest versions of the so-called abstract logic
programming languages in classical and intuitionistic logic. We then study the
reduction of classical and, derivatively, intuitionistic provability to uniform
provability via the addition to the assumption set of the negation of the
formula to be proved. Our focus here is on understanding the situations in
which this reduction is achieved. However, our discussions indicate the
structure of a proof procedure based on the reduction, a matter also considered
explicitly elsewhere.

<id>
cs/9809120v1
<category>
cs.LO
<abstract>
In this paper, we present a formalization of Kozen's propositional modal
$\mu$-calculus, in the Calculus of Inductive Constructions. We address several
problematic issues, such as the use of higher-order abstract syntax in
inductive sets in presence of recursive constructors, the encoding of modal
(``proof'') rules and of context sensitive grammars. The encoding can be used
in the \Coq system, providing an experimental computer-aided proof environment
for the interactive development of error-free proofs in the $\mu$-calculus. The
techniques we adopted can be readily ported to other languages and proof
systems featuring similar problematic issues.

<id>
cs/9810008v1
<category>
cs.LO
<abstract>
Flat iteration is a variation on the original binary version of the Kleene
star operation P*Q, obtained by restricting the first argument to be a sum of
atomic actions. It generalizes prefix iteration, in which the first argument is
a single action. Complete finite equational axiomatizations are given for five
notions of bisimulation congruence over basic CCS with flat iteration, viz.
strong congruence, branching congruence, eta-congruence, delay congruence and
weak congruence. Such axiomatizations were already known for prefix iteration
and are known not to exist for general iteration. The use of flat iteration has
two main advantages over prefix iteration: 1.The current axiomatizations
generalize to full CCS, whereas the prefix iteration approach does not allow an
elimination theorem for an asynchronous parallel composition operator. 2.The
greater expressiveness of flat iteration allows for much shorter completeness
proofs.
  In the setting of prefix iteration, the most convenient way to obtain the
completeness theorems for eta-, delay, and weak congruence was by reduction to
the completeness theorem for branching congruence. In the case of weak
congruence this turned out to be much simpler than the only direct proof found.
In the setting of flat iteration on the other hand, the completeness theorems
for delay and weak (but not eta-) congruence can equally well be obtained by
reduction to the one for strong congruence, without using branching congruence
as an intermediate step. Moreover, the completeness results for prefix
iteration can be retrieved from those for flat iteration, thus obtaining a
second indirect approach for proving completeness for delay and weak congruence
in the setting of prefix iteration.

<id>
cs/9903006v1
<category>
cs.LO
<abstract>
For arbitrary undirected graph $G$, we are designing SATISFIABILITY problem
(SAT) for HCP, using tools of Boolean algebra only. The obtained SAT be the
logic formulation of conditions for Hamiltonian cycle existence, and use $m$
Boolean variables, where $m$ is the number of graph edges. This Boolean
expression is true if and only if an initial graph is Hamiltonian. That is,
each satisfying assignment of the Boolean variables determines a Hamiltonian
cycle of $G$, and each Hamiltonian cycle of $G$ corresponds to a satisfying
assignment of the Boolean variables. In common case, the obtained Boolean
expression may has an exponential length (the number of Boolean literals).

<id>
cs/9907022v1
<category>
cs.LO
<abstract>
We define a hierarchy of circuit complexity classes LD^i, whose depth are the
inverse of a function in Ackermann hierarchy. Then we introduce extremely weak
versions of length induction and construct a bounded arithmetic theory L^i_2
whose provably total functions exactly correspond to functions computable by
LD^i circuits. Finally, we prove a non-conservation result between L^i_2 and a
weaker theory AC^0CA which corresponds to the class AC^0. Our proof utilizes
KPT witnessing theorem.

<id>
cs/9910023v4
<category>
cs.LO
<abstract>
This paper introduces a logical system, called BV, which extends
multiplicative linear logic by a non-commutative self-dual logical operator.
This extension is particularly challenging for the sequent calculus, and so far
it is not achieved therein. It becomes very natural in a new formalism, called
the calculus of structures, which is the main contribution of this work.
Structures are formulae submitted to certain equational laws typical of
sequents. The calculus of structures is obtained by generalising the sequent
calculus in such a way that a new top-down symmetry of derivations is observed,
and it employs inference rules that rewrite inside structures at any depth.
These properties, in addition to allow the design of BV, yield a modular proof
of cut elimination.

<id>
cs/0003026v1
<category>
cs.LO
<abstract>
Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there are definite programs and
constraint logic programs that compute a solution as an answer substitution to
a query containing the variables of the constraint satisfaction problem. On the
other hand there are approaches based on stable model semantics, abduction, and
first-order logic model generation that compute solutions as models of some
theory. This paper compares these different approaches from point of view of
knowledge representation (how declarative are the programs) and from point of
view of performance (how good are they at solving typical problems).

<id>
cs/0003045v1
<category>
cs.LO
<abstract>
Tabled logic programming is receiving increasing attention in the Logic
Programming community. It avoids many of the shortcomings of SLD execution and
provides a more flexible and often extremely efficient execution mechanism for
logic programs. In particular, tabled execution of logic programs terminates
more often than execution based on SLD-resolution. In this article, we
introduce two notions of universal termination of logic programming with
Tabling: quasi-termination and (the stronger notion of) LG-termination. We
present sufficient conditions for these two notions of termination, namely
quasi-acceptability and LG-acceptability, and we show that these conditions are
also necessary in case the tabling is well-chosen. Starting from these
conditions, we give modular termination proofs, i.e., proofs capable of
combining termination proofs of separate programs to obtain termination proofs
of combined programs. Finally, in the presence of mode information, we state
sufficient conditions which form the basis for automatically proving
termination in a constraint-based way.

<id>
cs/0003069v1
<category>
cs.LO
<abstract>
Proving failure of queries for definite logic programs can be done by
constructing a finite model of the program in which the query is false. A
general purpose model generator for first order logic can be used for this. A
recent paper presented at PLILP98 shows how the peculiarities of definite
programs can be exploited to obtain a better solution. There a procedure is
described which combines abduction with tabulation and uses a meta-interpreter
for heuristic control of the search. The current paper shows how similar
results can be obtained by direct execution under the standard tabulation of
the XSB-Prolog system. The loss of control is compensated for by better
intelligent backtracking and more accurate failure analysis.

<id>
cs/0003071v1
<category>
cs.LO
<abstract>
We introduce a set of eight universal Rules of Inference by which computer
programs with known properties (axioms) are transformed into new programs with
known properties (theorems). Axioms are presented to formalize a segment of
Number Theory, DataBase retrieval and Computability Theory. The resulting
Program Calculus is used to generate programs to (1) Determine if one number is
a factor of another. (2) List all employees who earn more than their manager.
(3) List the set of programs that halt no on themselves, thus proving that it
is recursively enumerable. The well-known fact that the set of programs that do
not halt yes on themselves is not recursively enumerable is formalized as a
program requirement that has no solution, an Incompleteness Axiom. Thus, any
axioms (programs) which could be used to generate this program are themselves
unattainable. Such proofs are presented to formally generate several additional
theorems, including (4) The halting problem is unsolvable.
  Open problems and future research is discussed, including the use of
temporary sort files, programs that calculate statistics (such as counts and
sums), the synthesis of programs to solve other well-known problems from Number
Theory, Logic, DataBase retrieval and Computability Theory, application to
Programming Language Semantics, and the formalization of incompleteness results
from Logic and the semantic paradoxes.

<id>
cs/0006010v1
<category>
cs.LO
<abstract>
This paper is a structured introduction to Light Affine Logic, and to its
intuitionistic fragment. Light Affine Logic has a polynomially costing cut
elimination (P-Time correctness), and encodes all P-Time Turing machines
(P-Time completeness). P-Time correctness is proved by introducing the Proof
nets for Intuitionistic Light Affine Logic. P-Time completeness is demonstrated
in full details thanks to a very compact program notation. On one side, the
proof of P-Time correctness describes how the complexity of cut elimination is
controlled, thanks to a suitable cut elimination strategy that exploits
structural properties of the Proof nets. This allows to have a good catch on
the meaning of the ``paragraph'' modality, which is a peculiarity of light
logics. On the other side, the proof of P-Time completeness, together with a
lot of programming examples, gives a flavor of the non trivial task of
programming with resource limitations, using Intuitionistic Light Affine Logic
derivations as programs.

<id>
cs/0007030v2
<category>
cs.LO
<abstract>
In existing simulation proof techniques, a single step in a lower-level
specification may be simulated by an extended execution fragment in a
higher-level one. As a result, it is cumbersome to mechanize these techniques
using general purpose theorem provers. Moreover, it is undecidable whether a
given relation is a simulation, even if tautology checking is decidable for the
underlying specification logic. This paper introduces various types of normed
simulations. In a normed simulation, each step in a lower-level specification
can be simulated by at most one step in the higher-level one, for any related
pair of states. In earlier work we demonstrated that normed simulations are
quite useful as a vehicle for the formalization of refinement proofs via
theorem provers. Here we show that normed simulations also have pleasant
theoretical properties: (1) under some reasonable assumptions, it is decidable
whether a given relation is a normed forward simulation, provided tautology
checking is decidable for the underlying logic; (2) at the semantic level,
normed forward and backward simulations together form a complete proof method
for establishing behavior inclusion, provided that the higher-level
specification has finite invisible nondeterminism.

<id>
cs/0007037v1
<category>
cs.LO
<abstract>
We study the topological models of a logic of knowledge for topological
reasoning, introduced by Larry Moss and Rohit Parikh. Among our results is a
solution of a conjecture by the formentioned contributors, finite satisfiability
property and decidability for the theory of topological models.

<id>
cs/9809009v1
<category>
cs.MS
<abstract>
The rapid and widespread adoption of Java has created a demand for reliable
and reusable mathematical software components to support the growing number of
compute-intensive applications now under development, particularly in science
and engineering. In this paper we address practical issues of the Java language
and environment which have an effect on numerical library design and
development. Benchmarks which illustrate the current levels of performance of
key numerical kernels on a variety of Java platforms are presented. Finally, a
strategy for the development of a fundamental numerical toolkit for Java is
proposed and its current status is described.

<id>
cs/9809105v1
<category>
cs.MS
<abstract>
A novel parallel algorithm for matrix multiplication is presented. The
hyper-systolic algorithm makes use of a one-dimensional processor abstraction.
The procedure can be implemented on all types of parallel systems. It can
handle matrix-vector multiplications as well as transposed matrix products.

<id>
cs/0004004v1
<category>
cs.MS
<abstract>
This paper provides some reflections on the field of mathematical software on
the occasion of John Rice's 65th birthday. I describe some of the common themes
of research in this field and recall some significant events in its evolution.
Finally, I raise a number of issues that are of concern to future developments.

<id>
cs/0101001v1
<category>
cs.MS
<abstract>
We discuss the role of automatic differentiation tools in optimization
software. We emphasize issues that are important to large-scale optimization
and that have proved useful in the installation of nonlinear solvers in the
NEOS Server. Our discussion centers on the computation of the gradient and
Hessian matrix for partially separable functions and shows that the gradient
and Hessian matrix can be computed with guaranteed bounds in time and memory
requirements

<id>
cs/0101018v1
<category>
cs.MS
<abstract>
GPCG is an algorithm within the Toolkit for Advanced Optimization (TAO) for
solving bound constrained, convex quadratic problems. Originally developed by
More' and Toraldo, this algorithm was designed for large-scale problems but had
been implemented only for a single processor. The TAO implementation is
available for a wide range of high-performance architecture, and has been
tested on up to 64 processors to solve problems with over 2.5 million
variables.

<id>
cs/0102001v2
<category>
cs.MS
<abstract>
We propose performance profiles-distribution functions for a performance
metric-as a tool for benchmarking and comparing optimization software. We show
that performance profiles combine the best features of other tools for
performance evaluation.

<id>
cs/0106051v1
<category>
cs.MS
<abstract>
SnadiOpt is a package that supports the use of the automatic differentiation
package ADIFOR with the optimization package Snopt. Snopt is a general-purpose
system for solving optimization problems with many variables and constraints.
It minimizes a linear or nonlinear function subject to bounds on the variables
and sparse linear or nonlinear constraints. It is suitable for large-scale
linear and quadratic programming and for linearly constrained optimization, as
well as for general nonlinear programs. The method used by Snopt requires the
first derivatives of the objective and constraint functions to be available.
The SnadiOpt package allows users to avoid the time-consuming and error-prone
process of evaluating and coding these derivatives. Given Fortran code for
evaluating only the values of the objective and constraints, SnadiOpt
automatically generates the code for evaluating the derivatives and builds the
relevant Snopt input files and sparse data structures.

<id>
cs/0107025v2
<category>
cs.MS
<abstract>
Most existing implementations of multiple precision arithmetic demand that
the user sets the precision {\em a priori}. Some libraries are said adaptable
in the sense that they dynamically change the precision of each intermediate
operation individually to deliver the target accuracy according to the actual
inputs. We present in this text a new adaptable numeric core inspired both from
floating point expansions and from on-line arithmetic.
  The numeric core is cut down to four tools. The tool that contains arithmetic
operations is proved to be correct. The proofs have been formally checked by
the Coq assistant. Developing the proofs, we have formally proved many results
published in the literature and we have extended a few of them. This work may
let users (i) develop application specific adaptable libraries based on the
toolset and / or (ii) write new formal proofs based on the set of validated
facts.

<id>
cs/0306127v1
<category>
cs.MS
<abstract>
We had assembled a Java package, known as MatrixPak, of four classes for the
purpose of numerical matrix computation. The classes are matrix,
matrix_operations, StrToMatrix, and MatrixToStr; all of which are inherited
from java.lang.Object class. Class matrix defines a matrix as a two-dimensional
array of float types, and contains the following mathematical methods:
transpose, adjoint, determinant, inverse, minor and cofactor. Class
matrix_operations contains the following mathematical methods: matrix addition,
matrix subtraction, matrix multiplication, and matrix exponential. Class
StrToMatrix contains methods necessary to parse a string representation (for
example, [[2 3 4]-[5 6 7]]) of a matrix into a matrix definition, whereas class
MatrixToStr does the reverse.

<id>
cs/0307009v1
<category>
cs.MS
<abstract>
When implementing regular enough functions (e.g., elementary or special
functions) on a computing system, we frequently use polynomial approximations.
In most cases, the polynomial that best approximates (for a given distance and
in a given interval) a function has coefficients that are not exactly
representable with a finite number of bits. And yet, the polynomial
approximations that are actually implemented do have coefficients that are
represented with a finite - and sometimes small - number of bits: this is due
to the finiteness of the floating-point representations (for software
implementations), and to the need to have small, hence fast and/or inexpensive,
multipliers (for hardware implementations). We then have to consider polynomial
approximations for which the degree-$i$ coefficient has at most $m_i$
fractional bits (in other words, it is a rational number with denominator
$2^{m_i}$). We provide a general method for finding the best polynomial
approximation under this constraint. Then, we suggest refinements than can be
used to accelerate our method.

<id>
cs/0310057v1
<category>
cs.MS
<abstract>
We give a gentle introduction to using various software tools for automatic
differentiation (AD). Ready-to-use examples are discussed, and links to further
information are presented. Our target audience includes all those who are
looking for a straightforward way to get started using the available AD
technology. The document is dynamic in the sense that its content will be
updated as the AD software evolves.

<id>
cs/0406049v1
<category>
cs.MS
<abstract>
This paper presents an algorithm for computing Sine-Cosine pairs to modest
accuracy, but in a manner which contains no conditional tests or branching,
making it highly amenable to vectorization. An exemplary implementation for
PowerPC AltiVec processors is included, but the algorithm should be easily
portable to other achitectures, such as Intel SSE.

<id>
cs/0407052v1
<category>
cs.MS
<abstract>
We present two add-ons for Mathematica for teaching mathematics to
undergraduate and high school students. These two applications, M@th Desktop
(MD) and M@th Desktop Tools (MDTools), include several palettes and notebooks
covering almost every field. The underlying didactic concept is so-called
"blended learning", in which these tools are meant to be used as a complement
to the professor or teacher rather than as a replacement, which other
e-learning applications do. They enable students to avoid the usual problem of
computer-based learning, namely that too large an amount of time is wasted
struggling with computer and program errors instead of actually learning the
mathematical concepts.
  M@th Desktop Tools is palette-based and provides easily accessible and
user-friendly templates for the most important functions in the fields of
Analysis, Algebra, Linear Algebra and Statistics. M@th Desktop, in contrast, is
a modern, interactive teaching and learning software package for mathematics
classes. It is comprised of modules for Differentiation, Integration, and
Statistics, and each module presents its topic with a combination of
interactive notebooks and palettes.
  Both packages can be obtained from Deltasoft's homepage at
http://www.deltasoft.at/ .

<id>
cs/0408029v1
<category>
cs.MS
<abstract>
The solution of large, sparse constrained least-squares problems is a staple
in scientific and engineering applications. However, currently available codes
for such problems are proprietary or based on MATLAB. We announce a freely
available C implementation of the fast block pivoting algorithm of Portugal,
Judice, and Vicente. Our version is several times faster than Matstoms' MATLAB
implementation of the same algorithm. Further, our code matches the accuracy of
MATLAB's built-in lsqnonneg function.

<id>
cs/0503014v1
<category>
cs.MS
<abstract>
ADF95 is a tool to automatically calculate numerical first derivatives for
any mathematical expression as a function of user defined independent
variables. Accuracy of derivatives is achieved within machine precision. ADF95
may be applied to any FORTRAN 77/90/95 conforming code and requires minimal
changes by the user. It provides a new derived data type that holds the value
and derivatives and applies forward differencing by overloading all FORTRAN
operators and intrinsic functions. An efficient indexing technique leads to a
reduced memory usage and a substantially increased performance gain over other
available tools with operator overloading. This gain is especially pronounced
for sparse systems with large number of independent variables. A wide class of
numerical simulations, e.g., those employing implicit solvers, can profit from
ADF95.

<id>
cs/0602005v1
<category>
cs.MS
<abstract>
We present in this paper a library to compute with Taylor models, a technique
extending interval arithmetic to reduce decorrelation and to solve differential
equations. Numerical software usually produces only numerical results. Our
library can be used to produce both results and proofs. As seen during the
development of Fermat's last theorem reported by Aczel 1996, providing a proof
is not sufficient. Our library provides a proof that has been thoroughly
scrutinized by a trustworthy and tireless assistant. PVS is an automatic proof
assistant that has been fairly developed and used and that has no internal
connection with interval arithmetic or Taylor models. We built our library so
that PVS validates each result as it is produced. As producing and validating a
proof, is and will certainly remain a bigger task than just producing a
numerical result our library will never be a replacement to imperative
implementations of Taylor models such as Cosy Infinity. Our library should
mainly be used to validate small to medium size results that are involved in
safety or life critical applications.

<id>
cs/0603001v1
<category>
cs.MS
<abstract>
BioSig is an open source software library for biomedical signal processing.
Most users in the field are using Matlab; however, significant effort was
undertaken to provide compatibility to Octave, too. This effort has been widely
successful, only some non-critical components relying on a graphical user
interface are missing. Now, installing BioSig on Octave is as easy as on
Matlab. Moreover, a benchmark test based on BioSig has been developed and the
benchmark results of several platforms are presented.

<id>
cs/0603052v2
<category>
cs.MS
<abstract>
The subject of our talk is the correct evaluation of interval extension of
the function specified by the expression x^y without any constraints on the
values of x and y. The core of our approach is a decomposition of the graph of
x^y into a small number of parts which can be transformed into subsets of the
graph of x^y for non-negative bases x. Because of this fact, evaluation of
interval extension of x^y, without any constraints on x and y, is not much
harder than evaluation of interval extension of x^y for non-negative bases x.

<id>
cs/0604006v1
<category>
cs.MS
<abstract>
There are many classes of mathematical problems which give rise to matrices,
where a large number of the elements are zero. In this case it makes sense to
have a special matrix type to handle this class of problems where only the
non-zero elements of the matrix are stored. Not only does this reduce the
amount of memory to store the matrix, but it also means that operations on this
type of matrix can take advantage of the a-priori knowledge of the positions of
the non-zero elements to accelerate their calculations. A matrix type that
stores only the non-zero elements is generally called sparse.
  Until recently Octave has lacked a full implementation of sparse matrices.
This article address the implementation of sparse matrices within Octave,
including their storage, creation, fundamental algorithms used, their
implementations and the basic operations and functions implemented for sparse
matrices. Mathematical issues such as the return types of sparse operations,
matrix fill-in and reordering for sparse matrix factorization is discussed in
the context of a real example.
  Benchmarking of Octave's implementation of sparse operations compared to
their equivalent in Matlab are given and their implications discussed. Results
are presented for multiplication and linear algebra operations for various
matrix orders and densities. Furthermore, the use of Octave's sparse matrix
implementation is demonstrated using a real example of a finite element model
(FEM) problem. Finally, the method of using sparse matrices with Octave's
oct-files is discussed. The means of creating, using and returning sparse
matrices within oct-files is discussed as well as the differences between
Octave's Sparse and Array classes.

<id>
cs/0604039v1
<category>
cs.MS
<abstract>
This paper announces the availability of a fixed point toolbox for the Matlab
compatible software package Octave. This toolbox is released under the GNU
Public License, and can be used to model the losses in algorithms implemented
in hardware. Furthermore, this paper presents as an example of the use of this
toolbox, the effects of a fixed point implementation on the precision of an
OFDM modulator.

<id>
cs/0604088v3
<category>
cs.MS
<abstract>
Mathematica is a versatile equipment for doing numeric and symbolic
computations and it has wide spread applications in all branches of science.
Mathematica has a complete consistency to design it at every stage that gives
it multilevel capability and helps advanced usage evolve naturally. Mathematica
functions work for any precision of number and it can be easily computed with
symbols, represented graphically to get the best answer. Mathematica is a
robust software development that can be used in any popular operating systems
and it can be communicated with external programs by using proper mathlink
commands.
  Sometimes it is quite desirable to run jobs in background of a computer which
can take considerable amount of time to finish, and this allows us to do work
on other tasks, while keeping the jobs running. Most of us are very familiar to
run jobs in background for the programs written in the languages like C, C++,
F77, F90, F95, etc. But the way of running jobs, written in a mathematica
notebook, in background is quite different from the conventional method. In
this article, we explore how to create a mathematica batch-file from a
mathematica notebook and run it in background. Here we concentrate our study
only for the Unix version, but one can run mathematica programs in background
for the Windows version as well by using proper mathematica batch-file.

<id>
cs/0605081v1
<category>
cs.MS
<abstract>
Les unit\'{e}s graphiques (Graphic Processing Units- GPU) sont d\'{e}sormais
des processeurs puissants et flexibles. Les derni\`{e}res g\'{e}n\'{e}rations
de GPU contiennent des unit\'{e}s programmables de traitement des sommets
(vertex shader) et des pixels (pixel shader) supportant des op\'{e}rations en
virgule flottante sur 8, 16 ou 32 bits. La repr\'{e}sentation flottante sur 32
bits correspond \`{a} la simple pr\'{e}cision de la norme IEEE sur
l'arithm\'{e}tique en virgule flottante (IEEE-754). Les GPU sont bien
adapt\'{e}s aux applications avec un fort parall\'{e}lisme de donn\'{e}es.
Cependant ils ne sont que peu utilis\'{e}s en dehors des calculs graphiques
(General Purpose computation on GPU -- GPGPU). Une des raisons de cet \'{e}tat
de faits est la pauvret\'{e} des documentations techniques fournies par les
fabricants (ATI et Nvidia), particuli\`{e}rement en ce qui concerne
l'implantation des diff\'{e}rents op\'{e}rateurs arithm\'{e}tiques
embarqu\'{e}s dans les diff\'{e}rentes unit\'{e}s de traitement. Or ces
informations sont essentielles pour estimer et contr\^{o}ler les erreurs
d'arrondi ou pour mettre en oeuvre des techniques de r\'{e}duction ou de
compensation afin de travailler en pr\'{e}cision double, quadruple ou
arbitrairement \'{e}tendue. Nous proposons dans cet article un ensemble de
programmes qui permettent de d\'{e}couvrir les caract\'{e}ristiques principales
des GPU en ce qui concerne l'arithm\'{e}tique \`{a} virgule flottante. Nous
donnons les r\'{e}sultats obtenus sur deux cartes graphiques r\'{e}centes: la
Nvidia 7800GTX et l'ATI RX1800XL.

<id>
cs/0606101v4
<category>
cs.MS
<abstract>
This paper provides a bound on the number of numeric operations (fixed or
floating point) that can safely be performed before accuracy is lost. This work
has important implications for control systems with safety-critical software,
as these systems are now running fast enough and long enough for their errors
to impact on their functionality. Furthermore, worst-case analysis would
blindly advise the replacement of existing systems that have been successfully
running for years. We present here a set of formal theorems validated by the
PVS proof assistant. These theorems will allow code analyzing tools to produce
formal certificates of accurate behavior. For example, FAA regulations for
aircraft require that the probability of an error be below $10^{-9}$ for a 10
hour flight.

<id>
cs/0610110v4
<category>
cs.MS
<abstract>
We provide a framework to bound the probability that accumulated errors were
never above a given threshold on hybrid systems. Such systems are used for
example to model an aircraft or a nuclear power plant on one side and its
software on the other side. This report contains simple formulas based on
L\'evy's and Markov's inequalities and it presents a formal theory of random
variables with a special focus on producing concrete results. We selected four
very common applications that fit in our framework and cover the common
practices of hybrid systems that evolve for a long time. We compute the number
of bits that remain continuously significant in the first two applications with
a probability of failure around one against a billion, where worst case
analysis considers that no significant bit remains. We are using PVS as such
formal tools force explicit statement of all hypotheses and prevent incorrect
uses of theorems.

<id>
cs/0701186v2
<category>
cs.MS
<abstract>
Gappa uses interval arithmetic to certify bounds on mathematical expressions
that involve rounded as well as exact operators. Gappa generates a theorem with
its proof for each bound treated. The proof can be checked with a higher order
logic automatic proof checker, either Coq or HOL Light, and we have developed a
large companion library of verified facts for Coq dealing with the addition,
multiplication, division, and square root, in fixed- and floating-point
arithmetics. Gappa uses multiple-precision dyadic fractions for the endpoints
of intervals and performs forward error analysis on rounded operators when
necessary. When asked, Gappa reports the best bounds it is able to reach for a
given expression in a given context. This feature is used to quickly obtain
coarse bounds. It can also be used to identify where the set of facts and
automatic techniques implemented in Gappa becomes insufficient. Gappa handles
seamlessly additional properties expressed as interval properties or rewriting
rules in order to establish more intricate bounds. Recent work showed that
Gappa is perfectly suited to the proof of correctness of small pieces of
software. Proof obligations can be written by designers, produced by
third-party tools or obtained by overloading arithmetic operators.

<id>
cs/0703040v1
<category>
cs.MS
<abstract>
The basic statistical methods of data representation did not change since
their emergence. Their simplicity was dictated by the intricacies of
computations in the before computers epoch. It turns out that such approach is
not uniquely possible in the presence of quick computers. The suggested here
method improves significantly the reliability of data processing and their
graphical representation. In this paper we show problems of the standard data
processing which can bring to incorrect results. A method solving these
problems is proposed. It is based on modification of data representation. The
method was implemented in a computer program Consensus5. The program
performances are illustrated through varied examples.

<id>
0707.2347v5
<category>
cs.MS
<abstract>
We propose several new schedules for Strassen-Winograd's matrix
multiplication algorithm, they reduce the extra memory allocation requirements
by three different means: by introducing a few pre-additions, by overwriting
the input matrices, or by using a first recursive level of classical
multiplication. In particular, we show two fully in-place schedules: one having
the same number of operations, if the input matrices can be overwritten; the
other one, slightly increasing the constant of the leading term of the
complexity, if the input matrices are read-only. Many of these schedules have
been found by an implementation of an exhaustive search algorithm based on a
pebble game.

<id>
0707.4651v1
<category>
cs.MS
<abstract>
This brief paper: (1) Discusses strategies to generate random test cases that
can be used to extensively test any Linear Distance Program (LDP) software. (2)
Gives three numerical examples of input cases generated by this strategy that
cause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard
matter of acceptable implementation procedures, that (unless it is done
internally in the software itself, but, in general, this seems to be much rarer
than one would expect) all users should test the returned output from any LDP
module for self-consistency since it incurs only a small amount of added
computational overhead and it is not hard to do.

<id>
0808.2794v1
<category>
cs.MS
<abstract>
On modern architectures, the performance of 32-bit operations is often at
least twice as fast as the performance of 64-bit operations. By using a
combination of 32-bit and 64-bit floating point arithmetic, the performance of
many dense and sparse linear algebra algorithms can be significantly enhanced
while maintaining the 64-bit accuracy of the resulting solution. The approach
presented here can apply not only to conventional processors but also to other
technologies such as Field Programmable Gate Arrays (FPGA), Graphical
Processing Units (GPU), and the STI Cell BE processor. Results on modern
processor architectures and the STI Cell BE are presented.

<id>
0811.1714v1
<category>
cs.MS
<abstract>
We describe an efficient implementation of a hierarchy of algorithms for
multiplication of dense matrices over the field with two elements (GF(2)). In
particular we present our implementation -- in the M4RI library -- of
Strassen-Winograd matrix multiplication and the "Method of the Four Russians"
multiplication (M4RM) and compare it against other available implementations.
Good performance is demonstrated on on AMD's Opteron and particulary good
performance on Intel's Core 2 Duo. The open-source M4RI library is available
stand-alone as well as part of the Sage mathematics software.
  In machine terms, addition in GF(2) is logical-XOR, and multiplication is
logical-AND, thus a machine word of 64-bits allows one to operate on 64
elements of GF(2) in parallel: at most one CPU cycle for 64 parallel additions
or multiplications. As such, element-wise operations over GF(2) are relatively
cheap. In fact, in this paper, we conclude that the actual bottlenecks are
memory reads and writes and issues of data locality. We present our empirical
findings in relation to minimizing these and give an analysis thereof.

<id>
cs/0306119v1
<category>
cs.MA
<abstract>
We present a method for solving service allocation problems in which a set of
services must be allocated to a set of agents so as to maximize a global
utility. The method is completely distributed so it can scale to any number of
services without degradation. We first formalize the service allocation problem
and then present a simple hill-climbing, a global hill-climbing, and a
bidding-protocol algorithm for solving it. We analyze the expected performance
of these algorithms as a function of various problem parameters such as the
branching factor and the number of agents. Finally, we use the sensor
allocation problem, an instance of a service allocation problem, to show the
bidding protocol at work. The simulations also show that phase transition on
the expected quality of the solution exists as the amount of communication
between agents increases.

<id>
cs/0401025v1
<category>
cs.MA
<abstract>
Objective-C is still the language of choice if users want to run their
simulation efficiently under the Swarm environment since the Swarm environment
itself was written in Objective-C. The language is a fast, object-oriented and
easy to learn. However, the language is less well known than, less expressive
than, and lacks support for many important features of C++ (eg. OpenMP for high
performance computing application). In this paper, we present a methodology and
software tools that we have developed for auto generating an Objective-C object
template (and all the necessary interfacing functions) from a given C++ model,
utilising the Classdesc's object description technology, so that the C++ model
can both be run and accessed under the Objective-C and C++ environments. We
also present a methodology for modifying an existing Swarm application to make
part of the model (eg. the heatbug's step method) run under the C++
environment.

<id>
cs/0401026v1
<category>
cs.MA
<abstract>
\EcoLab{} is an agent based modeling system for C++ programmers, strongly
influenced by the design of Swarm. This paper is just a brief outline of
\EcoLab's features, more details can be found in other published articles,
documentation and source code from the \EcoLab{} website.

<id>
cs/0407025v1
<category>
cs.MA
<abstract>
Agent Academy (AA) aims to develop a multi-agent society that can train new
agents for specific or general tasks, while constantly retraining existing
agents in a recursive mode. The system is based on collecting information both
from the environment and the behaviors of the acting agents and their related
successes/failures to generate a body of data, stored in the Agent Use
Repository, which is mined by the Data Miner module, in order to generate
useful knowledge about the application domain. Knowledge extracted by the Data
Miner is used by the Agent Training Module as to train new agents or to enhance
the behavior of agents already running. In this paper the Agent Academy
framework is introduced, and its overall architecture and functionality are
presented. Training issues as well as agent ontologies are discussed. Finally,
a scenario, which aims to provide environmental alerts to both individuals and
public contributorities, is described an AA-based use case.

<id>
cs/0502094v1
<category>
cs.MA
<abstract>
Solutions to the coalition formation problem commonly assume agent
rationality and, correspondingly, utility maximization. This in turn may
prevent agents from making compromises. As shown in recent studies, compromise
may facilitate coalition formation and increase agent utilities. In this study
we leverage on those new results. We devise a novel coalition formation
mechanism that enhances compromise. Our mechanism can utilize information on
task dependencies to reduce formation complexity. Further, it works well with
both cardinal and ordinal task values. Via experiments we show that the use of
the suggested compromise-based coalition formation mechanism provides
significant savings in the computation and communication complexity of
coalition formation. Our results also show that when information on task
dependencies is used, the complexity of coalition formation is further reduced.
We demonstrate successful use of the mechanism for collaborative information
filtering, where agents combine linguistic rules to analyze documents'
contents.

<id>
cs/0506092v1
<category>
cs.MA
<abstract>
This paper reviews recent attempts at modelling inequality of wealth as an
emergent phenomenon of interacting-agent processes. We point out that recent
models of wealth condensation which draw their inspiration from molecular
dynamics have, in fact, reinvented a process introduced quite some time ago by
Angle (1986) in the sociological literature. We emphasize some problematic
aspects of simple wealth exchange models and contrast them with a monetary
model based on economic principles of market mediated exchange. The paper also
reports new results on the influence of market power on the wealth distribution
in statistical equilibrium. As it turns out, inequality increases but market
power alone is not sufficient for changing the exponential tails of simple
exchange models into Pareto tails.

<id>
cs/0509037v1
<category>
cs.MA
<abstract>
By harvesting friendship networks from e-mail contacts or instant message
"buddy lists" Peer-to-Peer (P2P) applications can improve performance in low
trust environments such as the Internet. However, natural social networks are
not always suitable, reliable or available. We propose an algorithm (SLACER)
that allows peer nodes to create and manage their own friendship networks.
  We evaluate performance using a canonical test application, requiring
cooperation between peers for socially optimal outcomes. The Artificial Social
Networks (ASN) produced are connected, cooperative and robust - possessing many
of the disable properties of human friendship networks such as trust between
friends (directly linked peers) and short paths linking everyone via a chain of
friends.
  In addition to new application possibilities, SLACER could supply ASN to P2P
applications that currently depend on human social networks thus transforming
them into fully autonomous, self-managing systems.

<id>
cs/0509050v1
<category>
cs.MA
<abstract>
The recent commercial launch of twin-deck Very Large Transport Aircraft
(VLTA) such as the Airbus A380 has raised questions concerning the speed at
which they may be evacuated. The abnormal height of emergency exits on the
upper deck has led to speculation that emotional factors such as fear may lead
to door delay, and thus play a significant role in increasing overall
evacuation time. Full-scale evacuation tests are financially expensive and
potentially hazardous, and systematic studies of the evacuation of VLTA are
rare. Here we present a computationally cheap agent-based framework for the
general simulation of aircraft evacuation, and apply it to the particular case
of the Airbus A380. In particular, we investigate the effect of door delay, and
conclude that even a moderate average delay can lead to evacuation times that
exceed the maximum for safety certification. The model suggests practical ways
to minimise evacuation time, as well as providing a general framework for the
simulation of evacuation.

<id>
cs/0602056v1
<category>
cs.MA
<abstract>
Oftentimes, the need to build multidiscipline knowledge bases, oriented to
policy scenarios, entails the involvement of stakeholders in manifold domains,
with a juxtaposition of different languages whose semantics can hardly allow
inter-domain transfers. A useful support for planning is the building up of
durable IT based interactive platforms, where it is possible to modify initial
positions toward a semantic convergence. The present paper shows an area-based
application of these tools, for the integrated distance-management of different
forms of knowledge expressed by selected stakeholders about environmental
planning issues, in order to build alternative development scenarios.
  Keywords: Environmental planning, Scenario building, Multi-source knowledge,
IT-based

<id>
cs/0603126v1
<category>
cs.MA
<abstract>
Systems of systems differ from traditional systems in that they are open at
the top, open at the bottom, and continually (but slowly) evolving. "Open at
the top" means that there is no pre-defined top level application. New
applications may be created at any time. "Open at the bottom" means that the
system primitives are defined functionally rather than concretely. This allows
the implementation of these primitives to be modified as technology changes.
"Continually (but slowly) evolving" means that the system's functionality is
stable enough to be useful but is understood to be subject to modification.
Systems with these properties tend to be environments within which other
systems operate--and hence are systems of systems. It is also important to
understand the larger environment within which a system of systems exists.

<id>
cs/0603127v1
<category>
cs.MA
<abstract>
One may define a complex system as a system in which phenomena emerge as a
consequence of multiscale interaction among the system's components and their
environments. The field of Complex Systems is the study of such
systems--usually naturally occurring, either bio-logical or social. Systems
Engineering may be understood to include the conceptualising and building of
systems that consist of a large number of concurrently operating and
interacting components--usually including both human and non-human elements. It
has become increasingly apparent that the kinds of systems that systems
engineers build have many of the same multiscale characteristics as those of
naturally occurring complex systems. In other words, systems engineering is the
engineering of complex systems. This paper and the associated panel will
explore some of the connections between the fields of complex systems and
systems engineering.

<id>
cs/0609041v1
<category>
cs.MA
<abstract>
In this paper, we study the construction and transformation of
two-dimensional persistent graphs. Persistence is a generalization to directed
graphs of the undirected notion of rigidity. In the context of moving
autonomous agent formations, persistence characterizes the efficacy of a
directed structure of unilateral distances constraints seeking to preserve a
formation shape. Analogously to the powerful results about Henneberg sequences
in minimal rigidity theory, we propose different types of directed graph
operations allowing one to sequentially build any minimally persistent graph
(i.e. persistent graph with a minimal number of edges for a given number of
vertices), each intermediate graph being also minimally persistent. We also
consider the more generic problem of obtaining one minimally persistent graph
from another, which corresponds to the on-line reorganization of an autonomous
agent formation. We prove that we can obtain any minimally persistent formation
from any other one by a sequence of elementary local operations such that
minimal persistence is preserved throughout the reorganization process.

<id>
cs/0612014v1
<category>
cs.MA
<abstract>
In 2005, Railsback et al. proposed a very simple model ({\em Stupid
  Model}) that could be implemented within a couple of hours, and later
extended to demonstrate the use of common ABM platform functionality. They
provided implementations of the model in several agent based modelling
platforms, and compared the platforms for ease of implementation of this simple
model, and performance. In this paper, I implement Railsback et al's Stupid
Model in the EcoLab simulation platform, a C++ based modelling platform,
demonstrating that it is a feasible platform for these sorts of models, and
compare the performance of the implementation with Repast, Mason and Swarm
versions.

<id>
cs/0701087v2
<category>
cs.MA
<abstract>
This text provides with an introduction to the modern approach of
artificiality and simulation in social sciences. It presents the relationship
between complexity and artificiality, before introducing the field of
artificial societies which greatly benefited from the computer power fast
increase, gifting social sciences with formalization and experimentation tools
previously owned by "hard" sciences alone. It shows that as "a new way of doing
social sciences", artificial societies should undoubtedly contribute to a
renewed approach in the study of sociality and should play a significant part
in the elaboration of original theories of social phenomena.

<id>
cs/0702091v1
<category>
cs.MA
<abstract>
An edge-colored directed graph is \emph{observable} if an agent that moves
along its edges is able to determine his position in the graph after a
sufficiently long observation of the edge colors. When the agent is able to
determine his position only from time to time, the graph is said to be
\emph{partly observable}. Observability in graphs is desirable in situations
where autonomous agents are moving on a network and one wants to localize them
(or the agent wants to localize himself) with limited information. In this
paper, we completely characterize observable and partly observable graphs and
show how these concepts relate to observable discrete event systems and to
local automata. Based on these characterizations, we provide polynomial time
algorithms to decide observability, to decide partial observability, and to
compute the minimal number of observations necessary for finding the position
of an agent. In particular we prove that in the worst case this minimal number
of observations increases quadratically with the number of nodes in the graph.
  From this it follows that it may be necessary for an agent to pass through
the same node several times before he is finally able to determine his position
in the graph. We then consider the more difficult question of assigning colors
to a graph so as to make it observable and we prove that two different versions
of this problem are NP-complete.

<id>
0705.1757v1
<category>
cs.MA
<abstract>
A population of committees of agents that learn by using neural networks is
implemented to simulate the stock market. Each committee of agents, which is
regarded as a player in a game, is optimised by continually adapting the
architecture of the agents using genetic algorithms. The committees of agents
buy and sell stocks by following this procedure: (1) obtain the current price
of stocks; (2) predict the future price of stocks; (3) and for a given price
trade until all the players are mutually satisfied. The trading of stocks is
conducted by following these rules: (1) if a player expects an increase in
price then it tries to buy the stock; (2) else if it expects a drop in the
price, it sells the stock; (3)and the order in which a player participates in
the game is random. The proposed procedure is implemented to simulate trading
of three stocks, namely, the Dow Jones, the Nasdaq and the S&P 500. A linear
relationship between the number of players and agents versus the computational
time to run the complete simulation is observed. It is also found that no
player has a monopolistic advantage.

<id>
0705.3050v1
<category>
cs.MA
<abstract>
We develop a dynamic multi-agent model of an interbank payment system where
banks choose their level of available funds on the basis of private payoff
maximisation. The model consists of the repetition of a simultaneous move stage
game with incomplete information, incomplete monitoring, and stochastic
payoffs. Adaptation takes place with bayesian updating, with banks maximizing
immediate payoffs. We carry out numerical simulations to solve the model and
investigate two special scenarios: an operational incident and exogenous
throughput guidelines for payment submission. We find that the demand for
intraday credit is an S-shaped function of the cost ratio between intraday
credit costs and the costs associated with delaying payments. We also find that
the demand for liquidity is increased both under operational incidents and in
the presence of effective throughput guidelines.

<id>
0707.1558v1
<category>
cs.MA
<abstract>
This paper presents a model of autonomy called autonomy with regard to an
attribute applicable to cognitive and not cognitive artificial agents. Three
criteria (global / partial, social / nonsocial, absolute / relative) are
defined and used to describe the main characteristics of this type of autonomy.
A software agent autonomous with regard to the mobility illustrates a possible
implementation of this model.

<id>
0802.4450v1
<category>
cs.MA
<abstract>
We investigate convergence properties of a proposed distributed model
predictive control (DMPC) scheme, where agents negotiate to compute an optimal
consensus point using an incremental subgradient method based on primal
decomposition as described in Johansson et al. [2006, 2007]. The objective of
the distributed control strategy is to agree upon and achieve an optimal common
output value for a group of agents in the presence of constraints on the agent
dynamics using local predictive controllers. Stability analysis using a
receding horizon implementation of the distributed optimal consensus scheme is
performed. Conditions are given under which convergence can be obtained even if
the negotiations do not reach full consensus.

<id>
0806.3031v1
<category>
cs.MA
<abstract>
A new approach of coordination of decisions in a multi site system is
proposed. It is based this approach on a multi-agent concept and on the
principle of distributed network of enterprises. For this purpose, each
enterprise is defined as autonomous and performs simultaneously at the local
and global levels. The basic component of our approach is a so-called Virtual
Enterprise Node (VEN), where the enterprise network is represented as a set of
tiers (like in a product breakdown structure). Within the network, each partner
constitutes a VEN, which is in contact with several customers and suppliers.
Exchanges between the VENs ensure the autonomy of decision, and guarantiee the
consistency of information and material flows. Only two complementary VEN
agents are necessary: one for external interactions, the Negotiator Agent (NA)
and one for the planning of internal decisions, the Planner Agent (PA). If
supply problems occur in the network, two other agents are defined: the Tier
Negotiator Agent (TNA) working at the tier level only and the Supply Chain
Mediator Agent (SCMA) working at the level of the enterprise network. These two
agents are only active when the perturbation occurs. Otherwise, the VENs
process the flow of information alone. With this new approach, managing
enterprise network becomes much more transparent and looks like managing a
simple enterprise in the network. The use of a Multi-Agent System (MAS) allows
physical distribution of the decisional system, and procures a heterarchical
organization structure with a decentralized control that guaranties the
autonomy of each entity and the flexibility of the network.

<id>
0806.3032v1
<category>
cs.MA
<abstract>
The purpose of this paper is to propose a new approach for the supply chain
management. This approach is based on the virtual enterprise paradigm and the
used of multi-agent concept. Each entity (like enterprise) is autonomous and
must perform local and global goals in relation with its environment. The base
component of our approach is a Virtual Enterprise Node (VEN). The supply chain
is viewed as a set of tiers (corresponding to the levels of production), in
which each partner of the supply chain (VEN) is in relation with several
customers and suppliers. Each VEN belongs to one tier. The main customer gives
global objectives (quantity, cost and delay) to the supply chain. The Mediator
Agent (MA) is in charge to manage the supply chain in order to respect those
objectives as global level. Those objectives are taking over to Negotiator
Agent at the tier level (NAT). These two agents are only active if a
perturbation occurs; otherwise information flows are only exchange between
VENs. This architecture allows supply chains management which is completely
transparent seen from simple enterprise of the supply chain. The used of
Multi-Agent System (MAS) allows physical distribution of the decisional system.
Moreover, the hierarchical organizational structure with a decentralized
control guaranties, in the same time, the autonomy of each entity and the whole
flexibility.

<id>
0807.2028v4
<category>
cs.MA
<abstract>
We study a model of opinion dynamics introduced by Krause: each agent has an
opinion represented by a real number, and updates its opinion by averaging all
agent opinions that differ from its own by less than 1. We give a new proof of
convergence into clusters of agents, with all agents in the same cluster
holding the same opinion. We then introduce a particular notion of equilibrium
stability and provide lower bounds on the inter-cluster distances at a stable
equilibrium. To better understand the behavior of the system when the number of
agents is large, we also introduce and study a variant involving a continuum of
agents, obtaining partial convergence results and lower bounds on inter-cluster
distances, under some mild assumptions.

<id>
0905.0747v1
<category>
cs.MA
<abstract>
In this paper, we investigate the possibility to deterministically solve the
gathering problem (GP) with weak robots (anonymous, autonomous, disoriented,
deaf and dumb, and oblivious). We introduce strong multiplicity detection as
the ability for the robots to detect the exact number of robots located at a
given position. We show that with strong multiplicity detection, there exists a
deterministic self-stabilizing algorithm solving GP for n robots if, and only
if, n is odd.

<id>
0911.0753v1
<category>
cs.MA
<abstract>
In this paper we propose an XML-based multi-agent recommender system for
supporting online recruitment services. Our system is characterized by the
following features: {\em (i)} it handles user profiles for personalizing the
job search over the Internet; {\em (ii)} it is based on the Intelligent Agent
Technology; {\em (iii)} it uses XML for guaranteeing a light, versatile and
standard mechanism for information representation, storing and exchange. The
paper discusses the basic features of the proposed system, presents the results
of an experimental study we have carried out for evaluating its performance,
and makes a comparison between the proposed system and other e-recruitment
systems already presented in the past.

<id>
0911.0912v1
<category>
cs.MA
<abstract>
Coordination between organizations on strategic, tactical and operation
levels leads to more effective and efficient supply chains. Supply chain
management is increasing day by day in modern enterprises. The environment is
becoming competitive and many enterprises will find it difficult to survive if
they do not make their sourcing, production and distribution more efficient.
Multi-agent supply chain management has recognized as an effective methodology
for supply chain management. Multi-agent systems (MAS) offer new methods
compared to conventional, centrally organized architectures in the scope of
supply chain management (SCM). Since necessary data are not available within
the whole supply chain, an integrated approach for production planning and
control taking into account all the partners involved is not feasible. In this
study we show how MAS architecture interacts in the integrated SCM architecture
with the help of various intelligent agents to highlight the above problem.

<id>
0911.2902v1
<category>
cs.MA
<abstract>
The simulation of vehicular traffic as well as pedestrian dynamics meanwhile
both have a decades long history. The success of this conference series, PED
and others show that the interest in these topics is still strongly increasing.
This contribution deals with a combination of both systems: pedestrians
crossing a street. In a VISSIM simulation for varying demand jam sizes of
vehicles as well as pedestrians and the travel times of the pedestrians are
measured and compared. The study is considered as a study of VISSIM's con ict
area functionality as such, as there is no empirical data available to use for
calibration issues. Above a vehicle demand threshold the results show a
non-monotonic dependence of pedestrians' travel time on pedestrian demand.

<id>
0911.3723v1
<category>
cs.MA
<abstract>
Recently the dynamic distance potential field (DDPF) was introduced as a
computationally efficient method to make agents in a simulation of pedestrians
move rather on the quickest path than the shortest. It can be considered to be
an estimated-remaining-journey-time-based one-shot dynamic assignment method
for pedestrian route choice on the operational level of dynamics. In this
contribution the method is shortly introduced and the effect of the method on
RiMEA's test case 11 is investigated.

<id>
0912.3961v1
<category>
cs.MA
<abstract>
We are exploring the enhancement of models of agent behaviour with more
"human-like" decision making strategies than are presently available. Our
motivation is to developed with a view to as the decision analysis and support
for electric taxi company under the mission of energy saving and reduction of
CO2, in particular car-pool and car-sharing management policies. In order to
achieve the object of decision analysis for user, we provide a human-agents
interactive spatial behaviour to support user making decision real time. We
adopt passenger average waiting time and electric taxi average idle time as the
performance measures and decision support fro electric taxi company. Finally,
according to the analysis result, we demonstrate that our multi-agent
simulation and GUI can help users or companies quickly make a quality and
accurate decision to reduce the decision-making cost and time.

<id>
0912.3984v1
<category>
cs.MA
<abstract>
Information management and retrieval of all the citizen occurs in almost all
the public service functions. Electronic Government system is an emerging trend
in India through which efforts are made to strive maximum safety and security.
Various solutions for this have been proposed like Shibboleth, Public Key
Infrastructure, Smart Cards and Light Weight Directory Access Protocols. Still,
none of these guarantee 100 percent security. Efforts are being made to provide
common national identity solution to various diverse Government identity cards.
In this paper, we discuss issues related to these solutions.

<id>
0912.4637v1
<category>
cs.MA
<abstract>
We use the notion of a promise to define local trust between agents
possessing autonomous decision-making. An agent is trustworthy if it is
expected that it will keep a promise. This definition satisfies most
commonplace meanings of trust. Reputation is then an estimation of this
expectation value that is passed on from agent to agent.
  Our definition distinguishes types of trust, for different behaviours, and
decouples the concept of agent reliability from the behaviour on which the
judgement is based. We show, however, that trust is fundamentally heuristic, as
it provides insufficient information for agents to make a rational judgement. A
global trustworthiness, or community trust can be defined by a proportional,
self-consistent voting process, as a weighted eigenvector-centrality function
of the promise theoretical graph.

<id>
cs/0109041v2
<category>
cs.MM
<abstract>
In this paper we analyze the development of interactive TV in the U.S. and
Western Europe. We argue that despite the nascent character of the market there
are important regulatory issues at stake, as exemplified by the AOL/TW merger
and the British Interactive Broadcasting case. Absent rules that provide for
non-discriminatory access to network components (including terminal equipment
specifications), dominant platform operators are likely to leverage ownership
of delivery infrastructure into market power over interactive TV services.
While integration between platform operator, service provider and terminal
vendor may facilitate the introduction of services in the short-term, the
lasting result will be a collection of fragmented "walled gardens" offering
limited content and applications. Would interactive TV develop under such
model, the exciting opportunities for broad-based innovation and extended
access to multiple information, entertainment and educational services opened
by the new generation of broadcasting technologies will be foregone

<id>
cs/0506070v1
<category>
cs.MM
<abstract>
The modern multimedia technologies based on the whole palette of hardware and
software facilities of real-time high-speed information processing, in a
combination with effective facilities of the remote access to information
resources, allow us to visualize diverse types of information. Data
visualization facilities &#8211; is the face of the Automated Control System on
whom often judge about their efficiency. They take a special place, providing
visualization of the diverse information necessary for decision-making by a
final control link - the person allocated by certain powers.

<id>
cs/0603130v1
<category>
cs.MM
<abstract>
Many current watermarking algorithms insert data in the spatial or transform
domains like the discrete cosine, the discrete Fourier, and the discrete
wavelet transforms. In this paper, we present a data-hiding algorithm that
exploits the singular value decomposition (SVD) representation of the data. We
compute the SVD of the host image and the watermark and embed the watermark in
the singular vectors of the host image. The proposed method leads to an
imperceptible scheme for digital images, both in grey scale and color and is
quite robust against attacks like noise and JPEG compression.

<id>
cs/0607087v1
<category>
cs.MM
<abstract>
In the context of human action recognition in video sequences, a temporal
belief filter is presented. It allows to cope with human action disparity and
low quality videos. The whole system of action recognition is based on the
Transferable Belief Model (TBM) proposed by P. Smets. The TBM allows to
explicitly model the doubt between actions. Furthermore, the TBM emphasizes the
conflict which is exploited for action recognition. The filtering performance
is assessed on real video sequences acquired by a moving camera and under
several unknown view angles.

<id>
cs/0609131v1
<category>
cs.MM
<abstract>
In this paper, we propose a fast 2-D block-based motion estimation algorithm
called Particle Swarm Optimization - Zero-motion Prejudgment(PSO-ZMP) which
consists of three sequential routines: 1)Zero-motion prejudgment. The routine
aims at finding static macroblocks(MB) which do not need to perform remaining
search thus reduces the computational cost; 2)Predictive image coding and 3)PSO
matching routine. Simulation results obtained show that the proposed PSO-ZMP
algorithm achieves over 10 times of computation less than Diamond Search(DS)
and 5 times less than the recent proposed Adaptive Rood Pattern
Searching(ARPS). Meanwhile the PSNR performances using PSO-ZMP are very close
to that using DS and ARPS in some less-motioned sequences. While in some
sequences containing dense and complex motion contents, the PSNR performances
of PSO-ZMP are several dB lower than that using DS and ARPS but in an
acceptable degree.

<id>
0710.4819v1
<category>
cs.MM
<abstract>
Motion estimation is the most critical process in video coding systems. First
of all, it has a definitive impact on the rate-distortion performance given by
the video encoder. Secondly, it is the most computationally intensive process
within the encoding loop. For these reasons, the design of high-performance
low-cost motion estimators is a crucial task in the video compression field. An
adaptive cost block matching (ACBM) motion estimation technique is presented in
this paper, featuring an excellent tradeoff between the quality of the
reconstructed video sequences and the computational effort. Simulation results
demonstrate that the ACBM algorithm achieves a slight better rate-distortion
performance than the one given by the well-known full search algorithm block
matching algorithm with reductions of up to 95% in the computational load.

<id>
0710.4821v1
<category>
cs.MM
<abstract>
This paper surveys the characteristics of multimedia systems. Multimedia
applications today are dominated by compression and decompression, but
multimedia devices must also implement many other functions such as security
and file management. We introduce some basic concepts of multimedia algorithms
and the larger set of functions that multimedia systems-on-chips must
implement.

<id>
0710.4823v1
<category>
cs.MM
<abstract>
Visual information processing will play an increasingly important role in
future electronics systems. In many applications, e.g. video surveillance
cameras, data throughput of microprocessors is not sufficient and power
consumption is too high. Instruction profiling on a typical test algorithm has
shown that pixel address calculations are the dominant operations to be
optimized. Therefore AddressLib, a structured scheme for pixel addressing was
developed, that can be accelerated by AddressEngine, a coprocessor for visual
information processing. In this paper, the architectural design of
AddressEngine is described, which in the first step supports a subset of the
AddressLib. Dataflow and memory organization are optimized during architectural
design. AddressEngine was implemented in a FPGA and was tested with MPEG-7
Global Motion Estimation algorithm. Results on processing speed and circuit
complexity are given and compared to a pure software implementation. The next
step will be the support for the full AddressLib, including segment addressing.
An outlook on further investigations on dynamic reconfiguration capabilities is
given.

<id>
0804.0134v1
<category>
cs.MM
<abstract>
The Voice over Internet Protocol (VoIP) is becoming a more available and
popular way of communicating for Internet users. This also applies to
Peer-to-Peer (P2P) systems and merging these two have already proven to be
successful (e.g. Skype). Even the existing standards of VoIP provide an
assurance of security and Quality of Service (QoS), however, these features are
usually optional and supported by limited number of implementations. As a
result, the lack of mandatory and widely applicable QoS and security guaranties
makes the contemporary VoIP systems vulnerable to attacks and network
disturbances. In this paper we are facing these issues and propose the SecMon
system, which simultaneously provides a lightweight security mechanism and
improves quality parameters of the call. SecMon is intended specially for VoIP
service over P2P networks and its main advantage is that it provides
authentication, data integrity services, adaptive QoS and (D)DoS attack
detection. Moreover, the SecMon approach represents a low-bandwidth consumption
solution that is transparent to the users and possesses a self-organizing
capability. The above-mentioned features are accomplished mainly by utilizing
two information hiding techniques: digital audio watermarking and network
steganography. These techniques are used to create covert channels that serve
as transport channels for lightweight QoS measurement's results. Furthermore,
these metrics are aggregated in a reputation system that enables best route
path selection in the P2P network. The reputation system helps also to mitigate
(D)DoS attacks, maximize performance and increase transmission efficiency in
the network.

<id>
0808.0309v1
<category>
cs.MM
<abstract>
We propose a novel scheme for watermarking of digital images based on
singular value decomposition (SVD), which makes use of the fact that the SVD
subspace preserves significant amount of information of an image, as compared
to its singular value matrix, Zhang and Li (2005). The principal components of
the watermark are embedded in the original image, leaving the detector with a
complimentary set of singular vectors for watermark extraction. The above step
invariably ensures that watermark extraction from the embedded watermark image,
using a modified matrix, is not possible, thereby removing a major drawback of
an earlier proposed algorithm by Liu and Tan (2002).

<id>
0809.5154v1
<category>
cs.MM
<abstract>
In this paper, we propose an export architecture that provides a clear
separation of contributoring services from publication services. We illustrate this
architecture with the LimSee3 contributoring tool and several standard publication
formats: Timesheets, SMIL, and XHTML.

<id>
0810.2063v1
<category>
cs.MM
<abstract>
Initial offset placement in p2p streaming systems is studied in this paper.
Proportional placement (PP) scheme is proposed. In this scheme, peer places the
initial offset as the offset reported by other reference peer with a shift
proportional to the buffer width or offset lag of this reference peer. This
will introduce a stable placement that supports larger buffer width for peers
and small buffer width for tracker. Real deployed placement method in PPLive is
studied through measurement. It shows that, instead of based on offset lag, the
placement is based on buffer width of the reference peer to facilitate the
initial chunk fetching. We will prove that, such a PP scheme may not be stable
under arbitrary buffer occupation in the reference peer. The required average
buffer width then is derived. A simple good peer selection mechanism to check
the buffer occupation of reference peer is proposed for a stable PP scheme
based on buffer width

<id>
0811.1959v1
<category>
cs.MM
<abstract>
No single information source can be good enough to satisfy the divergent and
dynamic needs of users all the time. Integrating information from divergent
sources can be a solution to deficiencies in information content. We present
how Information from multimedia document can be collected based on associating
a generic database to a federated database. Information collected in this way
is brought into relevance by integrating the parameters of usage and user's
parameter for decision making. We identified seven different classifications of
multimedia document.

<id>
0812.2529v1
<category>
cs.MM
<abstract>
One of the current challenges of Information Systems is to ensure
semi-structured data transmission, such as multimedia data, in a distributed
and pervasive environment. Information Sytems must then guarantee users a
quality of service ensuring data accessibility whatever the hardware and
network conditions may be. They must also guarantee information coherence and
particularly intelligibility that imposes a personalization of the service.
Within this framework, we propose a design method based on original models of
multimedia applications and quality of service. We also define a supervision
platform Kalinahia using a user centered heuristic allowing us to define at any
moment which configuration of software components constitutes the best answers
to users' wishes in terms of service.

<id>
0812.2988v1
<category>
cs.MM
<abstract>
Needs of multimedia systems evolved due to the evolution of their
architecture which is now distributed into heterogeneous contexts. A critical
issue lies in the fact that they handle, process, and transmit multimedia data.
This data integrates several properties which should be considered since it
holds a considerable part of its semantics, for instance the lips
synchronization in a video. In this paper, we focus on the definition of a
model as a basic abstraction for describing and modeling media in multimedia
systems by taking into account their properties. This model will be used in
software architecture in order to handle data in efficient way. The provided
model is an interesting solution for the integration of media into
applications; we propose to consider and to handle them in a uniform way. This
model is proposed with synchronization policies to ensure synchronous transport
of media. Therefore, we use it in a component model that we develop for the
design and deployment of distributed multimedia systems.

<id>
0812.2989v1
<category>
cs.MM
<abstract>
Resource-constrained embedded and mobile devices are becoming increasingly
common. Since few years, some mobile and ubiquitous devices such as wireless
sensor, able to be aware of their physical environment, appeared. Such devices
enable proposing applications which adapt to user's need according the context
evolution. It implies the collaboration of sensors and software components
which differ on their nature and their communication mechanisms. This paper
proposes a unified component model in order to easily design applications based
on software components and sensors without taking care of their nature. Then it
presents a state of the art of communication problems linked to heterogeneous
components and proposes an interaction mechanism which ensures information
exchanges between wireless sensors and software components.

<id>
0902.3979v1
<category>
cs.MM
<abstract>
A single queue incorporating a retransmission protocol is investigated,
assuming that the sequence of per effort success probabilities in the Automatic
Retransmission reQuest (ARQ) chain is a priori defined and no channel state
information at the transmitter is available. A Markov Decision Problem with an
average cost criterion is formulated where the possible actions are to either
continue the retransmission process of an erroneous packet at the next time
slot or to drop the packet and move on to the next packet awaiting for
transmission. The cost per slot is a linear combination of the current queue
length and a penalty term in case dropping is chosen as action. The
investigation seeks policies that provide the best possible average packet
delay-dropping trade-off for Quality of Service guarantees. An optimal
deterministic stationary policy is shown to exist, several structural
properties of which are obtained. Based on that, a class of suboptimal
<L,K>-policies is introduced. These suggest that it is almost optimal to use a
K-truncated ARQ protocol as long as the queue length is lower than L, else send
all packets in one shot. The work concludes with an evaluation of the optimal
delay-dropping tradeoff using dynamic programming and a comparison between the
optimal and suboptimal policies.

<id>
0905.4205v1
<category>
cs.MM
<abstract>
This article presents a new concept of a multimedia interactive product. It
is a multiuser versatile platform that can be used for different purposes. The
first implementation of the platform is a multiplayer game called Texas Hold
'em, which is a very popular community card game. The paper shows the product's
multimedia structure where Hardware and Software work together in creating a
realistic feeling for the users.

<id>
0909.0245v1
<category>
cs.MM
<abstract>
The intent of the H.264 AVC project was to create a standard capable of
providing good video quality at substantially lower bit rates than previous
standards without increasing the complexity of design so much that it would be
impractical or excessively expensive to implement. An additional goal was to
provide enough flexibility to allow the standard to be applied to a wide
variety of applications. To achieve better coding efficiency, H.264 AVC uses
several techniques such as inter mode and intra mode prediction with variable
size motion compensation, which adopts Rate Distortion Optimization (RDO). This
increases the computational complexity of the encoder especially for devices
with lower processing capabilities such as mobile and other handheld devices.
In this paper, we propose an algorithm to reduce the number of mode and sub
mode evaluations in inter mode prediction. Experimental results show that this
fast intra mode selection algorithm can lessen about 75 percent encoding time
with little loss of bit rate and visual quality.

<id>
0909.2816v1
<category>
cs.MM
<abstract>
Playout buffers are used in VoIP systems to compensate for network delay
jitter by making a trade-off between delay and loss. In this work we propose a
playout buffer algorithm that makes the trade-off based on maximization of
conversational speech quality, aiming to keep the computational complexity
lowest possible. We model the network delay using a Pareto distribution and
show that it is a good compromise between providing an appropriate fit to the
network delay characteristics and yielding a low arithmetical complexity. We
use the ITU-T E-Model as the quality model and simplify its delay impairment
function. The proposed playout buffer algorithm finds the optimum playout delay
using a closed-form solution that minimizes the sum of the simplified delay
impairment factor and the loss-dependent equipment impairment factor of the
E-model. The simulation results show that our proposed algorithm outperforms
existing state-of-the-art algorithms with a reduced complexity for a
quality-based algorithm.

<id>
0911.0399v1
<category>
cs.MM
<abstract>
A novel video watermarking system operating in the three dimensional wavelet
transform is here presented. Specifically the video sequence is partitioned
into spatio temporal units and the single shots are projected onto the 3D
wavelet domain. First a grayscale watermark image is decomposed into a series
of bitplanes that are preprocessed with a random location matrix. After that
the preprocessed bitplanes are adaptively spread spectrum and added in 3D
wavelet coefficients of the video shot. Our video watermarking algorithm is
robust against the attacks of frame dropping, averaging and swapping.
Furthermore, it allows blind retrieval of embedded watermark which does not
need the original video and the watermark is perceptually invisible. The
algorithm design, evaluation, and experimentation of the proposed scheme are
described in this paper.

<id>
0912.4880v1
<category>
cs.MM
<abstract>
In this paper we present the new genre of interactive operas implemented on
personal computers. They differ from traditional ones not only because they are
virtual, but mainly because they offer to composers and listeners new
perspectives of combinations and interactions between music, text and visual
aspects.

<id>
1001.0442v1
<category>
cs.MM
<abstract>
Dance videos are interesting and semantics-intensive. At the same time, they
are the complex type of videos compared to all other types such as sports, news
and movie videos. In fact, dance video is the one which is less explored by the
researchers across the globe. Dance videos exhibit rich semantics such as macro
features and micro features and can be classified into several types. Hence,
the conceptual modeling of the expressive semantics of the dance videos is very
crucial and complex. This paper presents a generic Dance Video Semantics Model
(DVSM) in order to represent the semantics of the dance videos at different
granularity levels, identified by the components of the accompanying song. This
model incorporates both syntactic and semantic features of the videos and
introduces a new entity type called, Agent, to specify the micro features of
the dance videos. The instantiations of the model are expressed as graphs. The
model is implemented as a tool using J2SE and JMF to annotate the macro and
micro features of the dance videos. Finally examples and evaluation results are
provided to depict the effectiveness of the proposed dance video model.
Keywords: Agents, Dance videos, Macro features, Micro features, Video
annotation, Video semantics.

<id>
1001.0443v1
<category>
cs.MM
<abstract>
Educational media mining is the process of converting raw media data from
educational systems to useful information that can be used to design learning
systems, answer research questions and allow personalized learning experiences.
Knowledge discovery encompasses a wide range of techniques ranging from
database queries to more recent developments in machine learning and language
technology. Educational media mining techniques are now being used in IT
Services research worldwide. Multi-modal Lecture Recordings is one of the
important types of educational media and this paper explores the research
challenges for mining lecture recordings for the efficient personalized
learning experiences. Keywords: Educational Media Mining; Lecture Recordings,
Multimodal Information System, Personalized Learning; Online Course Ware;
Skills and Competences;

<id>
1001.3744v1
<category>
cs.MM
<abstract>
Admission control is a key component in multimedia servers, which will allow
the resources to be used by the client only when they are available. A problem
faced by numerous content serving machines is overload, when there are too many
clients who need to be served, the server tends to slow down. An admission
control algorithm for a multimedia server is responsible for determining if a
new request can be accepted without violating the QoS requirements of the
existing requests in the system. By caching and streaming only the data in the
interval between two successive requests on the same object, the following
request can be serviced directly from the buffer cache without disk operations
and within the deadline of the request. An admission control strategy based on
Popularity-aware interval caching for Prefix [3] scheme extends the interval
caching by considering different popularity of multimedia objects. The method
of Prefix caching with multicast transmission of popular objects utilizes the
hard disk and network bandwidth efficiently and increases the number of
requests being served.

<id>
1001.3774v1
<category>
cs.MM
<abstract>
- The aim of this paper is to propose a novel Voice On Demand (VoD)
architecture and implementation of an efficient load sharing algorithm to
achieve Quality of Service (QoS). This scheme reduces the transmission cost
from the Centralized Multimedia Sever (CMS) to Proxy Servers (PS) by sharing
the videos among the proxy servers of the Local Proxy Servers Group [LPSG] and
among the neighboring LPSGs, which are interconnected in a ring fashion. This
results in very low request rejection ratio, reduction in transmission time and
cost, reduction of load on the CMS and high QoS for the users. Simulation
results indicate acceptable initial startup latency, reduced transmission cost
and time, load sharing among the proxy servers, among the LPSGs and between the
CMS and the PS.

<id>
1001.4135v1
<category>
cs.MM
<abstract>
In this paper we have proposed an adaptive dynamic cache replacement
algorithm for a multimedia servers cache system. The goal is to achieve an
effective utilization of the cache memory which stores the prefix of popular
videos. A replacement policy is usually evaluated using hit ratio, the
frequency with which any video is requested. Usually discarding the least
recently used page is the policy of choice in cache management. The adaptive
dynamic replacement approach for prefix cache is a self tuning, low overhead
algorithm that responds online to changing access patterns. It constantly
balances between lru and lfu to improve combined result. It automatically
adapts to evolving workloads. Since in our algorithm we have considered a
prefix caching with multicast transmission of popular objects it utilizes the
hard disk and network bandwidth efficiently and increases the number of
requests being served.

<id>
1003.4049v1
<category>
cs.MM
<abstract>
In this paper we propose scalable proxy servers cluster architecture of
interconnected proxy servers for high quality and high availability services.
We also propose an optimal regional popularity based video prefix replication
strategy and a scene change based replica caching algorithm that utilizes the
zipf-like video popularity distribution to maximize the availability of videos
closer to the client and request-servicing rate thereby reducing the client
rejection ratio and the response time for the client. The simulation results of
our proposed architecture and algorithm show the greater achievement in
maximizing the availability of videos, client request-servicing rate and in
reduction of initial start-up latency and client rejection ratio.

<id>
1003.4083v1
<category>
cs.MM
<abstract>
Digital processing of speech signal and voice recognition algorithm is very
important for fast and accurate automatic voice recognition technology. The
voice is a signal of infinite information. A direct analysis and synthesizing
the complex voice signal is due to too much information contained in the
signal. Therefore the digital signal processes such as Feature Extraction and
Feature Matching are introduced to represent the voice signal. Several methods
such as Liner Predictive Predictive Coding (LPC), Hidden Markov Model (HMM),
Artificial Neural Network (ANN) and etc are evaluated with a view to identify a
straight forward and effective method for voice signal. The extraction and
matching process is implemented right after the Pre Processing or filtering
signal is performed. The non-parametric method for modelling the human auditory
perception system, Mel Frequency Cepstral Coefficients (MFCCs) are utilize as
extraction techniques. The non linear sequence alignment known as Dynamic Time
Warping (DTW) introduced by Sakoe Chiba has been used as features matching
techniques. Since it's obvious that the voice signal tends to have different
temporal rate, the alignment is important to produce the better
performance.This paper present the viability of MFCC to extract features and
DTW to compare the test patterns.

<id>
1003.4637v1
<category>
cs.MM
<abstract>
Tag recommendation is a common way to enrich the textual annotation of
multimedia contents. However, state-of-the-art recommendation methods are built
upon the pair-wised tag relevance, which hardly capture the context of the web
video, i.e., when who are doing what at where. In this paper we propose the
context-oriented tag recommendation (CtextR) approach, which expands tags for
web videos under the context-consistent constraint. Given a web video, CtextR
first collects the multi-form WWW resources describing the same event with the
video, which produce an informative and consistent context; and then, the tag
recommendation is conducted based on the obtained context. Experiments on an
80,031 web video collection show CtextR recommends various relevant tags to web
videos. Moreover, the enriched tags improve the performance of web video
categorization.

<id>
cs/9809030v1
<category>
cs.NI
<abstract>
Recent network traffic studies argue that network arrival processes are much
more faithfully modeled using statistically self-similar processes instead of
traditional Poisson processes [LTWW94,PF95]. One difficulty in dealing with
self-similar models is how to efficiently synthesize traces (sample paths)
corresponding to self-similar traffic. We present a fast Fourier transform
method for synthesizing approximate self-similar sample paths for one type of
self-similar process, Fractional Gaussian Noise, and assess its performance and
validity. We find that the method is as fast or faster than existing methods
and appears to generate close approximations to true self-similar sample paths.
We also discuss issues in using such synthesized sample paths for simulating
network traffic, and how an approximation used by our method can dramatically
speed up evaluation of Whittle's estimator for H, the Hurst parameter giving
the strength of long-range dependence present in a self-similar time series.

<id>
cs/9809039v1
<category>
cs.NI
<abstract>
Multipoint capabilities are essential for ATM networks to efficiently support
many applications, including IP multicasting and overlay applications. The
current signaling and routing specifications for ATM define point-to-multipoint
capabilities. Multipoint-to-point connection support is also being discussed by
the signaling and PNNI groups, and will be defined in the near future for the
unspecified bit rate (UBR) service. We examine point-to-multipoint and
multipoint-to-point flow control for the available bit rate (ABR) service, as
discussed in the traffic management working group.

<id>
cs/9809040v1
<category>
cs.NI
<abstract>
An explicit rate switch scheme monitors the load at each link and gives
feedback to the sources. We define the overload factor as the ratio of the
input rate to the available capacity. In this paper, we present four overload
based ABR switch schemes which provide MCR guarantees. The switch schemes
proposed use the overload factor and other quantities to calculate feedback
rates. A dynamic queue control mechanism is used to achieve efficient usage of
the link, control queues and, achieve constant queuing delay at steady state.
The proposed algorithms are studied and compared using several configurations.
The configurations were chosen to test the performance of the algorithms in
presence of link bottlenecks, source bottlenecks and transient sources. A
comparison of the proposed algorithms based on the simulation results is
presented.

<id>
cs/9809041v1
<category>
cs.NI
<abstract>
The main goals of a switch scheme are high utilization, low queuing delay and
fairness. To achieve high utilization the switch scheme can maintain non-zero
(small) queues in steady state which can be used if the sources do not have
data to send. Queue length (delay) can be controlled if part of the link
capacity is used for draining queues in the event of queue build up. In most
schemes a simple threshold function is used for queue control. Better control
of the queue and hence delay can be achieved by using sophisticated queue
control functions. It is very important to design and analyze such queue
control functions. We study step, linear, hyperbolic and inverse hyperbolic
queue control functions. Analytical explanation and simulation results
consistent with analysis are presented. From the study, we conclude that
inverse hyperbolic is the best control function and to reduce complexity the
linear control function can be used since it performs satisfactorily in most
cases.

<id>
cs/9809042v1
<category>
cs.NI
<abstract>
In this paper we give a general definition of weighted fairness and show how
this can achieve various fairness definitions, such as those mentioned in the
ATM Forum TM 4.0 Specifications. We discuss how a pricing policy can be mapped
to general weighted (GW) fairness. The GW fairness can be achieved by
calculating the $ExcessFairshare$ (weighted fairshare of the left over
bandwidth) for each VC. We show how a switch algorithm can be modified to
support the GW fairness by using the $ExcessFairshare$. We use ERICA+ as an
example switch algorithm and show how it can be modified to achieve the general
fairness. Simulations results are presented to demonstrate that the modified
switch algorithm achieves GW fairness. An analytical proof for convergence of
the modified ERICA+ algorithm is given in the appendix.

<id>
cs/9809043v1
<category>
cs.NI
<abstract>
ATM (asynchronous transfer mode) is the technology chosen for the Broadband
Integrated Services Digital Network (B-ISDN). The ATM ABR (available bit rate)
service can be used to transport ``best-effort'' traffic. In this paper, we
extend our earlier work on the buffer requirements problem for TCP over ABR.
Here, a worst case scenario is generated such that TCP sources send a burst of
data at the time when the sources have large congestion windows and the ACRs
(allowed cell rates) for ABR are high. We find that ABR using the ERICA+ switch
algorithm can control the maximum queue lengths (hence the buffer requirements)
even for the worst case. We present analytical arguments for the expected queue
length and simulation results for different number of sources values and
parameter values.

<id>
cs/9809045v1
<category>
cs.NI
<abstract>
Compressed video is well known to be self-similar in nature. We model VBR
carrying Long-Range Dependent (LRD), multiplexed MPEG-2 video sources. The
actual traffic for the model is generated using fast-fourier transform of
generate the fractional gaussian noise (FGN) sequence. Our model of compressed
video sources bears similarity to an MPEG-2 Transport Stream carrying video,
i.e., it is long-range dependent and generates traffic in a piecewise-CBR
fashion. We study the effect of such VBR traffic on ABR carrying TCP traffic.
The effect of such VBR traffic is that the ABR capacity is highly variant. We
find that a switch algorithm like ERICA+ can tolerate this variance in ABR
capacity while maintaining high throughput and low delay. We present simulation
results for terrestrial and satellite configurations.

<id>
cs/9809046v1
<category>
cs.NI
<abstract>
In multipoint-to-point connections, the traffic at the root (destination) is
the combination of all traffic originating at the leaves. A crucial concern in
the case of multiple senders is how to define fairness within a multicast group
and among groups and point-to-point connections. Fairness definition can be
complicated since the multipoint connection can have the same identifier
(VPI/VCI) on each link, and senders might not be distinguishable in this case.
Many rate allocation algorithms implicitly assume that there is only one sender
in each VC, which does not hold for multipoint-to-point cases. We give various
possibilities for defining fairness for multipoint connections, and show the
tradeoffs involved. In addition, we show that ATM bandwidth allocation
algorithms need to be adapted to give fair allocations for multipoint-to-point
connections.

<id>
cs/9809047v1
<category>
cs.NI
<abstract>
Asynchronous transfer mode (ATM) is the new generation of computer and
communication networks that are being deployed throughout the telecommunication
industry as well as in campus backbones. ATM technology distinguishes itself
from the previous networking protocols in that it has the latest traffic
management technology and thus allows guaranteeing delay, throughput, and other
performance measures. This in turn, allows users to integrate voice, video, and
data on the same network. Available bit rate (ABR) service in ATM has been
designed to fairly distribute all unused capacity to data traffic and is
specified in the ATM Forum's Traffic Management (TM4.0) standard. This paper
will describe the OPNET models that have been developed for ATM and ABR design
and analysis.

<id>
cs/9809048v1
<category>
cs.NI
<abstract>
In this paper we describe a hands-on laboratory oriented instructional
package that we have developed for data communications and networking. The
package consists of a software tool, together with instructional material for a
laboratory based networking curriculum. The software is based on a simulation
environment that enables the student to experiment with various networking
protocols, on an easy to use graphical user interface (GUI). Data message
flows, packet losses, control/routing message flows, virtual circuit setups,
link failures, bit errors etc., are some of the features that can be visualized
in this environment. The student can also modify the networking components
provided, as well as add new components using the C programming language. The
instructional material consists of a set of laboratory exercises for flow and
error control (HDLC), IEEE 802.3 CSMA/CD protocol, the token ring protocol,
interconnecting LANs via bridges, TCP congestion avoidance and control, IP
fragmentation and reassembly, ATM PNNI routing and ATM policing. The laboratory
exercises have facilitated the development of a networking curriculum based on
both the traditional computer networking principles, as well as the new
technologies in telecommunication networking. The laboratory environment has
been used in the networking curriculum at The Ohio State University, and is
being piloted at other universities. The entire package is freely available
over the Internet.

<id>
cs/9809052v1
<category>
cs.NI
<abstract>
In this paper we present a model to study the end-to-end delay performance of
a satellite-ATM netowrk. We describe a satellite-ATM network architecture. The
architecture presents a trade-off between the on-board switching/processing
features and the complexity of the satellite communication systems. The
end-to-end delay of a connection passing through a satellite constellation
consists of the transmission delay, the uplink and downlink ground
terminal-satellite propagation delay, the inter-satellite link delays, the
on-board switching, processing and buffering delays. In a broadband satellite
network, the propagation and the buffering delays have the most impact on the
overall delay. We present an analysis of the propagation and buffering delay
components for GEO and LEO systems. We model LEO constellations as satellites
evenly spaced in circular orbits around the earth. A simple routing algorithm
for LEO systems calculates locally optimal paths for the end-to-end connection.
This is used to calculate the end-to-end propagation delays for LEO networks.
We present a simulation model to calculate the buffering delay for TCP/IP
traffic over ATM ABR and UBR service categories. We apply this model to
calculate total end-to-end delays for TCP/IP over satellite-ATM networks.

<id>
cs/9809053v1
<category>
cs.NI
<abstract>
In this paper we study the design issues in improving TCP performance over
the ATM UBR service. ATM-UBR switches respond to congestion by dropping cells
when their buffers become full. TCP connections running over UBR can experience
low throughput and high unfairness. Intelligent switch drop policies and
end-system policies can improve the performance of TCP over UBR with limited
buffers. We describe the various design options available to the network as
well as to the end systems to improve TCP performance over UBR. We study the
effects of Early Packet Discard, and two per-VC accounting based buffer
management policies. We also study the effects of various TCP end system
congestion control policies including slow start and congestion avoidance, fast
retransmit and recovery and selective acknowledgments. We present simulation
results for various small and large latency configurations with varying buffer
sizes and number of sources.

<id>
cs/9809054v1
<category>
cs.NI
<abstract>
Recent enhancements have been proposed to the ATM Unspecified Bit Rate (UBR)
service that guarantee a minimum rate at the frame level to the UBR VCs. These
enhancements have been called Guaranteed Frame Rate (GFR). In this paper, we
discuss the motivation, design and implementation issues for GFR. We present
the design of buffer management and policing mechanisms to implement GFR. We
study the effects of policing, per-VC buffer allocation, and per-VC queuing on
providing GFR to TCP/IP traffic. We conclude that per-VC scheduling is
necessary to provide minimum rate guarantees to TCP traffic. We examine the
role of frame tagging in the presence of scheduling and buffer management for
providing minumum rate guarantees. The use of GFR to support the Internet
Controlled Load Service is also discussed.

<id>
cs/9809055v1
<category>
cs.NI
<abstract>
The ATM Guaranteed Frame Rate (GFR) service is intended for best effort
traffic that can benefit from minimum throughput guarantees. Edge devices
connecting LANs to an ATM network can use GFR to transport multiple TCP/IP
connections over a single GFR VC.These devices would typically multiplex VCs
into a single FIFO queue. It has been shown that in general, FIFO queuing is
not sufficient to provide rate guarantees, and per-VC queuing with scheduling
is needed. We show that under conditions of low buffer allocation, it is
possible to control TCP rates with FIFO queuing and buffer management. We
present analysis and simulation results on controlling TCP rates by buffer
management. We present a buffer management policy that provides loose rate
guarantees to SACK TCP sources when the total buffer allocation is low. We
study the performance of this buffer management scheme by simulation.

<id>
cs/9809056v1
<category>
cs.NI
<abstract>
In performance analysis and design of communication netword modeling data
traffic is important. With introduction of new applications, the
characteristics of the data traffic changes. We present a brief review the
different models of data traffic and how they have evolved. We present results
of data traffic analysis and simulated traffic, which demonstrates that the
packet train model fits the traffic at source destination level and long-memory
(self-similar) model fits the traffic at the aggregate level.

<id>
cs/9809057v1
<category>
cs.NI
<abstract>
The ABR service is designed to fairly allocate the bandwidth unused by higher
priority services. The network indicates to the ABR sources the rates at which
they should transmit to minimize their cell loss. Switches must constantly
measure the demand and available capacity, and divide the capacity fairly among
the contending connections. In order to compute the fair and efficient
allocation for each connection, a switch needs to determine the effective
number of active connections. In this paper, we propose a method for
determining the number of active connections and the fair bandwidth share for
each. We prove the efficiency and fairness of the proposed method analytically,
and simulate it by incorporating it into the ERICA switch algorithm.

<id>
cs/9809058v1
<category>
cs.NI
<abstract>
The OSU scheme is a rate-based congestion avoidance scheme for ATM networks
using explicit rate indication. This work was one of the first attempts to
define explicit rate switch mechanisms and the Resource Management (RM) cell
format in Asynchronous Transfer Mode (ATM) networks. The key features of the
scheme include explicit rate feedback, congestion avoidance, fair operation
while maintaining high utilization, use of input rate as a congestion metric,
O(1) complexity. This paper presents an overview of the scheme, presents those
features of the scheme that have now become common features of other switch
algorithms and discusses three extensions of the scheme.

<id>
cs/9809059v1
<category>
cs.NI
<abstract>
We propose an explicit rate indication scheme for congestion avoidance in ATM
networks. In this scheme, the network switches monitor their load on each link,
determining a load factor, the available capacity, and the number of currently
active virtual channels. This information is used to advise the sources about
the rates at which they should transmit. The algorithm is designed to achieve
efficiency, fairness, controlled queueing delays, and fast transient response.
The algorithm is also robust to measurement errors caused due to variation in
ABR demand and capacity. We present performance analysis of the scheme using
both analytical arguments and simulation results. The scheme is being
implemented by several ATM switch manufacturers.

<id>
cs/9809062v1
<category>
cs.NI
<abstract>
In this paper, we have provided a summary of the design options in
Satellite-ATM technology. A satellite ATM network consists of a space segment
of satellites connected by inter-satellite crosslinks, and a ground segment of
the various ATM networks. A satellite-ATM interface module connects the
satellite network to the ATM networks and performs various call and control
functions. A network control center performs various network management and
resource allocation functions. Several issues such as the ATM service model,
media access protocols, and traffic management issues must be considered when
designing a satellite ATM network to effectively transport Internet traffic. We
have presented the buffer requirements for TCP/IP traffic over ATM-UBR for
satellite latencies. Our results are based on TCP with selective
acknowledgments and a per-VC buffer management policy at the switches. A buffer
size of about 0.5 * RTT to 1 * RTT is sufficient to provide over 98% throughput
to infinite TCP traffic for long latency networks and a large number of
sources. This buffer requirement is independent of the number of sources. The
fairness is high for a large numbers of sources because of the per-VC buffer
management performed at the switches and the nature of TCP traffic.

<id>
cs/9809063v1
<category>
cs.NI
<abstract>
We model World Wide Web (WWW) servers and clients running over an ATM network
using the ABR (available bit rate) service. The WWW servers are modeled using a
variant of the SPECweb96 benchmark, while the WWW clients are based on a model
by Mah. The traffic generated by this application is typically bursty, i.e., it
has active and idle periods in transmission. A timeout occurs after given
amount of idle period. During idle period the underlying TCP congestion windows
remain open until a timeout expires. These open windows may be used to send
data in a burst when the application becomes active again. This raises the
possibility of large switch queues if the source rates are not controlled by
ABR. We study this problem and show that ABR scales well with a large number of
bursty TCP sources in the system.

<id>
cs/9809065v1
<category>
cs.NI
<abstract>
ABR traffic management for point-to-multipoint connections controls the
source rate to the minimum rate supported by all the branches of the multicast
tree. A number of algorithms have been developed for extending ABR congestion
avoidance algorithms to perform feedback consolidation at the branch points.
This paper discusses various design options and implementation alternatives for
the consolidation algorithms, and proposes a number of new algorithms. The
performance of the proposed algorithms and the previous algorithms is compared
under a variety of conditions. Results indicate that the algorithms we propose
eliminate the consolidation noise (caused if the feedback is returned before
all branches respond), while exhibiting a fast transient response.

<id>
cs/9809066v1
<category>
cs.NI
<abstract>
We study the performance of Selective Acknowledgments with TCP over the
ATM-UBR service category. We examine various UBR drop policies, TCP mechanisms
and network configurations to recommend optimal parameters for TCP over UBR. We
discuss various TCP congestion control mechanisms compare their performance for
LAN and WAN networks. We describe the effect of satellite delays on TCP
performance over UBR and present simulation results for LAN, WAN and satellite
networks. SACK TCP improves the performance of TCP over UBR, especially for
large delay networks. Intelligent drop policies at the switches are an
important factor for good performance in local area networks.

<id>
cs/9809067v1
<category>
cs.NI
<abstract>
Asynchronous transfer mode (ATM) networks must define multicast capabilities
in order to efficiently support numerous applications, such as video
conferencing and distributed applications, in addition to LAN emulation (LANE)
and Internet protocol (IP) multicasting. Several problems and issues arise in
ATM multicasting, such as signaling, routing, connection admission control, and
traffic management problems. IP integrated services over ATM poses further
challenges to ATM multicasting. Scalability and simplicity are the two main
concerns for ATM multicasting. This paper provides a survey of the current work
on multicasting problems in general, and ATM multicasting in particular. A
number of proposed schemes is examined, such as the schemes MARS, MCS, SEAM,
SMART, RSVP, and various multipoint traffic management and transport-layer
schemes. The paper also indicates a number of key open issues that remain
unresolved.

<id>
cs/9809068v1
<category>
cs.NI
<abstract>
The testing group at ATM Forum is working on developing a specification for
performance testing of ATM switches and networks. The emphasis is on the user
perceived frame-level performance. This paper explains what is different about
this new effort and gives its status.

<id>
cs/9809069v1
<category>
cs.NI
<abstract>
The Available Bit Rate (ABR) service in ATM networks has been specified to
allow fair and efficient support of data applications over ATM utilizing
capacity left over after servicing higher priority classes. One of the
architectural features in the ABR specification [tm4] is the Virtual
Source/Virtual Destination (VS/VD) option. This option allows a switch to
divide an end-to-end ABR connection into separately controlled ABR segments by
acting like a destination on one segment, and like a source on the other. The
coupling in the VS/VD switch between the two ABR control segments is
implementation specific. In this paper, we model a VS/VD ATM switch and study
the issues in designing coupling between ABR segments. We identify a number of
implementation options for the coupling. A good choice significantly improves
the stability and transient performance of the system and reduces the buffer
requirements at the switches.

<id>
cs/9809070v1
<category>
cs.NI
<abstract>
The Available Bit Rate (ABR) service has been developed to support 21st
century data applications over Asynchronous Transfer Mode (ATM). The ABR
service uses a closed-loop rate-based traffic management framework where the
network divides left-over bandwidth among contending sources. The ATM Forum
traffic management group also incorporated open-loop control capabilities to
make the ABR service robust to temporary network failures and source
inactivity. An important problem addressed was whether rate allocations of
sources should be taken away if sources do not use them. The proposed
solutions, popularly known as the Use-It-or-Lose-It (UILI) policies, have had
significant impact on the ABR service capabilities. In this paper we discuss
the design, development, and the final shape of these policies and their impact
on the ABR service. We compare the various alternatives through a performance
evaluation.

<id>
cs/9809071v1
<category>
cs.NI
<abstract>
ATM-UBR switches respond to congestion by dropping cells when their buffers
become full. TCP connections running over UBR experience low throughput and
high unfairness. For 100% TCP throughput each switch needs buffers equal to the
sum of the window sizes of all the TCP connections. Intelligent drop policies
can improve the performance of TCP over UBR with limited buffers. The UBR+
service proposes enhancements to UBR for intelligent drop. Early Packet Discard
improves throughput but does not attempt to improve fairness. Selective packet
drop based on per-connection buffer occupancy improves fairness. The Fair
Buffer Allocation scheme further improves both throughput and fairness.

<id>
cs/9809072v1
<category>
cs.NI
<abstract>
We extend our earlier studies of buffer requirements of TCP over ABR in two
directions. First, we study the performance of TCP over ABR in an ATM backbone.
On the backbone, the TCP queues are at the edge router and not inside the ATM
network. The router requires buffer equal to the sum of the receiver window
sizes of the participating TCP connections. Second, we introduce various
patterns of VBR background traffic. The VBR background introduces variance in
the ABR capacity and the TCP traffic introduces variance in the ABR demand.
Some simple switch schemes are unable to keep up with the combined effect of
highly varying demands and highly varying ABR capacity. We present our
experiences with refining the ERICA+ switch scheme to handle these conditions.

<id>
cs/9809073v1
<category>
cs.NI
<abstract>
The Asynchronous Transfer Mode (ATM) networks are quickly being adopted as
backbones over various parts of the Internet. This paper analyzes the
performance of TCP/IP protocols over ATM network's Available Bit Rate (ABR) and
Unspecified Bit Rate (UBR) services. It is shown that ABR pushes congestion to
the edges of the ATM network while UBR leaves it inside the ATM portion.

<id>
cs/9809074v1
<category>
cs.NI
<abstract>
We study the buffering requirements for zero cell loss for TCP/IP over
satellite links using the available bit rate (ABR) and unspecified bit rate
(UBR) services of asynchronous transfer mode (ATM) networks. For the ABR
service, we explore the effect of feedback delay (a factor which depends upon
the position of the bottleneck), the switch scheme used, and background
variable bit rate (VBR) traffic. It is shown that the buffer requirement for
TCP over ABR is independent of the number of TCP sources, but depends on the
aforementioned factors. For the UBR service, we show that the buffer
requirement is the sum of the TCP receiver window sizes. We substantiate our
arguments with simulation results.

<id>
cs/9809049v1
<category>
cs.NE
<abstract>
This paper examines the four main types of Evolutionary Design by computers:
Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial
Life Forms and Creative Evolutionary Design. Definitions for all four areas are
provided. A review of current work in each of these areas is given, with
examples of the types of applications that have been tackled. The different
properties and requirements of each are examined. Descriptions of typical
representations and evolutionary algorithms are provided and examples of
designs evolved using these techniques are shown. The paper then discusses how
the boundaries of these areas are beginning to merge, resulting in four new
'overlapping' types of Evolutionary Design: Integral Evolutionary Design,
Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and
Aesthetic Evolutionary Design. Finally, the last part of the paper discusses
some common problems faced by creators of Evolutionary Design systems,
including: interdependent elements in designs, epistasis, and constraint
handling.

<id>
cs/9812002v1
<category>
cs.NE
<abstract>
A new training algorithm is presented for delayed reinforcement learning
problems that does not assume the existence of a critic model and employs the
polytope optimization algorithm to adjust the weights of the action network so
that a simple direct measure of the training performance is maximized.
Experimental results from the application of the method to the pole balancing
problem indicate improved training performance compared with critic-based and
genetic reinforcement approaches.

<id>
cs/9902025v1
<category>
cs.NE
<abstract>
A mean field feedback artificial neural network algorithm is developed and
explored for the set covering problem. A convenient encoding of the inequality
constraints is achieved by means of a multilinear penalty function. An
approximate energy minimum is obtained by iterating a set of mean field
equations, in combination with annealing. The approach is numerically tested
against a set of publicly available test problems with sizes ranging up to
5x10^3 rows and 10^6 columns. When comparing the performance with exact results
for sizes where these are available, the approach yields results within a few
percent from the optimal solutions. Comparisons with other approximate methods
also come out well, in particular given the very low CPU consumption required
-- typically a few seconds. Arbitrary problems can be processed using the
algorithm via a public domain server.

<id>
cs/0110021v1
<category>
cs.NE
<abstract>
The process of evolutionary emergence of purposeful adaptive behavior is
investigated by means of computer simulations. The model proposed implies that
there is an evolving population of simple agents, which have two natural needs:
energy and reproduction. Any need is characterized quantitatively by a
corresponding motivation. Motivations determine goal-directed behavior of
agents. The model demonstrates that purposeful behavior does emerge in the
simulated evolutionary processes. Emergence of purposefulness is accompanied by
origin of a simple hierarchy in the control system of agents.

<id>
cs/0201024v1
<category>
cs.NE
<abstract>
In general, we can not use algebraic or enumerative methods to optimize a
quality control (QC) procedure so as to detect the critical random and
systematic analytical errors with stated probabilities, while the probability
for false rejection is minimum. Genetic algorithms (GAs) offer an alternative,
as they do not require knowledge of the objective function to be optimized and
search through large parameter spaces quickly. To explore the application of
GAs in statistical QC, we have developed an interactive GAs based computer
program that designs a novel near optimal QC procedure, given an analytical
process. The program uses the deterministic crowding algorithm. An illustrative
application of the program suggests that it has the potential to design QC
procedures that are significantly better than 45 alternative ones that are used
in the clinical laboratories.

<id>
cs/0210012v1
<category>
cs.NE
<abstract>
A new general procedure for a priori selection of more predictable events
from a time series of observed variable is proposed. The procedure is
applicable to time series which contains different types of events that feature
significantly different predictability, or, in other words, to heteroskedastic
time series. A priori selection of future events in accordance to expected
uncertainty of their forecasts may be helpful for making practical decisions.
The procedure first implies creation of two neural network based forecasting
models, one of which is aimed at prediction of conditional mean and other -
conditional dispersion, and then elaboration of the rule for future event
selection into groups of more and less predictable events. The method is
demonstrated and tested by the example of the computer generated time series,
and then applied to the real world time series, Dow Jones Industrial Average
index.

<id>
cs/0212019v1
<category>
cs.NE
<abstract>
Ever increasing computational power will require methods for automatic
programming. We present an alternative to genetic programming, based on a
general model of thinking and learning. The advantage is that evolution takes
place in the space of constructs and can thus exploit the mathematical
structures of this space. The model is formalized, and a macro language is
presented which allows for a formal yet intuitive description of the problem
under consideration. A prototype has been developed to implement the scheme in
PERL. This method will lead to a concentration on the analysis of problems, to
a more rapid prototyping, to the treatment of new problem classes, and to the
investigation of philosophical problems. We see fields of application in
nonlinear differential equations, pattern recognition, robotics, model
building, and animated pictures.

<id>
cs/0302002v1
<category>
cs.NE
<abstract>
GoTools is a program which solves life & death problems in the game of Go.
This paper describes experiments using a Genetic Algorithm to optimize
heuristic weights used by GoTools' tree-search. The complete set of heuristic
weights is composed of different subgroups, each of which can be optimized with
a suitable fitness function. As a useful side product, an MPI interface for
FreePascal was implemented to allow the use of a parallelized fitness function
running on a Beowulf cluster. The aim of this exercise is to optimize the
current version of GoTools, and to make tools available in preparation of an
extension of GoTools for solving open boundary life & death problems, which
will introduce more heuristic parameters to be fine tuned.

<id>
cs/0306125v1
<category>
cs.NE
<abstract>
In the present paper a newer application of Artificial Neural Network (ANN)
has been developed i.e., predicting response-function results of
electrical-mechanical system through ANN. This method is specially useful to
complex systems for which it is not possible to find the response-function
because of complexity of the system. The proposed approach suggests that how
even without knowing the response-function, the response-function results can
be predicted with the use of ANN to the system. The steps used are: (i)
Depending on the system, the ANN-architecture and the input & output parameters
are decided, (ii) Training & test data are generated from simplified circuits
and through tactic-superposition of it for complex circuits, (iii) Training the
ANN with training data through many cycles and (iv) Test-data are used for
predicting the response-function results. It is found that the proposed novel
method for response prediction works satisfactorily. Thus this method could be
used specially for complex systems where other methods are unable to tackle it.
In this paper the application of ANN is particularly demonstrated to
electrical-circuit system but can be applied to other systems too.

<id>
cs/0309038v1
<category>
cs.NE
<abstract>
We introduce a novel evolutionary formulation of the problem of finding a
maximum independent set of a graph. The new formulation is based on the
relationship that exists between a graph's independence number and its acyclic
orientations. It views such orientations as individuals and evolves them with
the aid of evolutionary operators that are very heavily based on the structure
of the graph and its acyclic orientations. The resulting heuristic has been
tested on some of the Second DIMACS Implementation Challenge benchmark graphs,
and has been found to be competitive when compared to several of the other
heuristics that have also been tested on those graphs.

<id>
cs/0309039v1
<category>
cs.NE
<abstract>
We introduce two novel evolutionary formulations of the problem of coloring
the nodes of a graph. The first formulation is based on the relationship that
exists between a graph's chromatic number and its acyclic orientations. It
views such orientations as individuals and evolves them with the aid of
evolutionary operators that are very heavily based on the structure of the
graph and its acyclic orientations. The second formulation, unlike the first
one, does not tackle one graph at a time, but rather aims at evolving a
`program' to color all graphs belonging to a class whose members all have the
same number of nodes and other common attributes. The heuristics that result
from these formulations have been tested on some of the Second DIMACS
Implementation Challenge benchmark graphs, and have been found to be
competitive when compared to the several other heuristics that have also been
tested on those graphs.

<id>
cs/0310009v3
<category>
cs.NE
<abstract>
This paper studies how the generalization ability of neurons can be affected
by mutual processing of different signals. This study is done on the basis of a
feedforward artificial neural network. The mutual processing of signals can
possibly be a good model of patterns in a set generalized by a neural network
and in effect may improve generalization. In this paper it is discussed that
the interference may also cause a highly random generalization. Adaptive
activation functions are discussed as a way of reducing that type of
generalization. A test of a feedforward neural network is performed that shows
the discussed random generalization.

<id>
cs/0310050v4
<category>
cs.NE
<abstract>
In this paper, feedforward neural networks are presented that have nonlinear
weight functions based on look--up tables, that are specially smoothed in a
regularization called the diffusion. The idea of such a type of networks is
based on the hypothesis that the greater number of adaptive parameters per a
weight function might reduce the total number of the weight functions needed to
solve a given problem. Then, if the computational complexity of a propagation
through a single such a weight function would be kept low, then the introduced
neural networks might possibly be relatively fast.
  A number of tests is performed, showing that the presented neural networks
may indeed perform better in some cases than the classic neural networks and a
number of other learning machines.

<id>
cs/0312047v1
<category>
cs.NE
<abstract>
Websites of a particular class form increasingly complex networks, and new
tools are needed to map and understand them. A way of visualizing this complex
network is by mapping it. A map highlights which members of the community have
similar interests, and reveals the underlying social network. In this paper, we
will map a network of websites using Kohonen's self-organizing map (SOM), a
neural-net like method generally used for clustering and visualization of
complex data sets. The set of websites considered has been the Blogalia weblog
hosting site (based at http://www.blogalia.com/), a thriving community of
around 200 members, created in January 2002. In this paper we show how SOM
discovers interesting community features, its relation with other
community-discovering algorithms, and the way it highlights the set of
communities formed over the network.

<id>
cs/0402047v1
<category>
cs.NE
<abstract>
This paper presents a parameter-less optimization framework that uses the
extended compact genetic algorithm (ECGA) and iterated local search (ILS), but
is not restricted to these algorithms. The presented optimization algorithm
(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),
where the parameters of a selecto-recombinative GA are eliminated. The approach
that we propose is tested on several well known problems. In the absence of
domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use
optimization method.

<id>
cs/0402049v1
<category>
cs.NE
<abstract>
This paper presents an architecture which is suitable for a massive
parallelization of the compact genetic algorithm. The resulting scheme has
three major advantages. First, it has low synchronization costs. Second, it is
fault tolerant, and third, it is scalable.
  The paper argues that the benefits that can be obtained with the proposed
approach is potentially higher than those obtained with traditional parallel
genetic algorithms. In addition, the ideas suggested in the paper may also be
relevant towards parallelizing more complex probabilistic model building
genetic algorithms.

<id>
cs/0402050v1
<category>
cs.NE
<abstract>
This paper makes a number of connections between life and various facets of
genetic and evolutionary algorithms research. Specifically, it addresses the
topics of adaptation, multiobjective optimization, decision making, deception,
and search operators, among others. It argues that human life, from birth to
death, is an adaptive or dynamic optimization problem where people are
continuously searching for happiness. More important, the paper speculates that
genetic algorithms can be used as a source of inspiration for helping people
make decisions in their everyday life.

<id>
cs/0403003v1
<category>
cs.NE
<abstract>
Recently, researchers have applied genetic algorithms (GAs) to address some
problems in quantum computation. Also, there has been some works in the
designing of genetic algorithms based on quantum theoretical concepts and
techniques. The so called Quantum Evolutionary Programming has two major
sub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic
Algorithms (QGAs). The former adopts qubit chromosomes as representations and
employs quantum gates for the search of the best solution. The later tries to
solve a key question in this field: what GAs will look like as an
implementation on quantum hardware? As we shall see, there is not a complete
answer for this question. An important point for QGAs is to build a quantum
algorithm that takes advantage of both the GA and quantum computing parallelism
as well as true randomness provided by quantum computers. In the first part of
this paper we present a survey of the main works in GAs plus quantum computing
including also our works in this area. Henceforth, we review some basic
concepts in quantum computation and GAs and emphasize their inherent
parallelism. Next, we review the application of GAs for learning quantum
operators and circuit design. Then, quantum evolutionary programming is
considered. Finally, we present our current research in this field and some
perspectives.

<id>
cs/0405062v1
<category>
cs.NE
<abstract>
This paper presents two different efficiency-enhancement techniques for
probabilistic model building genetic algorithms. The first technique proposes
the use of a mutation operator which performs local search in the sub-solution
neighborhood identified through the probabilistic model. The second technique
proposes building and using an internal probabilistic model of the fitness
along with the probabilistic model of variable interactions. The fitness values
of some offspring are estimated using the probabilistic model, thereby avoiding
computationally expensive function evaluations. The scalability of the
aforementioned techniques are analyzed using facetwise models for convergence
time and population sizing. The speed-up obtained by each of the methods is
predicted and verified with empirical results. The results show that for
additively separable problems the competent mutation operator requires O(k 0.5
logm)--where k is the building-block size, and m is the number of building
blocks--less function evaluations than its selectorecombinative counterpart.
The results also show that the use of an internal probabilistic fitness model
reduces the required number of function evaluations to as low as 1-10% and
yields a speed-up of 2--50.

<id>
cs/0405063v1
<category>
cs.NE
<abstract>
This paper analyzes the relative advantages between crossover and mutation on
a class of deterministic and stochastic additively separable problems. This
study assumes that the recombination and mutation operators have the knowledge
of the building blocks (BBs) and effectively exchange or search among competing
BBs. Facetwise models of convergence time and population sizing have been used
to determine the scalability of each algorithm. The analysis shows that for
additively separable deterministic problems, the BB-wise mutation is more
efficient than crossover, while the crossover outperforms the mutation on
additively separable problems perturbed with additive Gaussian noise. The
results show that the speed-up of using BB-wise mutation on deterministic
problems is O(k^{0.5}logm), where k is the BB size, and m is the number of BBs.
Likewise, the speed-up of using crossover on stochastic problems with fixed
noise variance is O(mk^{0.5}log m).

<id>
cs/0405064v1
<category>
cs.NE
<abstract>
This paper presents a competent selectomutative genetic algorithm (GA), that
adapts linkage and solves hard problems quickly, reliably, and accurately. A
probabilistic model building process is used to automatically identify key
building blocks (BBs) of the search problem. The mutation operator uses the
probabilistic model of linkage groups to find the best among competing building
blocks. The competent selectomutative GA successfully solves additively
separable problems of bounded difficulty, requiring only subquadratic number of
function evaluations. The results show that for additively separable problems
the probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and
requires O(k^{0.5}logm) less function evaluations than its selectorecombinative
counterpart, confirming theoretical results reported elsewhere (Sastry &
Goldberg, 2004).

<id>
cs/0405065v1
<category>
cs.NE
<abstract>
This paper studies fitness inheritance as an efficiency enhancement technique
for a class of competent genetic algorithms called estimation distribution
algorithms. Probabilistic models of important sub-solutions are developed to
estimate the fitness of a proportion of individuals in the population, thereby
avoiding computationally expensive function evaluations. The effect of fitness
inheritance on the convergence time and population sizing are modeled and the
speed-up obtained through inheritance is predicted. The results show that a
fitness-inheritance mechanism which utilizes information on building-block
fitnesses provides significant efficiency enhancement. For additively separable
problems, fitness inheritance reduces the number of function evaluations to
about half and yields a speed-up of about 1.75--2.25.

<id>
cs/0501005v1
<category>
cs.NE
<abstract>
In this paper we apply a heuristic method based on artificial neural networks
in order to trace out the efficient frontier associated to the portfolio
selection problem. We consider a generalization of the standard Markowitz
mean-variance model which includes cardinality and bounding constraints. These
constraints ensure the investment in a given number of different assets and
limit the amount of capital to be invested in each asset. We present some
experimental results obtained with the neural network heuristic and we compare
them to those obtained with three previous heuristic methods.

<id>
cs/0503078v1
<category>
cs.NE
<abstract>
This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network
(NFN-MK), an hybrid computational model that combines fuzzy system technique
and artificial neural networks. Its main task consists in the automatic
generation of membership functions, in particular, triangle forms, aiming a
dynamic modeling of a system. The model is tested by simulating real systems,
here represented by a nonlinear mathematical function. Comparison with the
results obtained by traditional neural networks, and correlated studies of
neurofuzzy systems applied in system identification area, shows that the NFN-MK
model has a similar performance, despite its greater simplicity.

<id>
cs/0504053v1
<category>
cs.NE
<abstract>
We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment.

<id>
cs/0505003v1
<category>
cs.NE
<abstract>
The Hopfield network has been applied to solve optimization problems over
decades. However, it still has many limitations in accomplishing this task.
Most of them are inherited from the optimization algorithms it implements. The
computation of a Hopfield network, defined by a set of difference equations,
can easily be trapped into one local optimum or another, sensitive to initial
conditions, perturbations, and neuron update orders. It doesn't know how long
it will take to converge, as well as if the final solution is a global optimum,
or not. In this paper, we present a Hopfield network with a new set of
difference equations to fix those problems. The difference equations directly
implement a new powerful optimization algorithm.

<id>
cs/0505016v1
<category>
cs.NE
<abstract>
The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines.

<id>
cs/0505019v1
<category>
cs.NE
<abstract>
The Artificial Neural network is a functional imitation of simplified model
of the biological neurons and their goal is to construct useful computers for
real world problems. The ANN applications have increased dramatically in the
last few years fired by both theoretical and practical applications in a wide
variety of applications. A brief theory of ANN is presented and potential areas
are identified and future trends are discussed.

<id>
cs/0505021v3
<category>
cs.NE
<abstract>
This paper discusses the notion of generalization of training samples over
long distances in the input space of a feedforward neural network. Such a
generalization might occur in various ways, that differ in how great the
contribution of different training features should be.
  The structure of a neuron in a feedforward neural network is analyzed and it
is concluded, that the actual performance of the discussed generalization in
such neural networks may be problematic -- while such neural networks might be
capable for such a distant generalization, a random and spurious generalization
may occur as well.
  To illustrate the differences in generalizing of the same function by
different learning machines, results given by the support vector machines are
also presented.

<id>
cs/0505065v2
<category>
cs.NE
<abstract>
A dissipative particle swarm optimization is developed according to the
self-organization of dissipative structure. The negative entropy is introduced
to construct an opening dissipative system that is far-from-equilibrium so as
to driving the irreversible evolution process with better fitness. The testing
of two multimodal functions indicates it improves the performance effectively

<id>
cs/9907005v1
<category>
cs.NA
<abstract>
We propose alternative discriminant measures for selecting the best basis
among a large collection of orthonormal bases for classification purposes. A
generalization of the Local Discriminant Basis Algorithm of Saito and Coifman
is constructed. The success of these new methods is evaluated and compared to
earlier methods in experiments.

<id>
cs/0102023v1
<category>
cs.NA
<abstract>
This note addresses the input and output of intervals in the sense of
interval arithmetic and interval constraints. The most obvious, and so far most
widely used notation, for intervals has drawbacks that we remedy with a new
notation that we propose to call factored notation. It is more compact and
allows one to find a good trade-off between interval width and ease of reading.
We describe how such a trade-off can be based on the information yield (in the
sense of information theory) of the last decimal shown.

<id>
cs/0201015v1
<category>
cs.NA
<abstract>
To analyse the significance of the digits used for interval bounds, we
clarify the philosophical presuppositions of various interval notations. We use
information theory to determine the information content of the last digit of
the numeral used to denote the interval's bounds. This leads to the notion of
efficiency of a decimal digit: the actual value as percentage of the maximal
value of its information content. By taking this efficiency into account, many
presentations of intervals can be made more readable at the expense of
negligible loss of information.

<id>
cs/0206031v1
<category>
cs.NA
<abstract>
We show that S.Vavasis' sufficient condition for global invertibility of a
polynomial mapping can be easily generalized to the case of a general Lipschitz
mapping. Keywords: Invertibility conditions, generalized Jacobian, nonsmooth
analysis.

<id>
cs/0210015v2
<category>
cs.NA
<abstract>
We consider the prospect of a processor that can perform interval arithmetic
at the same speed as conventional floating-point arithmetic. This makes it
possible for all arithmetic to be performed with the superior security of
interval methods without any penalty in speed. In such a situation the IEEE
floating-point standard needs to be compared with a version of floating-point
arithmetic that is ideal for the purpose of interval arithmetic. Such a
comparison requires a succinct and complete exposition of interval arithmetic
according to its recent developments. We present such an exposition in this
paper. We conclude that the directed roundings toward the infinities and the
definition of division by the signed zeros are valuable features of the
standard. Because the operations of interval arithmetic are always defined,
exceptions do not arise. As a result neither Nans nor exceptions are needed. Of
the status flags, only the inexact flag may be useful. Denormalized numbers
seem to have no use for interval arithmetic; in the use of interval
constraints, they are a handicap.

<id>
cs/0306015v1
<category>
cs.NA
<abstract>
There are several numerical methods for computing approximate zeros of a
given univariate polynomial. In this paper, we develop a simple and novel
method for determining sharp upper bounds on errors in approximate zeros of a
given polynomial using Rouche's theorem from complex analysis. We compute the
error bounds using non-linear optimization. Our bounds are scalable in the
sense that we compute sharper error bounds for better approximations of zeros.
We use high precision computations using the LEDA/real floating-point filter
for computing our bounds robustly.

<id>
cs/0404034v1
<category>
cs.NA
<abstract>
Numerical analysis has no satisfactory method for the more realistic
optimization models. However, with constraint programming one can compute a
cover for the solution set to arbitrarily close approximation. Because the use
of constraint propagation for composite arithmetic expressions is
computationally expensive, consistency is computed with interval arithmetic. In
this paper we present theorems that support, selective initialization, a simple
modification of constraint propagation that allows composite arithmetic
expressions to be handled efficiently.

<id>
cs/0407022v4
<category>
cs.NA
<abstract>
We consider linear systems arising from the use of the finite element method
for solving scalar linear elliptic problems. Our main result is that these
linear systems, which are symmetric and positive semidefinite, are well
approximated by symmetric diagonally dominant matrices. Our framework for
defining matrix approximation is support theory. Significant graph theoretic
work has already been developed in the support framework for preconditioners in
the diagonally dominant case, and in particular it is known that such systems
can be solved with iterative methods in nearly linear time. Thus, our
approximation result implies that these graph theoretic techniques can also
solve a class of finite element problems in nearly linear time. We show that
the support number bounds, which control the number of iterations in the
preconditioned iterative solver, depend on mesh quality measures but not on the
problem size or shape of the domain.

<id>
cs/0410045v4
<category>
cs.NA
<abstract>
We consider an algorithm called FEMWARP for warping triangular and
tetrahedral finite element meshes that computes the warping using the finite
element method itself. The algorithm takes as input a two- or three-dimensional
domain defined by a boundary mesh (segments in one dimension or triangles in
two dimensions) that has a volume mesh (triangles in two dimensions or
tetrahedra in three dimensions) in its interior. It also takes as input a
prescribed movement of the boundary mesh. It computes as output updated
positions of the vertices of the volume mesh. The first step of the algorithm
is to determine from the initial mesh a set of local weights for each interior
vertex that describes each interior vertex in terms of the positions of its
neighbors. These weights are computed using a finite element stiffness matrix.
After a boundary transformation is applied, a linear system of equations based
upon the weights is solved to determine the final positions of the interior
vertices. The FEMWARP algorithm has been considered in the previous literature
(e.g., in a 2001 paper by Baker). FEMWARP has been succesful in computing
deformed meshes for certain applications. However, sometimes FEMWARP reverses
elements; this is our main concern in this paper. We analyze the causes for
this undesirable behavior and propose several techniques to make the method
more robust against reversals. The most successful of the proposed methods
includes combining FEMWARP with an optimization-based untangler.

<id>
cs/0412009v1
<category>
cs.NA
<abstract>
In this paper, we show a way to exploit sparsity in the problem data in a
primal-dual potential reduction method for solving a class of semidefinite
programs. When the problem data is sparse, the dual variable is also sparse,
but the primal one is not. To avoid working with the dense primal variable, we
apply Fukuda et al.'s theory of partial matrix completion and work with partial
matrices instead. The other place in the algorithm where sparsity should be
exploited is in the computation of the search direction, where the gradient and
the Hessian-matrix product of the primal and dual barrier functions must be
computed in every iteration. By using an idea from automatic differentiation in
backward mode, both the gradient and the Hessian-matrix product can be computed
in time proportional to the time needed to compute the barrier functions of
sparse variables itself. Moreover, the high space complexity that is normally
associated with the use of automatic differentiation in backward mode can be
avoided in this case. In addition, we suggest a technique to efficiently
compute the determinant of the positive definite matrix completion that is
required to compute primal search directions. The method of obtaining one of
the primal search directions that minimizes the number of the evaluations of
the determinant of the positive definite completion is also proposed. We then
implement the algorithm and test it on the problem of finding the maximum cut
of a graph.

<id>
cs/0501041v1
<category>
cs.NA
<abstract>
The paper describes two iterative algorithms for solving general systems of M
simultaneous linear algebraic equations (SLAE) with real matrices of
coefficients. The system can be determined, underdetermined, and
overdetermined. Linearly dependent equations are also allowed. Both algorithms
use the method of Lagrange multipliers to transform the original SLAE into a
positively determined function F of real original variables X(i) (i=1,...,N)
and Lagrange multipliers Lambda(i) (i=1,...,M). Function F is differentiated
with respect to variables X(i) and the obtained relationships are used to
express F in terms of Lagrange multipliers Lambda(i). The obtained function is
minimized with respect to variables Lambda(i) with the help of one of two the
following minimization techniques: (1) relaxation method or (2) method of
conjugate gradients by Fletcher and Reeves. Numerical examples are given.

<id>
cs/0502059v1
<category>
cs.NA
<abstract>
Mathematical treatment of massive wall systems is a useful tool for
investigation of these solar applications. The objectives of this work are to
develop (and validate) a numerical solution model for predication the thermal
behaviour of passive solar systems with massive wall, to improve knowledge of
using indirect passive solar systems and assess its energy efficiency according
to climatic conditions in Bulgaria. The problem of passive solar systems with
massive walls is modelled by thermal and mass transfer equations. As a boundary
conditions for the mathematical problem are used equations, which describe
influence of weather data and constructive parameters of building on the
thermal performance of the passive system. The mathematical model is solved by
means of finite differences method and improved solution procedure. In article
are presented results of theoretical and experimental study for developing and
validating a numerical solution model for predication the thermal behaviour of
passive solar systems with massive wall.

<id>
cs/0502092v1
<category>
cs.NA
<abstract>
In this paper, we investigate the use of compactly supported divergence-free
wavelets for the representation of the Navier-Stokes solution. After reminding
the theoretical construction of divergence-free wavelet vectors, we present in
detail the bases and corresponding fast algorithms for 2D and 3D incompressible
flows. In order to compute the nonlinear term, we propose a new method which
provides in practice with the Hodge decomposition of any flow: this
decomposition enables us to separate the incompressible part of the flow from
its orthogonal complement, which corresponds to the gradient component of the
flow. Finally we show numerical tests to validate our approach.

<id>
cs/0503086v2
<category>
cs.NA
<abstract>
In this paper a new method of detection of homogeneous zones and singularity
parts of a 1D signal is proposed. The entropy function is used to transform
signal in piecewise linear one. The multiple regression permits to detect lines
and project them in the Hough parameters space in order to easily recognise
homogeneous zone and abrupt changes of the signal. Two application examples are
analysed, the first is a classical fractal signal and the other is issued from
a dynamic mechanical study.

<id>
cs/0609114v1
<category>
cs.NA
<abstract>
A finite-volume method for the one-dimensional shallow-water equations
including topographic source terms is presented. Exploiting an original idea by
Leroux, the system of partial-differential equations is completed by a trivial
equation for the bathymetry. By applying a change of variable, the system is
given a celerity-speed formulation, and linearized. As a result, an approximate
Riemann solver preserving the positivity of the celerity can be constructed,
permitting wetting and drying flow simulations to be performed. Finally, the
simulation of numerical test cases is presented.

<id>
cs/0611143v2
<category>
cs.NA
<abstract>
In many global optimization problems motivated by engineering applications,
the number of function evaluations is severely limited by time or cost. To
ensure that each evaluation contributes to the localization of good candidates
for the role of global minimizer, a sequential choice of evaluation points is
usually carried out. In particular, when Kriging is used to interpolate past
evaluations, the uncertainty associated with the lack of information on the
function can be expressed and used to compute a number of criteria accounting
for the interest of an additional evaluation at any given point. This paper
introduces minimizer entropy as a new Kriging-based criterion for the
sequential choice of points at which the function should be evaluated. Based on
\emph{stepwise uncertainty reduction}, it accounts for the informational gain
on the minimizer expected from a new evaluation. The criterion is approximated
using conditional simulations of the Gaussian process model behind Kriging, and
then inserted into an algorithm similar in spirit to the \emph{Efficient Global
Optimization} (EGO) algorithm. An empirical comparison is carried out between
our criterion and \emph{expected improvement}, one of the reference criteria in
the literature. Experimental results indicate major evaluation savings over
EGO. Finally, the method, which we call IAGO (for Informational Approach to
Global Optimization) is extended to robust optimization problems, where both
the factors to be tuned and the function evaluations are corrupted by noise.

<id>
cs/0701141v2
<category>
cs.NA
<abstract>
Expressions are not functions. Confusing the two concepts or failing to
define the function that is computed by an expression weakens the rigour of
interval arithmetic. We give such a definition and continue with the required
re-statements and proofs of the fundamental theorems of interval arithmetic and
interval analysis.
  Revision Feb. 10, 2009: added reference to and acknowledgement of P. Taylor.

<id>
cs/0703003v1
<category>
cs.NA
<abstract>
Interval arithmetic is hardly feasible without directed rounding as provided,
for example, by the IEEE floating-point standard. Equally essential for
interval methods is directed rounding for conversion between the external
decimal and internal binary numerals. This is not provided by the standard I/O
libraries. Conversion algorithms exist that guarantee identity upon conversion
followed by its inverse. Although it may be possible to adapt these algorithms
for use in decimal interval I/O, we argue that outward rounding in radix
conversion is computationally a simpler problem than guaranteeing identity.
Hence it is preferable to develop decimal interval I/O ab initio, which is what
we do in this paper.

<id>
cs/0703082v1
<category>
cs.NA
<abstract>
The fast marching algorithm computes an approximate solution to the eikonal
equation in O(N log N) time, where the factor log N is due to the
administration of a priority queue. Recently, Yatziv, Bartesaghi and Sapiro
have suggested to use an untidy priority queue, reducing the overall complexity
to O(N) at the price of a small error in the computed solution. In this paper,
we give an explicit estimate of the error introduced, which is based on a
discrete comparison principle. This estimates implies in particular that the
choice of an accuracy level that is independent of the speed function F results
in the complexity bound O(Fmax /Fmin N). A numerical experiment illustrates
this robustness problem for large ratios Fmax /Fmin .

<id>
cs/0703119v2
<category>
cs.NA
<abstract>
We use support theory, in particular the fretsaw extensions of Shklarski and
Toledo, to design preconditioners for the stiffness matrices of 2-dimensional
truss structures that are stiffly connected. Provided that all the lengths of
the trusses are within constant factors of each other, that the angles at the
corners of the triangles are bounded away from 0 and $\pi$, and that the
elastic moduli and cross-sectional areas of all the truss elements are within
constant factors of each other, our preconditioners allow us to solve linear
equations in the stiffness matrices to accuracy $\epsilon$ in time $O (n^{5/4}
(\log^{2}n \log \log n)^{3/4} \log (1/\epsilon))$.

<id>
0710.3170v2
<category>
cs.NA
<abstract>
An efficient method is introduced in this paper to find the intrinsic mode
function (IMF) components of time series data. This method is faster and more
predictable than the Empirical Mode Decomposition (EMD) method devised by the
contributor of Hilbert Huang Transform (HHT). The approach is to transforms the
original data function into a piecewise linear sawtooth function (or triangle
wave function), then directly constructs the upper envelope by connecting the
maxima and construct lower envelope by connecting minima with straight line
segments in the sawtooth space, the IMF is calculated as the difference between
the sawtooth function and the mean of the upper and lower envelopes. The
results found in the sawtooth space are reversely transformed into the original
data space as the required IMF and envelopes mean. This decomposition method
process the data in one pass to obtain a unique IMF component without the time
consuming repetitive sifting process of EMD method. An alternative
decomposition method with sawtooth function expansion is also presented.

<id>
0804.0581v2
<category>
cs.NA
<abstract>
Recently, a framework for the approximation of the entire set of
$\epsilon$-efficient solutions (denote by $E_\epsilon$) of a multi-objective
optimization problem with stochastic search algorithms has been proposed. It
was proven that such an algorithm produces -- under mild assumptions on the
process to generate new candidate solutions --a sequence of archives which
converges to $E_{\epsilon}$ in the limit and in the probabilistic sense. The
result, though satisfactory for most discrete MOPs, is at least from the
practical viewpoint not sufficient for continuous models: in this case, the set
of approximate solutions typically forms an $n$-dimensional object, where $n$
denotes the dimension of the parameter space, and thus, it may come to
perfomance problems since in practise one has to cope with a finite archive.
Here we focus on obtaining finite and tight approximations of $E_\epsilon$, the
latter measured by the Hausdorff distance. We propose and investigate a novel
archiving strategy theoretically and empirically. For this, we analyze the
convergence behavior of the algorithm, yielding bounds on the obtained
approximation quality as well as on the cardinality of the resulting
approximation, and present some numerical results.

<id>
0806.2159v3
<category>
cs.NA
<abstract>
We present parallel and sequential dense QR factorization algorithms that are
both optimal (up to polylogarithmic factors) in the amount of communication
they perform, and just as stable as Householder QR. Our first algorithm, Tall
Skinny QR (TSQR), factors m-by-n matrices in a one-dimensional (1-D) block
cyclic row layout, and is optimized for m >> n. Our second algorithm, CAQR
(Communication-Avoiding QR), factors general rectangular matrices distributed
in a two-dimensional block cyclic layout. It invokes TSQR for each block column
factorization.

<id>
0806.2548v1
<category>
cs.NA
<abstract>
Numerical methods based on interval arithmetic are efficient means to
reliably solve nonlinear systems of equations. Algorithm bc3revise is an
interval method that tightens variables' domains by enforcing a property called
box consistency. It has been successfully used on difficult problems whose
solving eluded traditional numerical methods. We present a new algorithm to
enforce box consistency that is simpler than bc3revise, faster, and easily data
parallelizable. A parallel implementation with Intel SSE2 SIMD instructions
shows that an increase in performance of up to an order of magnitude and more
is achievable.

<id>
0806.3099v2
<category>
cs.NA
<abstract>
In this paper we investigate the relationship between stabilized and enriched
finite element formulations for the Stokes problem. We also present a new
stabilized mixed formulation for which the stability parameter is derived
purely by the method of weighted residuals. This new formulation allows equal
order interpolation for the velocity and pressure fields. Finally, we show by
counterexample that a direct equivalence between subgrid-based stabilized
finite element methods and Galerkin methods enriched by bubble functions cannot
be constructed for quadrilateral and hexahedral elements using standard bubble
functions.

<id>
0806.3514v1
<category>
cs.NA
<abstract>
The following paper compares a consistent Newton-Raphson and fixed-point
iteration based solution strategy for a variational multiscale finite element
formulation for incompressible Navier-Stokes. The main contributions of this
work include a consistent linearization of the Navier-Stokes equations, which
provides an avenue for advanced algorithms that require origins in a consistent
method. We also present a comparison between formulations that differ only in
their linearization, but maintain all other equivalences. Using the variational
multiscale concept, we construct a stabilized formulation (that may be
considered an extension of the MINI element to nonlinear Navier-Stokes). We
then linearize the problem using fixed-point iteration and by deriving a
consistent tangent matrix for the update equation to obtain the solution via
Newton-Raphson iterations. We show that the consistent formulation converges in
fewer iterations, as expected, for several test problems. We also show that the
consistent formulation converges for problems for which fixed-point iteration
diverges. We present the results of both methods for problems of Reynold's
number up to 5000.

<id>
0806.3963v1
<category>
cs.NA
<abstract>
The following work presents a generalized (extended) finite element
formulation for the advection-diffusion equation. Using enrichment functions
that represent the exponential nature of the exact solution, smooth numerical
solutions are obtained for problems with steep gradients and high Peclet
numbers (up to Pe = 25) in one and two-dimensions. As opposed to traditional
stabilized methods that require the construction of stability parameters and
stabilization terms, the present work avoids numerical instabilities by
improving the classical Galerkin solution with an enrichment function. To
contextualize this method among other stabilized methods, we show by
decomposition of the solution (in a multiscale manner) an equivalence to both
Galerkin/least-squares type methods and those that use bubble functions. This
work also presents a strategy for constructing the enrichment function for
problems with complex geometries by employing a global-local approach.

<id>
0806.4286v1
<category>
cs.NA
<abstract>
We consider Cauchy problem for Fourier transformation of 3-dimensional
Navier-Stokes system with zero external force. Using initial data purposed by
Dong Li and Ya.G.Sinai we implement self-similar regime producing fast growing
behavior of the energy of solution while time tends to critical value.

<id>
0808.2827v3
<category>
cs.NA
<abstract>
The intrinsic mode function (IMF) provides adaptive function bases for
nonlinear and non-stationary time series data. A fast convergent iterative
method is introduced in this paper to find the IMF components of the data, the
method is faster and more predictable than the Empirical Mode Decomposition
method devised by the contributor of Hilbert Huang Transform. The approach is to
iteratively adjust the control points on the data function corresponding to the
extrema of the refining IMF, the control points of the residue function are
calculated as the median of the straight line segments passing through the data
control points, the residue function is then constructed as the cubic spline
function of the median points. The initial residue function is simply
constructed as the straight line segments passing through the extrema of the
first derivative of the data function. The refining IMF is the difference
between the data function and the improved residue function. The IMF found
reveals all the riding waves in the whole data set. A new data filtering method
on frequency and amplitude of IMF is also presented with the similar approach
of finding the residue on the part to be filtered out. The program to
demonstrate the method is distributed under BSD open source license.

<id>
0809.0062v1
<category>
cs.NA
<abstract>
To analyze the stability of It\^o stochastic differential equations with
multiplicative noise, we introduce the stochastic logarithmic norm. The
logarithmic norm was originally introduced by G. Dahlquist in 1958 as a tool to
study the growth of solutions to ordinary differential equations and for
estimating the error growth in discretization methods for their approximate
solutions. We extend the concept to the stability analysis of It\^o stochastic
differential equations with multiplicative noise. Stability estimates for
linear It\^o SDEs using the one, two and $\infty$-norms in the $l$-th mean,
where $1 \leq l < \infty $, are derived and the application of the stochastic
logarithmic norm is illustrated with examples.

<id>
cs/0111035v1
<category>
cs.OS
<abstract>
Modern control systems applications are often built on top of a real time
operating system (RTOS) which provides the necessary hardware abstraction as
well as scheduling, networking and other services. Several open source RTOS
solutions are publicly available, which is very attractive, both from an
economic (no licensing fees) as well as from a technical (control over the
source code) point of view. This contribution gives an overview of the RTLinux
and RTEMS systems (architecture, development environment, API etc.). Both
systems feature most popular CPUs, several APIs (including Posix), networking,
portability and optional commercial support. Some performance figures are
presented, focusing on interrupt latency and context switching delay.

<id>
cs/0411080v1
<category>
cs.OS
<abstract>
When a program is loaded into memory for execution, the relative position of
its basic blocks is crucial, since loading basic blocks that are unlikely to be
executed first places them high in the instruction-memory hierarchy only to be
dislodged as the execution goes on. In this paper we study the use of Bayesian
networks as models of the input history of a program. The main point is the
creation of a probabilistic model that persists as the program is run on
different inputs and at each new input refines its own parameters in order to
reflect the program's input history more accurately. As the model is thus
tuned, it causes basic blocks to be reordered so that, upon arrival of the next
input for execution, loading the basic blocks into memory automatically takes
into account the input history of the program. We report on extensive
experiments, whose results demonstrate the efficacy of the overall approach in
progressively lowering the execution times of a program on identical inputs
placed randomly in a sequence of varied inputs. We provide results on selected
SPEC CINT2000 programs and also evaluate our approach as compared to the gcc
level-3 optimization and to Pettis-Hansen reordering.

<id>
cs/0502027v1
<category>
cs.OS
<abstract>
Researchers have long proposed using economic approaches to resource
allocation in computer systems. However, few of these proposals became
operational, let alone commercial. Questions persist about the economic
approach regarding its assumptions, value, applicability, and relevance to
system design. The goal of this paper is to answer these questions. We find
that market-based resource allocation is useful, and more importantly, that
mechanism design and system design should be integrated to produce systems that
are both economically and computationally efficient.

<id>
cs/0511010v1
<category>
cs.OS
<abstract>
Virtualization, a technique once used to multiplex the resources of
high-priced mainframe hardware, is seeing a resurgence in applicability with
the increasing computing power of commodity computers. By inserting a layer of
software between the machine and traditional operating systems, this technology
allows access to a shared computing medium in a manner that is secure,
resource-controlled, and efficient. These properties are attractive in the
field of on-demand computing, where the fine-grained subdivision of resources
provided by virtualized systems allows potentially higher utilization of
computing resources.
  It this work, we survey a number of virtual machine systems with the goal of
finding an appropriate candidate to serve as the basis for the On-Demand Secure
Cluster Computing project at the National Center for Supercomputing
Applications. Contenders are reviewed on a number of desirable properties
including portability and security. We conclude with a comparison and
justification of our choice.

<id>
cs/0611055v1
<category>
cs.OS
<abstract>
This paper shows that it is possible to dramatically reduce the memory
consumption of classes loaded in an embedded Java virtual machine without
reducing its functionalities. We describe how to pack the constant pool by
deleting entries which are only used during the class loading process. We
present some benchmarks which demonstrate the efficiency of this mechanism. We
finally suggest some additional optimizations which can be applied if some
restrictions to the functionalities of the virtual machine can be tolerated.

<id>
cs/0612079v2
<category>
cs.OS
<abstract>
We notice a way to execute a binary file on Windows and ELF-based systems. It
can be used to create software installers and other applications not exceeding
64 kilo bytes.

<id>
0710.4635v1
<category>
cs.OS
<abstract>
Demands for implementing original OSs that can achieve high I/O performance
on PC/AT compatible hardware have recently been increasing, but conventional OS
debugging environments have not been able to simultaneously assure their
stability, be easily customized to new OSs and new I/O devices, and assure
efficient execution of I/O operations. We therefore developed a novel OS
debugging method using a lightweight virtual machine. We evaluated this
debugging method experimentally and confirmed that it can transfer data about
5.4 times as fast as the conventional virtual machine monitor.

<id>
0710.4746v1
<category>
cs.OS
<abstract>
This paper presents the methodology and the modeling constructs we have
developed to capture the real time aspects of RTOS simulation models in a
System Level Design Language (SLDL) like SystemC. We describe these constructs
and show how they are used to build a simulation model of an RTOS kernel
targeting the $\mu$-ITRON OS specification standard.

<id>
0712.2958v2
<category>
cs.OS
<abstract>
In this paper, we address the power-aware scheduling of sporadic
constrained-deadline hard real-time tasks using dynamic voltage scaling upon
multiprocessor platforms. We propose two distinct algorithms. Our first
algorithm is an off-line speed determination mechanism which provides an
identical speed for each processor. That speed guarantees that all deadlines
are met if the jobs are scheduled using EDF. The second algorithm is an on-line
and adaptive speed adjustment mechanism which reduces the energy consumption
while the system is running.

<id>
0801.4292v1
<category>
cs.OS
<abstract>
In this paper we study the global scheduling of periodic task systems upon
multiprocessor platforms. We first show two very general properties which are
well-known for uniprocessor platforms and which remain for multiprocessor
platforms: (i) under few and not so restrictive assumptions, we show that
feasible schedules of periodic task systems are periodic from some point with a
period equal to the least common multiple of task periods and (ii) for the
specific case of synchronous periodic task systems, we show that feasible
schedules repeat from the origin. We then present our main result: we
characterize, for task-level fixed-priority schedulers and for asynchronous
constrained or arbitrary deadline periodic task models, upper bounds of the
first time instant where the schedule repeats. We show that job-level
fixed-priority schedulers are predictable upon unrelated multiprocessor
platforms. For task-level fixed-priority schedulers, based on the upper bounds
and the predictability property, we provide for asynchronous constrained or
arbitrary deadline periodic task sets, exact feasibility tests. Finally, for
the job-level fixed-priority EDF scheduler, for which such an upper bound
remains unknown, we provide an exact feasibility test as well.

<id>
0806.0132v1
<category>
cs.OS
<abstract>
For microprocessors used in real-time embedded systems, minimizing power
consumption is difficult due to the timing constraints. Dynamic voltage scaling
(DVS) has been incorporated into modern microprocessors as a promising
technique for exploring the trade-off between energy consumption and system
performance. However, it remains a challenge to realize the potential of DVS in
unpredictable environments where the system workload cannot be accurately
known. Addressing system-level power-aware design for DVS-enabled embedded
controllers, this paper establishes an analytical model for the DVS system that
encompasses multiple real-time control tasks. From this model, a feedback
control based approach to power management is developed to reduce dynamic power
consumption while achieving good application performance. With this approach,
the unpredictability and variability of task execution times can be attacked.
Thanks to the use of feedback control theory, predictable performance of the
DVS system is achieved, which is favorable to real-time applications. Extensive
simulations are conducted to evaluate the performance of the proposed approach.

<id>
0806.1381v1
<category>
cs.OS
<abstract>
Embedded computing systems today increasingly feature resource constraints
and workload variability, which lead to uncertainty in resource availability.
This raises great challenges to software design and programming in multitasking
environments. In this paper, the emerging methodology of feedback scheduling is
introduced to address these challenges. As a closed-loop approach to resource
management, feedback scheduling promises to enhance the flexibility and
resource efficiency of various software programs through dynamically
distributing available resources among concurrent tasks based on feedback
information about the actual usage of the resources. With emphasis on the
behavioral design of feedback schedulers, we describe a general framework of
feedback scheduling in the context of real-time control applications. A simple
yet illustrative feedback scheduling algorithm is given. From a programming
perspective, we describe how to modify the implementation of control tasks to
facilitate the application of feedback scheduling. An event-driven paradigm
that combines time-triggered and event-triggered approaches is proposed for
programming of the feedback scheduler. Simulation results argue that the
proposed event-driven paradigm yields better performance than time-triggered
paradigm in dynamic environments where the workload varies irregularly and
unpredictably.

<id>
0809.1132v1
<category>
cs.OS
<abstract>
Energy efficient real-time task scheduling attracted a lot of attention in
the past decade. Most of the time, deterministic execution lengths for tasks
were considered, but this model fits less and less with the reality, especially
with the increasing number of multimedia applications. It's why a lot of
research is starting to consider stochastic models, where execution times are
only known stochastically. However, contributors consider that they have a pretty
much precise knowledge about the properties of the system, especially regarding
to the worst case execution time (or worst case execution cycles, WCEC).
  In this work, we try to relax this hypothesis, and assume that the WCEC can
vary. We propose miscellaneous methods to react to such a situation, and give
many simulation results attesting that with a small effort, we can provide very
good results, allowing to keep a low deadline miss rate as well as an energy
consumption similar to clairvoyant algorithms.

<id>
0809.4082v1
<category>
cs.OS
<abstract>
In this ongoing work, we are interested in multiprocessor energy efficient
systems, where task durations are not known in advance, but are know
stochastically. More precisely, we consider global scheduling algorithms for
frame-based multiprocessor stochastic DVFS (Dynamic Voltage and Frequency
Scaling) systems. Moreover, we consider processors with a discrete set of
available frequencies.

<id>
0809.5238v1
<category>
cs.OS
<abstract>
In this paper, we propose a synchronous protocol without periodicity for
scheduling multi-mode real-time systems upon identical multiprocessor
platforms. Our proposal can be considered to be a multiprocessor extension of
the uniprocessor protocol called "Minimal Single Offset protocol".

<id>
0912.0606v1
<category>
cs.OS
<abstract>
The main objective of this paper is to develop the two different ways in
which round robin architecture is modified and made suitable to be implemented
in real time and embedded systems. The scheduling algorithm plays a significant
role in the design of real time embedded systems. Simple round robin
architecture is not efficient to be implemented in embedded systems because of
higher context switch rate, larger waiting time and larger response time.
Missing of deadlines will degrade the system performance in soft real time
systems. The main objective of this paper is to develop the scheduling
algorithm which removes the drawbacks in simple round robin architecture. A
comparison with round robin architecture to the proposed architectures has been
made. It is observed that the proposed architectures solves the problems
encountered in round robin architecture in soft real time by decreasing the
number of context switches waiting time and response time thereby increasing
the system throughput.

<id>
0912.0926v2
<category>
cs.OS
<abstract>
The difficulty of developing reliable parallel software is generating
interest in deterministic environments, where a given program and input can
yield only one possible result. Languages or type systems can enforce
determinism in new code, and runtime systems can impose synthetic schedules on
legacy parallel code. To parallelize existing serial code, however, we would
like a programming model that is naturally deterministic without language
restrictions or artificial scheduling. We propose "deterministic consistency",
a parallel programming model as easy to understand as the "parallel assignment"
construct in sequential languages such as Perl and JavaScript, where concurrent
threads always read their inputs before writing shared outputs. DC supports
common data- and task-parallel synchronization abstractions such as fork/join
and barriers, as well as non-hierarchical structures such as producer/consumer
pipelines and futures. A preliminary prototype suggests that software-only
implementations of DC can run applications written for popular parallel
environments such as OpenMP with low (<10%) overhead for some applications.

<id>
1001.3727v1
<category>
cs.OS
<abstract>
All real time tasks which are termed as critical tasks by nature have to
complete its execution before its deadline, even in presence of faults. The
most popularly used real time task assignment algorithms are First Fit (FF),
Best Fit (BF), Bin Packing (BP).The common task scheduling algorithms are Rate
Monotonic (RM), Earliest Deadline First (EDF) etc.All the current approaches
deal with either fault tolerance or criticality in real time. In this paper we
have proposed an integrated approach with a new algorithm, called SASA (Sorting
And Sequential Assignment) which maps the real time task assignment with task
schedule and fault tolerance

<id>
1003.1336v2
<category>
cs.OS
<abstract>
Virtual memory of computers is usually implemented by demand paging. For some
page replacement algorithms the number of page faults may increase as the
number of page frames increases. Belady, Nelson and Shedler constructed
reference strings for which page replacement algorithm FIFO produces near twice
more page faults in a larger memory than in a smaller one. They formulated the
conjecture that 2 is a general bound. We prove that this ratio can be
arbitrarily large.

<id>
1003.4088v1
<category>
cs.OS
<abstract>
Memory hierarchy is used to compete the processors speed. Cache memory is the
fast memory which is used to conduit the speed difference of memory and
processor. The access patterns of Level 1 cache (L1) and Level 2 cache (L2) are
different, when CPU not gets the desired data in L1 then it accesses L2. Thus
the replacement algorithm which works efficiently on L1 may not be as efficient
on L2. Similarly various applications such as Matrix Multiplication, Web, Fast
Fourier Transform (FFT) etc will have varying access pattern. Thus same
replacement algorithm for all types of application may not be efficient. This
paper works for getting an efficient pair of replacement algorithm on L1 and L2
for the algorithm Merge Sort. With the memory reference string of Merge Sort,
we have analyzed the behavior of various existing replacement algorithms on L1.
The existing replacement algorithms which are taken into consideration are:
Least Recently Used (LRU), Least Frequently Used (LFU) and First In First Out
(FIFO). After Analyzing the memory reference pattern of Merge Sort, we have
proposed a Partition Based Replacement algorithm (PBR_L1)) on L1 Cache.
Furthermore we have analyzed various pairs of algorithms on L1 and L2
respectively, resulting in finding a suitable pair of replacement algorithms.
Simulation on L1 shows, among the considered existing replacement algorithms
FIFO is performing better than others. While the proposed replacement algorithm
PBR_L1 is working about 1.7% to 44 % better than FIFO for various cache sizes.

<id>
1003.5525v1
<category>
cs.OS
<abstract>
This note concerns a search for publications in which one can find statements
that explain the concept of an operating system, reasons for introducing
operating systems, a formalization of the concept of an operating system or
theory about operating systems based on such a formalization. It reports on the
way in which the search has been carried out and the outcome of the search. The
outcome includes not only what the search was meant for, but also some added
bonuses.

<id>
1006.0813v1
<category>
cs.OS
<abstract>
We dwell on how a definition of a theoretical concept of an operating system,
suitable to be incorporated in a mathematical theory of operating systems,
could look like. This is considered a valuable preparation for the development
of a mathematical theory of operating systems.

<id>
1006.2104v1
<category>
cs.OS
<abstract>
Is it possible for an Information Technology [IT] product to be both mature
and state-of-theart at the same time? In the case of the UNIX system, the
answer is an unqualified "Yes." The UNIX system has continued to develop over
the past twenty-five years. In millions of installations running on nearly
every hardware platform made, the UNIX system has earned its reputation for
stability and scalability. Over the years, UNIX system suppliers have steadily
assimilated new technologies so that UNIX systems today provide more
functionality as any other operating system.

<id>
1006.2617v1
<category>
cs.OS
<abstract>
In this paper we consider the scheduling of periodic and parallel rigid
tasks. We provide (and prove correct) an exact schedulability test for Fixed
Task Priority (FTP) Gang scheduler sub-classes: Parallelism Monotonic, Idling,
Limited Gang, and Limited Slack Reclaiming. Additionally, we study the
predictability of our schedulers: we show that Gang FJP schedulers are not
predictable and we identify several sub-classes which are actually predictable.
Moreover, we extend the definition of rigid, moldable and malleable jobs to
recurrent tasks.

<id>
1006.2637v1
<category>
cs.OS
<abstract>
Algorithms based on semi-partitioned scheduling have been proposed as a
viable alternative between the two extreme ones based on global and partitioned
scheduling. In particular, allowing migration to occur only for few tasks which
cannot be assigned to any individual processor, while most tasks are assigned
to specific processors, considerably reduces the runtime overhead compared to
global scheduling on the one hand, and improve both the schedulability and the
system utilization factor compared to partitioned scheduling on the other hand.
In this paper, we address the preemptive scheduling problem of hard real-time
systems composed of sporadic constrained-deadline tasks upon identical
multiprocessor platforms. We propose a new algorithm and a scheduling paradigm
based on the concept of semi-partitioned scheduling with restricted migrations
in which jobs are not allowed to migrate, but two subsequent jobs of a task can
be assigned to different processors by following a periodic strategy.

<id>
1012.2831v2
<category>
cs.OS
<abstract>
System energy models are important for energy optimization and management in
mobile systems. However, existing system energy models are built in lab with
the help from a second computer. Not only are they labor-intensive; but also
they will not adequately account for the great diversity in the hardware and
usage of mobile systems. Moreover, existing system energy models are intended
for energy estimation for time intervals of one second or longer; they do not
provide the required rate for fine-grain use such as per-application energy
accounting.
  In this work, we study a self-modeling paradigm in which a mobile system
automatically generates its energy model without any external assistance. Our
solution, Se-same, leverages the possibility of self power measurement through
the smart battery interface and employs a suite of novel techniques to achieve
accuracy and rate much higher than that of the smart battery interface.
  We report the implementation and evaluation of Se-same on a laptop and a
smartphone. The experiment results show that Sesame generates system energy
models of 95% accuracy at one estimation per second and 88% accuracy at one
estimation per 10ms, without any external assistance. A five-day field studies
with four laptop and four smartphones users further demonstrate the
effectiveness, efficiency, and noninvasiveness of Sesame.

<id>
1012.3452v1
<category>
cs.OS
<abstract>
Almost all of the current process scheduling algorithms which are used in
modern operating systems (OS) have their roots in the classical scheduling
paradigms which were developed during the 1970's. But modern computers have
different types of software loads and user demands. We think it is important to
run what the user wants at the current moment. A user can be a human, sitting
in front of a desktop machine, or it can be another machine sending a request
to a server through a network connection. We think that OS should become
intelligent to distinguish between different processes and allocate resources,
including CPU, to those processes which need them most. In this work, as a
first step to make the OS aware of the current state of the system, we consider
process dependencies and interprocess communications. We are developing a
model, which considers the need to satisfy interactive users and other possible
remote users or customers, by making scheduling decisions based on process
dependencies and interprocess communications. Our simple proof of concept
implementation and experiments show the effectiveness of this approach in the
real world applications. Our implementation does not require any change in the
software applications nor any special kind of configuration in the system,
Moreover, it does not require any additional information about CPU needs of
applications nor other resource requirements. Our experiments show significant
performance improvement for real world applications. For example, almost
constant average response time for Mysql data base server and constant frame
rate for mplayer under different simulated load values.

<id>
1012.4045v1
<category>
cs.OS
<abstract>
This paper describes a study of comparison of global and one-dimensional
local optimization methods to operating system scheduler tuning. The operating
system scheduler we use is the Linux 2.6.23 Completely Fair Scheduler (CFS)
running in simulator (LinSched). We have ported the Hackbench scheduler
benchmark to this simulator and use this as the workload. The global
optimization approach we use is Particle Swarm Optimization (PSO). We make use
of Response Surface Methodology (RSM) to specify optimal parameters for our PSO
implementation. The one-dimensional local optimization approach we use is the
Golden Section method. In order to use this approach, we convert the scheduler
tuning problem from one involving setting of three parameters to one involving
the manipulation of one parameter. Our results show that the global
optimization approach yields better response but the one- dimensional
optimization approach converges to a solution faster than the global
optimization approach.

<id>
1012.5695v1
<category>
cs.OS
<abstract>
Energy consumption is a critical design issue in real-time systems,
especially in battery- operated systems. Maintaining high performance, while
extending the battery life between charges is an interesting challenge for
system designers. Dynamic Voltage Scaling (DVS) allows a processor to
dynamically change speed and voltage at run time, thereby saving energy by
spreading run cycles into idle time. Knowing when to use full power and when
not, requires the cooperation of the operating system scheduler. Usually,
higher processor voltage and frequency leads to higher system throughput while
energy reduction can be obtained using lower voltage and frequency. Instead of
lowering processor voltage and frequency as much as possible, energy efficient
real-time scheduling adjusts voltage and frequency according to some
optimization criteria, such as low energy consumption or high throughput, while
it meets the timing constraints of the real-time tasks. As the quantity and
functional complexity of battery powered portable devices continues to raise,
energy efficient design of such devices has become increasingly important. Many
real-time scheduling algorithms have been developed recently to reduce energy
consumption in the portable devices that use DVS capable processors. Three
algorithms namely Red Tasks Only (RTO), Blue When Possible (BWP) and Red as
Late as Possible (RLP) are proposed in the literature to schedule the real-time
tasks in Weakly-hard real-time systems. This paper proposes optimal slack
management algorithms to make the above existing weakly hard real-time
scheduling algorithms energy efficient using DVS and DPD techniques.

<id>
1012.5929v1
<category>
cs.OS
<abstract>
In this paper we consider the scheduling problem of hard real-time systems
composed of periodic constrained-deadline tasks upon identical multiprocessor
platforms. We assume that tasks are scheduled by using the global-EDF
scheduler. We establish an exact schedulability test for this scheduler by
exploiting on the one hand its predictability property and by providing on the
other hand a feasibility interval so that if it is possible to find a valid
schedule for all the jobs contained in this interval, then the whole system
will be stamped feasible. In addition, we show by means of a counterexample
that the feasibility interval, and thus the schedulability test, proposed by
Leung [Leung 1989] is incorrect and we show which arguments are actually
incorrect.

<id>
cs/9903005v1
<category>
cs.OH
<abstract>
Generalizations of linear numeration systems in which the set of natural
numbers is recognizable by finite automata are obtained by describing an
arbitrary infinite regular language following the lexicographic ordering. For
these systems of numeration, we show that ultimately periodic sets are
recognizable. We also study the translation and the multiplication by constants
as well as the order dependence of the recognizability.

<id>
cs/9911010v1
<category>
cs.OH
<abstract>
In his Discourse on the Method of Rightly Conducting the Reason, and Seeking
Truth in the Sciences, Rene Descartes sought ``clear and certain knowledge of
all that is useful in life.'' Almost three centuries later, in ``The
foundations of mathematics,'' David Hilbert tried to ``recast mathematical
definitions and inferences in such a way that they are unshakable.'' Hilbert's
program relied explicitly on formal systems (equivalently, computational
systems) to provide certainty in mathematics. The concepts of computation and
formal system were not defined in his time, but Descartes' method may be
understood as seeking certainty in essentially the same way.
  In this article, I explain formal systems as concrete artifacts, and
investigate the way in which they provide a high level of certainty---arguably
the highest level achievable by rational discourse. The rich understanding of
formal systems achieved by mathematical logic and computer science in this
century illuminates the nature of programs, such as Descartes' and Hilbert's,
that seek certainty through rigorous analysis.

<id>
cs/0002018v2
<category>
cs.OH
<abstract>
Generating high-quality schedules for a rotating workforce is a critical task
in all settings where a certain staffing level must be guaranteed beyond the
capacity of single employees, such as for instance in industrial plants,
hospitals, or airline companies. Results from ergonomics \cite{BEST91} indicate
that rotating workforce schedules have a profound impact on the health and
social life of employees as well as on their performance at work. Moreover,
rotating workforce schedules must satisfy legal requirements and should also
meet the objectives of the employing organization. We describe our solution to
this problem. A basic design decision was to aim at quickly obtaining
high-quality schedules for realistically sized problems while maintaining human
control. The interaction between the decision maker and the algorithm therefore
consists in four steps: (1) choosing a set of lengths of work blocks (a work
block is a sequence of consecutive days of work shifts), (2) choosing a
particular sequence of work and days-off blocks among those that have optimal
weekend characteristics, (3) enumerating possible shift sequences for the
chosen work blocks subject to shift change constraints and bounds on sequences
of shifts, and (4) assignment of shift sequences to work blocks while
fulfilling the staffing requirements. The combination of constraint
satisfaction and problem-oriented intelligent backtracking algorithms in each
of the four steps allows to find good solutions for real-world problems in
acceptable time. Computational results from real-world problems and from
benchmark examples found in the literature confirm the viability of our
approach. The algorithms are now part of a commercial shift scheduling software
package.

<id>
cs/0003075v1
<category>
cs.OH
<abstract>
This paper describes necessary elements for constructing theoretical models
of network and system administration. Armed with a theoretical model it becomes
possible to determine best practices and optimal strategies in a way which
objectively relates policies and assumptions to results obtained. It is
concluded that a mixture of automation and human, or other intelligent
incursion is required to fully implement system policy with current technology.
Some aspects of the contributor's immunity model for automated system administration
are explained, as an example. A theoretical framework makes the prediction that
the optimal balance between resource availability and garbage collection
strategies is encompassed by the immunity model.

<id>
cs/0007034v1
<category>
cs.OH
<abstract>
Previous research has directly studied whether on-line retailing is more
competitive than conventional retail markets. The evidence from books and music
CDs is mixed. Here, I use an indirect approach to compare the competitiveness
of on-line with conventional markets. Focusing on the retail market for books,
I identify a peculiarity in the pricing of bestsellers relative to other
titles. Supposing that competitive barriers are lower in on-line retailing, I
analyze how the lower barriers would affect the relative pricing of
bestsellers. The empirical data indicates that on-line retailing is more
competitive than conventional retailing.

<id>
cs/0012016v1
<category>
cs.OH
<abstract>
The VJ-Lab is a project oriented to improve the students learning process of
Computer Science degree at the National University of La Plata. The VJ-Lab is a
Web application with Java based simulations. Java can be used to provide
simulation environments with simple pictorial interfaces that can help students
to understand the subject. There are many fields in which it is difficult to
give students a feel for the subject that they are learning. Computer based
simulations offer a fun and effective way to enable students to learn by doing.
Both, practicing skills and applying knowledge are both allowed in simulated
worlds. We will focus on the VJ-Lab project overview, the work in progress and
some Java based simulations running. They imitate the behavior of data network
protocol and data structure algorithms. These applets are produced by the
students of the 'Software Development Laboratory' course.

<id>
cs/0108012v1
<category>
cs.OH
<abstract>
This submission was removed because it contained proprietary information that
was distributed without permission.

<id>
cs/0110029v3
<category>
cs.OH
<abstract>
A study on future large accelerators [1] has considered a facility, which is
designed, built and operated by a worldwide collaboration of equal partner
institutions, and which is remote from most of these institutions. The full
range of operation was considered including commi-ssioning, machine
development, maintenance, trouble shooting and repair. Experience from existing
accele-rators confirms that most of these activities are already performed
'remotely'. The large high-energy physics ex-periments and astronomy projects,
already involve inter-national collaborations of distant institutions. Based on
this experience, the prospects for a machine operated remotely from far sites
are encouraging. Experts from each laboratory would remain at their home
institution but continue to participate in the operation of the machine after
construction. Experts are required to be on site only during initial
commissioning and for par-ticularly difficult problems. Repairs require an
on-site non-expert maintenance crew. Most of the interventions can be made
without an expert and many of the rest resolved with remote assistance. There
appears to be no technical obstacle to controlling an accelerator from a
distance. The major challenge is to solve the complex management and
communication problems.

<id>
cs/0110066v1
<category>
cs.OH
<abstract>
The Channel Archiver has been operational for more than two years at Los
Alamos National Laboratory and other sites. This paper introduces the available
components (data sampling engine, viewers, scripting interface, HTTP/CGI
integration and data management), presents updated performance measurements and
reviews operational experience with the Channel Archiver.

<id>
cs/0111001v1
<category>
cs.OH
<abstract>
Being easy to learn and well suited for a self-contained desktop laboratory
setup, many casual programmers prefer to use the National Instruments LabVIEW
environment to develop their logic. An ActiveX interface is presented that
allows integration into a plant-wide distributed environment based on the
Experimental Physics and Industrial Control System (EPICS). This paper
discusses the design decisions and provides performance information, especially
considering requirements for the Spallation Neutron Source (SNS) diagnostics
system.

<id>
cs/0111002v1
<category>
cs.OH
<abstract>
The starting point of this paper is the introduction of a new measure of
inclusion of fuzzy set A in fuzzy set B. Previously used inclusion measures
take values in the interval [0,1]; the inclusion measure proposed here takes
values in a Boolean lattice. In other words, inclusion is viewed as an L-fuzzy
valued relation between fuzzy sets. This relation is re exive, antisymmetric
and transitive, i.e. it is a fuzzy order relation; in addition it possesess a
number of properties which various contributors have postulated as axiomatically
appropriate for an inclusion measure. We also define an L-fuzzy valued measure
of similarity between fuzzy sets and and an L-fuzzy valued distance function
between fuzzy sets; these possess properties analogous to the ones of
real-valued similarity and distance functions.
  Keywords: Fuzzy Relations, inclusion measure, subsethood, L-fuzzy sets,
similarity, distance, transitivity.

<id>
cs/0111017v1
<category>
cs.OH
<abstract>
First Experiences Integrating PC Distributed I/O Into Argonne's ATLAS Control
System The roots of ATLAS (Argonne Tandem-Linac Accelerator System) date back
to the early 1960s. Located at the Argonne National Laboratory, the accelerator
has been designated a National User Facility, which focuses primarily on
heavy-ion nuclear physics. Like the accelerator it services, the control system
has been in a constant state of evolution. The present real-time portion of the
control system is based on the commercial product Vsystem [1]. While Vsystem
has always been capable of distributed I/O processing, the latest offering of
this product provides for the use of relatively inexpensive PC hardware and
software. This paper reviews the status of the ATLAS control system, and
describes first experiences with PC distributed I/O.

<id>
cs/0111020v1
<category>
cs.OH
<abstract>
The Gemini Observatory is planning to implement a Multi Conjugate Adaptive
Optics (MCAO) System as a facility instrument for the Gemini-South telescope.
The system will include 5 Laser Guide Stars, 3 Natural Guide Stars, and 3
Deformable mirrors optically conjugated at different altitudes to achieve
near-uniform atmospheric compensation over a 1 arc minute square field of view.
The control of such a system will be split into 3 main functions: the control
of the opto-mechanical assemblies of the whole system (including the Laser, the
Beam Transfer Optics and the Adaptive Optics bench), the control of the
Adaptive Optics System itself at a rate of 800FPS and the control of the safety
system. The control of the Adaptive Optics System is the most critical in terms
of real time performances. The control system will be an EPICS based system. In
this paper, we will describe the requirements for the whole MCAO control
system, preliminary designs for the control of the opto-mechanical devices and
architecture options for the control of the Adaptive Optics system and the
safety system.

<id>
cs/0111044v1
<category>
cs.OH
<abstract>
The SNS has developed a standard power supply interface for the approximately
350 magnet power supplies in the SNS accumulator ring, Linac and transport
lines. Power supply manufacturers are providing supplies compatible with the
standard interface. The SNS standard consists of a VME based power supply
controller module (PSC) and a power supply interface unit (PSI) that mounts on
the power supply. Communication between the two is via a pair of multimode
fibers. This PSI/PSC system supports one 16-bit analog reference, four 16-bit
analog readbacks, fifteen digital commands and sixteen digital status bits in a
single fiber-isolated module. The system can send commands to the supplies and
read data from them synchronized to an external signal at up to a 10KHz rate.
The PSC time stamps and stores this data in a circular buffer so historical
data leading up to a fault event can be analyzed. The PSC contains a serial
port so that local testing of hardware can be accomplished with a laptop. This
paper concentrates on the software being provided to control the power supply.
It includes the EPICS driver; software to test hardware and power supplies via
the serial port and VME interface.

<id>
cs/0111055v2
<category>
cs.OH
<abstract>
The National Spherical Torus Experiment (NSTX) is an innovative magnetic
fusion device that was constructed by the Princeton Plasma Physics Laboratory
(PPPL) in collaboration with the Oak Ridge National Laboratory, Columbia
University, and the University of Washington at Seattle. Since achieving first
plasma in 1999, the device has been used for fusion research through an
international collaboration of over twenty institutions. The NSTX is operated
through a collection of control systems that encompass a wide range of
technology, from hardwired relay controls to real-time control systems with
giga-FLOPS of capability. This paper presents a broad introduction to the
control systems used on NSTX, with an emphasis on the computing controls, data
acquisition, and synchronization systems.

<id>
cs/0206025v1
<category>
cs.OH
<abstract>
Given a reference lattice, we define fuzzy intervals to be the fuzzy sets
such that their p-cuts are crisp closed intervals. We show that: given a
complete reference lattice, the collection of its fuzzy intervals is a complete
lattice. Furthermore we show that: if the reference lattice is completely
distributive then the lattice of its fuzzy intervals is distributive.

<id>
cs/0207007v1
<category>
cs.OH
<abstract>
This paper presents case-study results on the application of information
theoretic approach to gate-level evolutionary circuit design. We introduce
information measures to provide better estimates of synthesis criteria of
digital circuits. For example, the analysis of signal propagation during
evolving gate-level synthesis can be improved by using information theoretic
measures that will make it possible to find the most effective geometry and
therefore predict the cost of the final design solution. The problem is
considered from the information engine point of view. That is, the process of
evolutionary gate-level circuit design is presented via such measures as
entropy, logical work and information vitality. Some examples of geometry
driven synthesis are provided to prove the above idea.

<id>
cs/0207019v1
<category>
cs.OH
<abstract>
This paper presents a method to detect and recognize symmetries in Boolean
functions. The idea is to use information theoretic measures of Boolean
functions to detect sub-space of possible symmetric variables. Coupled with the
new techniques of efficient estimations of information measures on Binary
Decision Diagrams (BDDs) we obtain promised results in symmetries detection for
large-scale functions.

<id>
cs/0207020v1
<category>
cs.OH
<abstract>
This paper introduces new technique for efficient calculation of different
Shannon information measures which operates Binary Decision Diagrams (BDDs). We
offer an algorithm of BDD reordering which demonstrates the improvement of the
obtaining outcomes over the existing reordering approaches. The technique and
the reordering algorithm have been implemented, and the results on circuits'
benchmarks are analyzed. We point out that the results are quite promising, the
algorithm is very fast, and it is easy to implement. Finally, we show that our
approach to BDD reordering can yield to reduction in the power dissipation for
the circuits derived from BDDs.

<id>
cs/0209026v1
<category>
cs.OH
<abstract>
We present two ways in which an infinite universal alphabet may be generated
using a novel rewrite system that conserves zero (a special character of the
alphabet and the symbol for that character) at every step. The recursive method
delivers the entire alphabet in one step when invoked with the zero character
as the initial subset alphabet. The iterative method with the same start
delivers characters that act as ciphers for properties that the developing
subset alphabet contains. These properties emerge in an arbitrary sequence and
there are an infinite number of ways they may be selected. The subset alphabets
in addition to having mathematical interpretation as algebra can also be
constrained to emerge in a minimal way which then has application as a
foundational physical system. Each subset alphabet may itself be the basis of a
rewrite system where rules that operate on symbols (representing characters) or
collections of symbols manipulate the specific properties in a dynamic way.

<id>
cs/0304042v2
<category>
cs.OH
<abstract>
We consider probabilistic automata on a general state space and study their
computational power. The model is based on the concept of language recognition
by probabilistic automata due to Rabin and models of analog computation in a
noisy environment suggested by Maass and Orponen, and Maass and Sontag. Our
main result is a generalization of Rabin's reduction theorem that implies that
under very mild conditions, the computational power of the automaton is limited
to regular languages.

<id>
cs/0305043v1
<category>
cs.OH
<abstract>
Modeling has been created for a Space-to-Surface system defined for an
optimal trajectory for targeting in terminal phase. The modeling includes
models for simulation atmosphere, speed of sound, aerodynamic flight and
navigation by an infrared system. The modeling simulation includes statistical
analysis of the modeling results.

<id>
cs/0305047v1
<category>
cs.OH
<abstract>
In January 1999, CERN began to develop CASTOR ("CERN Advanced STORage
manager"). This Hierarchical Storage Manager targetted at HEP applications has
been in full production at CERN since May 2001. It now contains more than two
Petabyte of data in roughly 9 million files. In 2002, 350 Terabytes of data
were stored for COMPASS at 45 MB/s and a Data Challenge was run for ALICE in
preparation for the LHC startup in 2007 and sustained a data transfer to tape
of 300 MB/s for one week (180 TB). The major functionality improvements were
the support for files larger than 2 GB (in collaboration with IN2P3) and the
development of Grid interfaces to CASTOR: GridFTP and SRM ("Storage Resource
Manager"). An ongoing effort is taking place to copy the existing data from
obsolete media like 9940 A to better cost effective offerings. CASTOR has also
been deployed at several HEP sites with little effort. In 2003, we plan to
continue working on Grid interfaces and to improve performance not only for
Central Data Recording but also for Data Analysis applications where thousands
of processes possibly access the same hot data. This could imply the selection
of another filesystem or the use of replication (hardware or software).

<id>
cs/0306014v1
<category>
cs.OH
<abstract>
Recently SCRAM (Software Configuration And Management) has been adopted by
the applications area of the LHC computing grid project as baseline
configuration management and build support infrastructure tool.
  SCRAM is a software engineering tool, that supports the configuration
management and management processes for software development. It resolves the
issues of configuration definition, assembly break-down, build, project
organization, run-time environment, installation, distribution, deployment, and
source code distribution. It was designed with a focus on supporting a
distributed, multi-project development work-model.
  We will describe the underlying technology, and the solutions SCRAM offers to
the above software engineering processes, while taking a users view of the
system under configuration management.

<id>
cs/0306024v1
<category>
cs.OH
<abstract>
The DESY Computer Center is the home of O(1000) computers supplying a wide
range of different services Monitoring such a large installation is a
challenge. After a long time running a SNMP based commercial Network Management
System, the evaluation of a new System was started. There are a lot of
different commercial and freeware products on the market, but none of them
fully satisfied all our requirements. After re-valuating our original
requirements we selected NAGIOS as our monitoring and alarming tool. After a
successful test we are in production since autumn 2002 and are extending the
service to fully support a distributed monitoring and alarming.

<id>
cs/0306033v1
<category>
cs.OH
<abstract>
We present a procedure for the construction of multi-valued t-norms and
t-conorms. Our procedure makes use of a pair of single-valued t-norms and the
respective dual t-conorms and produces interval-valued t-norms and t-conorms.
In this manner we combine desirable characteristics of different t-norms and
t-conorms; if we use the t-norm min and t-conorm max, then the resulting
structure is a superlattice, i.e. the multivalued analog of a lattice.

<id>
cs/0401013v1
<category>
cs.OH
<abstract>
We consider the problem of model--checking for Process Rewrite Systems (PRSs)
in normal form. In a PRS in normal form every rewrite rule either only deals
with procedure calls and procedure termination, possibly with value return,
(this kind of rules allows to capture Pushdown Processes), or only deals with
dynamic activation of processes and synchronization (this kind of rules allows
to capture Petri Nets). The model-checking problem for PRSs and action-based
linear temporal logic (ALTL) is undecidable. However, decidability of
model--checking for PRSs and some interesting fragment of ALTL remains an open
question. In this paper we state decidability results concerning generalized
acceptance properties about infinite derivations (infinite term rewritings) in
PRSs in normal form. As a consequence, we obtain decidability of the
model-checking (restricted to infinite runs) for PRSs in normal form and a
meaningful fragment of ALTL.

<id>
cs/0403004v1
<category>
cs.OH
<abstract>
$\cal{A}$ point $P \in \Real^n$ is represented in Parallel Coordinates by a
polygonal line $\bar{P}$ (see \cite{Insel99a} for a recent survey). Earlier
\cite{inselberg85plane}, a surface $\sigma$ was represented as the {\em
envelope} of the polygonal lines representing it's points. This is ambiguous in
the sense that {\em different} surfaces can provide the {\em same} envelopes.
Here the ambiguity is eliminated by considering the surface $\sigma$ as the
envelope of it's {\em tangent planes} and in turn, representing each of these
planes by $n$-1 points \cite{Insel99a}. This, with some future extension, can
yield a new and unambiguous representation, $\bar{\sigma}$, of the surface
consisting of $n$-1 planar regions whose properties correspond lead to the {\em
recognition} of the surfaces' properties i.e. developable, ruled etc.
\cite{hung92smooth}) and {\em classification} criteria.
  It is further shown that the image (i.e. representation) of an algebraic
surface of degree 2 in $\Real^n$ is a region whose boundary is also an
algebraic curve of degree 2. This includes some {\em non-convex} surfaces which
with the previous ambiguous representation could not be treated. An efficient
construction algorithm for the representation of the quadratic surfaces (given
either by {\em explicit} or {\em implicit} equation) is provided. The results
obtained are suitable for applications, to be presented in a future paper, and
in particular for the approximation of complex surfaces based on their {\em
planar} images. An additional benefit is the elimination of the
``over-plotting'' problem i.e. the ``bunching'' of polygonal lines which often
obscure part of the parallel-coordinate display.

<id>
cs/0403005v1
<category>
cs.OH
<abstract>
${\cal U}$ntil now the representation (i.e. plotting) of curve in Parallel
Coordinates is constructed from the point $\leftrightarrow$ line duality. The
result is a ``line-curve'' which is seen as the envelope of it's tangents.
Usually this gives an unclear image and is at the heart of the
``over-plotting'' problem; a barrier in the effective use of Parallel
Coordinates. This problem is overcome by a transformation which provides
directly the ``point-curve'' representation of a curve. Earlier this was
applied to conics and their generalizations. Here the representation, also
called dual, is extended to all planar algebraic curves. Specifically, it is
shown that the dual of an algebraic curve of degree $n$ is an algebraic of
degree at most $n(n - 1)$ in the absence of singular points. The result that
conics map into conics follows as an easy special case. An algorithm, based on
algebraic geometry using resultants and homogeneous polynomials, is obtained
which constructs the dual image of the curve. This approach has potential
generalizations to multi-dimensional algebraic surfaces and their
approximation. The ``trade-off'' price then for obtaining {\em planar}
representation of multidimensional algebraic curves and hyper-surfaces is the
higher degree of the image's boundary which is also an algebraic curve in
$\|$-coords.

<id>
cs/0403026v1
<category>
cs.OH
<abstract>
Software engineering (SE) and usability engineering (UE), as disciplines,
have reached substantial levels of maturity. Each of these two disciplines is
now well represented with respect to most computer science (CS) curricula. But,
the two disciplines are practiced almost independently - missing oppurtunities
to collaborate, coordinate and communicate about the overall design - and
thereby contributing to system failures. Today, a confluence of several
ingredients contribute to these failures: the increasing importance of the user
interface (UI) component in the overall system, the independent maturation of
the human computer interaction area, and the lack of a cohesive process model
to integrate the UI experts' UE development efforts with that of SE. This in
turn, we believe, is a result of a void in computing curricula: a lack of
education and training regarding the importance of communication, collaboration
and coordination between the SE and UE processes. In this paper we describe the
current approach to teaching SE and UE and its shortcomings. We identify and
analyze the barriers and issues involved in developing systems having
substantial interactive components. We then propose four major themes of
learning for a comprehensive computing curriculum integrating SE, UE, and
system architectures in a project environment.

<id>
cs/0103014v1
<category>
cs.PF
<abstract>
Recent manifestations of apparently faster-than-light effects confirmed our
predictions that the group velocity in transparent optical media can exceed c.
Special relativity is not violated by these phenomena. Moreover, in the
electronic domain, the causality principle does not forbid negative group
delays of analytic signals in electronic circuits, in which the peak of an
output pulse leaves the exit port of a circuit before the peak of the input
pulse enters the input port. Furthermore, pulse distortion for these
superluminal analytic signals can be negligible in both the optical and
electronic domains. Here we suggest an extension of these ideas to the
microelectronic domain. The underlying principle is that negative feedback can
be used to produce negative group delays. Such negative group delays can be
used to cancel out the positive group delays due to transistor latency (e.g.,
the finite RC rise time of MOSFETS caused by their intrinsic gate capacitance),
as well as the propagation delays due to the interconnects between transistors.
Using this principle, it is possible to speed up computer systems.

<id>
cs/0111034v1
<category>
cs.PF
<abstract>
The Common Object Request Broker Architecture (CORBA) is successfully used in
many control systems (CS) for data transfer and device modeling. Communication
rates below 1 millisecond, high reliability, scalability, language independence
and other features make it very attractive. For common types of applications
like error logging, alarm messaging or slow monitoring, one can benefit from
standard CORBA services that are implemented by third parties and save
tremendous amount of developing time. We have started using few CORBA services
on our previous CORBA-based control system for the light source ANKA [1] and
use now several CORBA services for the ALMA Common Software (ACS) [2], the core
of the control system of the Atacama Large Millimeter Array. Our experiences
with the interface repository (IFR), the implementation repository, the naming
service, the property service, telecom log service and the notify service from
different vendors are presented. Performance and scalability benchmarks have
been performed.

<id>
cs/0205062v1
<category>
cs.PF
<abstract>
A number of known techniques for improving cache performance in scientific
computations involve the reordering of the iteration space. Some of these
reorderings can be considered coverings of the iteration space with sets having
small surface-to-volume ratios. Use of such sets may reduce the number of cache
misses in computations of local operators having the iteration space as their
domain. First, we derive lower bounds on cache misses that any algorithm must
suffer while computing a local operator on a grid. Then, we explore coverings
of iteration spaces of structured and unstructured discretization grid
operators which allow us to approach these lower bounds. For structured grids
we introduce a covering by successive minima tiles based on the interference
lattice of the grid. We show that the covering has a small surface-to-volume
ratio and present a computer experiment showing actual reduction of the cache
misses achieved by using these tiles. For planar unstructured grids we show
existence of a covering which reduces the number of cache misses to the level
of that of structured grids. Next, we introduce a class of multidimensional
grids, called starry grids in this paper. These grids represent an abstraction
of unstructured grids used in, for example, molecular simulations and the
solution of partial differential equations. We show that starry grids can be
covered by sets having a low surface-to-volume ratio and, hence have the same
cache efficiency as structured grids. Finally, we present a triangulation of a
three-dimensional cube that has the property that any local operator on the
corresponding grid must incur a significantly larger number of cache misses
than a similar operator on a structured grid of the same size.

<id>
cs/0301005v1
<category>
cs.PF
<abstract>
It has been suggested in voice over IP that an appropriate choice of the
distribution used in modeling the delay jitters, can improve the play-out
algorithm. In this paper, we propose a tool using which, one can determine, at
a given instance, which distribution model best explains the jitter
distribution. This is done using Expectation Maximization, to choose amongst
possible distribution models which include, the i.i.d exponential distribution,
the gamma distribution etc.

<id>
cs/0304015v1
<category>
cs.PF
<abstract>
Monitoring and information services form a key component of a distributed
system, or Grid. A quantitative study of such services can aid in understanding
the performance limitations, advise in the deployment of the systems, and help
evaluate future development work. To this end, we study the performance of
three monitoring and information services for distributed systems: the Globus
Toolkit's Monitoring and Discovery Service (MDS), the European Data Grid
Relational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the
Condor project. We perform experiments to test their scalability with respect
to number of users, number of resources, and amount of data collected. Our
study shows that each approach has different behaviors, often due to their
different design goals. In the four sets of experiments we conducted to
evaluate the performance of the service components under different
circumstances, we found a strong advantage to caching or prefetching the data,
as well as the need to have primary components at well connected sites due to
high load seen by all systems.

<id>
cs/0305054v1
<category>
cs.PF
<abstract>
Monitoring large clusters is a challenging problem. It is necessary to
observe a large quantity of devices with a reasonably short delay between
consecutive observations. The set of monitored devices may include PCs, network
switches, tape libraries and other equipments. The monitoring activity should
not impact the performances of the system. In this paper we present PerfMC, a
monitoring system for large clusters. PerfMC is driven by an XML configuration
file, and uses the Simple Network Management Protocol (SNMP) for data
collection. SNMP is a standard protocol implemented by many networked
equipments, so the tool can be used to monitor a wide range of devices. System
administrators can display informations on the status of each device by
connecting to a WEB server embedded in PerfMC. The WEB server can produce
graphs showing the value of different monitored quantities as a function of
time; it can also produce arbitrary XML pages by applying XSL Transformations
to an internal XML representation of the cluster's status. XSL Transformations
may be used to produce HTML pages which can be displayed by ordinary WEB
browsers. PerfMC aims at being relatively easy to configure and operate, and
highly efficient. It is currently being used to monitor the Italian
Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU
Linux machines.

<id>
cs/0305060v1
<category>
cs.PF
<abstract>
We report on our investigations on some technologies that can be used to
build disk servers and networks of disk servers using commodity hardware and
software solutions. It focuses on the performance that can be achieved by these
systems and gives measured figures for different configurations.
  It is divided into two parts : iSCSI and other technologies and hardware and
software RAID solutions.
  The first part studies different technologies that can be used by clients to
access disk servers using a gigabit ethernet network. It covers block access
technologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for
different numbers of clients and servers.
  The second part compares a system based on 3ware hardware RAID controllers, a
system using linux software RAID and IDE cards and a system mixing both
hardware RAID and software RAID. Performance measurements for reading and
writing are given for different RAID levels.

<id>
cs/0404035v3
<category>
cs.PF
<abstract>
We present some measurements and ideas for response time statistics in ERP
systems. It is shown that the response time distribution of a given transaction
in a given system is generically a log-normal distribution or, in some
situations, a sum of two or more log-normal distributions. We present some
arguments for this form of the distribution based on heuristic rules for
response times, and we show data from performance measurements in actual
systems to support the log-normal form. Deviations of the log-normal form can
often be traced back to performance problems in the system. Consequences for
the interpretation of response time data and for service level agreements are
discussed.

<id>
cs/0608102v1
<category>
cs.PF
<abstract>
The application of decentralized reputation systems is a promising approach
to ensure cooperation and fairness, as well as to address random failures and
malicious attacks in Mobile Ad-Hoc Networks. However, they are potentially
vulnerable to liars. With our work, we provide a first step to analyzing
robustness of a reputation system based on a deviation test. Using a mean-field
approach to our stochastic process model, we show that liars have no impact
unless their number exceeds a certain threshold (phase transition). We give
precise formulae for the critical values and thus provide guidelines for an
optimal choice of parameters.

<id>
cs/0610173v1
<category>
cs.PF
<abstract>
Decentralized search aims to find the target node in a large network by using
only local information. The applications of it include peer-to-peer file
sharing, web search and anything else that requires locating a specific target
in a complex system. In this paper, we examine the degree-based decentralized
search method. Specifically, we evaluate the efficiency of the method in
different cases with different amounts of available local information. In
addition, we propose a simple refinement algorithm for significantly shortening
the length of the route that has been found. Some insights useful for the
future developments of efficient decentralized search schemes have been
achieved.

<id>
cs/0611087v1
<category>
cs.PF
<abstract>
E-commerce Web-servers often face overload conditions during which
revenue-generating requests may be dropped or abandoned due to an increase in
the browsing requests. In this paper we present a simple, yet effective,
mechanism for overload control of E-commerce Web-servers. We develop an
E-commerce workload model that separates the browsing requests from
revenue-generating transaction requests. During overload, we apply LIFO
discipline in the browsing queues and use a dynamic priority model to service
them. The transaction queues are given absolute priority over the browsing
queues. This is called the LIFO-Pri scheduling discipline. Experimental results
show that LIFO-Pri dramatically improves the overall Web-server throughput
while also increasing the completion rate of revenue-generating requests. The
Web-server was able to operate at nearly 60% of its maximum capacity even when
offered load was 1.5 times its capacity. Further, when compared to a single
queue FIFO system, there was a seven-fold increase in the number of completed
revenue-generating requests during overload.

<id>
cs/0612141v1
<category>
cs.PF
<abstract>
This paper shows how the steady-state availability and failure frequency can
be calculated in a single pass for very large systems, when the availability is
expressed as a product of matrices. We apply the general procedure to
$k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a
simple ladder network in which each edge and node may fail. We also give the
associated generating functions when the components have identical
availabilities and failure rates. For large systems, the failure rate of the
whole system is asymptotically proportional to its size. This paves the way to
ready-to-use formulae for various architectures, as well as proof that the
differential operator approach to failure frequency calculations is very useful
and straightforward.

<id>
cs/0612143v1
<category>
cs.PF
<abstract>
The exact calculation of network reliability in a probabilistic context has
been a long-standing issue of practical importance, but a difficult one, even
for planar graphs, with perfect nodes and with edges of identical reliability
p. Many approaches (determination of bounds, sums of disjoint products
algorithms, Monte Carlo evaluations, studies of the reliability polynomials,
etc.) can only provide approximations when the network's size increases. We
consider here a ladder graph of arbitrary size corresponding to real-life
network configurations, and give the exact, analytical solutions for the all-
and two-terminal reliabilities. These solutions use transfer matrices, in which
individual reliabilities of edges and nodes are taken into account. The special
case of identical edge and node reliabilities -- p and rho, respectively -- is
solved. We show that the zeros of the two-terminal reliability polynomial
exhibit structures which differ substantially for seemingly similar networks,
and we compare the sensitivity of various edges. We discuss how the present
work may be further extended to lead to a catalog of exactly solvable networks
in terms of reliability, which could be useful as elementary bricks for a new
and improved set of bounds or benchmarks in the general case.

<id>
cs/0701005v1
<category>
cs.PF
<abstract>
The two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the
generalized fan have been calculated exactly for arbitrary size as well as
arbitrary individual edge and node reliabilities, using transfer matrices of
dimension four at most. While the all-terminal reliabilities of these graphs
are identical, the special case of identical edge ($p$) and node ($\rho$)
reliabilities shows that their two-terminal reliabilities are quite distinct,
as demonstrated by their generating functions and the locations of the zeros of
the reliability polynomials, which undergo structural transitions at $\rho =
\displaystyle {1/2}$.

<id>
cs/0702135v1
<category>
cs.PF
<abstract>
We present the results of gravitational direct $N$-body simulations using the
commercial graphics processing units (GPU) NVIDIA Quadro FX1400 and GeForce
8800GTX, and compare the results with GRAPE-6Af special purpose hardware. The
force evaluation of the $N$-body problem was implemented in Cg using the GPU
directly to speed-up the calculations. The integration of the equations of
motions were, running on the host computer, implemented in C using the 4th
order predictor-corrector Hermite integrator with block time steps. We find
that for a large number of particles ($N \apgt 10^4$) modern graphics
processing units offer an attractive low cost alternative to GRAPE special
purpose hardware. A modern GPU continues to give a relatively flat scaling with
the number of particles, comparable to that of the GRAPE. Using the same time
step criterion the total energy of the $N$-body system was conserved better
than to one in $10^6$ on the GPU, which is only about an order of magnitude
worse than obtained with GRAPE. For $N\apgt 10^6$ the GeForce 8800GTX was about
20 times faster than the host computer. Though still about an order of
magnitude slower than GRAPE, modern GPU's outperform GRAPE in their low cost,
long mean time between failure and the much larger onboard memory; the
GRAPE-6Af holds at most 256k particles whereas the GeForce 8800GTF can hold 9
million particles in memory.

<id>
cs/0703086v2
<category>
cs.PF
<abstract>
Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.

<id>
0705.1915v3
<category>
cs.PF
<abstract>
Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.

<id>
0710.4633v1
<category>
cs.PF
<abstract>
New nanotechnology based devices are replacing CMOS devices to overcome CMOS
technology's scaling limitations. However, many such devices exhibit
non-monotonic I-V characteristics and uncertain properties which lead to the
negative differential resistance (NDR) problem and the chaotic performance.
This paper proposes a new circuit simulation approach that can effectively
simulate nanotechnology devices with uncertain input sources and negative
differential resistance (NDR) problem. The experimental results show a 20-30
times speedup comparing with existing simulators.

<id>
0710.4701v1
<category>
cs.PF
<abstract>
This paper presents a scheme for efficient channel usage between simulator
and accelerator where the accelerator models some RTL sub-blocks in the
accelerator-based hardware/software co-simulation while the simulator runs
transaction-level model of the remaining part of the whole chip being verified.
With conventional simulation accelerator, evaluations of simulator and
accelerator alternate at every valid simulation time, which results in poor
simulation performance due to startup overhead of simulator-accelerator channel
access. The startup overhead can be reduced by merging multiple transactions on
the channel into a single burst traffic. We propose a predictive packetizing
scheme for reducing channel traffic by merging as many transactions into a
burst traffic as possible based on 'prediction and rollback.' Under ideal
condition with 100% prediction accuracy, the proposed method shows a
performance gain of 1500% compared to the conventional one.

<id>
0710.4723v1
<category>
cs.PF
<abstract>
This paper reports a novel simulation methodology for analysis and prediction
of substrate noise impact on analog / RF circuits taking into account the role
of the parasitic resistance of the on-chip interconnect in the impact
mechanism. This methodology allows investigation of the role of the separate
devices (also parasitic devices) in the analog / RF circuit in the overall
impact. This way is revealed which devices have to be taken care of (shielding,
topology change) to protect the circuit against substrate noise. The developed
methodology is used to analyze impact of substrate noise on a 3 GHz LC-tank
Voltage Controlled Oscillator (VCO) designed in a high-ohmic 0.18 $\mu$m 1PM6
CMOS technology. For this VCO (in the investigated frequency range from DC to
15 MHz) impact is mainly caused by resistive coupling of noise from the
substrate to the non-ideal on-chip ground interconnect, resulting in analog
ground bounce and frequency modulation. Hence, the presented test-case reveals
the important role of the on-chip interconnect in the phenomenon of substrate
noise impact.

<id>
0807.0626v1
<category>
cs.PF
<abstract>
This paper deals with asymptotic expressions of the Mean Time To Failure
(MTTF) and higher moments for large, recursive, and non-repairable systems in
the context of two-terminal reliability. Our aim is to extend the well-known
results of the series and parallel cases. We first consider several exactly
solvable configurations of identical components with exponential failure-time
distribution functions to illustrate different (logarithmic or power-law)
behaviors as the size of the system, indexed by an integer n, increases. The
general case is then addressed: it provides a simple interpretation of the
origin of the power-law exponent and an efficient asymptotic expression for the
total reliability of large, recursive systems. Finally, we assess the influence
of the non-exponential character of the component reliability on the
n-dependence of the MTTF.

<id>
0807.0629v1
<category>
cs.PF
<abstract>
The calculation of network reliability in a probabilistic context has long
been an issue of practical and academic importance. Conventional approaches
(determination of bounds, sums of disjoint products algorithms, Monte Carlo
evaluations, studies of the reliability polynomials, etc.) only provide
approximations when the network's size increases, even when nodes do not fail
and all edges have the same reliability p. We consider here a directed, generic
graph of arbitrary size mimicking real-life long-haul communication networks,
and give the exact, analytical solution for the two-terminal reliability. This
solution involves a product of transfer matrices, in which individual
reliabilities of edges and nodes are taken into account. The special case of
identical edge and node reliabilities (p and rho, respectively) is addressed.
We consider a case study based on a commonly-used configuration, and assess the
influence of the edges being directed (or not) on various measures of network
performance. While the two-terminal reliability, the failure frequency and the
failure rate of the connection are quite similar, the locations of complex
zeros of the two-terminal reliability polynomials exhibit strong differences,
and various structure transitions at specific values of rho. The present work
could be extended to provide a catalog of exactly solvable networks in terms of
reliability, which could be useful as building blocks for new and improved
bounds, as well as benchmarks, in the general case.

<id>
0807.0993v1
<category>
cs.PF
<abstract>
With the advent of increasingly complex hardware in real-time embedded
systems (processors with performance enhancing features such as pipelines,
cache hierarchy, multiple cores), many processors now have a set-associative L2
cache. Thus, there is a need for considering cache hierarchies when validating
the temporal behavior of real-time systems, in particular when estimating
tasks' worst-case execution times (WCETs). To the best of our knowledge, there
is only one approach for WCET estimation for systems with cache hierarchies
[Mueller, 1997], which turns out to be unsafe for set-associative caches. In
this paper, we highlight the conditions under which the approach described in
[Mueller, 1997] is unsafe. A safe static instruction cache analysis method is
then presented. Contrary to [Mueller, 1997] our method supports set-associative
and fully associative caches. The proposed method is experimented on
medium-size and large programs. We show that the method is most of the time
tight. We further show that in all cases WCET estimations are much tighter when
considering the cache hierarchy than when considering only the L1 cache. An
evaluation of the analysis time is conducted, demonstrating that analysing the
cache hierarchy has a reasonable computation time.

<id>
0808.3100v1
<category>
cs.PF
<abstract>
New information technologies provide a lot of prospects for performance
improvement. One of them is "Dynamic Source Code Generation and Compilation".
This article shows how this way provides high performance for engineering
problems.

<id>
0811.1151v1
<category>
cs.PF
<abstract>
In this paper, we present a probabilistic adaptation of an Assume/Guarantee
contract formalism. For the sake of generality, we assume that the extended
state machines used in the contracts and implementations define sets of runs on
a given set of variables, that compose by intersection over the common
variables. In order to enable probabilistic reasoning, we consider that the
contracts dictate how certain input variables will behave, being either
non-deterministic, or probabilistic; the introduction of probabilistic
variables leading us to tune the notions of implementation, refinement and
composition. As shown in the report, this probabilistic adaptation of the
Assume/Guarantee contract theory preserves compositionality and therefore
allows modular reliability analysis, either with a top-down or a bottom-up
approach.

<id>
0812.0904v1
<category>
cs.PF
<abstract>
In this letter, we present a closed-form approximation of the outage
probability for the multi-hop amplify-and-forward (AF) relaying systems with
fixed gain in Rayleigh fading channel. The approximation is derived from the
outage event for each hop. The simulation results show the tightness of the
proposed approximation in low and high signal-to-noise ratio (SNR) region.

<id>
0901.0148v1
<category>
cs.PF
<abstract>
In order to achieve both fast and coordinated data transfer to collaborative
sites as well as to create a distribution of data over multiple sites,
efficient data movement is one of the most essential aspects in distributed
environment. With such capabilities at hand, truly distributed task scheduling
with minimal latencies would be reachable by internationally distributed
collaborations (such as ones in HENP) seeking for scavenging or maximizing on
geographically spread computational resources. But it is often not all clear
(a) how to move data when available from multiple sources or (b) how to move
data to multiple compute resources to achieve an optimal usage of available
resources. We present a method of creating a Constraint Programming (CP) model
consisting of sites, links and their attributes such as bandwidth for grid
network data transfer also considering user tasks as part of the objective
function for an optimal solution. We will explore and explain trade-off between
schedule generation time and divergence from the optimal solution and show how
to improve and render viable the solution's finding time by using search tree
time limit, approximations, restrictions such as symmetry breaking or grouping
similar tasks together, or generating sequence of optimal schedules by
splitting the input problem. Results of data transfer simulation for each case
will also include a well known Peer-2-Peer model, and time taken to generate a
schedule as well as time needed for a schedule execution will be compared to a
CP optimal solution. We will additionally present a possible implementation
aimed to bring a distributed datasets (multiple sources) to a given site in a
minimal time.

<id>
0902.1035v8
<category>
cs.PF
<abstract>
The community of program optimisation and analysis, code performance
evaluation, parallelisation and optimising compilation has published since many
decades hundreds of research and engineering articles in major conferences and
journals. These articles study efficient algorithms, strategies and techniques
to accelerate programs execution times, or optimise other performance metrics
(MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but
nobody is able to reproduce them exactly. The non-reproducibility of our
research results is a dark point of the art, and we cannot be qualified as {\it
computer scientists} if we do not provide rigorous experimental methodology.
This article provides a first effort towards a correct statistical protocol for
analysing and measuring speedups. As we will see, some common mistakes are done
by the community inside published articles, explaining part of the
non-reproducibility of the results. Our current article is not sufficient by
its own to deliver a complete experimental methodology, further efforts must be
done by the community to decide about a common protocol for our future
experiences. Anyway, our community should take care about the aspect of
reproducibility of the results in the future.

<id>
0902.3065v1
<category>
cs.PF
<abstract>
We propose a new exact solution algorithm for closed multiclass product-form
queueing networks that is several orders of magnitude faster and less memory
consuming than established methods for multiclass models, such as the Mean
Value Analysis (MVA) algorithm. The technique is an important generalization of
the recently proposed Method of Moments (MoM) which, differently from MVA,
recursively computes higher-order moments of queue-lengths instead of mean
values.
  The main contribution of this paper is to prove that the information used in
the MoM recursion can be increased by considering multiple recursive branches
that evaluate models with different number of queues. This reformulation allows
to formulate a simpler matrix difference equation which leads to large
computational savings with respect to the original MoM recursion. Computational
analysis shows several cases where the proposed algorithm is between 1,000 and
10,000 times faster and less memory consuming than the original MoM, thus
extending the range of multiclass models where exact solutions are feasible.

<id>
0907.3047v1
<category>
cs.PF
<abstract>
The efficiency and the performance of anagement systems is becoming a hot
research topic within the networks and services management community. This
concern is due to the new challenges of large scale managed systems, where the
management plane is integrated within the functional plane and where management
activities have to carry accurate and up-to-date information. We defined a set
of primary and secondary metrics to measure the performance of a management
approach. Secondary metrics are derived from the primary ones and quantifies
mainly the efficiency, the scalability and the impact of management activities.
To validate our proposals, we have designed and developed a benchmarking
platform dedicated to the measurement of the performance of a JMX manager-agent
based management system. The second part of our work deals with the collection
of measurement data sets from our JMX benchmarking platform. We mainly studied
the effect of both load and the number of agents on the scalability, the impact
of management activities on the user perceived performance of a managed server
and the delays of JMX operations when carrying variables values. Our findings
show that most of these delays follow a Weibull statistical distribution. We
used this statistical model to study the behavior of a monitoring algorithm
proposed in the literature, under heavy tail delays distribution. In this case,
the view of the managed system on the manager side becomes noisy and out of
date.

<id>
cs/9401102v1
<category>
cs.PL
<abstract>
This paper describes how to implement a documentation technique that helps
readers to understand large programs or collections of programs, by providing
local indexes to all identifiers that are visible on every two-page spread. A
detailed example is given for a program that finds all Hamiltonian circuits in
an undirected graph.

<id>
cs/9809008v1
<category>
cs.PL
<abstract>
The Asynchronous pi-calculus, as recently proposed by Boudol and,
independently, by Honda and Tokoro, is a subset of the pi-calculus which
contains no explicit operators for choice and output-prefixing. The
communication mechanism of this calculus, however, is powerful enough to
simulate output-prefixing, as shown by Boudol, and input-guarded choice, as
shown recently by Nestmann and Pierce. A natural question arises, then, whether
or not it is possible to embed in it the full pi-calculus. We show that this is
not possible, i.e. there does not exist any uniform, parallel-preserving,
translation from the pi-calculus into the asynchronous pi-calculus, up to any
``reasonable'' notion of equivalence. This result is based on the incapablity
of the asynchronous pi-calculus of breaking certain symmetries possibly present
in the initial communication graph. By similar arguments, we prove a separation
result between the pi-calculus and CCS.

<id>
cs/9809016v1
<category>
cs.PL
<abstract>
The inclusion of universal quantification and a form of implication in goals
in logic programming is considered. These additions provide a logical basis for
scoping but they also raise new implementation problems. When universal and
existential quantifiers are permitted to appear in mixed order in goals, the
devices of logic variables and unification that are employed in solving
existential goals must be modified to ensure that constraints arising out of
the order of quantification are respected. Suitable modifications that are
based on attaching numerical tags to constants and variables and on using these
tags in unification are described. The resulting devices are amenable to an
efficient implementation and can, in fact, be assimilated easily into the usual
machinery of the Warren Abstract Machine (WAM). The provision of implications
in goals results in the possibility of program clauses being added to the
program for the purpose of solving specific subgoals. A naive scheme based on
asserting and retracting program clauses does not suffice for implementing such
additions for two reasons. First, it is necessary to also support the
resurrection of an earlier existing program in the face of backtracking.
Second, the possibility for implication goals to be surrounded by quantifiers
requires a consideration of the parameterization of program clauses by bindings
for their free variables. Devices for supporting these additional requirements
are described as also is the integration of these devices into the WAM. Further
extensions to the machine are outlined for handling higher-order additions to
the language. The ideas presented here are relevant to the implementation of
the higher-order logic programming language lambda Prolog.

<id>
cs/9810027v1
<category>
cs.PL
<abstract>
Reflective systems allow their own structures to be altered from within. Here
we are concerned with a style of reflection, called linguistic reflection,
which is the ability of a running program to generate new program fragments and
to integrate these into its own execution. In particular we describe how this
kind of reflection may be provided in the compiler-based, strongly typed
object-oriented programming language Java. The advantages of the programming
technique include attaining high levels of genericity and accommodating system
evolution. These advantages are illustrated by an example taken from persistent
programming which shows how linguistic reflection allows functionality (program
code) to be generated on demand (Just-In-Time) from a generic specification and
integrated into the evolving running program. The technique is evaluated
against alternative implementation approaches with respect to efficiency,
safety and ease of use.

<id>
cs/9811001v1
<category>
cs.PL
<abstract>
A polymorphic analysis is an analysis whose input and output contain
parameters which serve as placeholders for information that is unknown before
analysis but provided after analysis. In this paper, we present a polymorphic
groundness analysis that infers parameterised groundness descriptions of the
variables of interest at a program point. The polymorphic groundness analysis
is designed by replacing two primitive operators used in a monomorphic
groundness analysis and is shown to be as precise as the monomorphic groundness
analysis for any possible values for mode parameters. Experimental results of a
prototype implementation of the polymorphic groundness analysis are given.

<id>
cs/9901007v1
<category>
cs.PL
<abstract>
The universal object oriented languages made programming more simple and
efficient. In the article is considered possibilities of using similar methods
in computer algebra. A clear and powerful universal language is useful if
particular problem was not implemented in standard software packages like
REDUCE, MATHEMATICA, etc. and if the using of internal programming languages of
the packages looks not very efficient.
  Functional languages like LISP had some advantages and traditions for
algebraic and symbolic manipulations. Functional and object oriented
programming are not incompatible ones. An extension of the model of an object
for manipulation with pure functions and algebraic expressions is considered.

<id>
cs/9911001v2
<category>
cs.PL
<abstract>
By paying more attention to semantics-based tool generation, programming
language semantics can significantly increase its impact. Ultimately, this may
lead to ``Language Design Assistants'' incorporating substantial amounts of
semantic knowledge.

<id>
cs/0001003v1
<category>
cs.PL
<abstract>
With no intent of starting a holy war, this paper lists several annoying C++
birthmarks that the contributor has come across developing GUI class libraries.
C++'s view of classes, instances and hierarchies appears tantalizingly close to
GUI concepts of controls, widgets, window classes and subwindows. OO models of
C++ and of a window system are however different. C++ was designed to be a
"static" language with a lexical name scoping, static type checking and
hierarchies defined at compile time. Screen objects on the other hand are
inherently dynamic; they usually live well beyond the procedure/block that
created them; the hierarchy of widgets is defined to a large extent by layout,
visibility and event flow. Many GUI fundamentals such as dynamic and geometric
hierarchies of windows and controls, broadcasting and percolation of events are
not supported directly by C++ syntax or execution semantics (or supported as
"exceptions" -- pun intended). Therefore these features have to be emulated in
C++ GUI code. This leads to duplication of a graphical toolkit or a window
manager functionality, code bloat, engaging in unsafe practices and forgoing of
many strong C++ features (like scoping rules and compile-time type checking).
This paper enumerates a few major C++/GUI sores and illustrates them on simple
examples.

<id>
cs/0001009v1
<category>
cs.PL
<abstract>
Restructuring compilers use dependence analysis to prove that the meaning of
a program is not changed by a transformation. A well-known limitation of
dependence analysis is that it examines only the memory locations read and
written by a statement, and does not assume any particular interpretation for
the operations in that statement. Exploiting the semantics of these operations
enables a wider set of transformations to be used, and is critical for
optimizing important codes such as LU factorization with pivoting.
  Symbolic execution of programs enables the exploitation of such semantic
properties, but it is intractable for all but the simplest programs. In this
paper, we propose a new form of symbolic analysis for use in restructuring
compilers. Fractal symbolic analysis compares a program and its transformed
version by repeatedly simplifying these programs until symbolic analysis
becomes tractable, ensuring that equality of simplified programs is sufficient
to guarantee equality of the original programs. We present a prototype
implementation of fractal symbolic analysis, and show how it can be used to
optimize the cache performance of LU factorization with pivoting.

<id>
cs/0003010v1
<category>
cs.PL
<abstract>
The Task System and Item Architecture (TSIA) is a model for transparent
application execution. In many real-world projects, a TSIA provides a simple
application with a transparent reliable, distributed, heterogeneous, adaptive,
dynamic, real-time, parallel, secure or other execution. TSIA is suitable for
many applications, not just for the simple applications served to date. This
presentation shows that TSIA is a dataflow model - a long-standing model for
transparent parallel execution. The advances to the dataflow model include a
simple semantics, as well as support for input/output, for modifiable items and
for other such effects.

<id>
cs/0004006v1
<category>
cs.PL
<abstract>
In (Ferrucci, Pacini and Sessa, 1995) an extended form of resolution, called
Reduced SLD resolution (RSLD), is introduced. In essence, an RSLD derivation is
an SLD derivation such that redundancy elimination from resolvents is performed
after each rewriting step. It is intuitive that redundancy elimination may have
positive effects on derivation process. However, undesiderable effects are also
possible. In particular, as shown in this paper, program termination as well as
completeness of loop checking mechanisms via a given selection rule may be
lost. The study of such effects has led us to an analysis of selection rule
basic concepts, so that we have found convenient to move the attention from
rules of atom selection to rules of atom scheduling. A priority mechanism for
atom scheduling is built, where a priority is assigned to each atom in a
resolvent, and primary importance is given to the event of arrival of new atoms
from the body of the applied clause at rewriting time. This new computational
model proves able to address the study of redundancy elimination effects,
giving at the same time interesting insights into general properties of
selection rules. As a matter of fact, a class of scheduling rules, namely the
specialisation independent ones, is defined in the paper by using not trivial
semantic arguments. As a quite surprising result, specialisation independent
scheduling rules turn out to coincide with a class of rules which have an
immediate structural characterisation (named stack-queue rules). Then we prove
that such scheduling rules are tolerant to redundancy elimination, in the sense
that neither program termination nor completeness of equality loop check is
lost passing from SLD to RSLD.

<id>
cs/0004011v1
<category>
cs.PL
<abstract>
Forty years ago Dijkstra introduced the current conventional execution of
routines. It places activation frames onto a stack. Each frame is the internal
state of an executing routine. The resulting application execution is not
easily helped by an external system. This presentation proposes an alternative
execution of routines. It places task frames onto the stack. A task frame is
the call of a routine to be executed. The feasibility of the alternative
execution is demonstrated by a crude implementation. As described elsewhere, an
application which executes in terms of tasks can be provided by an external
system with a transparent reliable, distributed, heterogeneous, adaptive,
dynamic, real-time, parallel, secure or other execution. By extending the crude
implementation, this presentation outlines a simple transparent parallel
execution.

<id>
cs/0005002v1
<category>
cs.PL
<abstract>
While application software does the real work, domain-specific languages
(DSLs) are tools to help produce it efficiently, and language design assistants
in turn are meta-tools to help produce DSLs quickly. DSLs are already in wide
use (HTML for web pages, Excel macros for spreadsheet applications, VHDL for
hardware design, ...), but many more will be needed for both new as well as
existing application domains. Language design assistants to help develop them
currently exist only in the basic form of language development systems. After a
quick look at domain-specific languages, and especially their relationship to
application libraries, we survey existing language development systems and give
an outline of future language design assistants.

<id>
cs/0005023v1
<category>
cs.PL
<abstract>
The aim of this work is to define and implement an extended C++ language to
support the SIMD programming paradigm. The C++ programming language has been
extended to express all the potentiality of an abstract SIMD machine consisting
of a central Control Processor and a N-dimensional toroidal array of Numeric
Processors. Very few extensions have been added to the standard C++ with the
goal of minimising the effort for the programmer in learning a new language and
to keep very high the performance of the compiled code. The proposed language
has been implemented as a porting of the GNU C++ Compiler on a SIMD
supercomputer.

<id>
cs/0005033v1
<category>
cs.PL
<abstract>
The goal of this paper is the description and analysis of multimethod
implementation in a new object-oriented, class-based programming language
called OOLANG. The implementation of the multimethod typecheck and selection,
deeply analyzed in the paper, is performed in two phases in order to allow
static typechecking and separate compilation of modules. The first phase is
performed at compile time, while the second is executed at link time and does
not require the modules' source code. OOLANG has syntax similar to C++; the
main differences are the absence of pointers and the realization of
polymorphism through subsumption. It adopts the C++ object model and supports
multiple inheritance as well as virtual base classes. For this reason, it has
been necessary to define techniques for realigning argument and return value
addresses when performing multimethod invocations.

<id>
cs/0006034v1
<category>
cs.PL
<abstract>
Type classes are an elegant extension to traditional, Hindley-Milner based
typing systems. They are used in modern, typed languages such as Haskell to
support controlled overloading of symbols. Haskell 98 supports only
single-parameter and constructor type classes. Other extensions such as
multi-parameter type classes are highly desired but are still not officially
supported by Haskell. Subtle issues arise with extensions, which may lead to a
loss of feasible type inference or ambiguous programs. A proper logical basis
for type class systems seems to be missing. Such a basis would allow extensions
to be characterised and studied rigorously. We propose to employ Constraint
Handling Rules as a tool to study and develop type class systems in a uniform
way.

<id>
cs/0009029v1
<category>
cs.PL
<abstract>
Aldwych is proposed as the foundation of a general purpose language for
parallel applications. It works on a rule-based principle, and has aspects
variously of concurrent functional, logic and object-oriented languages, yet it
forms an integrated whole. It is intended to be applicable both for small-scale
parallel programming, and for large-scale open systems.

<id>
cs/0010009v1
<category>
cs.PL
<abstract>
We describe an approach to programming rule-based systems in Standard ML,
with a focus on so-called overlapping rules, that is rules that can still be
active when other rules are fired. Such rules are useful when implementing
rule-based reactive systems, and to that effect we show a simple implementation
of Loyall's Active Behavior Trees, used to control goal-directed agents in the
Oz virtual environment. We discuss an implementation of our framework using a
reactive library geared towards implementing those kind of systems.

<id>
cs/0010016v1
<category>
cs.PL
<abstract>
This paper illustrates how the diagram programming language DiaPlan can be
used to program visual systems. DiaPlan is a visual rule-based language that is
founded on the computational model of graph transformation. The language
supports object-oriented programming since its graphs are hierarchically
structured. Typing allows the shape of these graphs to be specified recursively
in order to increase program security. Thanks to its genericity, DiaPlan allows
to implement systems that represent and manipulate data in arbitrary diagram
notations. The environment for the language exploits the diagram editor
generator DiaGen for providing genericity, and for implementing its user
interface and type checker.

<id>
cs/0011025v1
<category>
cs.PL
<abstract>
We present a new approach to termination analysis of logic programs. The
essence of the approach is that we make use of general term-orderings (instead
of level mappings), like it is done in transformational approaches to logic
program termination analysis, but that we apply these orderings directly to the
logic program and not to the term-rewrite system obtained through some
transformation. We define some variants of acceptability, based on general
term-orderings, and show how they are equivalent to LD-termination. We develop
a demand driven, constraint-based approach to verify these
acceptability-variants.
  The advantage of the approach over standard acceptability is that in some
cases, where complex level mappings are needed, fairly simple term-orderings
may be easily generated. The advantage over transformational approaches is that
it avoids the transformation step all together.

<id>
cs/0011036v1
<category>
cs.PL
<abstract>
For logic programs with arithmetic predicates, showing termination is not
easy, since the usual order for the integers is not well-founded. A new method,
easily incorporated in the TermiLog system for automatic termination analysis,
is presented for showing termination in this case.
  The method consists of the following steps: First, a finite abstract domain
for representing the range of integers is deduced automatically. Based on this
abstraction, abstract interpretation is applied to the program. The result is a
finite number of atoms abstracting answers to queries which are used to extend
the technique of query-mapping pairs. For each query-mapping pair that is
potentially non-terminating, a bounded (integer-valued) termination function is
guessed. If traversing the pair decreases the value of the termination
function, then termination is established. Simple functions often suffice for
each query-mapping pair, and that gives our approach an edge over the classical
approach of using a single termination function for all loops, which must
inevitably be more complicated and harder to guess automatically. It is worth
noting that the termination of McCarthy's 91 function can be shown
automatically using our method.
  In summary, the proposed approach is based on combining a finite abstraction
of the integers with the technique of the query-mapping pairs, and is
essentially capable of dividing a termination proof into several cases, such
that a simple termination function suffices for each case. Consequently, the
whole process of proving termination can be done automatically in the framework
of TermiLog and similar systems.

<id>
cs/0012008v1
<category>
cs.PL
<abstract>
This paper describes a general framework for automatic termination analysis
of logic programs, where we understand by ``termination'' the finitenes s of
the LD-tree constructed for the program and a given query. A general property
of mappings from a certain subset of the branches of an infinite LD-tree into a
finite set is proved. From this result several termination theorems are
derived, by using different finite sets. The first two are formulated for the
predicate dependency and atom dependency graphs. Then a general result for the
case of the query-mapping pairs relevant to a program is proved (cf.
\cite{Sagiv,Lindenstrauss:Sagiv}). The correctness of the {\em TermiLog} system
described in \cite{Lindenstrauss:Sagiv:Serebrenik} follows from it. In this
system it is not possible to prove termination for programs involving
arithmetic predicates, since the usual order for the integers is not
well-founded. A new method, which can be easily incorporated in {\em TermiLog}
or similar systems, is presented, which makes it possible to prove termination
for programs involving arithmetic predicates. It is based on combining a finite
abstraction of the integers with the technique of the query-mapping pairs, and
is essentially capable of dividing a termination proof into several cases, such
that a simple termination function suffices for each case. Finally several
possible extensions are outlined.

<id>
cs/0101025v1
<category>
cs.PL
<abstract>
Complementation, the inverse of the reduced product operation, is a technique
for systematically finding minimal decompositions of abstract domains. File'
and Ranzato advanced the state of the art by introducing a simple method for
computing a complement. As an application, they considered the extraction by
complementation of the pair-sharing domain PS from the Jacobs and Langen's
set-sharing domain SH. However, since the result of this operation was still
SH, they concluded that PS was too abstract for this. Here, we show that the
source of this result lies not with PS but with SH and, more precisely, with
the redundant information contained in SH with respect to ground-dependencies
and pair-sharing. In fact, a proper decomposition is obtained if the
non-redundant version of SH, PSD, is substituted for SH. To establish the
results for PSD, we define a general schema for subdomains of SH that includes
PSD and Def as special cases. This sheds new light on the structure of PSD and
exposes a natural though unexpected connection between Def and PSD. Moreover,
we substantiate the claim that complementation alone is not sufficient to
obtain truly minimal decompositions of domains. The right solution to this
problem is to first remove redundancies by computing the quotient of the domain
with respect to the observable behavior, and only then decompose it by
complementation.

<id>
cs/0102025v2
<category>
cs.PL
<abstract>
In this paper we investigate the theoretical foundation of a new bottom-up
semantics for linear logic programs, and more precisely for the fragment of
LinLog that consists of the language LO enriched with the constant 1. We use
constraints to symbolically and finitely represent possibly infinite
collections of provable goals. We define a fixpoint semantics based on a new
operator in the style of Tp working over constraints. An application of the
fixpoint operator can be computed algorithmically. As sufficient conditions for
termination, we show that the fixpoint computation is guaranteed to converge
for propositional LO. To our knowledge, this is the first attempt to define an
effective fixpoint semantics for linear logic programs. As an application of
our framework, we also present a formal investigation of the relations between
LO and Disjunctive Logic Programming. Using an approach based on abstract
interpretation, we show that DLP fixpoint semantics can be viewed as an
abstraction of our semantics for LO. We prove that the resulting abstraction is
correct and complete for an interesting class of LO programs encoding Petri
Nets.

<id>
cs/0102030v1
<category>
cs.PL
<abstract>
It is important that practical data-flow analyzers are backed by reliably
proven theoretical results. Abstract interpretation provides a sound
mathematical framework and necessary generic properties for an abstract domain
to be well-defined and sound with respect to the concrete semantics. In logic
programming, the abstract domain Sharing is a standard choice for sharing
analysis for both practical work and further theoretical study. In spite of
this, we found that there were no satisfactory proofs for the key properties of
commutativity and idempotence that are essential for Sharing to be well-defined
and that published statements of the soundness of Sharing assume the
occurs-check. This paper provides a generalization of the abstraction function
for Sharing that can be applied to any language, with or without the
occurs-check. Results for soundness, idempotence and commutativity for abstract
unification using this abstraction function are proven.

<id>
cs/0105011v1
<category>
cs.PL
<abstract>
Prolog was once the main host for implementing constraint solvers.
  It seems that it is no longer so. To be useful, constraint solvers have to be
integrable into industrial applications written in imperative or
object-oriented languages; to be efficient, they have to interact with other
solvers. To meet these requirements, many solvers are now implemented in the
form of extensible object-oriented libraries. Following Pfister and Szyperski,
we argue that ``objects are not enough,'' and we propose to design solvers as
component-oriented libraries. We illustrate our approach by the description of
the architecture of a prototype, and we assess its strong points and
weaknesses.

<id>
cs/0109002v1
<category>
cs.PL
<abstract>
We propose an extension of the asynchronous pi-calculus with a notion of
random choice. We define an operational semantics which distinguishes between
probabilistic choice, made internally by the process, and nondeterministic
choice, made externally by an adversary scheduler. This distinction will allow
us to reason about the probabilistic correctness of algorithms under certain
schedulers. We show that in this language we can solve the electoral problem,
which was proved not possible in the asynchronous $\pi$-calculus. Finally, we
show an implementation of the probabilistic asynchronous pi-calculus in a
Java-like language.

<id>
cs/0109003v1
<category>
cs.PL
<abstract>
We consider a generalization of the dining philosophers problem to arbitrary
connection topologies. We focus on symmetric, fully distributed systems, and we
address the problem of guaranteeing progress and lockout-freedom, even in
presence of adversary schedulers, by using randomized algorithms. We show that
the well-known algorithms of Lehmann and Rabin do not work in the generalized
case, and we propose an alternative algorithm based on the idea of letting the
philosophers assign a random priority to their adjacent forks.

<id>
cs/0109024v1
<category>
cs.PL
<abstract>
ELAN is a powerful language and environment for specifying and prototyping
deduction systems in a language based on rewrite rules controlled by
strategies. Timed automata is a class of continuous real-time models of
reactive systems for which efficient model-checking algorithms have been
devised. In this paper, we show that these algorithms can very easily be
prototyped in the ELAN system. This paper argues through this example that
rewriting based systems relying on rules and strategies are a good framework to
prototype, study and test rather efficiently symbolic model-checking
algorithms, i.e. algorithms which involve combination of graph exploration
rules, deduction rules, constraint solving techniques and decision procedures.

<id>
cs/0109033v1
<category>
cs.PL
<abstract>
Nomadic applications create replicas of shared objects that evolve
independently while they are disconnected. When reconnecting, the system has to
reconcile the divergent replicas. In the log-based approach to reconciliation,
such as in the IceCube system, the input is a common initial state and logs of
actions that were performed on each replica. The output is a consistent global
schedule that maximises the number of accepted actions. The reconciler merges
the logs according to the schedule, and replays the operations in the merged
log against the initial state, yielding to a reconciled common final state.
  In this paper, we show the NP-completeness of the log-based reconciliation
problem and present two programs for solving it. Firstly, a constraint logic
program (CLP) that uses integer constraints for expressing precedence
constraints, boolean constraints for expressing dependencies between actions,
and some heuristics for guiding the search. Secondly, a stochastic local search
method with Tabu heuristic (LS), that computes solutions in an incremental
fashion but does not prove optimality. One difficulty in the LS modeling lies
in the handling of both boolean variables and integer variables, and in the
handling of the objective function which differs from a max-CSP problem.
Preliminary evaluation results indicate better performance for the CLP program
which, on somewhat realistic benchmarks, finds nearly optimal solutions up to a
thousands of actions and proves optimality up to a hundreds of actions.

<id>
cs/0212022v1
<category>
cs.RO
<abstract>
We develop and analyze algorithms for dispersing a swarm of primitive robots
in an unknown environment, R. The primary objective is to minimize the
makespan, that is, the time to fill the entire region. An environment is
composed of pixels that form a connected subset of the integer grid.
  There is at most one robot per pixel and robots move horizontally or
vertically at unit speed. Robots enter R by means of k>=1 door pixels
  Robots are primitive finite automata, only having local communication, local
sensors, and a constant-sized memory.
  We first give algorithms for the single-door case (i.e., k=1), analyzing the
algorithms both theoretically and experimentally. We prove that our algorithms
have optimal makespan 2A-1, where A is the area of R.
  We next give an algorithm for the multi-door case (k>1), based on a
wall-following version of the leader-follower strategy. We prove that our
strategy is O(log(k+1))-competitive, and that this bound is tight for our
strategy and other related strategies.

<id>
cs/0212027v1
<category>
cs.RO
<abstract>
A double pendulum subject to external torques is used as a model to study the
stability of a planar manipulator with two links and two rotational driven
joints. The hamiltonian equations of motion and the fixed points (stationary
solutions) in phase space are determined. Under suitable conditions, the
presence of constant torques does not change the number of fixed points, and
preserves the topology of orbits in their linear neighborhoods; two equivalent
invariant manifolds are observed, each corresponding to a saddle-center fixed
point.

<id>
cs/0311019v1
<category>
cs.RO
<abstract>
Deterministic replay is a method for allowing complex multitasking real-time
systems to be debugged using standard interactive debuggers. Even though
several replay techniques have been proposed for parallel, multi-tasking and
real-time systems, the solutions have so far lingered on a prototype academic
level, with very little results to show from actual state-of-the-practice
commercial applications. This paper describes a major deterministic replay
debugging case study performed on a full-scale industrial robot control system,
as well as a minor replay instrumentation case study performed on a military
aircraft radar system. In this article, we will show that replay debugging is
feasible in complex multi-million lines of code software projects running on
top of off-the-shelf real-time operating systems. Furthermore, we will discuss
how replay debugging can be introduced in existing systems without
impracticable analysis efforts. In addition, we will present benchmarking
results from both studies, indicating that the instrumentation overhead is
acceptable and affordable.

<id>
cs/0411020v1
<category>
cs.RO
<abstract>
Mobile robots have received a great deal of research in recent years. A
significant amount of research has been published in many aspects related to
mobile robots. Most of the research is devoted to design and develop some
control techniques for robot motion and path planning. A large number of
researchers have used kinematic models to develop motion control strategy for
mobile robots. Their argument and assumption that these models are valid if the
robot has low speed, low acceleration and light load. However, dynamic
modelling of mobile robots is very important as they are designed to travel at
higher speed and perform heavy duty work. This paper presents and discusses a
new approach to develop a dynamic model and control strategy for wheeled mobile
robot which I modelled as a rigid body that roles on two wheels and a castor.
The motion control strategy consists of two levels. The first level is dealing
with the dynamic of the system and denoted as Low level controller. The second
level is developed to take care of path planning and trajectory generation.

<id>
cs/0411021v1
<category>
cs.RO
<abstract>
An adaptive Monte Carlo localization algorithm based on coevolution mechanism
of ecological species is proposed. Samples are clustered into species, each of
which represents a hypothesis of the robots pose. Since the coevolution between
the species ensures that the multiple distinct hypotheses can be tracked
stably, the problem of premature convergence when using MCL in highly symmetric
environments can be solved. And the sample size can be adjusted adaptively over
time according to the uncertainty of the robots pose by using the population
growth model. In addition, by using the crossover and mutation operators in
evolutionary computation, intra-species evolution can drive the samples move
towards the regions where the desired posterior density is large. So a small
size of samples can represent the desired density well enough to make precise
localization. The new algorithm is termed coevolution based adaptive Monte
Carlo localization (CEAMCL). Experiments have been carried out to prove the
efficiency of the new localization algorithm.

<id>
cs/0411023v1
<category>
cs.RO
<abstract>
The study of the collaboration, coordination and negotiation among different
agents in a multi-agent system (MAS) has always been the most challenging yet
popular in the research of distributed artificial intelligence. In this paper,
we will suggest for RoboCup simulation, a typical MAS, a general
decision-making model, rather than define a different algorithm for each tactic
(e.g. ball handling, pass, shoot and interception, etc.) in soccer games as
most RoboCup simulation teams did. The general decision-making model is based
on two critical factors in soccer games: the vertical distance to the goal line
and the visual angle for the goalpost. We have used these two parameters to
formalize the defensive and offensive decisions in RoboCup simulation and the
results mentioned above had been applied in NOVAURO, original name is UJDB, a
RoboCup simulation team of Jiangsu University, whose decision-making model,
compared with that of Tsinghua University, the world champion team in 2001, is
a universal model and easier to be implemented.

<id>
cs/0411024v1
<category>
cs.RO
<abstract>
In this second of three short papers, I introduce some of the basic concepts
of space robotics with an emphasis on some specific challenging areas of
research that are peculiar to the application of robotics to space
infrastructure development. The style of these short papers is pedagogical and
the concepts in this paper are developed from fundamental manipulator robotics.
This second paper considers the application of space manipulators to on-orbit
servicing (OOS), an application which has considerable commercial application.
I provide some background to the notion of robotic on-orbit servicing and
explore how manipulator control algorithms may be modified to accommodate space
manipulators which operate in the micro-gravity of space.

<id>
cs/0412050v1
<category>
cs.RO
<abstract>
The single wheel, gyroscopically stabilized robot - Gyrover, is a dynamically
stable but statically unstable, underactuated system. In this paper, based on
the dynamic model of the robot, we investigate two classes of nonholonomic
constraints associated with the system. Then, based on the backstepping
technology, we propose a control law for balance control of Gyrover. Next,
through transferring the systems states from Cartesian coordinate to polar
coordinate, control laws for point-to-point control and line tracking in
Cartesian space are provided.

<id>
cs/0412051v1
<category>
cs.RO
<abstract>
The sewer inspection robot MAKRO is an autonomous multi-segment robot with
worm-like shape driven by wheels. It is currently under development in the
project MAKRO-PLUS. The robot has to navigate autonomously within sewer
systems. Its first tasks will be to take water probes, analyze it onboard, and
measure positions of manholes and pipes to detect polluted-loaded sewage and to
improve current maps of sewer systems. One of the challenging problems is the
controller software, which should enable the robot to navigate in the sewer
system and perform the inspection tasks autonomously, not inflicting any
self-damage. This paper focuses on the route planning and replanning aspect of
the robot. The robots software has four different levels, of which the planning
system is the highest level, and the remaining three are controller levels each
with a different degree of abstraction. The planner coordinates the sequence of
actions that are to be successively executed by the robot.

<id>
cs/0412052v1
<category>
cs.RO
<abstract>
Cyberbotics Ltd. develops WebotsTM, a mobile robotics simulation software
that provides you with a rapid prototyping environment for modelling,
programming and simulating mobile robots. The provided robot libraries enable
you to transfer your control programs to several commercially available real
mobile robots. WebotsTM lets you define and modify a complete mobile robotics
setup, even several different robots sharing the same environment. For each
object, you can define a number of properties, such as shape, color, texture,
mass, friction, etc. You can equip each robot with a large number of available
sensors and actuators. You can program these robots using your favorite
development environment, simulate them and optionally transfer the resulting
programs onto your real robots. WebotsTM has been developed in collaboration
with the Swiss Federal Institute of Technology in Lausanne, thoroughly tested,
well documented and continuously maintained for over 7 years. It is now the
main commercial product available from Cyberbotics Ltd.

<id>
cs/0412053v1
<category>
cs.RO
<abstract>
A rigid-flexible manipulator may be assigned tasks in a moving environment
where the winds or vibrations affect the position and/or orientation of surface
of operation. Consequently, losses of the contact and perhaps degradation of
the performance may occur as references are changed. When the environment is
moving, knowledge of the angle &#945; between the contact surface and the
horizontal is required at every instant. In this paper, different profiles for
the time varying angle &#945; are proposed to investigate the effect of this
change into the contact force and the joint torques of a rigid-flexible
manipulator. The coefficients of the equation of the proposed rotating surface
are changing with time to determine the new X and Y coordinates of the moving
surface as the surface rotates.

<id>
cs/0412054v1
<category>
cs.RO
<abstract>
The contributors propose the implementation of hybrid Fuzzy Logic-Genetic
Algorithm (FL-GA) methodology to plan the automatic assembly and disassembly
sequence of products. The GA-Fuzzy Logic approach is implemented onto two
levels. The first level of hybridization consists of the development of a Fuzzy
controller for the parameters of an assembly or disassembly planner based on
GAs. This controller acts on mutation probability and crossover rate in order
to adapt their values dynamically while the algorithm runs. The second level
consists of the identification of theoptimal assembly or disassembly sequence
by a Fuzzy function, in order to obtain a closer control of the technological
knowledge of the assembly/disassembly process. Two case studies were analyzed
in order to test the efficiency of the Fuzzy-GA methodologies.

<id>
cs/0412055v1
<category>
cs.RO
<abstract>
Traditionally, cardiac surgery has been performed through a median
sternotomy, which allows the surgeon generous access to the heart and
surrounding great vessels. As a paradigm shift in the size and location of
incisions occurs in cardiac surgery, new methods have been developed to allow
the surgeon the same amount of dexterity and accessibility to the heart in
confined spaces and in a less invasive manner. Initially, long instruments
without pivot points were used, however, more recent robotic telemanipulation
systems have been applied that allow for improved dexterity, enabling the
surgeon to perform cardiac surgery from a distance not previously possible. In
this rapidly evolving field, we review the recent history and clinical results
of using robotics in cardiac surgery.

<id>
cs/0412056v1
<category>
cs.RO
<abstract>
This paper introduces a six-legged autonomous robot managed by a single
controller and a software core modeled on subsumption architecture. We begin by
discussing the features and capabilities of IsoPod, a new processor for
robotics which has enabled a streamlined implementation of our project. We
argue that this processor offers a unique set of hardware and software
features, making it a practical development platform for robotics in general
and for subsumption-based control architectures in particular. Next, we
summarize original ideas on subsumption architecture implementation for a
six-legged robot, as presented by its inventor Rodney Brooks in 1980s. A
comparison is then made to a more recent example of a hexapod control
architecture based on subsumption. The merits of both systems are analyzed and
a new subsumption architecture layout is formulated as a response. We conclude
with some remarks regarding the development of this project as a hint at new
potentials for intelligent robot design, opened by a recent development in
embedded controller market.

<id>
cs/0412057v1
<category>
cs.RO
<abstract>
In this paper is presented an approach to achieving on-line modification of
nominal biped gait without recomputing entire dynamics when steady motion is
performed. Straight, dynamically balanced walk was used as a nominal gait, and
applied modifications were speed-up and slow-down walk and turning left and
right. It is shown that the disturbances caused by these modifications
jeopardize dynamic stability, but they can be simply compensated to enable walk
continuation.

<id>
cs/0504081v1
<category>
cs.RO
<abstract>
We present methods that generate cooperative strategies for multi-vehicle
control problems using a decomposition approach. By introducing a set of tasks
to be completed by the team of vehicles and a task execution method for each
vehicle, we decomposed the problem into a combinatorial component and a
continuous component. The continuous component of the problem is captured by
task execution, and the combinatorial component is captured by task assignment.
In this paper, we present a solver for task assignment that generates
near-optimal assignments quickly and can be used in real-time applications. To
motivate our methods, we apply them to an adversarial game between two teams of
vehicles. One team is governed by simple rules and the other by our algorithms.
In our study of this game we found phase transitions, showing that the task
assignment problem is most difficult to solve when the capabilities of the
adversaries are comparable. Finally, we implement our algorithms in a
multi-level architecture with a variable replanning rate at each level to
provide feedback on a dynamically changing and uncertain environment.

<id>
cs/0505042v1
<category>
cs.RO
<abstract>
Mixed integer linear programming (MILP) is a powerful tool for planning and
control problems because of its modeling capability and the availability of
good solvers. However, for large models, MILP methods suffer computationally.
In this paper, we present iterative MILP algorithms that address this issue. We
consider trajectory generation problems with obstacle avoidance requirements
and minimum time trajectory generation problems. The algorithms use fewer
binary variables than standard MILP methods and require less computational
effort.

<id>
cs/0510059v1
<category>
cs.RO
<abstract>
Automobile has become the dominant transport mode in the world in the last
century. In order to meet a continuously growing demand for transport, one
solution is to change the control approach for vehicle to full driving
automation, which removes the driver from the control loop to improve
efficiency and reduce accidents. Recent work shows that there are several
realistic paths towards this deployment : driving assistance on passenger cars,
automated commercial vehicles on dedicated infrastructures, and new forms of
urban transport (car-sharing and cybercars). Cybercars have already been put
into operation in Europe, and it seems that this approach could lead the way
towards full automation on most urban, and later interurban infrastructures.
The European project CyberCars has brought many improvements in the technology
needed to operate cybercars over the last three years. A new, larger European
project is now being prepared to carry this work further in order to meet more
ambitious objectives in terms of safety and efficiency. This paper will present
past and present technologies and will focus on the future developments.

<id>
cs/0511067v1
<category>
cs.RO
<abstract>
It is very important for quadruped walking machine to keep its stability in
high speed walking. It has been indicated that moment around the supporting
diagonal line of quadruped in trotting gait largely influences walking
stability. In this paper, moment around the supporting diagonal line of
quadruped in trotting gait is modeled and its effects on body attitude are
analyzed. The degree of influence varies with different initial stances of
quadruped and we get the optimal initial stance of quadruped in trotting gait
with maximal walking stability. Simulation results are presented. Keywords:
quadruped, trotting, attitude, walking stability.

<id>
cs/0511068v1
<category>
cs.RO
<abstract>
The goal is the development of a simultaneous, dynamic, technological as well
as logistical real-time planning and an organizational control of the
production by the production units themselves, working in the production
network under the use of Multi-Agent-Technology. The design of the
multi-agent-based manufacturing management system, the models of the single
agents, algorithms for the agent-based, decentralized dispatching of orders,
strategies and data management concepts as well as their integration into the
SCM, basing on the solution described, will be explained in the following.
  Keywords: production engineering and management, dynamic manufacturing
planning and control, multi-agentsystems (MAS), supply-chain-management (SCM),
e-manufacturing

<id>
cs/0511069v1
<category>
cs.RO
<abstract>
The approximate nonlinear receding-horizon control law is used to treat the
trajectory tracking control problem of rigid link robot manipulators. The
derived nonlinear predictive law uses a quadratic performance index of the
predicted tracking error and the predicted control effort. A key feature of
this control law is that, for their implementation, there is no need to perform
an online optimization, and asymptotic tracking of smooth reference
trajectories is guaranteed. It is shown that this controller achieves the
positions tracking objectives via link position measurements. The stability
convergence of the output tracking error to the origin is proved. To enhance
the robustness of the closed loop system with respect to payload uncertainties
and viscous friction, an integral action is introduced in the loop. A nonlinear
observer is used to estimate velocity. Simulation results for a two-link rigid
robot are performed to validate the performance of the proposed controller.
  Keywords: receding-horizon control, nonlinear observer, robot manipulators,
integral action, robustness.

<id>
cs/0601040v1
<category>
cs.RO
<abstract>
In the past few years, the European Commission has financed several projects
to examine how new technologies could improve the sustainability of European
cities. These technologies concern new public transportation modes such as
guided buses to form high capacity networks similar to light rail but at a
lower cost and better flexibility, PRT (Personal Rapid Transit) and cybercars
(small urban vehicles with fully automatic driving capabilities to be used in
carsharing mode, mostly as a complement to mass transport). They also concern
private vehicles with technologies which could improve the efficiency of the
vehicles as well as their safety (Intelligent Speed Adaptation, Adaptive Cruise
>.Control, Stop&Go, Lane Keeping,...) and how these new vehicles can complement
mass transport in the form of car-sharing services.

<id>
cs/0601053v1
<category>
cs.RO
<abstract>
Path planning and obstacle avoidance are the two major issues in any
navigation system. Wavefront propagation algorithm, as a good path planner, can
be used to determine an optimal path. Obstacle avoidance can be achieved using
possibility theory. Combining these two functions enable a robot to
autonomously navigate to its destination. This paper presents the approach and
results in implementing an autonomous navigation system for an indoor mobile
robot. The system developed is based on a laser sensor used to retrieve data to
update a two dimensional world model of therobot environment. Waypoints in the
path are incorporated into the obstacle avoidance. Features such as ageing of
objects and smooth motion planning are implemented to enhance efficiency and
also to cater for dynamic environments.

<id>
cs/0601054v1
<category>
cs.RO
<abstract>
This paper presents a robust control scheme for flexible link robotic
manipulators, which is based on considering the flexible mechanical structure
as a system with slow (rigid) and fast (flexible) modes that can be controlled
separately. The rigid dynamics is controlled by means of a robust sliding-mode
approach with wellestablished stability properties while an LQR optimal design
is adopted for the flexible dynamics. Experimental results show that this
composite approach achieves good closed loop tracking properties both for the
rigid and the flexible dynamics.

<id>
cs/0601055v1
<category>
cs.RO
<abstract>
This paper presents a new architecture called FAIS for imple- menting
intelligent agents cooperating in a special Multi Agent environ- ment, namely
the RoboCup Rescue Simulation System. This is a layered architecture which is
customized for solving fire extinguishing problem. Structural decision making
algorithms are combined with heuristic ones in this model, so it's a hybrid
architecture.

<id>
cs/0601056v1
<category>
cs.RO
<abstract>
This paper investigates the problem of the dynamic balance control of
multi-arm free-floating space robot during capturing an active object in close
proximity. The position and orientation of space base will be affected during
the operation of space manipulator because of the dynamics coupling between the
manipulator and space base. This dynamics coupling is unique characteristics of
space robot system. Such a disturbance will produce a serious impact between
the manipulator hand and the object. To ensure reliable and precise operation,
we propose to develop a space robot system consisting of two arms, with one arm
(mission arm) for accomplishing the capture mission, and the other one (balance
arm) compensating for the disturbance of the base. We present the coordinated
control concept for balance of the attitude of the base using the balance arm.
The mission arm can move along the given trajectory to approach and capture the
target with no considering the disturbance from the coupling of the base. We
establish a relationship between the motion of two arm that can realize the
zeros reaction to the base. The simulation studies verified the validity and
efficiency of the proposed control method.

<id>
cs/0601057v1
<category>
cs.RO
<abstract>
A resolved acceleration control (RAC) and proportional-integral active force
control (PIAFC) is proposed as an approach for the robust motion control of a
mobile manipulator (MM) comprising a differentially driven wheeled mobile
platform with a two-link planar arm mounted on top of the platform. The study
emphasizes on the integrated kinematic and dynamic control strategy in which
the RAC is used to manipulate the kinematic component while the PIAFC is
implemented to compensate the dynamic effects including the bounded
known/unknown disturbances and uncertainties. The effectivenss and robustness
of the proposed scheme are investigated through a rigorous simulation study and
later complemented with experimental results obtained through a number of
experiments performed on a fully developed working prototype in a laboratory
environment. A number of disturbances in the form of vibratory and impact
forces are deliberately introduced into the system to evaluate the system
performances. The investigation clearly demonstrates the extreme robustness
feature of the proposed control scheme compared to other systems considered in
the study.

<id>
cs/0601058v1
<category>
cs.RO
<abstract>
This paper is a summary of the recently accomplished research work on
flexible gripping systems. The goal is to develop a gripper which can be used
for a great amount of geometrically variant workpieces. The economic aspect is
of particular importance during the whole development. The high flexibility of
the gripper is obtained by three parallel used principles. These are human and
computer based analysis of the gripping object as well as mechanical adaptation
of the gripper to the object with the help of servo motors. The focus is on the
gripping of free-form surfaces with suction cup.

<id>
cs/0601059v1
<category>
cs.RO
<abstract>
At present, the research on robot team cooperation is still in qualitative
analysis phase and lacks the description model that can quantitatively describe
the dynamical evolution of team cooperative relationships with constantly
changeable task demand in Multi-robot field. First this paper whole and static
describes organization model HWROM of robot team, then uses Markov course and
Bayesian theorem for reference, dynamical describes the team cooperative
relationships building. Finally from cooperative entity layer, ability layer
and relative layer we research team formation and cooperative mechanism, and
discuss how to optimize relative action sets during the evolution. The dynamic
evolution model of robot team and cooperative relationships between robot teams
proposed and described in this paper can not only generalize the robot team as
a whole, but also depict the dynamic evolving process quantitatively. Users can
also make the prediction of the cooperative relationship and the action of the
robot team encountering new demands based on this model. Journal web page & a
lot of robotic related papers www.ars-journal.com

<id>
cs/0601060v1
<category>
cs.RO
<abstract>
There is a belief that complexity and chaos are essential for adaptability.
But life deals with complexity every moment, without the chaos that engineers
fear so, by invoking goal-directed behaviour. Goals can be programmed. That is
why living organisms give us hope to achieve adaptability in robots. In this
paper a method for the description of a goal-directed, or programmed,
behaviour, interacting with uncertainty of environment, is described. We
suggest reducing the structural (goals, intentions) and stochastic components
(probability to realise the goal) of individual behaviour to random variables
with nominal values to apply probabilistic approach. This allowed us to use a
Normalized Entropy Index to detect the system state by estimating the
contribution of each agent to the group behaviour. The number of possible group
states is 27. We argue that adaptation has a limited number of possible paths
between these 27 states. Paths and states can be programmed so that after
adjustment to any particular case of task and conditions, adaptability will
never involve chaos. We suggest the application of the model to operation of
robots or other devices in remote and/or dangerous places.

<id>
cs/9810022v1
<category>
cs.SE
<abstract>
We apply the Gurevich Abstract State Machine methodology to a benchmark
specification problem of Broy and Lamport.

<id>
cs/9810023v1
<category>
cs.SE
<abstract>
In a recent provocative paper, Lamport points out "the insubstantiality of
processes" by proving the equivalence of two different decompositions of the
same intuitive algorithm by means of temporal formulas. We point out that the
correct equivalence of algorithms is itself in the eye of the beholder. We
discuss a number of related issues and, in particular, whether algorithms can
be proved equivalent directly.

<id>
cs/9810024v1
<category>
cs.SE
<abstract>
We describe an automated partial evaluator for evolving algebras implemented
at the University of Michigan.

<id>
cs/9810025v1
<category>
cs.SE
<abstract>
We describe the architecture of an evolving algebra partial evaluator, a
program which specializes an evolving algebra with respect to a portion of its
input. We discuss the particular analysis, specialization, and optimization
techniques used and show an example of its use.

<id>
cs/9810026v1
<category>
cs.SE
<abstract>
We give an evolving algebra solution for the well-known railroad crossing
problem and use the occasion to experiment with agents that perform
instantaneous actions in continuous time and in particular with agents that
fire at the moment they are enabled.

<id>
cs/9811007v1
<category>
cs.SE
<abstract>
The second Software Engineering Institute Product Line Practice Workshop was
a hands-on meeting held in November 1997 to share industry practices in
software product lines and to explore the technical and non-technical issues
involved. This report synthesizes the workshop presentations and discussions,
which identified factors involved in product line practices and analyzed issues
in the areas of software engineering, technical management, and enterprise
management.

<id>
cs/9811011v1
<category>
cs.SE
<abstract>
This paper presents a method for analyzing the survivability of distributed
network systems and an example of its application.

<id>
cs/9811014v1
<category>
cs.SE
<abstract>
An annotated bibliography of papers which deal with or use Abstract State
Machines (ASMs), as of January 1998.

<id>
cs/9902008v1
<category>
cs.SE
<abstract>
Systematic testing of object-oriented software turned out to be much more
complex than testing conventional software. Especially the highly incremental
and iterative development cycle demands both many more changes and partially
implemented resp. re-implemented classes. Much more integration and regression
testing has to be done to reach stable stages during the development. In this
presentation we propose a diagram capturing all possible dependencies and
interactions in an object-oriented program. Then we give algorithms and
coverage criteria to identify integration resp. regression test strategys and
all test cases to be executed after some implementation resp. modification
activities. Finally, we summarize some practical experiences and heuristics.

<id>
cs/9903018v1
<category>
cs.SE
<abstract>
Scripting languages are becoming more and more important as a tool for
software development, as they provide great flexibility for rapid prototyping
and for configuring componentware applications. In this paper we present
LuaJava, a scripting tool for Java. LuaJava adopts Lua, a dynamically typed
interpreted language, as its script language. Great emphasis is given to the
transparency of the integration between the two languages, so that objects from
one language can be used inside the other like native objects. The final result
of this integration is a tool that allows the construction of configurable Java
applications, using off-the-shelf components, in a high abstraction level.

<id>
cs/9906030v1
<category>
cs.SE
<abstract>
This paper gives an overview of SCR3 -- a toolset designed to increase the
usability of formal methods for software development. Formal requirements are
specified in SCR3 in an easy to use and review format, and then used in
checking requirements for correctness and in verifying consistency between
annotated code and requirements.
  In this paper we discuss motivations behind this work, describe several tools
which are part of SCR3, and illustrate their operation on an example of a
Cruise Control system.

<id>
cs/9906031v1
<category>
cs.SE
<abstract>
For over a decade, researchers in formal methods tried to create formalisms
that permit natural specification of systems and allow mathematical reasoning
about their correctness. The availability of fully-automated reasoning tools
enables more non-specialists to use formal methods effectively --- their
responsibility reduces to just specifying the model and expressing the desired
properties. Thus, it is essential that these properties be represented in a
language that is easy to use and sufficiently expressive.
  Linear-time temporal logic is a formalism that has been extensively used by
researchers for specifying properties of systems. When such properties are
closed under stuttering, i.e. their interpretation is not modified by
transitions that leave the system in the same state, verification tools can
utilize a partial-order reduction technique to reduce the size of the model and
thus analyze larger systems. If LTL formulas do not contain the ``next''
operator, the formulas are closed under stuttering, but the resulting language
is not expressive enough to capture many important properties, e.g., properties
involving events. Determining if an arbitrary LTL formula is closed under
stuttering is hard --- it has been proven to be PSPACE-complete.
  In this paper we relax the restriction on LTL that guarantees closure under
stuttering, introduce the notion of edges in the context of LTL, and provide
theorems that enable syntactic reasoning about closure under stuttering of LTL
formulas.

<id>
cs/9906032v1
<category>
cs.SE
<abstract>
This paper describes a case study conducted in collaboration with Nortel to
demonstrate the feasibility of applying formal modeling techniques to
telecommunication systems. A formal description language, SDL, was chosen by
our qualitative CASE tool evaluation to model a multimedia-messaging system
described by an 80-page natural language specification. Our model was used to
identify errors in the software requirements document and to derive test
suites, shadowing the existing development process and keeping track of a
variety of productivity data.

<id>
cs/9907019v1
<category>
cs.SE
<abstract>
A reasonable C++ Java Native Interface (JNI) technique termed C++ Wrappered
JNI (C++WJ) is presented. The technique simplifies current error-prone JNI
development by wrappering JNI calls. Provided development is done with the aid
of a C++ compiler, C++WJ offers type checking and behind the scenes caching. A
tool (jH) patterned on javah automates the creation of C++WJ classes.
  The paper presents the rationale behind the choices that led to C++WJ.
Handling of Java class and interface hierarchy including Java type downcasts is
discussed. Efficiency considerations in the C++WJ lead to two flavors of C++
classes: jtypes and Jtypes. A jtype is a lightweight less than full wrapper of
a JNI object reference. A Jtype is a heavyweight full wrapper of a JNI object
reference.

<id>
cs/9912018v1
<category>
cs.SE
<abstract>
One of the key concepts in testing is that of adequate test sets. A test
selection criterion decides which test sets are adequate. In this paper, a
language schema for specifying a large class of test selection criteria is
developed; the schema is based on two operations for building complex criteria
from simple ones. Basic algebraic properties of the two operations are derived.
  In the second part of the paper, a simple language-an instance of the general
schema-is studied in detail, with the goal of generating small adequate test
sets automatically. It is shown that one version of the problem is intractable,
while another is solvable by an efficient algorithm. An implementation of the
algorithm is described.

<id>
cs/0011029v1
<category>
cs.SE
<abstract>
Although attribute grammars are commonly used for compiler construction,
little investigation has been conducted on debugging attribute grammars. The
paper proposes two types of systematic debugging methods, an algorithmic
debugging and slice-based debugging, both tailored for attribute grammars. By
means of query-based interaction with the developer, our debugging methods
effectively narrow the potential bug space in the attribute grammar description
and eventually identify the incorrect attribution rule. We have incorporated
this technology in our visual debugging tool called Aki.

<id>
cs/0012009v1
<category>
cs.SE
<abstract>
A program fails. Under which circumstances does this failure occur? One
single algorithm, the delta debugging algorithm, suffices to determine these
failure-inducing circumstances. Delta debugging tests a program systematically
and automatically to isolate failure-inducing circumstances such as the program
input, changes to the program code, or executed statements.

<id>
cs/0012014v1
<category>
cs.SE
<abstract>
Slicing is a program analysis technique originally developed for imperative
languages. It facilitates understanding of data flow and debugging.
  This paper discusses slicing of Constraint Logic Programs. Constraint Logic
Programming (CLP) is an emerging software technology with a growing number of
applications. Data flow in constraint programs is not explicit, and for this
reason the concepts of slice and the slicing techniques of imperative languages
are not directly applicable.
  This paper formulates declarative notions of slice suitable for CLP. They
provide a basis for defining slicing techniques (both dynamic and static) based
on variable sharing. The techniques are further extended by using groundness
information.
  A prototype dynamic slicer of CLP programs implementing the presented ideas
is briefly described together with the results of some slicing experiments.

<id>
cs/0105008v1
<category>
cs.SE
<abstract>
Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural specifications) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintenance, and reuse will become an important issue.
This paper introduces a new form of slicing, named architectural slicing, to
aid architectural understanding and reuse. In contrast to traditional slicing,
architectural slicing is designed to operate on the architectural specification
of a software system, rather than the source code of a program. Architectural
slicing provides knowledge about the high-level structure of a software system,
rather than the low-level implementation details of a program. In order to
compute an architectural slice, we present the architecture information flow
graph which can be used to represent information flows in a software
architecture. Based on the graph, we give a two-phase algorithm to compute an
architectural slice.

<id>
cs/0105009v1
<category>
cs.SE
<abstract>
Software architecture is receiving increasingly attention as a critical
design level for software systems. As software architecture design resources
(in the form of architectural descriptions) are going to be accumulated, the
development of techniques and tools to support architectural understanding,
testing, reengineering, maintaining, and reusing will become an important
issue. In this paper we introduce a new dependence analysis technique, named
architectural dependence analysis to support software architecture development.
In contrast to traditional dependence analysis, architectural dependence
analysis is designed to operate on an architectural description of a software
system, rather than the source code of a conventional program. Architectural
dependence analysis provides knowledge of dependences for the high-level
architecture of a software system, rather than the low-level implementation
details of a conventional program.

<id>
cs/0105010v1
<category>
cs.SE
<abstract>
This paper proposes some new architectural metrics which are appropriate for
evaluating the architectural attributes of a software system. The main feature
of our approach is to assess the complexity of a software architecture by
analyzing various types of architectural dependences in the architecture.

<id>
cs/0109019v1
<category>
cs.SE
<abstract>
Test suites are designed to validate the operation of a system against
requirements. One important aspect of a test suite design is to ensure that
system operation logic is tested completely. A test suite should drive a system
through all abstract states to exercise all possible cases of its operation.
This is a difficult task. Code coverage tools support test suite designers by
providing the information about which parts of source code are covered during
system execution. Unfortunately, code coverage tools produce only source code
coverage information. For a test engineer it is often hard to understand what
the noncovered parts of the source code do and how they relate to requirements.
We propose a generic approach that provides design coverage of the executed
software simplifying the development of new test suites. We demonstrate our
approach on common design abstractions such as statecharts, activity diagrams,
message sequence charts and structure diagrams. We implement the design
coverage using Third Eye tracing and trace analysis framework. Using design
coverage, test suites could be created faster by focussing on untested design
elements.

<id>
cs/0111013v1
<category>
cs.SE
<abstract>
The strategy used to develop the NIF Integrated Computer Control System
(ICCS) calls for incremental cycles of construction and formal test to deliver
a total of 1 million lines of code. Each incremental release takes four to six
months to implement specific functionality and culminates when offline tests
conducted in the ICCS Integration and Test Facility verify functional,
performance, and interface requirements. Tests are then repeated on line to
confirm integrated operation in dedicated laser laboratories or ultimately in
the NIF. Test incidents along with other change requests are recorded and
tracked to closure by the software change control board (SCCB). Annual
independent audits advise management on software process improvements.
Extensive experience has been gained by integrating controls in the prototype
laser preamplifier laboratory. The control system installed in the preamplifier
lab contains five of the ten planned supervisory subsystems and seven of
sixteen planned front-end processors (FEPs). Beam alignment, timing, diagnosis
and laser pulse amplification up to 20 joules was tested through an automated
series of shots. Other laboratories have provided integrated testing of six
additional FEPs. Process measurements including earned-value, product size, and
defect densities provide software project controls and generate confidence that
the control system will be successfully deployed.

<id>
cs/0111014v1
<category>
cs.SE
<abstract>
Visual DCT is an EPICS configuration tool completely written in Java and
therefore supported in various systems. It was developed to provide features
missing in existing configuration tools as Capfast and GDCT. Visually Visual
DCT resembles GDCT - records can be created, moved and linked, fields and links
can be easily modified. But Visual DCT offers more: using groups, records can
be grouped together in a logical block, which allows a hierarchical design.
Additionally indication of data flow direction using arrows makes the design
easier to understand. Visual DCT has a powerful DB parser, which allows
importing existing DB and DBD files. Output file is also DB file, all comments
and record order is preserved and visual data saved as comment, which allows
DBs to be edited in other tools or manually. Great effort has been taken and
many tricks used to optimize the performance in order to compensate for the
fact that Java is an interpreted language.

<id>
cs/0111016v1
<category>
cs.SE
<abstract>
The NIF Integrated Computer Control System (ICCS) application software uses a
set of service frameworks that assures uniform behavior spanning the front-end
processors (FEPs) and supervisor programs. This uniformity is visible both in
the way each program employs shared services and in the flexibility it affords
for attaching graphical user interfaces (GUIs). Uniformity of structure across
applications is desired for the benefit of programmers who will be maintaining
the many programs that constitute the ICCS. In this paper, the framework
components that have the greatest impact on the application structure are
discussed.

<id>
cs/0111019v2
<category>
cs.SE
<abstract>
The Swiss Light Source (SLS) has in the order of 500 magnet power supplies
(PS) installed, ranging from from 3 A/20 V four-quadrant PS to a 950 A/1000 V
two-quadrant 3 Hz PS. All magnet PS have a local digital controller for a
digital regulation loop and a 5 MHz optical point-to-point link to the VME
level. The PS controller is running a pulse width/pulse repetition regulation
scheme, optional with multiple slave regulation loops. Many internal regulation
parameters and controller diagnostics are readable by the control system.
Industry Pack modules with standard VME carrier cards are used as VME hardware
interface with the high control density of eight links per VME card. The low
level EPICS interface is identical for all 500 magnet PS, including insertion
devices. The digital PS have proven to be very stable and reliable during
commissioning of the light source. All specifications were met for all PS. The
advanced diagnostic for the magnet PS turned out to be very useful not only for
the diagnostic of the PS but also to identify problems on the magnets.

<id>
cs/0111021v3
<category>
cs.SE
<abstract>
The commissioning of the Swiss Light Source (SLS) started in Feb. 2000 with
the Linac, continued in May 2000 with the booster synchrotron and by Dec. 2000
first light in the storage ring were produced. The first four beam lines had to
be operational by August 2001. The thorough integration of all subsystems to
the control system and a high level of automation was prerequisite to meet the
tight time schedule. A careful balanced distribution of functionality into high
level and low level applications allowed an optimization of short development
cycles and high reliability of the applications. High level applications were
implemented as CORBA based client/server applications (tcl/tk and Java based
clients, C++ based servers), IDL applications using EZCA, medm/dm2k screens and
tcl/tk applications using CDEV. Low level applications were mainly built as
EPICS process databases, SNL state machines and customized drivers.
Functionality of the high level application was encapsulated and pushed to
lower levels whenever it has proven to be adequate. That enabled to reduce
machine setups to a handful of physical parameters and allow the usage of
standard EPICS tools for display, archiving and processing of complex physical
values. High reliability and reproducibility were achieved with that approach.

<id>
cs/0111045v1
<category>
cs.SE
<abstract>
The Integrated Computer Control System (ICCS) for the National Ignition
Facility (NIF) is a layered architecture of 300 front-end processors (FEP)
coordinated by supervisor subsystems including automatic beam alignment and
wavefront control, laser and target diagnostics, pulse power, and shot control
timed to 30 ps. FEP computers incorporate either VxWorks on PowerPC or Solaris
on UltraSPARC processors that interface to over 45,000 control points attached
to VME-bus or PCI-bus crates respectively. Typical devices are stepping motors,
transient digitizers, calorimeters, and photodiodes. The front-end layer is
divided into another segment comprised of an additional 14,000 control points
for industrial controls including vacuum, argon, synthetic air, and safety
interlocks implemented with Allen-Bradley programmable logic controllers
(PLCs). The computer network is augmented asynchronous transfer mode (ATM) that
delivers video streams from 500 sensor cameras monitoring the 192 laser beams
to operator workstations. Software is based on an object-oriented framework
using CORBA distribution that incorporates services for archiving, machine
configuration, graphical user interface, monitoring, event logging, scripting,
alert management, and access control. Software coding using a mixed language
environment of Ada95 and Java is one-third complete at over 300 thousand source
lines. Control system installation is currently under way for the first 8
beams, with project completion scheduled for 2008.

<id>
cs/0201023v1
<category>
cs.SE
<abstract>
In this paper we outline a software development process for safety-critical
systems that aims at combining some of the specific strengths of model-based
development with those of programming language based development using
safety-critical subsets of Ada. Model-based software development and
model-based test case generation techniques are combined with code generation
techniques and tools providing a transition from model to code both for a
system itself and for its test cases. This allows developers to combine
domain-oriented, model-based techniques with source code based validation
techniques, as required for conformity with standards for the development of
safety-critical software, such as the avionics standard RTCA/DO-178B. We
introduce the AutoFocus and Validator modeling and validation toolset and
sketch its usage for modeling, test case generation, and code generation in a
combined approach, which is further illustrated by a simplified leading edge
aerospace model with built-in fault tolerance.

<id>
cs/0201028v1
<category>
cs.SE
<abstract>
The validation of modern software systems incorporates both functional and
quality requirements. This paper proposes a validation approach for software
quality requirement - its power consumption. This approach validates whether
the software produces the desired results with a minimum expenditure of energy.
We present energy requirements and an approach for their validation using a
power consumption model, test-case specification, software traces, and power
measurements. Three different approaches for power data gathering are
described. The power consumption of mobile phone applications is obtained and
matched against the power consumption model.

<id>
cs/0005022v2
<category>
cs.SD
<abstract>
While traditional implementations of variable-length digital delay lines are
based on a circular buffer accessed by two pointers, we propose an
implementation where a single fractional pointer is used both for read and
write operations. On modern general-purpose architectures, the proposed method
is nearly as efficient as the popularinterpolated circular buffer, and it
behaves well for delay-length modulations commonly found in digital audio
effects. The physical interpretation of the new implementation shows that it is
suitable for simulating tension or density modulations in wave-propagating
media.

<id>
cs/0007014v1
<category>
cs.SD
<abstract>
Computing practice today depends on visual output to drive almost all user
interaction. Other senses, such as audition, may be totally neglected, or used
tangentially, or used in highly restricted specialized ways. We have excellent
audio rendering through D-A conversion, but we lack rich general facilities for
modeling and manipulating sound comparable in quality and flexibility to
graphics. We need co-ordinated research in several disciplines to improve the
use of sound as an interactive information channel.
  Incremental and separate improvements in synthesis, analysis, speech
processing, audiology, acoustics, music, etc. will not alone produce the
radical progress that we seek in sonic practice. We also need to create a new
central topic of study in digital audio research. The new topic will assimilate
the contributions of different disciplines on a common foundation. The key
central concept that we lack is sound as a general-purpose information channel.
We must investigate the structure of this information channel, which is driven
by the co-operative development of auditory perception and physical sound
production. Particular audible encodings, such as speech and music, illuminate
sonic information by example, but they are no more sufficient for a
characterization than typography is sufficient for a characterization of visual
information.

<id>
cs/0010017v2
<category>
cs.SD
<abstract>
A rectangular enclosure has such an even distribution of resonances that it
can be accurately and efficiently modelled using a feedback delay network.
Conversely, a non rectangular shape such as a sphere has a distribution of
resonances that challenges the construction of an efficient model. This work
proposes an extension of the already known feedback delay network structure to
model the resonant properties of a sphere. A specific frequency distribution of
resonances can be approximated, up to a certain frequency, by inserting an
allpass filter of moderate order after each delay line of a feedback delay
network. The structure used for rectangular boxes is therefore augmented with a
set of allpass filters allowing parametric control over the enclosure size and
the boundary properties. This work was motivated by informal listening tests
which have shown that it is possible to identify a basic shape just from the
distribution of its audible resonances.

<id>
cs/0103005v1
<category>
cs.SD
<abstract>
This paper describes a method for decomposing steady-state instrument data
into excitation and formant filter components. The input data, taken from
several series of recordings of acoustical instruments is analyzed in the
frequency domain, and for each series a model is built, which most accurately
represents the data as a source-filter system. The source part is taken to be a
harmonic excitation system with frequency-invariant magnitudes, and the filter
part is considered to be responsible for all spectral inhomogenieties. This
method has been applied to the SHARC database of steady state instrument data
to create source-filter models for a large number of acoustical instruments.
Subsequent use of such models can have a wide variety of applications,
including improvements to wavetable and physical modeling synthesis, high
quality pitch shifting, and creation of "hybrid" instrument timbres.

<id>
cs/0103006v1
<category>
cs.SD
<abstract>
Modal synthesis is an important area of physical modeling whose exploration
in the past has been held back by a large number of control parameters, the
scarcity of general-purpose design tools and the difficulty of obtaining the
computational power required for real-time synthesis. This paper presents an
overview of a flexible software framework facilitating the design and control
of instruments based on modal synthesis. The framework is designed as a
hierarchy of polymorphic synthesis objects, representing modal structures of
various complexity. As a method of generalizing all interactions among the
elements of a modal system, an abstract notion of {\it energy} is introduced,
and a set of energy transfer functions is provided. Such abstraction leads to a
design where the dynamics of interactions can be largely separated from the
specifics of particular modal structures, yielding an easily configurable and
expandable system. A real-time version of the framework has been implemented as
a set of C++ classes along with an integrating shell and a GUI, and is
currently being used to design and play modal instruments, as well as to survey
fundamental properties of various modal algorithms.

<id>
cs/0701177v4
<category>
cs.SD
<abstract>
In this paper, a method of pitch tracking based on variance minimization of
locally periodic subsamples of an acoustic signal is presented. Replicates
along the length of the periodically sampled data of the signal vector are
taken and locally averaged sample variances are minimized to estimate the
fundamental frequency. Using this method, pitch tracking of any text
independent voiced signal is possible for different speakers.

<id>
0812.0706v1
<category>
cs.SD
<abstract>
The notes which play the most important and second most important roles in
expressing a raga are called Vadi and Samvadi swars respectively in (North)
Indian Classical music. Like Bageshree, Bhairavi, Shankara, Hamir and Kalingra,
Rageshree is another controversial raga so far as the choice of Vadi-Samvadi
selection is concerned where there are two different opinions. In the present
work, a two minute vocal recording of raga Rageshree is subjected to a careful
statistical analysis. Our analysis is broken into three phases: first half,
middle half and last half. Under a multinomial model set up holding appreciably
in the first two phases, only one opinion is found acceptable. In the last
phase the distribution seems to be quasi multinomial, characterized by an
unstable nature of relative occurrence of pitch of all the notes and although
the note whose relative occurrence of pitch suddenly shoots is the Vadi swar
selected from our analysis of the first two phases, we take it as an outlier
demanding a separate treatment like any other in statistics. Selection of
Vadi-Samvadi notes in a quasi-multinomial set up is still an open research
problem. An interesting musical cocktail is proposed, however, embedding
several ideas like melodic property of notes, note combinations and pitch
movements between notes, using some weighted combination of psychological and
statistical stability of notes along with watching carefully the sudden shoot
of one or more notes whenever there is enough evidence that multinomial model
has broken down.

<id>
0901.2416v1
<category>
cs.SD
<abstract>
An effective way to increase the noise robustness of automatic speech
recognition is to label noisy speech features as either reliable or unreliable
(missing) prior to decoding, and to replace the missing ones by clean speech
estimates. We present a novel method to obtain such clean speech estimates.
Unlike previous imputation frameworks which work on a frame-by-frame basis, our
method focuses on exploiting information from a large time-context. Using a
sliding window approach, denoised speech representations are constructed using
a sparse representation of the reliable features in an overcomplete basis of
fixed-length exemplar fragments. We demonstrate the potential of our approach
with experiments on the AURORA-2 connected digit database.

<id>
0901.3902v1
<category>
cs.SD
<abstract>
In this paper, we are presenting a new model for interactive music. Unlike
most interactive systems, our model is based on file organization, but does not
require digital audio treatments. This model includes a definition of a
constraints system and its solver. The products of this project are intended
for the general public, inexperienced users, as well as professional musicians,
and will be distributed commercially. We are here presenting three products of
this project. The difficulty of this project is to design a technology and
software products for interactive music which must be easy to use by the
general public and by professional composers.

<id>
0902.2783v2
<category>
cs.SD
<abstract>
This paper has been withdrawn by the contributor ali pourmohammad.

<id>
0907.3220v1
<category>
cs.SD
<abstract>
Music genre classification is an essential tool for music information
retrieval systems and it has been finding critical applications in various
media platforms. Two important problems of the automatic music genre
classification are feature extraction and classifier design. This paper
investigates inter-genre similarity modelling (IGS) to improve the performance
of automatic music genre classification. Inter-genre similarity information is
extracted over the mis-classified feature population. Once the inter-genre
similarity is modelled, elimination of the inter-genre similarity reduces the
inter-genre confusion and improves the identification rates. Inter-genre
similarity modelling is further improved with iterative IGS modelling(IIGS) and
score modelling for IGS elimination(SMIGS). Experimental results with promising
classification improvements are provided.

<id>
0909.2363v1
<category>
cs.SD
<abstract>
In this paper, an improved strategy for automated text dependent speaker
identification system has been proposed in noisy environment. The
identification process incorporates the Neuro- Genetic hybrid algorithm with
cepstral based features. To remove the background noise from the source
utterance, wiener filter has been used. Different speech pre-processing
techniques such as start-end point detection algorithm, pre-emphasis filtering,
frame blocking and windowing have been used to process the speech utterances.
RCC, MFCC, MFCC, MFCC, LPC and LPCC have been used to extract the features.
After feature extraction of the speech, Neuro-Genetic hybrid algorithm has been
used in the learning and identification purposes. Features are extracted by
using different techniques to optimize the performance of the identification.
According to the VALID speech database, the highest speaker identification rate
of 100.000 percent for studio environment and 82.33 percent for office
environmental conditions have been achieved in the close set text dependent
speaker identification system.

<id>
0911.3538v1
<category>
cs.SD
<abstract>
Speech analyzing in special periods of time has been presented in this paper.
One of the most important periods in signal processing is near to Zero. By this
paper, we analyze noise speech signals when these signals are near to Zero. Our
strategy is defining some subfunctions and compress histograms when a noise
speech signal is in a special period. It can be so useful for wavelet signal
processing and spoken systems analyzing.

<id>
0911.5171v1
<category>
cs.SD
<abstract>
We are looking for a mathematical model of monophonic sounds with independent
time and phase dimensions. With such a model we can resynthesise a sound with
arbitrarily modulated frequency and progress of the timbre. We propose such a
model and show that it exactly fulfils some natural properties, like a kind of
time-invariance, robustness against non-harmonic frequencies, envelope
preservation, and inclusion of plain resampling as a special case. The
resulting algorithm is efficient and allows to process data in a streaming
manner with phase and shape modulation at sample rate, what we demonstrate with
an implementation in the functional language Haskell. It allows a wide range of
applications, namely pitch shifting and time scaling, creative FM synthesis
effects, compression of monophonic sounds, generating loops for sampled sounds,
synthesise sounds similar to wavetable synthesis, or making ultrasound audible.

<id>
0912.0745v1
<category>
cs.SD
<abstract>
The objective of this paper is to understand the critical parameters that
need to be addressed while designing a guitar tuner. The focus of the design
lies in developing a suitable algorithm to accurately detect the fundamental
frequency of a plucked guitar string from its frequency spectrum. A
userfriendly graphical interface is developed using Matlab to allow any user to
easily tune his guitar using the developed program.

<id>
1001.4190v1
<category>
cs.SD
<abstract>
Speech signals of the letter 'zha' in Tamil language of 3 males and 3 females
were coded using an improved version of Linear Predictive Coding (LPC). The
sampling frequency was at 16 kHz and the bit rate was at 15450 bits per second,
where the original bit rate was at 128000 bits per second with the help of wave
surfer audio tool. The output LPC cepstrum is implemented in first order three
state Hidden Markov Model(HMM) chain.

<id>
1003.4908v1
<category>
cs.SD
<abstract>
Among environmental sounds, we have chosen to study a class of action-related
impact sounds: automobile door closure sounds. We propose to describe these
sounds using a model composed of perceptual properties. The development of the
perceptual model was derived from the evaluation of many door closure sounds
measured under controlled laboratory listening conditions. However, listening
to such sounds normally occurs within a natural context, which probably
modifies their perception. We therefore need to study differences between the
real situation and the laboratory situation by following standard practices in
order to specify the precise listening conditions and observe the influence of
previous learning, expectations, action-perception interactions, and attention
given to sounds. Our process consists in doing in situ experiments that are
compared with specific laboratory experiments in order to isolate certain
influential, context dependent components.

<id>
1005.2465v2
<category>
cs.SD
<abstract>
The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi.

<id>
1006.0831v1
<category>
cs.SD
<abstract>
This work introduces an economic solution for the problems of sound
insulation of recording studios. Sound insulation at wall resonance frequency
is weak. Instead of acoustical treatment, a digital filter is used to eliminate
the effects of wall resonance and coincidence phenomena on recording of speech.
Sound insulation of studio is measured to calculate the wall resonance
frequency and the coincidence frequency. Pole /zero placement technique is used
to calculate the IIR filter coefficients. The digital filter is designed,
simulated and implemented. The proposed system is used to treat these problems
and it is shown to be effective in recording the noisy speech. In this work
digital signal processing is used instead of the acoustic treatment to
eliminate the effect of noise at the studio wall resonance. This technique is
cheap and effective in canceling the noise at the desired frequencies. Field
Programmable Gate Array (FPGA) is used for hardware implementation of the
proposed filter structure which provides fast and cheap solution for processing
real time audio signals. The implementation is carried out using Spartan chip
from Xinlinx achieving higher performance than commercially available software
solutions.

<id>
1006.0866v1
<category>
cs.SD
<abstract>
"Hopscotch" is a world-wide game for children to play since the times in the
ancient Roman Empire and China. Here we present a study mainly focused on the
research and discussion of the application on the children's well-know
edutainment via the physical interactive design to provide the sensing of the
times for the conventional hopscotch, which is a new type of experiment for the
technology aided edutainment. The innovated hopscotch music game involves the
sound samples of various animals and the characters of cartoon, and the
algorithmic composition via the development of the music technology based
interactive game, to gradually make the children perceive the world of digits,
sound, and music. It can guide the growing children's personality and character
from disorder into clarity. Furthermore, the traditional teaching materials can
be improved via the implementation of the electrical sensing devices,
electrical I/O module, and the computer music program Max/MSP, to integrate the
interactive computer music with the interactive and immersive soundscapes
composition, and the teaching tool with educational gaming is completely
accomplished eventually.

<id>
1009.2796v1
<category>
cs.SD
<abstract>
This paper addresses the problem of infants' cry fundamental frequency
estimation. The fundamental frequency is estimated using a modified simple
inverse filtering tracking (SIFT) algorithm. The performance of the modified
SIFT is studied using a real database of infants' cry. It is shown that the
algorithm is capable of overcoming the problem of under-estimation and
over-estimation of the cry fundamental frequency, with an estimation accuracy
of 6.15% and 3.75%, for hyperphonated and phonated cry segments, respectively.
Some typical examples of the fundamental frequency contour in typical cases of
pathological and healthy cry signals are presented and discussed.

<id>
1012.2797v1
<category>
cs.SD
<abstract>
In this paper, we ask what properties makes a large corpus more or less
useful. We suggest that size, by itself, should not be the ultimate goal of
building a corpus. Large-scale corpora are considered desirable because they
offer statistical stability and rich variation. But this rich variation means
more factors to control and evaluate, which can limit the advantages of size.
We discuss the use of multi-channel data to complement large-scale speech
corpora. Even though multi-channel data may limit the scale of a corpus (due to
the complex and labor-intensive nature of data collection) they can offer
information that allows us to tease apart various factors related to speech
production.

<id>
1107.4185v1
<category>
cs.SD
<abstract>
In this paper, envelope detection of speech is discussed to distinguish the
pathological cases of speech disabled children. The speech signal samples of
children of age between five to eight years are considered for the present
study. These speech signals are digitized and are used to determine the speech
envelope. The envelope is subjected to ratio mean analysis to estimate the
disability. This analysis is conducted on ten speech signal samples which are
related to both place of articulation and manner of articulation. Overall
speech disability of a pathological subject is estimated based on the results
of above analysis.

<id>
1107.5492v1
<category>
cs.SD
<abstract>
This paper presents a new method on the use of the gammachirp auditory filter
based on a continuous wavelet analysis. The gammachirp auditory filter is
designed to provide a spectrum reflecting the spectral properties of the
cochlea, which is responsible for frequency analysis in the human auditory
system. The impulse response of the theoretical gammachirp auditory filter that
has been developed by Irino and Patterson can be used as the kernel for wavelet
transform which approximates the frequency response of the cochlea. This study
implements the gammachirp auditory filter described by Irino as an analytical
wavelet and examines its application to a different speech signals.
  The obtained results will be compared with those obtained by two other
predefined wavelet families that are Morlet and Mexican Hat. The results show
that the gammachirp wavelet family gives results that are comparable to ones
obtained by Morlet and Mexican Hat wavelet family.

<id>
1206.1450v1
<category>
cs.SD
<abstract>
The sensitivity of human ear is dependent on frequency which is nonlinearly
resolved across the audio spectrum .Now to improve the recognition performance
in a similar non linear approach requires a front -end design, suggested by
empirical evidences. A popular alternative to linear prediction based analysis
is therefore filter bank analysis since this provides a much more
straightforward route to obtain the desired non-linear frequency resolution.
MEL filter bank and BARK filter bank are two popular filter bank analysis
techniques. This paper presents FPGA based implementation of MEL filter bank
and BARK filter bank with different bandwidths and different signal spectrum
ranges. The designs have been implemented using VHDL, simulated and verified
using Xilinx 11.1.For each filter bank, the basic building block is implemented
in Spartan 3E. A comparative study among these two mentioned filter banks is
also done in this paper.

<id>
1301.0265v1
<category>
cs.SD
<abstract>
Usable speech criteria are proposed to extract minimally corrupted speech for
speaker identification (SID) in co-channel speech. In co-channel speech, either
speaker can randomly appear as the stronger speaker or the weaker one at a
time. Hence, the extracted usable segments are separated in time and need to be
organized into speaker streams for SID. In this paper, we focus to organize
extracted usable speech segment into a single stream for the same speaker by
speaker assignment system. For this, we develop model-based speaker assignment
method based on posterior probability and exhaustive search algorithm.
Evaluation of this method is performed on TIMIT database. The system is
evaluated on co-channel speech and results show a significant improvement.

<id>
1301.0278v1
<category>
cs.SD
<abstract>
Many applications of speech communication and speaker identification suffer
from the problem of co-channel speech. This paper deals with a multi-resolution
dyadic wavelet transform method for usable segments of co-channel speech
detection that could be processed by a speaker identification system.
Evaluation of this method is performed on TIMIT database referring to the
Target to Interferer Ratio measure. Co-channel speech is constructed by mixing
all possible gender speakers. Results do not show much difference for different
mixtures. For the overall mixtures 95.76% of usable speech is correctly
detected with false alarms of 29.65%.

<id>
0903.3080v1
<category>
cs.SD
<abstract>
Time-frequency representations such as the spectrogram are commonly used to
analyze signals having a time-varying distribution of spectral energy, but the
spectrogram is constrained by an unfortunate tradeoff between resolution in
time and frequency. A method of achieving high-resolution spectral
representations has been independently introduced by several parties. The
technique has been variously named reassignment and remapping, but while the
implementations have differed in details, they are all based on the same
theoretical and mathematical foundation. In this work, we present a brief
history of work on the method we will call the method of time-frequency
reassignment, and present a unified mathematical description of the technique
and its derivation. We will focus on the development of time-frequency
reassignment in the context of the spectrogram, and conclude with a discussion
of some current applications of the reassigned spectrogram.

<id>
0903.3198v1
<category>
cs.SD
<abstract>
Using the AURORA-2 digit recognition task, we show that recognition
accuracies obtained with classical, SNR based oracle masks can be substantially
improved by using a state-dependent mask estimation technique.

<id>
1004.4478v1
<category>
cs.SD
<abstract>
Biometric authentication techniques are more consistent and efficient than
conventional authentication techniques and can be used in monitoring,
transaction authentication, information retrieval, access control, forensics,
etc. In this paper, we have presented a detailed comparative analysis between
Principle Component Analysis (PCA) and Independent Component Analysis (ICA)
which are used for feature extraction on the basis of different Artificial
Neural Network (ANN) such as Back Propagation (BP), Radial Basis Function (RBF)
and Learning Vector Quantization (LVQ). In this paper, we have chosen "TULIPS1
database, (Movellan, 1995)" which is a small audiovisual database of 12
subjects saying the first 4 digits in English for the incorporation of above
methods. The six geometric lip features i.e. height of the outer corners of the
mouth, width of the outer corners of the mouth, height of the inner corners of
the mouth, width of the inner corners of the mouth, height of the upper lip,
and height of the lower lip which extracts the identity relevant information
are considered for the research work. After the comprehensive analysis and
evaluation a maximum of 91.07% accuracy in speaker recognition is achieved
using PCA and RBF and 87.36% accuracy is achieved using ICA and RBF. Speaker
identification has a wide scope of applications such as access control,
monitoring, transaction authentication, information retrieval, forensics, etc.

<id>
cs/0206032v1
<category>
cs.SC
<abstract>
In this note, we fill a gap in the proof of the heuristic GCD in the
multivariate case made by Char, Geddes and Gonnet (JSC 1989) and give some
additionnal information on this method.

<id>
cs/0207006v1
<category>
cs.SC
<abstract>
This paper developed a systematic strategy establishing RBF on the wavelet
analysis, which includes continuous and discrete RBF orthonormal wavelet
transforms respectively in terms of singular fundamental solutions and
nonsingular general solutions of differential operators. In particular, the
harmonic Bessel RBF transforms were presented for high-dimensional data
processing. It was also found that the kernel functions of convection-diffusion
operator are feasible to construct some stable ridgelet-like RBF transforms. We
presented time-space RBF transforms based on non-singular solution and
fundamental solution of time-dependent differential operators. The present
methodology was further extended to analysis of some known RBFs such as the MQ,
Gaussian and pre-wavelet kernel RBFs.

<id>
cs/0207034v1
<category>
cs.SC
<abstract>
Recently, Bert, Wang and Striz [1, 2] applied the differential quadrature
(DQ) and harmonic differential quadrature (HDQ) methods to analyze static and
dynamic behaviors of anisotropic plates. Their studies showed that the methods
were conceptually simple and computationally efficient in comparison to other
numerical techniques. Based on some recent work by the present contributor [3, 4],
the purpose of this note is to further simplify the formulation effort and
improve computing efficiency in applying the DQ and HDQ methods for these
cases.

<id>
cs/0208031v1
<category>
cs.SC
<abstract>
The theme of symbolic computation in algebraic categories has become of
utmost importance in the last decade since it enables the automatic modeling of
modern algebra theories. On this theoretical background, the present paper
reveals the utility of the parameterized categorical approach by deriving a
multivariate polynomial category (over various coefficient domains), which is
used by our Mathematica implementation of Buchberger's algorithms for
determining the Groebner basis. These implementations are designed according to
domain and category parameterization principles underlining their advantages:
operation protection, inheritance, generality, easy extendibility. In
particular, such an extension of Mathematica, a widely used symbolic
computation system, with a new type system has a certain practical importance.
The approach we propose for Mathematica is inspired from D. Gruntz and M.
Monagan's work in Gauss, for Maple.

<id>
cs/0301029v1
<category>
cs.SC
<abstract>
A method is presented that reduces the number of terms of systems of linear
equations (algebraic, ordinary and partial differential equations). As a
byproduct these systems have a tendency to become partially decoupled and are
more likely to be factorizable or integrable. A variation of this method is
applicable to non-linear systems. Modifications to improve efficiency are given
and examples are shown. This procedure can be used in connection with the
computation of the radical of a differential ideal (differential Groebner
basis).

<id>
cs/0304003v2
<category>
cs.SC
<abstract>
Inevitability properties in branching temporal logics are of the syntax
forall eventually \phi, where \phi is an arbitrary (timed) CTL formula. In the
sense that "good things will happen", they are parallel to the "liveness"
properties in linear temporal logics. Such inevitability properties in
dense-time logics can be analyzed with greatest fixpoint calculation. We
present algorithms to model-check inevitability properties both with and
without requirement of non-Zeno computations. We discuss a technique for early
decision on greatest fixpoints in the temporal logics, and experiment with the
effect of non-Zeno computations on the evaluation of greatest fixpoints. We
also discuss the TCTL subclass with only universal path quantifiers which
allows for the safe abstraction analysis of inevitability properties. Finally,
we report our implementation and experiments to show the plausibility of our
ideas.

<id>
cs/0304004v2
<category>
cs.SC
<abstract>
Fast algorithms for arithmetic on real or complex polynomials are well-known
and have proven to be not only asymptotically efficient but also very
practical. Based on Fast Fourier Transform (FFT), they for instance multiply
two polynomials of degree up to N or multi-evaluate one at N points
simultaneously within quasi-linear time O(N.polylog N). An extension to (and in
fact the mere definition of) polynomials over the skew-field H of quaternions
is promising but still missing. The present work proposes three such
definitions which in the commutative case coincide but for H turn out to
differ, each one satisfying some desirable properties while lacking others. For
each notion we devise algorithms for according arithmetic; these are
quasi-optimal in that their running times match lower complexity bounds up to
polylogarithmic factors.

<id>
cs/0309008v1
<category>
cs.SC
<abstract>
We present a novel scheme to the coverage problem, introducing a quantitative
way to estimate the interaction between a block and its enviroment.This is
achieved by setting a discrete version of Green`s theorem, specially adapted
for Model Checking based verification of integrated circuits.This method is
best suited for the coverage problem since it enables one to quantify the
incompleteness or, on the other hand, the redundancy of a set of rules,
describing the model under verification.Moreover this can be done continuously
throughout the verification process, thus enabling the user to pinpoint the
stages at which incompleteness/redundancy occurs. Although the method is
presented locally on a small hardware example, we additionally show its
possibility to provide precise coverage estimation also for large scale
systems. We compare this method to others by checking it on the same
test-cases.

<id>
cs/0404008v2
<category>
cs.SC
<abstract>
We want to achieve efficiency for the exact computation of the dot product of
two vectors over word-size finite fields. We therefore compare the practical
behaviors of a wide range of implementation techniques using different
representations. The techniques used include oating point representations,
discrete logarithms, tabulations, Montgomery reduction, delayed modulus.

<id>
cs/0405060v1
<category>
cs.SC
<abstract>
The aim of this work is to show how we can decompose a module (if
decomposable) into an indecomposable module with the help of the minimization
process.

<id>
cs/0406002v1
<category>
cs.SC
<abstract>
A prototype for an extensible interactive graphical term manipulation system
is presented that combines pattern matching and nondeterministic evaluation to
provide a convenient framework for doing tedious algebraic manipulations that
so far had to be done manually in a semi-automatic fashion.

<id>
cs/0409048v1
<category>
cs.SC
<abstract>
We give a brief introduction to FORM, a symbolic programming language for
massive batch operations, designed by J.A.M. Vermaseren. In particular, we
stress various methods to efficiently use FORM under the UNIX operating system.
Several scripts and examples are given, and suggestions on how to use the vim
editor as development platform.

<id>
cs/0411063v1
<category>
cs.SC
<abstract>
In this paper we present our recent work in developing a computer-algebra
tool for systems of partial differential equations (PDEs), termed "Kranc". Our
work is motivated by the problem of finding solutions of the Einstein equations
through numerical simulations. Kranc consists of Mathematica based
computer-algebra packages, that facilitate the task of dealing with symbolic
tensorial calculations and realize the conversion of systems of partial
differential evolution equations into parallelized C or Fortran code.

<id>
cs/0412005v1
<category>
cs.SC
<abstract>
In this paper, we present a determinist Jordan normal form algorithms based
on the Fadeev formula: \[(\lambda \cdot I-A) \cdot B(\lambda)=P(\lambda) \cdot
I\] where $B(\lambda)$ is $(\lambda \cdot I-A)$'s comatrix and $P(\lambda)$ is
$A$'s characteristic polynomial. This rational Jordan normal form algorithm
differs from usual algorithms since it is not based on the Frobenius/Smith
normal form but rather on the idea already remarked in Gantmacher that the
non-zero column vectors of $B(\lambda_0)$ are eigenvectors of $A$ associated to
$\lambda_0$ for any root $\lambda_0$ of the characteristical polynomial. The
complexity of the algorithm is $O(n^4)$ field operations if we know the
factorization of the characteristic polynomial (or $O(n^5 \ln(n))$ operations
for a matrix of integers of fixed size). This algorithm has been implemented
using the Maple and Giac/Xcas computer algebra systems.

<id>
cs/0501074v2
<category>
cs.SC
<abstract>
This article deals with the computation of the characteristic polynomial of
dense matrices over small finite fields and over the integers. We first present
two algorithms for the finite fields: one is based on Krylov iterates and
Gaussian elimination. We compare it to an improvement of the second algorithm
of Keller-Gehrig. Then we show that a generalization of Keller-Gehrig's third
algorithm could improve both complexity and computational time. We use these
results as a basis for the computation of the characteristic polynomial of
integer matrices. We first use early termination and Chinese remaindering for
dense matrices. Then a probabilistic approach, based on integer minimal
polynomial and Hensel factorization, is particularly well suited to sparse
and/or structured matrices.

<id>
cs/0503073v2
<category>
cs.SC
<abstract>
GPL Maxima is an open-source computer algebra system based on DOE-MACSYMA.
GPL Maxima included two tensor manipulation packages from DOE-MACSYMA, but
these were in various states of disrepair. One of the two packages, CTENSOR,
implemented component-based tensor manipulation; the other, ITENSOR, treated
tensor symbols as opaque, manipulating them based on their index properties.
The present paper describes the state in which these packages were found, the
steps that were needed to make the packages fully functional again, and the new
functionality that was implemented to make them more versatile. A third
package, ATENSOR, was also implemented; fully compatible with the identically
named package in the commercial version of MACSYMA, ATENSOR implements abstract
tensor algebras.

<id>
cs/0510057v1
<category>
cs.SC
<abstract>
We propose a new diagrammatic modeling language, DML. The paradigm used is
that of the category theory and in particular of the pushout tool. We show that
most of the object-oriented structures can be described with this tool and have
many examples in C++, ranging from virtual inheritance and polymorphism to
template genericity. With this powerful tool, we propose a quite simple
description of the C++ LinBox library. This library has been designed for
efficiency and genericity and therefore makes heavy usage of complex template
and polymorphic mecanism. Be reverse engineering, we are able to describe in a
simple manner the complex structure of archetypes in LinBox.

<id>
cs/0510093v1
<category>
cs.SC
<abstract>
We report on the status of our project of parallelization of the symbolic
manipulation program FORM. We have now parallel versions of FORM running on
Cluster- or SMP-architectures. These versions can be used to run arbitrary FORM
programs in parallel.

<id>
cs/0511033v1
<category>
cs.SC
<abstract>
For a linearly recurrent vector sequence P[n+1] = A(n) * P[n], consider the
problem of calculating either the n-th term P[n] or L<=n arbitrary terms
P[n_1],...,P[n_L], both for the case of constant coefficients A(n)=A and for a
matrix A(n) with entries polynomial in n. We improve and extend known
algorithms for this problem and present new applications for it. Specifically
it turns out that for instance * any family (p_n) of classical orthogonal
polynomials admits evaluation at given x within O(n^{1/2} log n) operations
INDEPENDENT of the family (p_n) under consideration. * For any L indices
n_1,...,n_L <= n, the values p_{n_i}(x) can be calculated simultaneously using
O(n^{1/2} log n + L log(n/L)) arithmetic operations; again this running time
bound holds uniformly. * Every hypergeometric (or, more generally, holonomic)
function admits approximate evaluation up to absolute error e>0 within
O((log(1/e)^{1/2} loglog(1/e)) -- as opposed to O(log(1/e)) -- arithmetic
steps. * Given m and a polynomial p of degree d over a field of characteristic
zero, the coefficient of p^m to term X^n can be computed within O(d^2
M(n^{1/2})) steps where M(n) denotes the cost of multiplying two degree-n
polynomials. * The same time bound holds for the joint calculation of any
L<=n^{1/2} desired coefficients of p^m to terms X^{n_i}, n_1,...,n_L <= n.

<id>
cs/0511066v5
<category>
cs.SC
<abstract>
We present an algorithm computing the determinant of an integer matrix A. The
algorithm is introspective in the sense that it uses several distinct
algorithms that run in a concurrent manner. During the course of the algorithm
partial results coming from distinct methods can be combined. Then, depending
on the current running time of each method, the algorithm can emphasize a
particular variant. With the use of very fast modular routines for linear
algebra, our implementation is an order of magnitude faster than other existing
implementations. Moreover, we prove that the expected complexity of our
algorithm is only O(n^3 log^{2.5}(n ||A||)) bit operations in the dense case
and O(Omega n^{1.5} log^2(n ||A||) + n^{2.5}log^3(n||A||)) in the sparse case,
where ||A|| is the largest entry in absolute value of the matrix and Omega is
the cost of matrix-vector multiplication in the case of a sparse matrix.

<id>
cs/0601133v3
<category>
cs.SC
<abstract>
In the past two decades, some major efforts have been made to reduce exact
(e.g. integer, rational, polynomial) linear algebra problems to matrix
multiplication in order to provide algorithms with optimal asymptotic
complexity. To provide efficient implementations of such algorithms one need to
be careful with the underlying arithmetic. It is well known that modular
techniques such as the Chinese remainder algorithm or the p-adic lifting allow
very good practical performance, especially when word size arithmetic are used.
Therefore, finite field arithmetic becomes an important core for efficient
exact linear algebra libraries. In this paper, we study high performance
implementations of basic linear algebra routines over word size prime fields:
specially the matrix multiplication; our goal being to provide an exact
alternate to the numerical BLAS library. We show that this is made possible by
a carefull combination of numerical computations and asymptotically faster
algorithms. Our kernel has several symbolic linear algebra applications enabled
by diverse matrix multiplication reductions: symbolic triangularization, system
solving, determinant and matrix inverse implementations are thus studied.

<id>
cs/0602064v1
<category>
cs.SC
<abstract>
In this paper, a set of programs enhancing the Kenzo system is presented.
Kenzo is a Common Lisp program designed for computing in Algebraic Topology, in
particular it allows the user to calculate homology and homotopy groups of
complicated spaces. The new programs presented here entirely compute Serre and
Eilenberg-Moore spectral sequences, in particular the groups and differential
maps for arbitrary r. They also determine when the spectral sequence has
converged and describe the filtration of the target homology groups induced by
the spectral sequence.

<id>
cs/0603082v1
<category>
cs.SC
<abstract>
We propose a new algorithm to solve sparse linear systems of equations over
the integers. This algorithm is based on a $p$-adic lifting technique combined
with the use of block matrices with structured blocks. It achieves a sub-cubic
complexity in terms of machine operations subject to a conjecture on the
effectiveness of certain sparse projections. A LinBox-based implementation of
this algorithm is demonstrated, and emphasizes the practical benefits of this
new method over the previous state of the art.

<id>
cs/0604060v1
<category>
cs.SC
<abstract>
Lie group theory states that knowledge of a $m$-parameters solvable group of
symmetries of a system of ordinary differential equations allows to reduce by
$m$ the number of equation. We apply this principle by finding dilatations and
translations that are Lie point symmetries of considered ordinary differential
system. By rewriting original problem in an invariant coordinates set for these
symmetries, one can reduce the involved number of parameters. This process is
classically call nondimensionalisation in dimensional analysis. We present an
algorithm based on this standpoint and show that its arithmetic complexity is
polynomial in input's size.

<id>
cs/0604101v1
<category>
cs.SC
<abstract>
We propose new algorithms for the computation of the first N terms of a
vector (resp. a basis) of power series solutions of a linear system of
differential equations at an ordinary point, using a number of arithmetic
operations which is quasi-linear with respect to N. Similar results are also
given in the non-linear case. This extends previous results obtained by Brent
and Kung for scalar differential equations of order one and two.

<id>
cs/0605021v1
<category>
cs.SC
<abstract>
This seminar report is concerned with expressing LPO-termination of term
rewrite systems as a satisfiability problem in propositional logic. After
relevant algorithms are explained, experimental results are reported.

<id>
cs/0605068v1
<category>
cs.SC
<abstract>
We consider two kinds of problems: the computation of polynomial and rational
solutions of linear recurrences with coefficients that are polynomials with
integer coefficients; indefinite and definite summation of sequences that are
hypergeometric over the rational numbers. The algorithms for these tasks all
involve as an intermediate quantity an integer $N$ (dispersion or root of an
indicial polynomial) that is potentially exponential in the bit size of their
input. Previous algorithms have a bit complexity that is at least quadratic in
$N$. We revisit them and propose variants that exploit the structure of
solutions and avoid expanding polynomials of degree $N$. We give two
algorithms: a probabilistic one that detects the existence or absence of
nonzero polynomial and rational solutions in $O(\sqrt{N}\log^{2}N)$ bit
operations; a deterministic one that computes a compact representation of the
solution in $O(N\log^{3}N)$ bit operations. Similar speed-ups are obtained in
indefinite and definite hypergeometric summation. We describe the results of an
implementation.

<id>
cs/0606031v1
<category>
cs.SC
<abstract>
Consider a system of n polynomial equations and r polynomial inequations in n
indeterminates of degree bounded by d with coefficients in a polynomial ring of
s parameters with rational coefficients of bit-size at most $\sigma$. From the
real viewpoint, solving such a system often means describing some
semi-algebraic sets in the parameter space over which the number of real
solutions of the considered parametric system is constant. Following the works
of Lazard and Rouillier, this can be done by the computation of a discriminant
variety. In this report we focus on the case where for a generic specialization
of the parameters the system of equations generates a radical zero-dimensional
ideal, which is usual in the applications. In this case, we provide a
deterministic method computing the minimal discriminant variety reducing the
problem to a problem of elimination. Moreover, we prove that the degree of the
computed minimal discriminant variety is bounded by $D:=(n+r)d^{(n+1)}$ and
that the complexity of our method is $\sigma^{\mathcal{O}(1)}
D^{\mathcal{O}(n+s)}$ bit-operations on a deterministic Turing machine.

<id>
cs/0610051v2
<category>
cs.SC
<abstract>
Let f1, ..., fs be a polynomial family in Q[X1,..., Xn] (with s less than n)
of degree bounded by D. Suppose that f1, ..., fs generates a radical ideal, and
defines a smooth algebraic variety V. Consider a projection P. We prove that
the degree of the critical locus of P restricted to V is bounded by
D^s(D-1)^(n-s) times binomial of n and n-s. This result is obtained in two
steps. First the critical points of P restricted to V are characterized as
projections of the solutions of Lagrange's system for which a bi-homogeneous
structure is exhibited. Secondly we prove a bi-homogeneous B\'ezout Theorem,
which bounds the sum of the degrees of the equidimensional components of the
radical of an ideal generated by a bi-homogeneous polynomial family. This
result is improved when f1,..., fs is a regular sequence. Moreover, we use
Lagrange's system to design an algorithm computing at least one point in each
connected component of a smooth real algebraic set. This algorithm generalizes,
to the non equidimensional case, the one of Safey El Din and Schost. The
evaluation of the output size of this algorithm gives new upper bounds on the
first Betti number of a smooth real algebraic set. Finally, we estimate its
arithmetic complexity and prove that in the worst cases it is polynomial in n,
s, D^s(D-1)^(n-s) and the binomial of n and n-s, and the complexity of
evaluation of f1,..., fs.

<id>
cs/0610136v4
<category>
cs.SC
<abstract>
This note presents absolute bounds on the size of the coefficients of the
characteristic and minimal polynomials depending on the size of the
coefficients of the associated matrix. Moreover, we present algorithms to
compute more precise input-dependant bounds on these coefficients. Such bounds
are e.g. useful to perform deterministic chinese remaindering of the
characteristic or minimal polynomial of an integer matrix.

<id>
nlin/0001019v1
<category>
nlin.AO
<abstract>
We discuss the problem of parameter estimation in nonlinear stochastic
differential equations based on sampled time series. A central message from the
theory of integrating stochastic differential equations is that there exists in
general two time scales, i.e. that of integrating these equations and that of
sampling. We argue that therefore maximum likelihood estimation is
computational extremely expensive. We discuss the relation between maximum
likelihood and quasi maximum likelihood estimation. In a simulation study, we
compare the quasi maximum likelihood method with an approach for parameter
estimation in nonlinear stochastic differential equations that disregards the
existence of the two time scales.

<id>
nlin/0002004v1
<category>
nlin.AO
<abstract>
In the standard minority game, each agent in the minority group receives the
same payoff regardless of the size of the minority group. Of great interest for
real social and biological systems are cases in which the payoffs to members of
the minority group depend on the size of the minority group. This latter
includes the fixed sum game. We find, remarkably, that the phase structure and
general scaling behavior of the standard minority game persists when the payoff
function depends on the size of the minority group. there is still a phase
transition at the same value of z, the ratio of the dimension of the strategy
space to the number of agents playing the game. We explain the persistence of
the phase structure and argue that it is due to the absence of temporal
cooperation in the dynamics of the minority game. We also discuss the behavior
of average agent wealth and the wealth distribution in these variable payoff
games.

<id>
nlin/0002007v1
<category>
nlin.AO
<abstract>
A family of multi-value cellular automaton (CA) associated with traffic flow
is presented in this paper. The family is obtained by extending the rule-184
CA, which is an ultradiscrete analogue to the Burgers equation. CA models in
the family show both metastable states and stop-and-go waves, which are often
observed in real traffic flow. Metastable states in the models exist not only
on a prominent part of a free phase but also in a congested phase.

<id>
nlin/0003030v1
<category>
nlin.AO
<abstract>
Based on the LISSOM model and the OFC earthquake model, we introduce a
self-organized feature map Neural Network model . It displays a "Self Organized
Criticality"(SOC) behavior. It can be seen that the feature area (synchronized
area) produced by self-organized process brings about some definite effect on
SOC behavior and the system evolves into a "partly-synchronized" state. For
explaining this phenomena, a quasi-OFC earthquake model is simulated.

<id>
nlin/0003042v1
<category>
nlin.AO
<abstract>
World records in athletics provide a measure of physical as well as
physiological human performance. Here we analyse running records and show that
the mean speed as a function of race time can be described by two scaling laws
that have a breakpoint at about 150-170 seconds (corresponding to the ~1,000 m
race). We interpret this as being the transition time between anaerobic and
aerobic energy expenditure by athletes.

<id>
nlin/0003053v1
<category>
nlin.AO
<abstract>
The physical and chemical properties of metal nanoparticles differ
significantly from those of free metal atoms as well as from the properties of
bulk metals, and therefore, they may be viewed as a transition regime between
the two physical states. Within this nanosize regime, there is a wide
fluctuation of properties, particularly chemical reactivity, as a function of
the size, geometry, and electronic state of the metal nanoparticles. In recent
years, great advancements have been made in the attempts to control and
manipulate the growth of metal particles to pre-specified dimensions. One of
the main synthetic methods utilized in this endeavor, is the capping of the
growing clusters with a variety of molecules, e.g. polymers. In this paper we
attempt to model such a process and show the relationship between the
concentration of the polymer present in the system and the final metal particle
size obtained. The theoretical behavior which we obtained is compared with
experimental results for the cobalt-polystyrene system.

<id>
nlin/0004033v1
<category>
nlin.AO
<abstract>
We study the emergence of a power law distribution in the systems which can
be characterized by a hierarchically organized supplying network. It is shown
that conservation laws on the branches of the network can, at some
approximation, impose power law properties on the systems. Some simple examples
taken from economics, biophysics etc. are considered.

<id>
nlin/0005018v2
<category>
nlin.AO
<abstract>
We look at price formation in a retail setting, that is, companies set
prices, and consumers either accept prices or go someplace else. In contrast to
most other models in this context, we use a two-dimensional spatial structure
for information transmission, that is, consumers can only learn from nearest
neighbors. Many aspects of this can be understood in terms of generalized
evolutionary dynamics. In consequence, we first look at spatial competition and
cluster formation without price. This leads to establishement size
distributions, which we compare to reality. After some theoretical
considerations, which at least heuristically explain our simulation results, we
finally return to price formation, where we demonstrate that our simple model
with nearly no organized planning or rationality on the part of any of the
agents indeed leads to an economically plausible price.

<id>
nlin/0006048v1
<category>
nlin.AO
<abstract>
In the light of a recently derived evolution equation for genetic algorithms
we consider the schema theorem and the building block hypothesis. We derive a
schema theorem based on the concept of effective fitness showing that schemata
of higher than average effective fitness receive an exponentially increasing
number of trials over time. The equation makes manifest the content of the
building block hypothesis showing how fit schemata are constructed from fit
sub-schemata. However, we show that generically there is no preference for
short, low-order schemata. In the case where schema reconstruction is favored
over schema destruction large schemata tend to be favored. As a corollary of
the evolution equation we prove Geiringer's theorem.

<id>
nlin/0006049v1
<category>
nlin.AO
<abstract>
We analyze the schema theorem and the building block hypothesis using a
recently derived, exact schemata evolution equation. We derive a new schema
theorem based on the concept of effective fitness showing that schemata of
higher than average effective fitness receive an exponentially increasing
number of trials over time. The building block hypothesis is a natural
consequence in that the equation shows how fit schemata are constructed from
fit sub-schemata. However, we show that generically there is no preference for
short, low-order schemata. In the case where schema reconstruction is favoured
over schema destruction large schemata tend to be favoured. As a corollary of
the evolution equation we prove Geiringer's theorem. We give supporting
numerical evidence for our claims in both non-epsitatic and epistatic
landscapes.

<id>
nlin/0007005v1
<category>
nlin.AO
<abstract>
This paper discusses dynamic evolutionary economics, and introduces a model
of such.

<id>
nlin/0007006v1
<category>
nlin.AO
<abstract>
Self-adjusting, or adaptive systems have gathered much recent interest. We
present a model for self-adjusting systems which treats the control parameters
of the system as slowly varying, rather than constant. The dynamics of these
parameters is governed by a low-pass filtered feedback from the dynamical
variables of the system. We apply this model to the logistic map and examine
the behavior of the control parameter. We find that the parameter leaves the
chaotic regime. We observe a high probability of finding the parameter at the
boundary between periodicity and chaos. We therefore find that this system
exhibits adaptation to the edge of chaos.

<id>
nlin/0007013v1
<category>
nlin.AO
<abstract>
In spite of their seemingly "obvious" virtues as a search strategy, genetic
algorithms have ended up playing only a modest role as design tools in science
and engineering. We review the reasons for this apparent failure, and we
suggest a more relaxed view of their utility.

<id>
nlin/0008008v1
<category>
nlin.AO
<abstract>
We present a model describing the competition between information
transmission and decision making in financial markets. The solution of this
simple model is recalled, and possible variations discussed. It is shown
numerically that despite its simplicity, it can mimic a size effect comparable
to a crash. Two extensions of this model are presented that allow to simulate
the demand process. One of these extensions has a coherent stable equilibrium
and is self-organized, while the other has a bistable equilibrium, with a
spontaneous segregation of the population of agents. A new model is introduced
to generate a transition between those two equilibriums. We show that the
coherent state is dominant up to an equal mixing of the two extensions. We
focuss our attention on the microscopic structure of the investment rate, which
is the main parameter of the original model. A constant investment rate seems
to be a very good approximation.

<id>
nlin/0009018v1
<category>
nlin.AO
<abstract>
The main problem we address in this paper is whether function determines form
when a society of agents organizes itself for some purpose or whether the
organizing method is more important than the functionality in determining the
structure of the ensemble. As an example, we use a neural network that learns
the same function by two different learning methods. For sufficiently large
networks, very different structures may indeed be obtained for the same
functionality. Clustering, characteristic path length and hierarchy are
structural differences, which in turn have implications on the robustness and
adaptability of the networks. In networks, as opposed to simple graphs, the
connections between the agents are not necessarily symmetric and may have
positive or negative signs. New characteristic coefficients are introduced to
characterize this richer connectivity structure.

<id>
nlin/0009042v1
<category>
nlin.AO
<abstract>
The emergence of dynamical structures in multi-agent systems is analysed.
Three different mechanisms are identified, namely: (1) sensitive-dependence and
convex coupling, (2) sensitive-dependence and extremal dynamics and (3)
interaction through a collectively generated field. The dynamical origin of the
emergent structures is traced back either to a modification, by interaction, of
the Lyapunov spectrum or to multistable dynamics.

<id>
nlin/0010013v1
<category>
nlin.AO
<abstract>
We analyze the synchronization transition for a pair of coupled identical
Kauffman networks in the chaotic phase. The annealed model for Kauffman
networks shows that synchronization appears through a transcritical
bifurcation, and provides an approximate description for the whole dynamics of
the coupled networks. We show that these analytical predictions are in good
agreement with numerical results for sufficiently large networks, and study
finite-size effects in detail. Preliminary analytical and numerical results for
partially disordered networks are also presented.

<id>
nlin/0010037v1
<category>
nlin.AO
<abstract>
A dynamical model for the distribution of resources between competing agents
is studied. While global competition leads to the accumulation of all the
resources by a single agent, local competition allows for a wider resource
distribution. Multiplicative processes give rise to almost-ordered spatial
structures, thourgh the enhancement of random fluctuations.

<id>
nlin/0010051v1
<category>
nlin.AO
<abstract>
Computers and routers on the Internet send each other error messages (called
ICMP datagrams) to signal conditions such as network congestion or blackouts.
While these datagrams are very rare, less than 0.001% of total traffic, they
hold very important global information about problems and congestions elsewhere
in the Net. A measurement of the flow of such error messages in our local
cluster shows a very pathological distribution of inter-message times:
$P(\Delta t) \approx 1/\Delta t$. This scaling extends for about seven decades,
and is only punctuated by extraneously periodic signals from automatons. More
than a half of these error messages were themselves generated erroneously.

<id>
nlin/0011021v1
<category>
nlin.AO
<abstract>
We investigate the interface dynamic in Laplacian growth model, using the
conformal mapping technique. Starting from the governing equation for the
conformal map, obtained by B.Shraiman and D.Bensimon, we derive different
possible forms of the equation. Some of them have evident physical
representation and are convenient for computer simulation, the results of which
are presented in this paper.

<id>
nlin/0012002v1
<category>
nlin.AO
<abstract>
Mortgage prepayments play a crucial role in the pricing and hedging of
mortgage backed securities. An important feature of mortgage prepayment
modeling is burnout; as time goes on those borrowers who have the greatest
tendency to refinance are removed from the pool leaving only those that are
less likely to refinance. In this paper we examine the implications of burnout
on the late time prepayment rate using rather general assumptions. Analytic
formulas are derived for the average prepayment rate in the N'th month, P_N,
and the fraction of borrowers left in the pool in the N'th month, y_N. In the
case where the incentive to refinance, and other relevent economic factors, are
constant these results are particularly simple. For example, P_N=p+(1-p)/N+...
, where p is a constant and the ellipses denote terms suppressed by more powers
of 1/N or exponentially suppressed. The term of order 1/N indicates that
burnout causes the probability of prepayment to decrease as a very simple
function of N.

<id>
nlin/0101006v1
<category>
nlin.AO
<abstract>
Numerous definitions for complexity have been proposed over the last half
century, with little consensus achieved on how to use the term. A definition of
complexity is supplied here that is closely related to the Kolmogorov
Complexity and Shannon Entropy measures widely used as complexity measures, yet
addresses a number of concerns raised against these measures. However, the
price of doing this is to introduce context dependence into the definition of
complexity. It is argued that such context dependence is an inherent property
of complexity, and related concepts such as entropy and emergence. Scientists
are uncomfortable with such context dependence, which smacks of subjectivity,
and this is perhaps the reason why little agreement has been found on the
meaning of these terms.

<id>
nlin/0102029v1
<category>
nlin.AO
<abstract>
We propose a generalization of small world networks, in which the
reconnection of links is governed by a function that depends on the distance
between the elements to be linked. An adequate choice of this function lets us
control the clusterization of the system. Control of the clusterization, in
turn, allows the generation of a wide variety of topologies.

<id>
nlin/0103038v1
<category>
nlin.AO
<abstract>
We show that the way in which the Shannon entropy of sequences produced by an
information source converges to the source's entropy rate can be used to
monitor how an intelligent agent builds and effectively uses a predictive model
of its environment. We introduce natural measures of the environment's apparent
memory and the amounts of information that must be (i) extracted from
observations for an agent to synchronize to the environment and (ii) stored by
an agent for optimal prediction. If structural properties are ignored, the
missed regularities are converted to apparent randomness. Conversely, using
representations that assume too much memory results in false predictability.

<id>
nlin/0104002v1
<category>
nlin.AO
<abstract>
Autoresonance is a phase locking phenomenon occurring in nonlinear
oscillatory system, which is forced by oscillating perturbation. Many physical
applications of the autoresonance are known in nonlinear physics. The essence
of the phenomenon is that the nonlinear oscillator selfadjusts to the varying
external conditions so that it remains in resonance with the driver for a long
time. This long time resonance leads to a strong increase in the response
amplitude under weak driving perturbation. An analytic treatment of a simple
mathematical model is done here by means of asymptotic analysis using a small
driving parameter. The main result is finding threshold for entering the
autoresonance.

<id>
nlin/0104016v1
<category>
nlin.AO
<abstract>
Recently it has been shown that a large variety of different networks have
power-law (scale-free) distributions of connectivities. We investigate the
robustness of such a distribution in discrete threshold networks under neutral
evolution. The guiding principle for this is robustness in the resulting
phenotype. The numerical results show that a power-law distribution is not
stable under such an evolution, and the network approaches a homogeneous form
where the overall distribution of connectivities is given by a Poisson
distribution.

<id>
nlin/0105059v3
<category>
nlin.AO
<abstract>
We study the two time correlation for the noise driven dynamics of the
double-well oscillator in the Suzuki regime. It is seen that for very small
noise strength the correaltion function shows a lack of translational
invariance for very long times characterised by the Suzuki scaling variable. We
see that in this strongly out-of-equilibrium situation, the conventional
mode-coupling approximation is not a convenient tool.

<id>
nlin/0106006v2
<category>
nlin.AO
<abstract>
The order of `noun and adposition' is the important parameter of word
ordering rules in the world's languages. The seven parameters, `adverb and
verb' and others, have a strong dependence on the `noun and adposition'.
Japanese as well as Korean, Tamil and several other languages seem to have a
stable structure of word ordering rules, as well as Thai and other languages
which have the opposite word ordering rules to Japanese. It seems that each
language in the world fluctuates between these two structures like the Ising
model for finite lattice.

<id>
nlin/0106027v1
<category>
nlin.AO
<abstract>
The immune system is a cognitive system of complexity comparable to the brain
and its computational algorithms suggest new solutions to engineering problems
or new ways of looking at these problems. Using immunological principles, a two
(or three-) module algorithm is developed which is capable of launching a
specific response to an anomalous situation. Applications are being developed
for electromechanical drives and network power transformers. Experimental
results illustrate an application to fault detection in squirrel-cage electric
motors.

<id>
nlin/0107026v1
<category>
nlin.AO
<abstract>
We show mathematical structure of the function dynamics, i.e., the dynamics
of interval maps $f_{n+1} = (1-\e)f_n + \e f_n\circ f_n$ and clarify the types
of fixed points, the self-referential structure and the hierarchical structure.

<id>
nlin/0101037v1
<category>
nlin.CG
<abstract>
We conjecture that for a wide class of interacting particle systems evolving
in discrete time, namely conservative cellular automata with piecewise linear
flow diagram, relaxation to the limit set follows the same power law at
critical points. We further describe the structure of the limit sets of such
systems as unions of shifts of finite type. Relaxation to the equilibrium
resembles ballistic annihilation, with ``defects'' propagating in opposite
direction annihilating upon collision.

<id>
nlin/0102035v1
<category>
nlin.CG
<abstract>
We compare several definitions for number-conserving cellular automata that
we prove to be equivalent. A necessary and sufficient condition for \cas to be
number-conserving is proved. Using this condition, we give a linear-time
algorithm to decide number-conservation. The dynamical behavior of
number-conserving \cas is studied and a classification that focuses on
chaoticity is given.

<id>
nlin/0105045v1
<category>
nlin.CG
<abstract>
We analyze the steady-state flow as a function of the initial density for a
class of deterministic cellular automata rules (``traffic rules'') with
periodic boundary conditions [H. Fuks and N. Boccara, Int. J. Mod. Phys. C 9, 1
(1998)]. We are able to predict from simple considerations the observed,
unexpected cutoff of the average flow at unity. We also present an efficient
algorithm for determining the exact final flow from a given finite initial
state. We analyze the behavior of this algorithm in the infinite limit to
obtain for R_m,k an exact polynomial equation maximally of 2(m+k)th degree in
the flow and density.

<id>
nlin/0204045v5
<category>
nlin.CG
<abstract>
In addition to the $\lambda$ parameter, we have found another parameter which
characterize the class III, class II and class IV patterns more quantitatively.
It explains why the different classes of patterns coexist at the same
$\lambda$. With this parameter, the phase diagram for an one dimensional
cellular automata is obtained. Our result explains why the edge of chaos(class
IV) is scattered rather wide range in $\lambda$ around 0.5, and presents an
effective way to control the pattern classes. \noindent PACS: 89.75.-k Complex
Systems

<id>
nlin/0205068v1
<category>
nlin.CG
<abstract>
Book Review for: "A New Kind of Science", by Stephen Wolfram (Wolfram Media,
Inc. Champaign IL 2002).

<id>
nlin/0206025v1
<category>
nlin.CG
<abstract>
The most efficient known method for solving certain computational problems is
to construct an iterated map whose fixed points are by design the problem's
solution. Although the origins of this idea go back at least to Newton, the
clearest expression of its logical basis is an example due to Mermin. A
contemporary application in image recovery demonstrates the power of the
method.

<id>
nlin/0206033v1
<category>
nlin.CG
<abstract>
Derivation of the lattice Boltzmann method from the continuous kinetic theory
[X. He and L. S. Luo, {\it Phys. Rev. E} {\bf 55}, R6333 (1997); X. Shan and X.
He, {\it Phys. Rev. Lett.} {\bf 80}, 65 (1998)] is extended in order to obtain
boundary conditions for the method. For the model of a diffusively reflecting
moving solid wall, the boundary condition for the discrete set of velocities is
derived, and the error of the discretization is estimated. Numerical results
are presented which demonstrate convergence to the hydrodynamic limit. In
particular, the Knudsen layer in the Kramers' problem is reproduced correctly
for small Knudsen numbers.

<id>
nlin/0206042v1
<category>
nlin.CG
<abstract>
We present an exact solution of headway distribution of the asymmetric simple
exclusion model with open boundary conditions and compare it to the headway
distributions of the highway traffic.

<id>
nlin/0206043v1
<category>
nlin.CG
<abstract>
We present a cellular automaton simulating the behavior of public bus
transport in several Mexican cities. The headway statistics obtained from the
model is compared to the measured time intervals between subsequent bus
arrivals to a given bus stop and to a spacing distribution resulting from a
random matrix theory.

<id>
nlin/0207017v1
<category>
nlin.CG
<abstract>
Coarse timesteppers provide a bridge between microscopic / stochastic system
descriptions and macroscopic tasks such as coarse stability/bifurcation
computations. Exploiting this computational enabling technology, we present a
framework for designing observers and controllers based on microscopic
simulations, that can be used for their coarse control. The proposed
methodology provides a bridge between traditional numerical analysis and
control theory on the one hand and microscopic simulation on the other.

<id>
nlin/0207047v1
<category>
nlin.CG
<abstract>
We demonstrate that a local mapping f in a space of bisequences over {0,1}
which conserves the number of nonzero sites can be viewed as a deterministic
particle system evolving according to a local mapping in a space of increasing
bisequences over Z. We present an algorithm for determination of the local
mapping in the space of particle coordinates corresponding to the local mapping
f.

<id>
nlin/0210062v1
<category>
nlin.CG
<abstract>
We consider the long time dependence for the moments of displacement < |r|^q
> of infinite horizon billiards, given a bounded initial distribution of
particles. For a variety of billiard models we find <|r|^q> ~ t^g(q) (up to
factors of log t). The time exponent, g(q), is piecewise linear and equal to
q/2 for q<2 and q-1 for q>2. We discuss the lack of dependence of this result
on the initial distribution of particles and resolve apparent discrepancies
between this time dependence and a prior result. The lack of dependence on
initial distribution follows from a remarkable scaling result that we obtain
for the time evolution of the distribution function of the angle of a
particle's velocity vector.

<id>
nlin/0210063v1
<category>
nlin.CG
<abstract>
The long time algebraic relaxation process in spatially periodic billiards
with infinite horizon is shown to display a self-similar time asymptotic form.
This form is identical for a class of such billiards, but can be different in
an important special case.

<id>
nlin/0211015v1
<category>
nlin.CG
<abstract>
It is shown that for the N-neighbor and K-state cellular automata, the class
II, class III and class IV patterns coexist at least in the range $\frac{1}{K}
\le \lambda \le 1-\frac{1}{K} $. The mechanism which determines the difference
between the pattern classes at a fixed $\lambda$ is found, and it is studied
quantitatively by introducing a new parameter $F$. Using the parameter F and
$\lambda$, the phase diagram of cellular automata is obtained for 5-neighbor
and 4-state cellular automata. PACS: 89.75.-k Complex Systems

<id>
nlin/0302015v1
<category>
nlin.CG
<abstract>
We present a probabilistic cellular automaton (CA) with two absorbing states
which performs classification of binary strings in a non-deterministic sense.
In a system evolving under this CA rule, empty sites become occupied with a
probability proportional to the number of occupied sites in the neighborhood,
while occupied sites become empty with a probability proportional to the number
of empty sites in the neighborhood. The probability that all sites become
eventually occupied is equal to the density of occupied sites in the initial
string.

<id>
nlin/0303066v1
<category>
nlin.CG
<abstract>
The mechanism which discriminates the pattern classes at the same $\lambda$,
is found. It is closely related to the structure of the rule table and
expressed by the numbers of the rules which break the strings of the quiescent
states. It is shown that for the N-neighbor and K-state cellular automata, the
class I, class II, class III and class IV patterns coexist at least in the
range, $\frac{1}{K} \le \lambda \le 1-\frac{1}{K} $. The mechanism is studied
quantitatively by introducing a new parameter $F$, which we call quiescent
string dominance parameter. It is taken to be orthogonal to $\lambda$. Using
the parameter F and $\lambda$, the rule tables of one dimensional 5-neighbor
and 4-state cellular automata are classified. The distribution of the four
pattern classes in ($\lambda$,F) plane shows that the rule tables of class III
pattern class are distributed in larger $F$ region, while those of class II and
class I pattern classes are found in the smaller $F$ region and the class IV
behaviors are observed in the overlap region between them. These distributions
are almost independent of $\lambda$ at least in the range $0.25 \leq \lambda
\leq 0.75$, namely the overlapping region in $F$, where the class III and class
II patterns coexist, has quite gentle $\lambda$ dependence in this $\lambda$
region. Therefore the relation between the pattern classes and the $\lambda$
parameter is not observed. PACS: 89.75.-k Complex Systems

<id>
nlin/0304010v1
<category>
nlin.CG
<abstract>
The Sato-Crutchfield equations are studied analytically and numerically. The
Sato-Crutchfield formulation is corresponding to losing memory. Then
Sato-Crutchfield formulation is applied for some different types of games
including hawk-dove, prisoner's dilemma and the battle of the sexes games. The
Sato-Crutchfield formulation is found not to affect the evolutionarily stable
strategy of the ordinary games. But choosing a strategy becomes purely random
independent on the previous experiences, initial conditions, and the rules of
the game itself. Sato-Crutchfield formulation for the prisoner's dilemma game
can be considered as a theoretical explanation for the existence of cooperation
in a population of defectors.

<id>
nlin/0305051v1
<category>
nlin.CG
<abstract>
We demonstrate that the concept of a conservation law can be naturally
extended from deterministic to probabilistic cellular automata (PCA) rules. The
local function for conservative PCA must satisfy conditions analogous to
conservation conditions for deterministic cellular automata. Conservation
condition for PCA can also be written in the form of a current conservation
law. For deterministic nearest-neighbour CA the current can be computed
exactly. Local structure approximation can partially predict the equilibrium
current for non-deterministic cases. For linear segments of the fundamental
diagram it actually produces exact results.

<id>
nlin/0306022v2
<category>
nlin.CG
<abstract>
The virtual ant introduced by C. Langton has an interesting behavior, which
has been studied in several contexts. Here we give a construction to calculate
any boolean circuit with the trajectory of a single ant. This proves the
P-hardness of the system and implies, through the simulation of one dimensional
cellular automata and Turing machines, the universality of the ant and the
undecidability of some problems associated to it.

<id>
nlin/0306032v1
<category>
nlin.CG
<abstract>
Number-conserving cellular automata (NCCA) are particularly interesting, both
because of their natural appearance as models of real systems, and because of
the strong restrictions that number-conservation implies. Here we extend the
definition of the property to include cellular automata with any set of states
in $\Zset$, and show that they can be always extended to ``usual'' NCCA with
contiguous states. We show a way to simulate any one dimensional CA through a
one dimensional NCCA, proving the existence of intrinsically universal NCCA.
Finally, we give an algorithm to decide, given a CA, if its states can be
labeled with integers to produce a NCCA, and to find this relabeling if the
answer is positive.

<id>
nlin/0306040v2
<category>
nlin.CG
<abstract>
Number-conserving (or {\em conservative}) cellular automata have been used in
several contexts, in particular traffic models, where it is natural to think
about them as systems of interacting particles. In this article we consider
several issues concerning one-dimensional cellular automata which are
conservative, monotone (specially ``non-increasing''), or that allow a weaker
kind of conservative dynamics. We introduce a formalism of ``particle
automata'', and discuss several properties that they may exhibit, some of
which, like anticipation and momentum preservation, happen to be intrinsic to
the conservative CA they represent. For monotone CA we give a characterization,
and then show that they too are equivalent to the corresponding class of
particle automata. Finally, we show how to determine, for a given CA and a
given integer $b$, whether its states admit a $b$-neighborhood-dependent
relabelling whose sum is conserved by the CA iteration; this can be used to
uncover conservative principles and particle-like behavior underlying the
dynamics of some CA. Complements at {\tt http://www.dim.uchile.cl/\verb'
'anmoreir/ncca}

<id>
nlin/0307016v1
<category>
nlin.CG
<abstract>
In "equation-free" multiscale computation a dynamic model is given at a fine,
microscopic level; yet we believe that its coarse-grained, macroscopic dynamics
can be described by closed equations involving only coarse variables. These
variables are typically various low-order moments of the distributions evolved
through the microscopic model. We consider the problem of integrating these
unavailable equations by acting directly on kinetic Monte Carlo microscopic
simulators, thus circumventing their derivation in closed form. In particular,
we use projective multi-step integration to solve the coarse initial value
problem forward in time as well as backward in time (under certain conditions).
Macroscopic trajectories are thus traced back to unstable, source-type, and
even sometimes saddle-like stationary points, even though the microscopic
simulator only evolves forward in time. We also demonstrate the use of such
projective integrators in a shooting boundary value problem formulation for the
computation of "coarse limit cycles" of the macroscopic behavior, and the
approximation of their stability through estimates of the leading "coarse
Floquet multipliers".

<id>
nlin/0309024v1
<category>
nlin.CG
<abstract>
We present a computer-assisted approach to approximating coarse optimal
switching policies for systems described by microscopic/stochastic evolution
rules. The coarse timestepper constitutes a bridge between the underlying
kinetic Monte Carlo simulation and traditional, continuum numerical
optimization techniques formulated in discrete time. The approach is
illustrated through a simplified kinetic Monte Carlo simulation of NO reduction
on a Pt catalyst: a switch between two coexisting stable steady states is
implemented by minimal manipulation of a system parameter.

<id>
nlin/0310008v1
<category>
nlin.CG
<abstract>
We derive the Euler equations as the hydrodynamic limit of a stochastic model
of a hard-sphere gas on a lattice. We show that the system does not produce
entropy.

<id>
nlin/0312003v1
<category>
nlin.CG
<abstract>
Evidence and results suggesting that a Noether--like theorem for conservation
laws in 1D RCA can be obtained. Unlike Noether's theorem, the connection here
is to the maximal congruences rather than the automorphisms of the local
dynamics.
  We take the results of Takesue and Hattori (1992) on the space of additive
conservation laws in one dimensional cellular automata. In reversible automata,
we show that conservation laws correspond to the null spaces of certain
well-structured matrices.
  It is shown that a class of conservation laws exist that correspond to the
maximal congruences of index 2. In all examples investigated, this is all the
conservation laws. Thus we conjecture that there is an equality here,
corresponding to a Noether--like theorem.

<id>
nlin/0402016v2
<category>
nlin.CG
<abstract>
Cellular Automata (CA) are a class of discrete dynamical systems that have
been widely used to model complex systems in which the dynamics is specified at
local cell-scale. Classically, CA are run on a regular lattice and with perfect
synchronicity. However, these two assumptions have little chance to truthfully
represent what happens at the microscopic scale for physical, biological or
social systems. One may thus wonder whether CA do keep their behavior when
submitted to small perturbations of synchronicity.
  This work focuses on the study of one-dimensional (1D) asynchronous CA with
two states and nearest-neighbors. We define what we mean by ``the behavior of
CA is robust to asynchronism'' using a statistical approach with macroscopic
parameters. and we present an experimental protocol aimed at finding which are
the robust 1D elementary CA. To conclude, we examine how the results exposed
can be used as a guideline for the research of suitable models according to
robustness criteria.

<id>
nlin/0402043v1
<category>
nlin.CG
<abstract>
We study Markovian and non-Markovian behaviour of stochastic processes
generated by $p$-adic random dynamical systems. Given a family of $p$-adic
monomial random mappings generating a random dynamical system. Under which
conditions do the orbits under such a random dynamical system form Markov
chains? It is necessary that the mappings are Markov dependent. We show,
however, that this is in general not sufficient. In fact, in many cases we have
to require that the mappings are independent. Moreover we investigate some
geometric and algebraic properties for $p-$adic monomial mappings as well as
for the $p-$adic power function which are essential to the formation of
attractors. $p$-adic random dynamical systems can be useful in so called
$p$-adic quantum phytsics as well as in some cognitive models.

<id>
nlin/0404003v2
<category>
nlin.CG
<abstract>
A brief introduction to Wolfram's work on cellular automata.

<id>
nlin/0404018v3
<category>
nlin.CG
<abstract>
Within the class of stochastic cellular automata models of traffic flows, we
look at the velocity dependent randomization variant (VDR-TCA) whose parameters
take on a specific set of extreme values. These initial conditions lead us to
the discovery of the emergence of four distinct phases. Studying the
transitions between these phases, allows us to establish a rigorous
classification based on their tempo-spatial behavioral characteristics. As a
result from the system's complex dynamics, its flow-density relation exhibits a
non-concave region in which forward propagating density waves are encountered.
All four phases furthermore share the common property that moving vehicles can
never increase their speed once the system has settled into an equilibrium.

<id>
nlin/0405061v2
<category>
nlin.CG
<abstract>
An experimental analysis of the asynchronous version of the "Game of Life" is
performed to estimate how topology perturbations modify its evolution. We focus
on the study of a phase transition from an "inactive-sparse phase" to a
"labyrinth phase" and produce experimental data to quantify these changes as a
function of the density of the initial configuration, the value of the
synchrony rate, and the topology missing-link rate. An interpretation of the
experimental results is given using the hypothesis that initial "germs"
colonize the whole lattice and the validity of this hypothesis is tested.

<id>
nlin/9904201v1
<category>
nlin.CD
<abstract>
A new class of integro-partial differential equation models is derived for
the prediction of granular flow dynamics. These models are obtained using a
novel limiting averaging method (inspired by techniques employed in the
derivation of infinite-dimensional dynamical systems models) on the Newtonian
equations of motion of a many-particle system incorporating widely used
inelastic particle-particle force formulas. By using Taylor series expansions,
these models can be approximated by a system of partial differential equations
of the Navier-Stokes type. The exact or approximate governing equations
obtained are far from simple, but they are less complicated than most of the
continuum models now being used to predict particle flow behavior. Solutions of
the new models for granular flows down inclined planes and in vibrating beds
are compared with known experimental and analytical results and good agreement
is obtained.

<id>
nlin/9907202v1
<category>
nlin.CD
<abstract>
This paper aims to cast some new light on controlling chaos using the OGY-
and the Zero-Spectral-Radius methods. In deriving those methods we use a
generalized procedure differing from the usual ones. This procedure allows us
to conveniently treat maps to be controlled bringing the orbit to both various
saddles and to sources with both real and complex eigenvalues. We demonstrate
the procedure and the subsequent control on a variety of maps. We evaluate the
control by examining the basins of attraction of the relevant controlled
systems graphically and in some cases analytically.

<id>
nlin/0001002v1
<category>
nlin.CD
<abstract>
We present a method to derive an upper bound for the entropy density of
coupled map lattices with local interactions from local observations. To do
this, we use an embedding technique being a combination of time delay and
spatial embedding. This embedding allows us to identify the local character of
the equations of motion. Based on this method we present an approximate
estimate of the entropy density by the correlation integral.

<id>
nlin/0001011v1
<category>
nlin.CD
<abstract>
We compute the full Lyapunov spectra for a hard-disk fluid under temperature
gradient and shear. The system is thermalized by deterministic and
time-reversible scattering at the boundary. This thermostating mechanism allows
for energy fluctuations around a mean value which is reflected by only two
vanishing Lyapunov exponents in equilibrium and nonequilibrium. The Lyapunov
exponents are calculated with a recently developed formalism for systems with
elastic hard collisions. In a nonequilibrium steady state the average phase
space volume is contracted onto a fractal attractor leading to a negative sum
of Lyapunov exponents. Since the system is driven inhomogeneously we do not
expect the conjugate pairing rule to hold which is confirmed numerically.

<id>
nlin/0001013v2
<category>
nlin.CD
<abstract>
Resonances of the time evolution (Frobenius-Perron) operator P for phase
space densities have recently been shown to play a key role for the
interrelations of classical, semiclassical and quantum dynamics. Efficient
methods to determine resonances are thus in demand, in particular for
Hamiltonian systems displaying a mix of chaotic and regular behavior. We
present a powerful method based on truncating P to a finite matrix which not
only allows to identify resonances but also the associated phase space
structures. It is demonstrated to work well for a prototypical dynamical
system.

<id>
nlin/0001015v1
<category>
nlin.CD
<abstract>
We analyze statistical properties of the city bus transport in Cuernavaca
(Mexico) and show that the bus arrivals display probability distributions
conforming those given by the Unitary Ensemble of random matrices.

<id>
nlin/0001017v1
<category>
nlin.CD
<abstract>
In the symmetric and the asymmetric trapezoid maps, as a slope of the
trapezoid is increased, the period doubling cascade occurs and the symbolic
sequence of periodic points is the Metropolis-Stein-Stein sequence and the
convergence of the onset point of the period 2^m solution to the accumulation
point is exponentially fast. We reported these results previously. In this
paper, we give the detailed description of the proof on the results. Further,
we study the period doubling cascade starting from period p solution and show
the superconvergence of the period doubling cascade.

<id>
nlin/0001024v1
<category>
nlin.CD
<abstract>
Periodic orbit action correlations are studied for the piecewise linear,
area-preserving Baker map. Semiclassical periodic orbit formulae together with
universal spectral statistics in the corresponding quantum Baker map suggest
the existence of universal periodic orbit correlations. The calculation of
periodic orbit sums for the Baker map can be performed with the help of a
Perron-Frobenius type operator. This makes it possible to study periodic orbit
correlations for orbits with period up to 500 iterations of the map. Periodic
orbit correlations are found to agree quantitatively with the predictions from
random matrix theory up to a critical length determined by the semiclassical
error. Exponentially increasing terms dominate the correlations for longer
orbits which are due to the violation of unitarity in the semiclassical
approximation.

<id>
nlin/0001025v1
<category>
nlin.CD
<abstract>
Quantum graphs have recently been introduced as model systems to study the
spectral statistics of linear wave problems with chaotic classical limits. It
is proposed here to generalise this approach by considering arbitrary, directed
graphs with unitary transfer matrices. An exponentially increasing contribution
to the form factor is identified when performing a diagonal summation over
periodic orbit degeneracy classes. A special class of graphs, so-called binary
graphs, is studied in more detail. For these, the conditions for periodic orbit
pairs to be correlated (including correlations due to the unitarity of the
transfer matrix) can be given explicitly. Using combinatorial techniques it is
possible to perform the summation over correlated periodic orbit pair
contributions to the form factor for some low--dimensional cases. Gradual
convergence towards random matrix results is observed when increasing the
number of vertices of the binary graphs.

<id>
nlin/0001027v2
<category>
nlin.CD
<abstract>
Numerical and physical experiments on two-dimensional (2d) turbulence show
that the differences of transverse components of velocity field are well
described by a gaussian statistics and Kolmogorov scaling exponents. In this
case the dissipation fluctuations are irrelevant in the limit of small
viscosity. In general, one can assume existence of critical
space-dimensionality $d=d_{c}$, at which the energy flux and all odd-order
moments of velocity difference change sign and the dissipation fluctuations
become dynamically unimportant. At $d<d_{c}$ the flow can be described by the
``mean-field theory'', leading to the observed gaussian statistics and
Kolmogorov scaling of transverse velocity differences. It is shown that in the
vicinity of $d=d_{c}$ the ratio of the relaxation and translation
characteristic times decreases to zero, thus giving rise to a small parameter
of the theory. The expressions for pressure and dissipation contributions to
the exact equation for the generating function of transverse velocity
differences are derived in the vicinity of $d=d_{c}$. The resulting equation
describes experimental data on two-dimensional turbulence and demonstrate onset
of intermittency as $d-d_{c}>0$ and $r/L\to 0$ in three-dimensional flows in
close agreement with experimental data. In addition, some new exact relations
between correlation functions of velocity differences are derived. It is also
predicted that the single-point pdf of transverse velocity difference in
developing as well as in the large-scale stabilized two-dimensional turbulence
is a gaussian.

<id>
nlin/0001031v2
<category>
nlin.CD
<abstract>
We observe oscillatory decay in the two-point, non-equal time, velocity
correlation function of homogeneous, isotropic turbulence. We found this
through a direct numerical simulation (DNS) of the three dimensional
Navier-Stokes ($3-D$ NS) equation. We give an approximate analytic theory which
explains this oscillatory behaviour. The wave-number and frequency dependent
effective viscosity turns out to be complex; the imaginary part gives rise to
the temporal oscillation. We find that, at least for the decay at short times,
data collapse occur among the inertial range velocity wave-vector modes with
the long time dynamic exponent $z=2/3$, but the time period of the temporal
oscillation is not universal.

<id>
nlin/0001032v1
<category>
nlin.CD
<abstract>
Some pressure and pressure-velocity correlation in a direct numerical
simulations of a three-dimensional turbulent flow at moderate Reynolds numbers
have been analyzed. We have identified a set of pressure-velocity correlations
which posseses a good scaling behaviour. Such a class of pressure-velocity
correlations are determined by looking at the energy-balance across any
sub-volume of the flow. According to our analysis, pressure scaling is
determined by the dimensional assumption that pressure behaves as a ``velocity
squared'', unless finite-Reynolds effects are overwhelming. The SO(3)
decompositions of pressure structure functions has also been applied in order
to investigate anisotropic effects on the pressure scaling.

<id>
nlin/0001033v1
<category>
nlin.CD
<abstract>
We construct an approximate renormalization transformation for Hamiltonian
systems with three degrees of freedom in order to study the break-up of
invariant tori with three incommensurate frequencies which belong to the cubic
field $Q(\tau)$, where $\tau^3+\tau^2-2\tau-1=0$. This renormalization has two
fixed points~: a stable one and a hyperbolic one with a codimension one stable
manifold. We compute the associated critical exponents that characterize the
universality class for the break-up of the invariant tori we consider.

<id>
nlin/0001035v1
<category>
nlin.CD
<abstract>
We demonstrate that the Fokker-Planck equation can be generalized into a
'Fractional Fokker-Planck' equation, i.e. an equation which includes fractional
space differentiations, in order to encompass the wide class of anomalous
diffusions due to a Levy stable stochastic forcing. A precise determination of
this equation is obtained by substituting a Levy stable source to the classical
gaussian one in the Langevin equation. This yields not only the anomalous
diffusion coefficient, but a non trivial fractional operator which corresponds
to the possible asymmetry of the Levy stable source. Both of them cannot be
obtained by scaling arguments. The (mono-) scaling behaviors of the Fractional
Fokker-Planck equation and of its solutions are analysed and a generalization
of the Einstein relation for the anomalous diffusion coefficient is obtained.
This generalization yields a straightforward physical interpretation of the
parameters of Levy stable distributions. Furthermore, with the help of
important examples, we show the applicability of the Fractional Fokker-Planck
equation in physics.

<id>
nlin/0001037v4
<category>
nlin.CD
<abstract>
We introduce three area preserving maps with phase space structures which
resemble circle packings. Each mapping is derived from a kicked Hamiltonian
system with one of three different phase space geometries (planar, hyperbolic
or spherical) and exhibits an infinite number of coexisting stable periodic
orbits which appear to `pack' the phase space with circular resonances.

<id>
nlin/0001041v1
<category>
nlin.CD
<abstract>
We study the statistical properties of the scattering matrix associated with
generic quantum graphs. The scattering matrix is the quantum analogue of the
classical evolution operator on the graph. For the energy-averaged spectral
form factor of the scattering matrix we have recently derived an exact
combinatorial expression. It is based on a sum over families of periodic orbits
which so far could only be performed in special graphs. Here we present a
simple algorithm implementing this summation for any graph. Our results are in
excellent agreement with direct numerical simulations for various graphs.
Moreover we extend our previous notion of an ensemble of graphs by considering
ensemble averages over random boundary conditions imposed at the vertices. We
show numerically that the corresponding form factor follows the predictions of
random-matrix theory when the number of vertices is large---even when all bond
lengths are degenerate. The corresponding combinatorial sum has a structure
similar to the one obtained previously by performing an energy average under
the assumption of incommensurate bond lengths.

<id>
nlin/0001042v1
<category>
nlin.CD
<abstract>
An information theoretic measure is derived that quantifies the statistical
coherence between systems evolving in time. The standard time delayed mutual
information fails to distinguish information that is actually exchanged from
shared information due to common history and input signals. In our new
approach, these influences are excluded by appropriate conditioning of
transition probabilities. The resulting transfer entropy is able to distinguish
driving and responding elements and to detect asymmetry in the coupling of
subsystems.

<id>
nlin/0001044v1
<category>
nlin.CD
<abstract>
We present here a modification of the Lagrangian measures technique, which
allows a reliable detection of interdependency among simultaneous measurements
of different variables. This method is applied to a simulated multivariate time
series and to a bivariate cardiorespiratory signal. By using this methodology,
it is possible to reveal a nonlinear interaction among cardiac and respiration
rhythms in pathological conditions

<id>
nlin/0001058v1
<category>
nlin.CD
<abstract>
Analysis of the PPF chaos control method used in biological experiments shows
that it can robustly control a wider class of systems than previously believed,
including those without stable manifolds. This can be exploited to find the
locations of unstable periodic orbits by varying the parameters of the control
system.

<id>
nlin/0001062v2
<category>
nlin.CD
<abstract>
We investigate the connections between microscopic chaos, defined on a
dynamical level and arising from collisions between molecules, and diffusion,
characterized by a mean square displacement proportional to the time. We use a
number of models involving a single particle moving in two dimensions and
colliding with fixed scatterers. We find that a number of microscopically
nonchaotic models exhibit diffusion, and that the standard methods of chaotic
time series analysis are ill suited to the problem of distinguishing between
chaotic and nonchaotic microscopic dynamics. However, we show that periodic
orbits play an important role in our models, in that their different properties
in chaotic and nonchaotic systems can be used to distinguish such systems at
the level of time series analysis, and in systems with absorbing boundaries.
Our findings are relevant to experiments aimed at verifying the existence of
chaoticity and related dynamical properties on a microscopic level in diffusive
systems.

<id>
nlin/0001063v1
<category>
nlin.CD
<abstract>
We give a brief overview of systems that show spiral patterns and
spatiotemporally chaotic states. We concentrate on two physical systems: (1)
the oxidation of CO on Pt(110) and (2) ventricular fibrillation in hearts. The
equations that have been suggested as simple models for these two different
systems are closely related for they are both {\it excitable media}. We present
these equations and give a short summary of the phenomena they yield.}

<id>
nlin/0001067v1
<category>
nlin.CD
<abstract>
The problem of anomalous scaling in passive scalar advection, especially with
$\delta$-correlated velocity field (the Kraichnan model) has attracted a lot of
interest since the exponents can be computed analytically in certain limiting
cases. In this paper we focus, rather than on the evaluation of the exponents,
on elucidating the {\em physical mechanism} responsible for the anomaly. We
show that the anomalous exponents $\zeta_n$ stem from the Lagrangian dynamics
of shapes which characterize configurations of n points in space. Using the
shape-to-shape transition probability, we define an operator whose eigenvalues
determine the anomalous exponents for all n, in all the sectors of the SO(3)
symmetry group.

<id>
nlin/0001070v1
<category>
nlin.CD
<abstract>
We present a graphical analysis of the mechanisms underlying the occurrences
of bubbling sequences and bistability regions in the bifurcation scenario of a
special class of one dimensional two parameter maps. The main result of the
analysis is that whether it is bubbling or bistability is decided by the sign
of the third derivative at the inflection point of the map function.

<id>
nlin/0002006v1
<category>
nlin.CD
<abstract>
In the inertial range of fully developed turbulence, we model the vertex
network dynamics by an iterated unimodular map having the universal behavior.
Inertial range anomalous scaling for the pair correlation functions of the
velocity and the local energy dissipation is established as a consequence of
the chaotic behavior of the unimodular map when the Feigenbaum attractor looses
stability. The anomalous exponents determined by the Feigenbaum constant $\eta$
to the Kolmogorov's spectra are larger than those observed in experiments.

<id>
nlin/0002014v1
<category>
nlin.CD
<abstract>
We consider the dynamics of Rydberg states of the hydrogen atom driven by a
microwave field of elliptical polarization, with a possible additional static
electric field. We concentrate on the effect of a resonant weak field - whose
frequency is close to the Kepler frequency of the electron around the nucleus -
which essentially produces no ionization of the atom, but completely mixes the
various states inside an hydrogenic manifold of fixed principal quantum number.
For sufficiently small fields, a perturbative approach (both in classical and
quantum mechanics) is relevant. For some configurations of the fields, the
classical secular motion (i.e. evolution in time of the elliptical electronic
trajectory) is shown to be predominantly chaotic. Changing the orientation of
the static field with respect to the polarization of the microwave field allows
us to investigate the effect of generalized time-reversal symmetry breaking on
the statistical properties of energy levels.

<id>
nlin/0002019v1
<category>
nlin.CD
<abstract>
We study the dynamics of a lattice of coupled nonidentical Fitz Hugh-Nagumo
system subject to independent external noise. It is shown that these stochastic
oscillators can lead to global synchronization behavior {\sl without an
external signal}. With the increase of the noise intensity, the system exhibits
coherence resonance behavior. Coupling can enhance greatly the noise-induced
coherence in the system.

<id>
nlin/0002021v1
<category>
nlin.CD
<abstract>
We present a conformal theory for intermittent scalar fields. As an example,
we consider the energy flux from large to small scales in the developed
turbulent flow. The conformal correlation functions are found in the inertial
range of scales. In the simplest case, the theory leads to the log-normal
model. Parameters of the model are expressed via integrals of the conformal
correlation functions. Non-Gaussian conformal correlation functions of high
order are studied.

<id>
nlin/0002024v1
<category>
nlin.CD
<abstract>
We introduce a method to estimate the initial conditions of a mutivariable
dynamical system from a scalar signal. The method is based on a modified
multidimensional Newton-Raphson method which includes the time evolution of the
system. The method can estimate initial conditions of periodic and chaotic
systems and the required length of scalar signal is very small. Also, the
method works even when the conditional Lyapunov exponent is positive. An
important application of our method is that synchronization of two chaotic
signals using a scalar signal becomes trivial and instantaneous.

<id>
nlin/0002025v1
<category>
nlin.CD
<abstract>
We consider a dynamic method, based on synchronization and adaptive control,
to estimate unknown parameters of a nonlinear dynamical system from a given
scalar chaotic time series. We present an important extension of the method
when time series of a scalar function of the variables of the underlying
dynamical system is given. We find that it is possible to obtain
synchronization as well as parameter estimation using such a time series. We
then consider a general quadratic flow in three dimensions and discuss
applicability of our method of parameter estimation in this case. In practical
situations one expects only a finite time series of a system variable to be
known. We show that the finite time series can be repeatedly used to estimate
unknown parameters with an accuracy which improves and then saturates to a
constant value with repeated use of the time series. Finally we propose that
the method can be used to confirm the correctness of a trial function modeling
an external unknown perturbation to a known system. We show that our method
produces exact synchronization with the given time series only when the trial
function has a form identical to that of the perturbation.

<id>
nlin/0002026v2
<category>
nlin.CD
<abstract>
A method allowing to distinguish interacting from non-interacting systems
based on available time series is proposed and investigated. Some facts
concerning generalized Renyi dimensions that form the basis of our method are
proved. We show that one can find the dimension of the part of the attractor of
the system connected with interaction between its parts. We use our method to
distinguish interacting from non-interacting systems on the examples of
logistic and H\'enon maps. A classification of all possible interaction schemes
is given.

<id>
nlin/9804201v1
<category>
nlin.SI
<abstract>
Under the Neumann constraints, each equation of the KdV hierarchy is
decomposed into two finite dimensional systems, including the well-known
Neumann model. Like in the case of the Bargmann constraint, the explicit Lax
representations are deduced from the adjoint representation of the auxiliary
spectral problem. It is shown that the Lax operator satisfies the r-matrix
relation in the Dirac bracket. Thus, the integrabilities of these resulting
systems with the Neumann constraints are obtained.

<id>
nlin/9804202v1
<category>
nlin.SI
<abstract>
Considering the coupled envelope equations in nonlinear couplers, the
question of integrability is attempted. It is explicitly shown that Hirota's
bilinear method is one of the simple and alternative techniques to Painlev\'e
analysis to obtain the integrability conditions of the coupled nonlinear
Schr\"odinger (CNLS) type equations. We also show that the coupled Hirota
equation introduced by Tasgal and Potasek is the next hierarchy of the inverse
scattering solvable CNLS equation. The results are in agreement with the known
results.

<id>
nlin/9901201v1
<category>
nlin.SI
<abstract>
An eigenvalue problem with a reference function and the corresponding
hierarchy of nonlinear evolution equations are proposed. The bi-Hamiltonian
structure of the hierarchy is established by using the trace identity. The
isospectral problem is nonlinearized as to be finite-dimensional completely
integrable systems in Liouville sense under Neumann and Bargmann constraints.

<id>
nlin/9907201v1
<category>
nlin.SI
<abstract>
In this paper we discuss a universal integrable model, given by a sum of two
Wess-Zumino-Witten-Novikov (WZWN) actions, corresponding to two different
orbits of the coadjoint action of a loop group on its dual, and the
Polyakov-Weigmann cocycle describing their interactions. This is an effective
action for free fermions on a torus with nontrivial boundary conditions. It is
universal in the sense that all other known integrable models can be derived as
reductions of this model. Hence our motivation is to present an unified
description of different integrable models. We present a proof of this
universal action from the action of the trivial dynamical system on the
cotangent bundles of the loop group. We also present some examples of
reductions.

<id>
nlin/0001001v4
<category>
nlin.SI
<abstract>
We present the fermionic representation for the q-deformed hypergeometric
functions related to Schur polynomials considered by S.Milne \cite{Milne}. For
$q=1$ these functions are also known as hypergeometric functions of matrix
argument which are related to zonal spherical polynomials for $GL(N,C)/U(N)$
symmetric space. We show that these multivariable hypergeometric functions are
tau-functions of the KP hierarchy. At the same time they are the ratios of Toda
lattice tau-functions considered by Takasaki in \cite{Tinit}, \cite{T}
evaluated at certain values of higher Toda lattice times. The variables of the
hypergeometric functions are related to the higher times of those hierarchies
via Miwa change of variables. The discrete Toda lattice variable shifts
parameters of hypergeometric functions. Hypergeometric functions of type
${}_pF_s$ can be also viewed as group 2-cocycle for the $\Psi$DO on the circle
of the order $p-s \leq 1$ (the group times are higher times of TL hierarchy and
the arguments of hypergeometric function). We get the determinant
representation and the integral representation of special type of KP
tau-functions, these results generalize some of Milne's results in
\cite{Milne}. We write down a system of linear differential and difference
equations for these tau-functions (string equations). We present also fermionic
representation for special type of Gelfand-Graev hypergeometric functions.

<id>
nlin/0001006v1
<category>
nlin.SI
<abstract>
We report for the first time exact solutions of a completely integrable
nonlinear lattice system for which the dynamical variables satisfy a q-deformed
Lie algebra - the Lie-Poisson algebra su_q(2). The system considered is a
q-deformed lattice for which in continuum limit the equations of motion become
the envelope Maxwell-Bloch (or SIT) equations describing the resonant
interaction of light with a nonlinear dielectric. Thus the N-soliton solutions
we here report are the natural q-deformations, necessary for a lattice, of the
well-known multi-soliton and breather solutions of self-induced transparency
(SIT). The method we use to find these solutions is a generalization of the
Darboux-Backlund dressing method. The extension of these results to quantum
solitons is sketched.

<id>
nlin/0001010v1
<category>
nlin.SI
<abstract>
Degrees of freedom for high-order binary constrained flows of soliton
equations admitting $2\times 2$ Lax matrices are $2N+k_0$. It is known that
$N+k_0$ pairs of canonical separated variables for their separation of
variables can be introduced directly via their Lax matrices. In present paper
we propose a new method to introduce the additional $N$ pairs of canonical
separated variables and $N$ additional separated equations. The Jacobi
inversion problems for high-order binary constrained flows and for soliton
equations are also established. This new method can be applied to all
high-order binary constrained flows admitting $2\times 2$ Lax matrices.

<id>
nlin/0001012v1
<category>
nlin.SI
<abstract>
We propose a nonlinear $\sigma$-model in a curved space as a general
integrable elliptic model. We construct its exact solutions and obtain energy
estimates near the critical point. We consider the Pohlmeyer transformation in
Euclidean space and investigate the gauge equivalence conditions for a broad
class of elliptic equations. We develop the inverse scattering transform method
for the $\sinh$-Gordon equation and evaluate its exact and asymptotic
solutions.

<id>
nlin/0001022v1
<category>
nlin.SI
<abstract>
In the Painleve analysis of nonintegrable partial differential equations one
obtains differential constraints describing the movable singularity manifold.
We show, for a class of n-dimensional wave equations, that these constraints
have a general structure which is related to the $n$-dimensional Bateman
equation. In particular, we derive the exact expressions of the singularity
manifold constraints for the n-dimensional sine-Gordon -, Liouville -,
Mikhailov -, and double sine-Gordon equation, as well as two 2-dimensional
polynomial field theory equations, and prove that their singularity manifold
conditions are satisfied by the n-dimensional Bateman equation. Finally we give
some examples.

<id>
nlin/0001028v1
<category>
nlin.SI
<abstract>
The general method of the cojmplex supersymmetrization
(supercomplexifications) of the soliton equations with the odd (bi)
hamiltoninan structure is established. New version of the supercomplexified
Kadomtsev-Petvishvili hierarchy is given. The second odd Hamiltonina operator
of the SUSY KdV equation generates the odd N=2 SUSY Virasoro like algebra.

<id>
nlin/0001036v1
<category>
nlin.SI
<abstract>
We investigate the bi-Hamiltonian structures associated with constrained
dispersionless modified KP hierarchies which are constructed from truncations
of the Lax operator of the dispersionless modified KP hierarchy. After
transforming their second Hamiltonian structures to those of Gelfand-Dickey
type, we obtain the Poisson algebras of the coefficient functions of the
truncated Lax operators. Then we study the conformal property and free-field
realizations of these Poisson algebras. Some examples are worked out explicitly
to illustrate the obtained results.

<id>
nlin/0001040v2
<category>
nlin.SI
<abstract>
Employing the graded versions of the Yang-Baxter equation and the reflection
equations, we construct two kinds of integrable impurities for a small-polaron
model with general open boundary conditions: (a) we shift the spectral
parameter of the local Lax operator at arbitrary sites in the bulk, and (b) we
embed the impurity fermion vertex at each boundary of the chain. The
Hamiltonians with different types of impurity terms are given explicitly. The
Bethe ansatz equations, as well as the eigenvalues of the Hamiltonians, are
constructed by means of the quantum inverse scattering method. In addition, we
discuss the ground-state properties in the thermodynamic limit.

<id>
nlin/0001048v1
<category>
nlin.SI
<abstract>
We consider in detail the self-trapping of a soliton from a wave pulse that
passes from a defocussing region into a focussing one in a spatially
inhomogeneous nonlinear waveguide, described by a nonlinear Schrodinger
equation in which the dispersion coefficient changes its sign from normal to
anomalous. The model has direct applications to dispersion-decreasing nonlinear
optical fibers, and to natural waveguides for internal waves in the ocean. It
is found that, depending on the (conserved) energy and (nonconserved) mass of
the initial pulse, four qualitatively different outcomes of the pulse
transformation are possible: decay into radiation; self-trapping into a single
soliton; formation of a breather; and formation of a pair of counterpropagating
solitons. A corresponding chart is drawn on a parametric plane, which
demonstrates some unexpected features. In particular, it is found that any kind
of soliton(s) (including the breather and counterpropagating pair) eventually
decays into pure radiation with the increase of the energy, the initial mass
being kept constant. It is also noteworthy that a virtually direct transition
from a single soliton into a pair of symmetric counterpropagating ones seems
possible. An explanation for these features is proposed. In two cases when
analytical approximations apply, viz., a simple perturbation theory for broad
initial pulses, or the variational approximation for narrow ones, comparison
with the direct simulations shows reasonable agreement.

<id>
nlin/0001052v1
<category>
nlin.SI
<abstract>
We generalize to the supersymmetric case the representation of the KP
hierarchy as a set of conservation laws for the generating series of the
conserved densities. We show that the hierarchy so obtained is isomorphic to
the JSKP of Mulase and Rabin. We identify its ``bosonic content'' with the
so-called Darboux-KP hierarchy, which geometrically encompasses the theory of
Darboux-B\"acklund transformations, and is an extension both of the KP theory
and of the modified KP theory. Finally, we show how the hierarchy can be
linearized and how the supersymmetric counterpart of a wide class of rational
solution can be quite explicitly worked out.

<id>
nlin/0001053v1
<category>
nlin.SI
<abstract>
Cubic invariants for two-dimensional degenerate Hamiltonian systems are
considered by using variables of separation of the associated St\"ackel
problems with quadratic integrals of motion. For the superintegrable St\"ackel
systems the cubic invariant is shown to admit new algebro-geometric
representation that is far more elementary than the all the known
representations in physical variables. A complete list of all known systems on
the plane which admit a cubic invariant is discussed.

<id>
nlin/0001066v2
<category>
nlin.SI
<abstract>
The Volterra and Toda chains equations are considered. A class of special
reductions for these equations are derived.

<id>
nlin/0001071v1
<category>
nlin.SI
<abstract>
A simple and general approach for calculating the elliptic finite-gap
solutions of the Korteweg-de Vries (KdV) equation is proposed. Our approach is
based on the use of the finite-gap equations and the general representation of
these solutions in the form of rational functions of the elliptic Weierstrass
function. The calculation of initial elliptic finite-gap solutions is reduced
to the solution of the finite-band equations with respect to the parameters of
the representation. The time evolution of these solutions is described via the
dynamic equations of their poles, integrated with the help of the finite-gap
equations. The proposed approach is applied by calculating the elliptic 1-, 2-
and 3-gap solutions of the KdV equations.

<id>
nlin/0001072v1
<category>
nlin.SI
<abstract>
The B\"acklund transformations for the relativistic lattices of the Toda type
and their discrete analogues can be obtained as the composition of two duality
transformations. The condition of invariance under this composition allows to
distinguish effectively the integrable cases. Iterations of the B\"acklund
transformations can be described in the terms of nonrelativistic lattices of
the Toda type. Several multifield generalizations are presented.

<id>
nlin/0002005v1
<category>
nlin.SI
<abstract>
The finite-genus solutions for the Hirota's bilinear difference equation are
constructed using the Fay's identities for the theta-functions of compact
Riemann surfaces.

<id>
nlin/0002008v1
<category>
nlin.SI
<abstract>
We discuss the bihamiltonian geometry of the Toda lattice (periodic and
open). Using some recent results on the separation of variables for
bihamiltonian manifolds, we show that these systems can be explicitly
integrated via the classical Hamilton-Jacobi method in the so-called
Darboux-Nijenhuis coordinates.

<id>
nlin/0002009v1
<category>
nlin.SI
<abstract>
The aim of these lectures is to show that the methods of classical
Hamiltonian mechanics can be profitably used to solve certain classes of
nonlinear partial differential equations. The prototype of these equations is
the well-known Korteweg-de Vries (KdV) equation. In these lectures we touch the
following subjects: i) The birth and the role of the method of Poisson pairs
inside the theory of the KdV equation; ii) the theoretical basis of the method
of Poisson pairs; iii) the Gel'fand-Zakharevich theory of integrable systems on
bihamiltonian manifolds; iv) the Hamiltonian interpretation of the Sato picture
of the KdV flows and of its linearization on an infinite-dimensional
Grassmannian manifold. v)the reduction technique and its use to construct
classes of solutions; iv) the role of the technique of separation of variables
in the study of the reduced systems. vii) some relations intertwining the
method of Poisson pairs with the method of Lax pairs.

<id>
nlin/0002020v1
<category>
nlin.SI
<abstract>
It is shown, that any sufficiently smooth periodic solution of the
self-focusing Nonlinear Schr\"odinger equation can be approximated by periodic
finite-gap ones with an arbitrary small error. As a corollary an analogous
result for the motion of closed curves in ${\Bbb R}^3$ guided by the Filament
equation is proved. This equation describes the dynamics of very thin filament
vortices in a fluid.

<id>
nlin/0002036v1
<category>
nlin.SI
<abstract>
Conformal symmetry underlies the mathematical description of various
two-dimensional integrable models (e.g. for their Lax representation, Poisson
algebra, zero curvature representation,...) or of conformal models (for the
anomalous Ward identities, operator product expansion, Krichever-Novikov
algebra,...) and of W-algebras. Here, we review the construction of conformally
covariant differential operators which allow to render the conformal covariance
manifest. The N=1 and N=2 supersymmetric generalizations of these results are
also indicated and it is shown that they involve nonstandard matrix formats of
Lie superalgebras.

<id>
nlin/0002037v1
<category>
nlin.SI
<abstract>
We analyze the dispersionless limits of the SUSY TB-B (sTB-B) and the SUSY TB
(sTB) hierarchies. We present the Lax description for each of these models, as
well as the N=2 sTB hierarchy and bring out various properties associated with
them. We also discuss open questions that need to be addressed in connection
with these models.

<id>
nlin/0002047v1
<category>
nlin.SI
<abstract>
The soliton equations can be factorized by two commuting x- and t-constrained
flows. We propose a method to derive N-soliton solutions of soliton equations
directly from the x- and t-constrained flows.

<id>
nlin/0002048v1
<category>
nlin.SI
<abstract>
A discrete version of the inverse scattering method proposed by Ablowitz and
Ladik is generalized to study an integrable full-discretization (discrete time
and discrete space) of the coupled nonlinear Schr\"{o}dinger equations. The
generalization enables one to solve the initial-value problem. Soliton
solutions and conserved quantities of the full-discrete system are constructed.

<id>
nlin/0002049v1
<category>
nlin.SI
<abstract>
Using the dbar-problem and dual dbar-problem, we derive bilinear relations
which allows us to construct integrable hierarchies in different
parametrizations, their Darboux-B\"{a}cklund transformations and to analyze
constraints for them ina very simple way. Scalar KP, BKP and CKP hierarchies
are considered as examples.

<id>
nlin/0002050v1
<category>
nlin.SI
<abstract>
We investigate various aspects of the integrability of the vertex models
associated to the $D_n^2$ affine Lie algebra with open boundaries. We first
study the solutions of the corresponding reflection equation compatible with
the minimal symmetry of this system. We find three classes of general
solutions, one diagonal solution and two non-diagonal families with a free
parameter. Next we perform the Bethe ansatz analysis for some of the associated
open $D_2^2$ spin chains and we identify the boundary having quantum group
invariance. We also discuss a new $D_2^2$ $R$-matrix.

<id>
nlin/0003002v1
<category>
nlin.SI
<abstract>
We suggest the procedure of the construction of Baxter Q-operators for Toda
chain . Apart from the one-paramitric family of Q-operators, considered in our
recent paper (hep-th/9908179) we also give the construction of two basic
Q-operators and the derivation of the functional relations for these operators.
Also we have found the relation of the basic Q-operators with Bloch solutions
of the quantum linear problem.

<id>
nlin/0003020v1
<category>
nlin.SI
<abstract>
We present a fairly new and comprehensive approach to the study of stationary
flows of the Korteweg-de Vries hierarchy. They are obtained by means of a
double restriction process from a dynamical system in an infinite number of
variables. This process naturally provides us with a Lax representation of the
flows, which is used to find their bi-Hamiltonian formulation. Then we prove
the separability of these flows making use of their bi-Hamiltonian structure,
and we show that the variables of separation are supplied by the Poisson pair.

<id>
nlin/0001014v2
<category>
nlin.PS
<abstract>
Possibility of asymmetric square convection is investigated numerically using
a few mode Lorenz-like model for thermal convection in Boussinesq fluids
confined between two stress free and conducting flat boundaries. For relatively
large value of Rayleigh number, the stationary rolls become unstable and
asymmetric squares appear as standing waves at the onset of secondary
instability. Asymmetric squares, two dimensional rolls and again asymmetric
squares with their corners shifted by half a wavelength form a stable limit
cycle.

<id>
nlin/0001043v1
<category>
nlin.PS
<abstract>
Stability of cylindrical and spherical crystals growing from a supersaturated
solution (in Mullins-Sekerka's approximation) is considered using the maximum
entropy production principle. The concept of the binodal of the nonequilibrium
(morphological) phase transition is introduced for interpretation of the
obtained results. The limits of the metastable regions are determined. The
morphological phase diagrams of stable-unstable growth in the plane (surface
energy, supersaturation) are given.

<id>
nlin/0001046v2
<category>
nlin.PS
<abstract>
We discuss several novel types of multi-component (temporal and spatial)
envelope solitary waves that appear in fiber and waveguide nonlinear optics. In
particular, we describe multi-channel solitary waves in bit-parallel-wavelength
fiber transmission systems for high performance computer networks, multi-colour
parametric spatial solitary waves due to cascaded nonlinearities of quadratic
materials, and quasiperiodic envelope solitons due to quasi-phase-matching in
Fibonacci optical superlattices.

<id>
nlin/0001049v1
<category>
nlin.PS
<abstract>
The Busse-Heikes dynamical model is described in terms of relaxational and
nonrelaxational dynamics. Within this dynamical picture a diverging alternating
period is calculated in a reduced dynamics given by a time-dependent
Hamiltonian with decreasing energy. A mean period is calculated which results
from noise stabilization of a mean energy. The consideration of
spatial-dependent amplitudes leads to vertex formation. The competition of
front motion around the vertices and the Kuppers-Lortz instability in
determining an alternating period is discussed.

<id>
nlin/0001055v1
<category>
nlin.PS
<abstract>
We report experiments on spatial switching dynamics and steady state
structures of passive nonlinear semiconductor resonators of large Fresnel
number. Extended patterns and switching front dynamics are observed and
investigated. Evidence of localization of structures is given.

<id>
nlin/0001056v1
<category>
nlin.PS
<abstract>
We show experimentally the existence of bright and dark spatial solitons in a
passive quantum-well-semiconductor resonator of large Fresnel number. For the
wavelength of observation the nonlinearity is mixed absorptive/defocusing.
Bright solitons appear more stable than dark ones.

<id>
nlin/0001059v1
<category>
nlin.PS
<abstract>
We report the observation of Raman solitons on numerical simulations of
transient stimulated Raman scattering (TSRS) with small group velocity
dispersion. The theory proceeds with the inverse scattering transform (IST) for
initial-boundary value problems and it is shown that the explicit theoretical
solution obtained by IST for a semi-infinite medium fits strikingly well the
numerical solution for a finite medium. We understand this from the rapid
decrease of the medium dynamical variable (the potential of the scattering
theory). The spectral transform reflection coefficient can be computed directly
from the values of the input and output fields and this allows to see the
generation of the Raman solitons from the numerical solution. We confirm the
presence of these nonlinear modes in the medium dynamical variable by the use
of a discrete spectral analysis.

<id>
nlin/0001065v2
<category>
nlin.PS
<abstract>
Patterns forming spontaneously in extended, three-dimensional, dissipative
systems are likely to excite several homogeneous soft modes ($\approx$
hydrodynamic modes) of the underlying physical system, much more than quasi
one- and two-dimensional patterns are. The reason is the lack of damping
boundaries. This paper compares two analytic techniques to derive the patten
dynamics from hydrodynamics, which are usually equivalent but lead to different
results when applied to multiple homogeneous soft modes. Dielectric
electroconvection in nematic liquid crystals is introduced as a model for
three-dimensional pattern formation. The 3D pattern dynamics including soft
modes are derived. For slabs of large but finite thickness the description is
reduced further to a two-dimensional one. It is argued that the range of
validity of 2D descriptions is limited to a very small region above threshold.
The transition from 2D to 3D pattern dynamics is discussed. Experimentally
testable predictions for the stable range of ideal patterns and the electric
Nusselt numbers are made. For most results analytic approximations in terms of
material parameters are given.

<id>
nlin/0001069v1
<category>
nlin.PS
<abstract>
A new nonlinear equation governing asymptotic dynamics of ripples is derived
by using a short wave perturbative expansion on a generalized version of the
Green-Naghdi system. It admits peakon solutions with amplitude, velocity and
width in interrelation and static compacton solutions with amplitude and width
in interrelation. Short wave pattern formation is shown to result from a
balance between linear dispersion and nonlinearity.

<id>
nlin/0002001v1
<category>
nlin.PS
<abstract>
We present a simple laboratory experiment to illustrate some aspects of the
soliton theory in discrete lattices with a system that models the dynamics of
dislocations in a crystal or the properties of adsorbed atomic layers. The
apparatus not only shows the role of the Peierls-Nabarro potential but also
illustrates the hierarchy of depinning transitions and the importance of the
collective motion in mass transport.

<id>
nlin/0002012v1
<category>
nlin.PS
<abstract>
An explicit perturbative solution to all orders is given for a general class
of nonlinear differential equations. This solution is written as a sum indexed
by rooted trees and uses the Green function of a linearization of the
equations. The modifications due to the presence of zero-modes is considered.
Possible divergence of the integrals can be avoided by using approximate Green
functions.

<id>
nlin/0002038v1
<category>
nlin.PS
<abstract>
Breaking the chiral symmetry, rotation induces a secondary Hopf bifurcation
in weakly nonlinear hexagon patterns which gives rise to oscillating hexagons.
We study the stability of the oscillating hexagons using three coupled
Ginzburg-Landau equations. Close to the bifurcation point we derive reduced
equations for the amplitude of the oscillation, coupled to the phase of the
underlying hexagons. Within these equation we identify two types of long-wave
instabilities and study the ensuing dynamics using numerical simulations of the
three coupled Ginzburg-Landau equations.

<id>
nlin/0002039v1
<category>
nlin.PS
<abstract>
We consider the Allen-Cahn equations with memory (a partial
integro-differential convolution equation). The prototype kernels are
exponentially decreasing functions of time and they reduce the
integrodifferential equation to a hyperbolic one, the damped Klein-Gordon
equation. By means of a formal asymptotic analysis we show that to the leading
order and under suitable assumptions on the kernels, the integro-differential
equation behave like a hyperbolic partial differential equation obtained by
considering prototype kernels: the evolution of fronts is governed by the
extended, damped Born-Infeld equation. We also apply our method to a system of
partial integro-differential equations which generalize the classical phase
field equations with a non-conserved order parameter and describe the process
of phase transitions where memory effects are present.

<id>
nlin/0002041v2
<category>
nlin.PS
<abstract>
Recent experiments (Kudrolli, Pier and Gollub, 1998) on two-frequency
parametrically excited surface waves exhibit an intriguing "superlattice" wave
pattern near a codimension-two bifurcation point where both subharmonic and
harmonic waves onset simultaneously, but with different spatial wavenumbers.
The superlattice pattern is synchronous with the forcing, spatially periodic on
a large hexagonal lattice, and exhibits small-scale triangular structure.
Similar patterns have been shown to exist as primary solution branches of a
generic 12-dimensional $D_6\dot{+}T^2$-equivariant bifurcation problem, and may
be stable if the nonlinear coefficients of the bifurcation problem satisfy
certain inequalities (Silber and Proctor, 1998). Here we use the spatial and
temporal symmetries of the problem to argue that weakly damped harmonic waves
may be critical to understanding the stabilization of this pattern in the
Faraday system. We illustrate this mechanism by considering the equations
developed by Zhang and Vinals (1997, J. Fluid Mech. 336) for small amplitude,
weakly damped surface waves on a semi-infinite fluid layer. We compute the
relevant nonlinear coefficients in the bifurcation equations describing the
onset of patterns for excitation frequency ratios of 2/3 and 6/7. For the 2/3
case, we show that there is a fundamental difference in the pattern selection
problems for subharmonic and harmonic instabilities near the codimension-two
point. Also, we find that the 6/7 case is significantly different from the 2/3
case due to the presence of additional weakly damped harmonic modes. These
additional harmonic modes can result in a stabilization of the superpatterns.

<id>
nlin/0003004v1
<category>
nlin.PS
<abstract>
We demonstrate experimentally the bistable nature of the bright spatial
solitons in a semiconductor microresonator and show that they can be created
and destroyed by incoherent local optical injection.

<id>
nlin/0003006v1
<category>
nlin.PS
<abstract>
In this paper we consider two models of soliton dynamics (the sine Gordon and
the \phi^4 equations) on a 1-dimensional lattice. We are interested in
particular in the behavior of their kink-like solutions inside the Peierls-
Nabarro barrier and its variation as a function of the discreteness parameter.
We find explicitly the asymptotic states of the system for any value of the
discreteness parameter and the rates of decay of the initial data to these
asymptotic states. We show that genuinely periodic solutions are possible and
we identify the regimes of the discreteness parameter for which they are
expected to persist. We also prove that quasiperiodic solutions cannot exist.
Our results are verified by numerical simulations.

<id>
nlin/0003011v1
<category>
nlin.PS
<abstract>
In this work we report the first realization of odd dark beams of finite
length under controllable initial conditions. The mixed edge-screw phase
dislocations are obtained by reproducing binary computer-generated holograms.
Two effective ways to control the steering of the beams are analyzed
experimentally and compared with numerical simulations.

<id>
nlin/0003015v1
<category>
nlin.PS
<abstract>
We describe experiments testing the existence and investigating the
properties of spatial solitons in nonlinear resonators. We investigate the
properties of stationary and moving spatial solitons in lasers with saturable
absorber, with a subcritical bifurcation, as well as their manipulation. As
opposed, spatial solitons relying on a supercritical bifurcation are shown to
exist in degenerate 4-wave mixing (DOPO). With a view to technical applications
in parallel information processing or communication, experiments on spatial
solitons in large area quantum well semiconductor resonators are conducted.

<id>
nlin/0003021v1
<category>
nlin.PS
<abstract>
We propose an indirect approach to the generation of a two-dimensional
quasiperiodic (QP) pattern in convection and similar nonlinear dissipative
systems where a direct generation of stable uniform QP planforms is not
possible. An {\it eightfold} QP pattern can be created as a broad transient
layer between two domains filled by square cells (SC) oriented under the angle
of 45 degrees relative to each other. A simplest particular type of the
transient layer is considered in detail. The structure of the pattern is
described in terms of a system of coupled real Ginzburg-Landau (GL) equations,
which are solved by means of combined numerical and analytical methods. It is
found that the transient ``quasicrystallic'' pattern exists exactly in a
parametric region in which the uniform SC pattern is stable. In fact, the
transient layer consists of two different sublayers, with a narrow additional
one between them. The width of one sublayer (which locally looks like the
eightfold QP pattern) is large, while the other sublayer (that seems like a
pattern having a quasiperiodicity only in one spatial direction) has a width
$\sim 1$. Similarly, a broad stripe of a {\it % twelvefold} QP pattern can be
generated as a transient region between two domains of hexagonal cells oriented
at the angle of 30 degrees.

<id>
nlin/0003047v1
<category>
nlin.PS
<abstract>
We investigate pattern formation in self-oscillating systems forced by an
external periodic perturbation. Experimental observations and numerical studies
of reaction-diffusion systems and an analysis of an amplitude equation are
presented. The oscillations in each of these systems entrain to rational
multiples of the perturbation frequency for certain values of the forcing
frequency and amplitude. We focus on the subharmonic resonant case where the
system locks at one fourth the driving frequency, and four-phase rotating
spiral patterns are observed at low forcing amplitudes. The spiral patterns are
studied using an amplitude equation for periodically forced oscillating
systems. The analysis predicts a bifurcation (with increasing forcing) from
rotating four-phase spirals to standing two-phase patterns. This bifurcation is
also found in periodically forced reaction-diffusion equations, the
FitzHugh-Nagumo and Brusselator models, even far from the onset of oscillations
where the amplitude equation analysis is not strictly valid. In a
Belousov-Zhabotinsky chemical system periodically forced with light we also
observe four-phase rotating spiral wave patterns. However, we have not observed
the transition to standing two-phase patterns, possibly because with increasing
light intensity the reaction kinetics become excitable rather than oscillatory.

<id>
nlin/0003056v2
<category>
nlin.PS
<abstract>
The slow dynamics of nearly stationary patterns in a FitzHugh-Nagumo model
are studied using a phase dynamics approach. A Cross-Newell phase equation
describing slow and weak modulations of periodic stationary solutions is
derived. The derivation applies to the bistable, excitable, and the Turing
unstable regimes. In the bistable case stability thresholds are obtained for
the Eckhaus and the zigzag instabilities and for the transition to traveling
waves. Neutral stability curves demonstrate the destabilization of stationary
planar patterns at low wavenumbers to zigzag and traveling modes. Numerical
solutions of the model system support the theoretical findings.

<id>
nlin/0004027v1
<category>
nlin.PS
<abstract>
We consider the Taylor-Couette problem in an infinitely extended cylindrical
domain. There exist modulated front solutions which describe the spreading of
the stable Taylor vortices into the region of the unstable Couette flow. These
transient solutions have the form of a front-like envelope advancing in the
laboratory frame and leaving behind the stationary, spatially periodic Taylor
vortices. We prove the nonlinear stability of these solutions with respect to
small spatially localized perturbations.

<id>
nlin/0004028v1
<category>
nlin.PS
<abstract>
We consider front solutions of the Swift-Hohenberg equation $\partial_t u=
-(1+\partial_x^2)^2 u +\epsilon ^2 u -u^3$. These are traveling waves which
leave in their wake a periodic pattern in the laboratory frame. Using
renormalization techniques and a decomposition into Bloch waves, we show the
non-linear stability of these solutions. It turns out that this problem is
closely related to the question of stability of the trivial solution for the
model problem $\partial_t u(x,t) = \partial_x^2 u
(x,t)+(1+\tanh(x-ct))u(x,t)+u(x,t)^p$ with $p>3$. In particular, we show that
the instability of the perturbation ahead of the front is entirely compensated
by a diffusive stabilization which sets in once the perturbation has hit the
bulk behind the front.

<id>
nlin/0004035v1
<category>
nlin.PS
<abstract>
We demonstrate the spontaneous formation of spatial patterns in
  a damped, ac-driven cubic Klein-Gordon
  lattice. These
  patterns are composed of arrays of intrinsic localized modes
  characteristic for nonlinear lattices. We analyze the modulation
  instability leading to this spontaneous pattern formation. Our
  calculation of the modulational instability is applicable in one
  and two-dimensional lattices, however in the analyses of the
  emerging patterns we concentrate particularly on the two-dimensional case.

<id>
nlin/0005004v1
<category>
nlin.PS
<abstract>
We demonstrate that weak parametric interaction of a fundamental beam with
its third harmonic field in Kerr media gives rise to a rich variety of families
of non-fundamental (multi-humped) solitary waves. Making a comprehensive
comparison between bifurcation phenomena for these families in bulk media and
planar waveguides, we discover two novel types of soliton bifurcations and
other interesting findings. The later includes (i) multi-humped solitary waves
without even or odd symmetry and (ii) multi-humped solitary waves with large
separation between their humps which, however, may not be viewed as bound
states of several distinct one-humped solitons.

<id>
nlin/0005010v1
<category>
nlin.PS
<abstract>
We report an experimental study of the dynamics of an air-fluidized thin
granular layer. Near-onset behavior of this shallow fluidized bed was described
in the earlier paper (Tsimring et al, 1999). Above the threshold of
fluidization the system exhibits a Hopf bifurcation as the layer starts to
oscillate at a certain frequency due to a feedback between the layer dilation
and the airflow drag force. After application of temporal band-pass filtering
of this frequency we discovered the spatio-temporal dynamics in the form of
defect turbulence. This type of dynamics is natural for spatio-temporal systems
close to the threshold of a Hopf bifurcation. At high flow rates, low-frequency
short-wavelength structures appear in addition to the long-wavelength
excitations. A simple model describing the instability and occurrence of
oscillations in a shallow fluidized bed, is proposed.

<id>
nlin/0005017v1
<category>
nlin.PS
<abstract>
The formation of textured patterns has been predicted to occur in two stages.
The first is an early time, domain-forming stage with dynamics characterized by
a disorder function $\bar\delta (\beta) \sim t^{-\sigma_{E}}$, with $\sigma_{E}
= {1/2}\beta$; this decay is universal. Coarsening of domains occurs in the
second stage, in which $\bar\delta (\beta) \sim t^{-\sigma_{L}}$, where
$\sigma_{L}$ is a nonlinear function of $\beta$ whose form is system and model
dependent. Our experiments on a vertically oscillated granular layer are in
accord with theory, yielding $\sigma_{E}\approx 0.5\beta$, and $\sigma_{L}$ a
nonlinear function of $\beta$.

<id>
nlin/0005028v1
<category>
nlin.PS
<abstract>
We analyze two-color spatially localized modes formed by parametrically
coupled fundamental and second-harmonic fields excited at quadratic (or chi-2)
nonlinear interfaces embedded into a linear layered structure --- a
quasi-one-dimensional quadratic nonlinear photonic crystal. For a periodic
lattice of nonlinear interfaces, we derive an effective discrete model for the
amplitudes of the fundamental and second-harmonic waves at the interfaces (the
so-called discrete chi-2 equations), and find, numerically and analytically,
the spatially localized solutions --- discrete gap solitons. For a single
nonlinear interface in a linear superlattice, we study the properties of
two-color localized modes, and describe both similarities and differences with
quadratic solitons in homogeneous media.

<id>
nlin/0005041v1
<category>
nlin.PS
<abstract>
Qualitative information about breather initial profiles in the weak coupling
limit of a chain of identical one-dimensional anharmonic oscillators is found
by studying the linearized equations of motion at a one-site breather. In
particular, information is found about how the breather initial profile depends
on its period T. Numerical work shows two different kinds of breathers to exist
and to occur in alternating T-bands. Genericity of certain aspects of the
observed behaviour is proved.

<id>
nlin/0005043v1
<category>
nlin.PS
<abstract>
Drifting pattern domains (DPDs), moving localized patches of traveling waves
embedded in a stationary (Turing) pattern background and vice versa, are
observed in simulations of a reaction-diffusion model with nonlocal coupling.
Within this model, a region of bistability between Turing patterns and
traveling waves arises from a codimension-2 Turing-wave bifurcation (TWB). DPDs
are found within that region in a substantial distance from the TWB. We
investigated the dynamics of single interfaces between Turing and wave
patterns. It is found that DPDs exist due to a locking of the interface
velocities, which is imposed by the absence of space-time defects near these
interfaces.

<id>
math/9204225v1
<category>
math.AG
<abstract>
Let $X$ be a compact K\"ahler manifold. The set $\cha(X)$ of one-dimensional
complex valued characters of the fundamental group of $X$ forms an algebraic
group. Consider the subset of $\cha(X)$ consisting of those characters for
which the corresponding local system has nontrivial cohomology in a given
degree $d$. This set is shown to be a union of finitely many components that
are translates of algebraic subgroups of $\cha(X)$. When the degree $d$ equals
1, it is shown that some of these components are pullbacks of the character
varieties of curves under holomorphic maps. As a corollary, it is shown that
the number of equivalence classes (under a natural equivalence relation) of
holomorphic maps, with connected fibers, of $X$ onto smooth curves of a fixed
genus $>1$ is a topological invariant of $X$. In fact it depends only on the
fundamental group of $X$.

<id>
math/9204230v1
<category>
math.AG
<abstract>
We introduce the notion of an algebraic cocycle as the algebraic analogue of
a map to an Eilenberg-MacLane space. Using these cocycles we develop a
``cohomology theory" for complex algebraic varieties. The theory is bigraded,
functorial, and admits Gysin maps. It carries a natural cup product and a
pairing to $L$-homology. Chern classes of algebraic bundles are defined in the
theory. There is a natural transformation to (singular) integral cohomology
theory that preserves cup products. Computations in special cases are carried
out. On a smooth variety it is proved that there are algebraic cocycles in each
algebraic rational $(p,p)$-cohomology class.

<id>
math/9304212v1
<category>
math.AG
<abstract>
We characterize the Zariski topologies over an algebraically closed field in
terms of general dimension-theoretic properties. Some applications are given to
complex manifold and to strongly minimal sets.

<id>
math/9410219v1
<category>
math.AG
<abstract>
The space of holomorphic maps from $S^2$ to a complex algebraic variety $X$,
i.e. the space of parametrized rational curves on $X$, arises in several areas
of geometry. It is a well known problem to determine an integer $n(D)$ such
that the inclusion of this space in the corresponding space of continuous maps
induces isomorphisms of homotopy groups up to dimension $n(D)$, where $D$
denotes the homotopy class of the maps. The solution to this problem is known
for an important but special class of varieties, the generalized flag
manifolds: such an integer may be computed, and $n(D)\to\infty$ as
$D\to\infty$. We consider the problem for another class of varieties, namely,
toric varieties. For smooth toric varieties and certain singular ones, $n(D)$
may be computed, and $n(D)\to\infty$ as $D\to\infty$. For other singular toric
varieties, however, it turns out that $n(D)$ cannot always be made arbitrarily
large by a suitable choice of $D$.

<id>
math/9411233v1
<category>
math.AG
<abstract>
We prove an existence result for stable vector bundles with arbitrary rank on
an algebraic surface, and determine the birational structure of certain moduli
space of stable bundles on a rational ruled surface.

<id>
math/9606215v1
<category>
math.AG
<abstract>
Given any polynomial system with fixed monomial term structure, we give
explicit formulae for the generic number of roots with specified coordinate
vanishing restrictions. For the case of affine space minus an arbitrary union
of coordinate hyperplanes, these formulae are also the tightest possible upper
bounds on the number of isolated roots. We also characterize, in terms of
sparse resultants, precisely when these upper bounds are attained. Finally, we
reformulate and extend some of the prior combinatorial results of the contributor on
which subsets of coefficients must be chosen generically for our formulae to be
exact.
  Our underlying framework provides a new toric variety setting for
computational intersection theory in affine space minus an arbitrary union of
coordinate hyperplanes. We thus show that, at least for root counting, it is
better to work in a naturally associated toric compactification instead of
always resorting to products of projective spaces.

<id>
math/9607213v1
<category>
math.AG
<abstract>
For any Lagrangean K\"ahler submanifold $M \subset T^*{\Bbb C}^n$, there
exists a canonical hyper K\"ahler metric on $T^*M$. A K\"ahler potential for
this metric is given by the generalized Calabi Ansatz of the theoretical
physicists Cecotti, Ferrara and Girardello. This correspondence provides a
method for the construction of (pseudo) hyper K\"ahler manifolds with large
automorphism group. Using it, a class of pseudo hyper K\"ahler manifolds of
complex signature $(2,2n)$ is constructed. For any hyper K\"ahler manifold $N$
in this class a group of automorphisms with a codimension one orbit on $N$ is
specified. Finally, it is shown that the bundle of intermediate Jacobians over
the moduli space of gauged Calabi Yau 3-folds admits a natural pseudo hyper
K\"ahler metric of complex signature $(2,2n)$.

<id>
math/9607217v1
<category>
math.AG
<abstract>
The purpose of this paper is to translate positivity properties of the
tangent bundle (and the anti-canonical bundle) of an algebraic manifold into
existence and movability properties of rational curves and to investigate the
impact on the global geometry of the manifold $X$. Among the results we prove
are these:
  \quad If $X$ is a projective manifold, and ${\cal E} \subset T_X$ is an ample
locally free sheaf with $n-2\ge rk {\cal E}\ge n$, then $X \simeq \EP_n$.
  \quad Let $X$ be a projective manifold. If $X$ is rationally connected, then
there exists a free $T_X$-ample family of (rational) curves. If $X$ admits a
free $T_X$-ample family of curves, then $X$ is rationally generated.

<id>
math/9607223v1
<category>
math.AG
<abstract>
In this paper, we attempt to determine the quantum cohomology of projective
bundles over the projective space P^n. In contrast to the previous examples,
the relevant moduli spaces in our case frequently do not have expected
dimensions. It makes the calculation more difficult. We overcome this
difficulty by using excessive intersection theory.

<id>
math/9607224v1
<category>
math.AG
<abstract>
In this paper, it is proved that certain stable rank-3 vector bundles can be
written as extensions of line bundles and stable rank-2 bundles. As an
application, we show the rationality of certain moduli spaces of stable rank-3
bundles over the projective plane P^2.

<id>
math/9607225v1
<category>
math.AG
<abstract>
In this paper, we compare the moduli spaces of rank-3 vector bundles stable
with respect to different ample divisors over rational ruled surfaces. We also
discuss the irreducibility, unirationality, and rationality of these moduli
spaces.

<id>
math/9608207v1
<category>
math.AG
<abstract>
A quadric in $\R P^3$ cuts a curve of degree 6 on a cubic surface in $\R
P^3$. The papers classifies the nonsingular curves cut in this way on
non-singular cubic surfaces up to homeomorphism.
  Two issues new in the study related to the first part of the 16th Hilbert
problem appear in this classification. One is the distribution of the
components of the curve between the components of the non-connected cubic
surface which turns out to depend on the patterns of arrangements (see Theorem
1). The other is presence of positive genus among the components of the
complement and genus-related restrictions (see Theorems 3 and 4).

<id>
math/9608209v1
<category>
math.AG
<abstract>
The zero set of a real polynomial in two variable is a curve in $\mathbb
R^2$. For a generic choice of its coefficients this is a non-singular curve, a
collection of circles and lines properly embedded in $\mathbb R^2$. What
topological arrangements of these circles and lines appear for the polynomials
of a given degree? This question arised in the 19th century in the works of
Harnack and Hilbert and was included by Hilbert into his 16th problem. Several
partial results were obtained since then. However the complete answer is known
only for polynomials of degree 5 or less. The paper presents a new partial
result toward the solution of the 16th Hilbert problem.
  The proof makes use of the proof by Kronheimer and Mrowka of the Thom
conjecture in $\mathbb C P^2$.

<id>
math/9612217v1
<category>
math.AG
<abstract>
The enumeration of points on (or off) the union of some linear or affine
subspaces over a finite field is dealt with in combinatorics via the
characteristic polynomial and in algebraic geometry via the zeta function. We
discuss the basic relations between these two points of view. Counting points
is also related to the $\ell$-adic cohomology of the arrangement (as a
variety). We describe the eigenvalues of the Frobenius map acting on this
cohomology, which corresponds to a finer decomposition of the zeta function.
The $\ell$-adic cohomology groups and their decomposition into eigenspaces are
shown to be fully determined by combinatorial data. Finally, it is shown that
the zeta function is determined by the topology of the corresponding complex
variety in some important cases.

<id>
math/9705215v1
<category>
math.AG
<abstract>
We study the rigidity questions and the Albanese Variety for Complex
Parallelizable Manifolds. Both are related to the study of the cohomology group
$H^1(X,\mathcal O)$. In particular we show that a compact complex
parallelizable manifold is rigid iff $b_1(X)=0$ iff Alb$(X)=\{e\}$ iff
$H^1(X,\mathcal O)=0$.

<id>
math/9705216v1
<category>
math.AG
<abstract>
We study flat vector bundles over complex parallelizable manifolds.

<id>
math/9706217v1
<category>
math.AG
<abstract>
We give an elementary proof of the Pieri-type formula in the cohomology of a
Grassmannian of maximal isotropic subspaces of an odd orthogonal or symplectic
vector space. This proof proceeds by explicitly computing a triple intersection
of Schubert varieties. The decisive step is an exact description of the
intersection of two Schubert varieties, from which the multiplicities (which
are powers of 2) in the Pieri-type formula are immediately obvious.

<id>
math/9710213v1
<category>
math.AG
<abstract>
The main goal of this paper is to give a unified description for the
structure of the small quantum cohomology rings for all homogeneous spaces of
SL_n(C).

<id>
math/9711218v1
<category>
math.AG
<abstract>
The purpose of this paper is to formulate a number of conjectures giving a
rather complete description of the tautological ring of M_g and to discuss the
evidence for these conjectures.

<id>
math/9711219v1
<category>
math.AG
<abstract>
Looijenga recently proved that the tautological ring of M_g vanishes in
degree d>g-2 and is at most one-dimensional in degree g-2, generated by the
class of the hyperelliptic locus. Here we show that K_{g-2} is non-zero on M_g.
The proof uses the Witten conjecture, proven by Kontsevich. With similar
methods, we expect to be able to prove some (possibly all) of the identities in
degree g-2 in the tautological ring that are part of the contributor's conjectural
explicit description of the ring.

<id>
math/9801004v3
<category>
math.AG
<abstract>
Let $V$ be a smooth, projective, convex variety. We define tautological
$\psi$ and $\kappa$ classes on the moduli space of stable maps $\M_{0,n}(V)$,
give a (graphical) presentation for these classes in terms of boundary strata,
derive differential equations for the generating functions of the Gromov-Witten
invariants of $V$ twisted by these tautological classes, and prove that these
intersection numbers are completely determined by the Gromov-Witten invariants
of $V$. This results in families of Frobenius manifold structures on the
cohomology ring of $V$ which includes the quantum cohomology as a special case.

<id>
math/9801033v1
<category>
math.AG
<abstract>
We show that hypergeometric differential equations, unitary and Gauss-Manin
connections give rise to de Rham cohomology sheaves which do not admit a
Bloch-Ogus resolution. The latter is in contrast to Panin's theorem asserting
that corresponding \'etale cohomology sheaves do fulfill Bloch-Ogus theory.

<id>
math/9801050v2
<category>
math.AG
<abstract>
We will show that the duality for regular weight systems introduced by K.
Saito can be interpreted as the duality for orbifoldized Poincare polynomials.

<id>
math/9801070v2
<category>
math.AG
<abstract>
We study tori attached to the fundamental groups of plane curves with
arbitrary singularities. These tori provide complete information about homology
of finite abelian covers of the plane branched along the curve. We calculate
these tori in terms of certain linear systems determined by the singularities
of the curve. In the case of the complements to a union of lines they can be
calculated from the lattice of the arrangement and are closely related to the
components of the space of Aomoto complexes with prescribed homology.

<id>
math/9801075v2
<category>
math.AG
<abstract>
These notes are based on the lecture courses given at the
Ruhr-Universit{\"a}t-Bochum (03--08.02.1997) and at the Universit{\'e} Paul
Sabatier (Toulouse, 08-12.01.1996).

<id>
math/9801076v2
<category>
math.AG
<abstract>
We study a kind of modification of an affine domain which produces another
affine domain. First appeared in passing in the basic paper of O. Zariski
(1942), it was further considered by E.D. Davis (1967). The first named contributor
applied its geometric counterpart to construct contractible smooth affine
varieties non-isomorphic to Euclidean spaces. Here we provide certain
conditions which guarantee preservation of the topology under a modification.
  As an application, we show that the group of biregular automorphisms of the
affine hypersurface $X \subset C^{k+2}$ given by the equation
$uv=p(x_1,...,x_k)$ where $p \in C[x_1,...,x_k],$ acts $m-$transitively on the
smooth part reg$X$ of $X$ for any $m \in N.$ We present examples of such
hypersurfaces diffeomorphic to Euclidean spaces.

<id>
math/9801080v1
<category>
math.AG
<abstract>
In the paper we show that for a normal-crossings degeneration $Z$ over the
ring of integers of a local field with $X$ as generic fibre, the local
monodromy operator and its powers determine invariant cocycle classes under the
decomposition group in the cohomology of the product $X \times X$. More
precisely, they also define algebraic cycles on the special fibre of a
resolution of $Z \times Z$. In the paper, we give an explicit description of
these cycles for a degeneration with at worst triple points as singularities.
These cycles explain geometrically the presence of poles on specific local
factors of the L-function related to $X \times X$.

<id>
math/9801086v1
<category>
math.AG
<abstract>
This is a survey on the topic explained in the title, for the proceedings on
the K-theory 1997 summer institute in Seattle.

<id>
math/9801092v1
<category>
math.AG
<abstract>
The rank 4 locus of a general skew-symmetric 7x7 matrix gives the pfaffian
variety in P^20 which is not defined as a complete intersection. Intersecting
this with a general P^6 gives a Calabi-Yau manifold. An orbifold construction
seems to give the 1-parameter mirror-family of this. However, corresponding to
two points in the 1-parameter family of complex structures, both with maximally
unipotent monodromy, are two different mirror-maps: one corresponding to the
general pfaffian section, the other to a general intersection of G(2,7) in P^20
with a P^13. Apparently, the pfaffian and G(2,7) sections constitute different
parts of the A-model (Kahler structure related) moduli space, and, thus,
represent different parts of the same conformal field theory moduli space.

<id>
math/9801093v1
<category>
math.AG
<abstract>
We use the notion of Milnor fibres of the germ of a meromorphic function and
the method of partial resolutions for a study of topology of a polynomial map
at infinity (mainly for calculation of the zeta-function of a monodromy). It
gives effective methods of computation of the zeta-function for a number of
cases and a criterium for a value to be atypical at infinity.

<id>
math/9503230v1
<category>
math.AT
<abstract>
In this paper we compute the integral cohomology of the discrete groups
SL(2,Z[1/p]), where p is any prime.

<id>
math/9508217v1
<category>
math.AT
<abstract>
We give a combinatorial description of homotopy groups of $\Sigma K(\pi,1)$.
In particular, all of the homotopy groups of the $3$-sphere are combinatorially
given.

<id>
math/9509219v1
<category>
math.AT
<abstract>
The homology with coefficients in a field of the configuration spaces
$C(M\times \bold R ^n,M_o\times \bold R ^n;X)$ is determined in this paper.

<id>
math/9510209v1
<category>
math.AT
<abstract>
We give some formulas of the James-Hopf maps by using combinatorial methods.
An application is to give a product decomposition of the spaces
$\Omega\Sigma^2(X)$.

<id>
math/9510210v1
<category>
math.AT
<abstract>
We give a specific product decomposition of the base-point path connected
component of the triple loop space of the suspension of the projective plane.

<id>
math/9510218v1
<category>
math.AT
<abstract>
Homotopy Lie groups, recently invented by W.G. Dwyer and C.W. Wilkerson,
represent the culmination of a long evolution. The basic philosophy behind the
process was formulated almost 25 years ago by Rector in his vision of a
homotopy theoretic incarnation of Lie group theory. What was then technically
impossible has now become feasible thanks to modern advances such as Miller's
proof of the Sullivan conjecture and Lannes's division functors. Today, with
Dwyer and Wilkerson's implementation of Rector's vision, the tantalizing
classification theorem seems to be within grasp. Supported by motivating
examples and clarifying exercises, this guide quickly leads, without ignoring
the context or the proof strategy, from classical finite loop spaces to the
important definitions and striking results of this new theory.

<id>
math/9706228v1
<category>
math.AT
<abstract>
This paper surveys some results and methods in topological transformation
groups.

<id>
math/9707219v1
<category>
math.AT
<abstract>
The study of the action of the Steenrod algebra on the mod $p$ cohomology of
spaces has many applications to the topological structure of those spaces. In
this paper we present combinatorial formulas for the action of Steenrod
operations on the cohomology of Grassmannians, both in the Borel and the
Schubert picture. We consider integral lifts of Steenrod operations, which lie
in a certain Hopf algebra of differential operators. The latter has been
considered recently as a realization of the Landweber-Novikov algebra in
complex cobordism theory; it also has connections with the action of the
Virasoro algebra on the boson Fock space. Our formulas for Steenrod operations
are based on combinatorial methods which have not been used before in this
area, namely Hammond operators and the combinatorics of Schur functions. We
also discuss several applications of our formulas to the geometry of
Grassmannians.

<id>
math/9801077v2
<category>
math.AT
<abstract>
The long hunt for a symmetric monoidal category of spectra finally ended in
success with the simultaneous discovery of the third contributor's discovery of
symmetric spectra and the Elmendorf-Kriz-Mandell-May category of S-modules. In
this paper we define and study the model category of symmetric spectra, based
on simplicial sets and topological spaces. We prove that the category of
symmetric spectra is closed symmetric monoidal and that the symmetric monoidal
structure is compatible with the model structure. We prove that the model
category of symmetric spectra is Quillen equivalent to Bousfield and
Friedlander's category of spectra. We show that the monoidal axiom holds, so
that we get model categories of ring spectra and modules over a given ring
spectrum.

<id>
math/9801079v1
<category>
math.AT
<abstract>
Symmetric spectra were introduced by Jeff Smith as a symmetric monoidal
category of spectra. In this paper, a detection functor is defined which
detects stable equivalences of symmetric spectra. This detection functor is
useful because the classic stable homotopy groups do not detect stable
equivalences in symmetric spectra.
  One of the advantages of a symmetric monoidal category of spectra is that one
can define topological Hochschild homology on ring spectra simply by mimicking
the Hochschild complex from algebra. Using the detection functor mentioned
above, this definition of topological Hochschild homology is shown to agree
with Bokstedt's original definition. In particular, this shows that Bokstedt's
definition is correct even for non-connective non-convergent symmetric ring
spectra.

<id>
math/9801082v1
<category>
math.AT
<abstract>
We construct model category structures for monoids and modules in symmetric
monoidal model categories which satisfy an extra axiom, the monoidal axiom,
with applications to symmetric spectra and $\Gamma$-spaces.

<id>
math/9801103v1
<category>
math.AT
<abstract>
Using Ohkawa's theorem that the collection of Bousfield classes is a set, we
perform a number of constructions with Bousfield classes. In particular, we
describe a greatest lower bound operator; we also note that a certain subset DL
of the Bousfield lattice is a frame, and we examine some consequences of this
observation. We make several conjectures about the structure of the Bousfield
lattice and DL. In particular, we conjecture that DL is obtained by killing
"strange" spectra, such as the Brown-Comenetz dual of the sphere. We introduce
a new "Boolean algebra of spectra" cBA, which contains Bousfield's BA and is
complete. Our conjectures allow us to identify cBA as being isomorphic to the
complete atomic Boolean algebra on {K(n) : n>= 0}, {A(n) : n>= 2}, and HF_p.
Our conjectures imply that BA is the subBoolean algebra consisting of finite
wedges of the K(n) and A(n), and their complements.

<id>
math/9801125v1
<category>
math.AT
<abstract>
We compute the completed E(n) cohomology of the classifying spaces of the
symmetric groups, and relate the answer to the theory of finite subgroups of
formal groups.

<id>
math/9802008v1
<category>
math.AT
<abstract>
We study phantom maps and homology theories in a stable homotopy category S
via a certain Abelian category A. We express the group P(X,Y) of phantom maps X
-> Y as an Ext group in A, and give conditions on X or Y which guarantee that
it vanishes. We also determine P(X,HB). We show that any composite of two
phantom maps is zero, and use this to reduce Margolis's axiomatisation
conjecture to an extension problem. We show that a certain functor S -> A is
the universal example of a homology theory with values in an AB 5 category and
compare this with some results of Freyd.

<id>
math/9803002v1
<category>
math.AT
<abstract>
A monoidal model category is a model category with a compatible closed
monoidal structure. Such things abound in nature; simplicial sets and chain
complexes of abelian groups are examples. Given a monoidal model category, one
can consider monoids and modules over a given monoid. We would like to be able
to study the homotopy theory of these monoids and modules. This question was
first addressed by Stefan Schwede and Brooke Shipley in "Algebras and modules
in monoidal model categories", who showed that under certain conditions, there
are model categories of monoids and of modules over a given monoid. This paper
is a follow-up to that one. We study what happens when the conditions of
Schwede-Shipley do not hold. This will happen in any topological situation, and
in particular, in topological symmetric spectra. We find that, with no
conditions on our monoidal model category except that it be cofibrantly
generated and that the unit be cofibrant, we still obtain a homotopy category
of monoids, and that this homotopy category is homotopy invariant in an
appropriate sense.

<id>
math/9803055v1
<category>
math.AT
<abstract>
The question of whether a given H-space X is, up to homotopy, a loop space
has been studied from a variety of viewpoints. Here we address this question
from the aspect of homotopy operations, in the classical sense of operations on
homotopy groups.
  First, we show how an H-space structure on X can be used to define the action
of the primary homotopy operations on the shifted homotopy groups \pi_{*-1} X
(which are isomorphic to \pi_* Y, if X=\Omega\Y. This action will behave
properly with respect to composition of operations if X is
homotopy-associative, and will lift to a topological action of the monoid of
all maps between spheres if and only if X is a loop space. The obstructions to
having such a topological action may be formulated in the framework of an
obstruction theory for realizing \Pi-algebras, which is simplified here by
showing that any (suitable) \Delta-simplicial space may be made into a full
simplicial space (a result which may be useful in other contexts).

<id>
math/9803068v1
<category>
math.AT
<abstract>
We show that in a generalized Adams spectral sequence, the presence of a
vanishing line of fixed slope (at some term of the spectral sequence, with some
intercept) is a generic property.

<id>
math/9803087v1
<category>
math.AT
<abstract>
We use obstruction theory to prove that if alpha(n)=2, then RP^{16n+8} cannot
be immersed in R^{32n+3} and RP^{16n+10} cannot be immersed in R^{32n+11}, and
that if alpha(n)>2, then RP^{8n+4} can be embedded in R^{16n+1}. These are new
results.

<id>
math/9806021v2
<category>
math.AT
<abstract>
A Lefschetz-type coincidence theorem for two maps f,g:X->Y from an arbitrary
topological space X to a manifold Y is given: I(f,g)=L(f,g), the coincidence
index is equal to the Lefschetz number. It follows that if L(f,g) is not equal
to zero then there is an x in X such that f(x)=g(x). In particular, the theorem
contains some well-known coincidence results for (i) X,Y manifolds and (ii) f
with acyclic fibers.

<id>
math/9807053v1
<category>
math.AT
<abstract>
We describe an alternative approach to some results of Vassiliev on spaces of
polynomials, by using the scanning method which was used by Segal in his
investigation of spaces of rational functions. We explain how these two
approaches are related by the Smale-Hirsch Principle or the h-Principle of
Gromov. We obtain several generalizations, which may be of interest in their
own right.

<id>
math/9807071v3
<category>
math.AT
<abstract>
We begin by showing that in a triangulated category, specifying a projective
class is equivalent to specifying an ideal I of morphisms with certain
properties, and that if I has these properties, then so does each of its
powers. We show how a projective class leads to an Adams spectral sequence and
give some results on the convergence and collapsing of this spectral sequence.
We use this to study various ideals. In the stable homotopy category we examine
phantom maps, skeletal phantom maps, superphantom maps, and ghosts. (A ghost is
a map which induces the zero map of homotopy groups.) We show that ghosts lead
to a stable analogue of the Lusternik-Schnirelmann category of a space, and we
calculate this stable analogue for low-dimensional real projective spaces. We
also give a relation between ghosts and the Hopf and Kervaire invariant
problems. In the case of A-infinity modules over an A-infinity ring spectrum,
the ghost spectral sequence is a universal coefficient spectral sequence. From
the phantom projective class we derive a generalized Milnor sequence for
filtered diagrams of finite spectra, and from this it follows that the group of
phantom maps from X to Y can always be described as a lim^1 group. The last two
sections focus on algebraic examples. In the derived category of an abelian
category we study the ideal of maps inducing the zero map of homology groups
and find a natural setting for a result of Kelly on the vanishing of composites
of such maps. We also explain how pure exact sequences relate to phantom maps
in the derived category of a ring and give an example showing that phantoms can
compose non-trivially.

<id>
math/9808089v2
<category>
math.AT
<abstract>
This paper discusses the question of how to recognize whether an operad is
E_n (ie. equivalent to the little n-cubes operad). A construction is given
which produces many new examples of E_n operads. This construction is developed
in the context of an infinite family of right adjoint constructions for
operads. Some other related constructions of E_n operads, so-called generalized
tensor products, are also described.

<id>
math/9808101v2
<category>
math.AT
<abstract>
The aim of this brief note is mainly to advocate our approach to homotopy
algebras based on the minimal model of an operad. Our exposition is motivated
by two examples which we discuss very explicitly - the example of strongly
homotopy associative algebras and the example of strongly homotopy Lie
algebras.
  We then indicate what must be proved in order to show that these homotopy
algebraic structures are really `stable under a homotopy.'
  The paper is based on a talk given by the contributor on June 16, 1998, at
University of Osnabrueck, Germany.

<id>
math/9808141v1
<category>
math.AT
<abstract>
We construct splittings of some completions of the Z/(p)-Tate cohomology of
E(n) and some related spectra. In particular, we split (a completion of) tE(n)
as a (completion of) a wedge of E(n-1)'s as a spectrum, where t is shorthand
for the fixed points of the Z/(p)-Tate cohomology spectrum (ie Mahowald's
inverse limit of P_{-k} smash SE(n)). We also give a multiplicative splitting
of tE(n) after a suitable base extension.

<id>
math/9809151v1
<category>
math.AT
<abstract>
This paper proves that the functor $C(*)$ that sends pointed,
simply-connected CW-complexes to their chain-complexes equipped with diagonals
and iterated higher diagonals, determines their integral homotopy type --- even
inducing an equivalence of categories between the category of CW-complexes up
to homotopy equivalence and a certain category of chain-complexes equipped with
higher diagonals. Consequently, $C(*)$ is an algebraic model for integral
homotopy types similar to Quillen's model of rational homotopy types. For
finite CW complexes, our model is finitely generated. Our result implies that
the geometrically induced diagonal map with all ``higher diagonal'' maps (like
those used to define Steenrod operations) collectively determine integral
homotopy type.

<id>
math/9809195v1
<category>
math.AT
<abstract>
\newcommand{\rhomi}[1]{\widetilde{H}_{#1}} \newcommand{\rbeti}[1]{\beta_{#1}}
\newcommand{\kk}{\mathbf k} \newcommand{\dimk}{\dim_{\kk}}
  We show that algebraically shifting a pair of simplicial complexes weakly
increases their relative homology Betti numbers in every dimension.
  More precisely, let $\Delta(K)$ denote the algebraically shifted complex of
simplicial complex $K$, and let $\rbeti{j}(K,L)=\dimk \rhomi{j}(K,L;\kk)$ be
the dimension of the $j$th reduced relative homology group over a field $\kk$
of a pair of simplicial complexes $L \subseteq K$. Then $\rbeti{j}(K,L) \leq
\rbeti{j}(\Delta(K),\Delta(L))$ for all $j$.
  The theorem is motivated by somewhat similar results about Gr\"obner bases
and generic initial ideals. Parts of the proof use Gr\"obner basis techniques.

<id>
math/9810068v1
<category>
math.AT
<abstract>
By considering labeled configurations of ``bounded multiplicity'', one can
construct a functor that fits between homology and stable homotopy. Based on
previous work, we are able to give an equivalent description of this labeled
construction in terms of loop space functors and symmetric products. This
yields a direct generalization of the May-Milgram model for iterated loop
spaces, and answers questions of Carlsson and Milgram posed in the handbook. We
give a classifying space formulation of our results hence extending an older
result of Segal. We finally relate our labeled construction to a theory of Lesh
and give a generalization of a well-known theorem of Quillen, Barratt and
Priddy.

<id>
math/9810178v1
<category>
math.AT
<abstract>
Let p be a prime. The Smith-Toda complex V(k) is a finite spectrum whose
BP-homology is isomorphic to BP_*/(p,v_1,...,v_k). For example, V(-1) is the
sphere spectrum and V(0) the mod p Moore spectrum. In this paper we show that
if p > 5, then V((p+3)/2) does not exist and V((p+1)/2), if it exists, is not a
ring spectrum. The proof uses the new homotopy fixed point spectral sequences
of Hopkins and Miller.

<id>
math/9811027v1
<category>
math.AT
<abstract>
In the first part, we determine conditions on spectra X and Y under which
either every map from X to Y is phantom, or no nonzero maps are. We also
address the question of whether such all or nothing behaviour is preserved when
X is replaced with V smash X for V finite. In the second part, we introduce
chromatic phantom maps. A map is n-phantom if it is null when restricted to
finite spectra of type at least n. We define divisibility and finite type
conditions which are suitable for studying n-phantom maps. We show that the
duality functor W_{n-1} defined by Mahowald and Rezk is the analog of
Brown-Comenetz duality for chromatic phantom maps, and give conditions under
which the natural map Y --> W_{n-1}^2 Y is an isomorphism.

<id>
math/9812035v1
<category>
math.AT
<abstract>
We define inductively a sequence of purely algebraic invariants - namely,
classes in the Quillen cohomology of the Pi-algebra \pi_* X - for
distinguishing between different homotopy types of spaces. Another sequence of
such cohomology classes allows one to decide whether a given abstract
Pi-algebra can be realized as the homotopy Pi-algebra of a space in the first
place.
  The paper is written for a relatively general "resolution model category", so
it also applies, for example, to rational homotopy types.

<id>
math/9201260v1
<category>
math.AP
<abstract>
Certain second-order partial differential operators, which are expressed as
sums of squares of real-analytic vector fields in $\Bbb R^3$ and which are well
known to be $C^\infty$ hypoelliptic, fail to be analytic hypoelliptic.

<id>
math/9201261v1
<category>
math.AP
<abstract>
In this announcement we present a general and new approach to analyzing the
asymptotics of oscillatory Riemann-Hilbert problems. Such problems arise, in
particular, in evaluating the long-time behavior of nonlinear wave equations
solvable by the inverse scattering method. We will restrict ourselves here
exclusively to the modified Korteweg de Vries (MKdV) equation,
  $$y_t-6y^2y_x+y_{xxx}=0,\qquad -\infty<x<\infty,\ t\ge0, y(x,t=0)=y_0(x),$$
  but it will be clear immediately to the reader with some experience in the
field, that the method extends naturally and easily to the general class of
wave equations solvable by the inverse scattering method, such as the KdV,
nonlinear Schr\"odinger (NLS), and Boussinesq equations, etc., and also to
``integrable'' ordinary differential equations such as the Painlev\'e
transcendents.

<id>
math/9201268v1
<category>
math.AP
<abstract>
We survey existence and regularity results for semi-linear wave equations. In
particular, we review the recent regularity results for the $u^5$-Klein Gordon
equation by Grillakis and this contributor and give a self-contained, slightly
simplified proof.

<id>
math/9204239v1
<category>
math.AP
<abstract>
For all functions on an arbitrary open set $\Omega\subset\R^3$ with zero
boundary values, we prove the optimal bound \[ \sup_{\Omega}|u| \leq
(2\pi)^{-1/2} \left(\int_{\Omega}|\nabla u|^2 \,dx\, \int_{\Omega}|\Delta u|^2
\,dx\right)^{1/4}. \] The method of proof is elementary and admits
generalizations. The inequality is applied to establish an existence theorem
for the Burgers equation.

<id>
math/9207212v1
<category>
math.AP
<abstract>
The notion of viscosity solutions of scalar fully nonlinear partial
differential equations of second order provides a framework in which startling
comparison and uniqueness theorems, existence theorems, and theorems about
continuous dependence may now be proved by very efficient and striking
arguments. The range of important applications of these results is enormous.
This article is a self-contained exposition of the basic theory of viscosity
solutions.

<id>
math/9210226v1
<category>
math.AP
<abstract>
We consider the Einstein/Yang-Mills equations in $3+1$ space time dimensions
with $\SU(2)$ gauge group and prove rigorously the existence of a globally
defined smooth static solution. We show that the associated Einstein metric is
asymptotically flat and the total mass is finite. Thus, for non-abelian gauge
fields the Yang/Mills repulsive force can balance the gravitational attractive
force and prevent the formation of singularities in spacetime.

<id>
math/9301218v1
<category>
math.AP
<abstract>
Given $\Bbb R^2, $ with a ``good'' complete metric, we show that the unique
solution of the Ricci flow approaches a soliton at time infinity. Solitons are
solutions of the Ricci flow, which move only by diffeomorphism. The Ricci flow
on $\Bbb R^2$ is the limiting case of the porous medium equation when $m$ is
zero. The results in the Ricci flow may therefore be interpreted as sufficient
conditions on the initial data, which guarantee that the corresponding unique
solution for the porous medium equation on the entire plane asymptotically
behaves like a ``soliton-solution''.

<id>
math/9504226v1
<category>
math.AP
<abstract>
In this announcement we consider an eigenvalue problem which arises in the
study of rectangular membranes. The mathematical model is an elliptic equation,
in potential form, with Dirichlet boundary conditions. We have shown that the
potential is uniquely determined, up to an additive constant, by a subset of
the nodal lines of the eigenfunctions. A formula is given which, when the
additive constant is fixed, yields an approximation to the potential at a dense
set of points. An estimate is presented for the error made by the formula.

<id>
math/9512215v1
<category>
math.AP
<abstract>
A necessary and sufficient condition for local solvability is presented for
the linear partial differential operators $-X^2-Y^2+ia(x)[X,Y]$ in $\bold
R^3=\{(x,y,t)\}$, where $X=\partial_x,\; Y=\partial_y+x^k\partial_t$, and $a\in
C^{\infty}(\bold R^1)$ is real valued, for each positive integer $k$.

<id>
math/9512216v1
<category>
math.AP
<abstract>
Examples are given of degenerate elliptic operators on smooth, compact
manifolds that are not globally regular in $C^\infty$. These operators
degenerate only in a rather mild fashion. Certain weak regularity results are
proved, and an interpretation of global irregularity in terms of the associated
heat semigroup is given.

<id>
math/9512218v1
<category>
math.AP
<abstract>
Local solvability is analyzed for natural families of partial differential
operators having double characteristics. In some families the set of all
operators that are not locally solvable is shown to have both infinite
dimension and infinite codimension.

<id>
math/9707229v1
<category>
math.AP
<abstract>
This work is devoted to the study of a family of almost periodic
one-dimensional Schr\"odinger equations. We define a monodromy matrix for this
family. We study the asymptotic behavior of this matrix in the adiabatic case.
Therefore, w develop a complex WKB method for adiabatic perturbations of
periodic Schr\"odinger equations. At last, the study of the monodromy matrix
enables us to get some spectral results for the initial family of almost
periodic equations.

<id>
math/9709222v1
<category>
math.AP
<abstract>
We prove local well-posedness results for the semi-linear wave equation for
data in $H^\gamma$, $0 < \gamma < \frac{n-3}{2(n-1)}$, extending the previously
known results for this problem. The improvement comes from an introduction of a
two-scale Lebesgue space $X^{r,p}_k$.

<id>
math/9710206v1
<category>
math.AP
<abstract>
We consider two variational evolution problems related to Monge-Kantorovich
mass transfer. These problems provide models for collapsing sandpiles and for
compression molding. We prove the following connection between these problems
and nonlocal geometric curvature motion: The distance functions to surfaces
moving according to certain nonlocal geometric laws are solutions of the
variational evolution problems. Thus we do the first step of the proof of
heuristics developed in earlier works. The main techniques we use are
differential equations methods in the Monge-Kantorovich theory.

<id>
math/9712253v1
<category>
math.AP
<abstract>
The question of complete integrability of evolution equations associated to
$n\times n$ first order isospectral operators is investigated using the inverse
scattering method. It is shown that for $n>2$, e.g. for the three-wave
interaction, additional (nonlinear) pointwise flows are necessary for the
assertion of complete integrability. Their existence is demonstrated by
constructing action-angle variables. This construction depends on the analysis
of a natural 2-form and symplectic foliation for the groups GL(n) and SU(n).}

<id>
math/9712254v1
<category>
math.AP
<abstract>
Using the scattering transform for $n^{th}$ order linear scalar operators,
the Poisson bracket found by Gel'fand and Dikii, which generalizes the Gardner
Poisson bracket for the KdV hierarchy, is computed on the scattering side.
Action-angle variables are then constructed. Using this, complete integrability
is demonstrated in the strong sense. Real action-angle variables are
constructed in the self-adjoint case.

<id>
math/9801146v1
<category>
math.AP
<abstract>
If $u(t,x)$ is a solution of a one--dimensional, parabolic, second--order,
linear partial differential equation (PDE), then it is known that, under
suitable conditions, the number of zero--crossings of the function $u(t,\cdot)$
decreases (that is, does not increase) as time $t$ increases. Such theorems
have applications to the study of blow--up of solutions of semilinear PDE, time
dependent Sturm Liouville theory, curve shrinking problems and control theory.
We generalise the PDE results by showing that the transition operator of a
(possibly time--inhomogenous) one--dimensional diffusion reduces the number of
zero--crossings of a function or even, suitably interpreted, a signed measure.
Our proof is completely probabilistic and depends in a transparent manner on
little more than the sample--path continuity of diffusion processes.

<id>
math/9804149v1
<category>
math.AP
<abstract>
In this paper we study the following nonlinear Maxwell's equations \\
$\varepsilon \E_{t}+\sigma(x,|\E|)\E= \g \vh +\F,\, \vh_{t}+\g \E=0$, where
$\sigma(x,s)$ is a monotone graph of $s$. It is shown that the system has a
unique weak solution. Moreover, the limit of the solution as
$\varepsilon\rightarrow 0$ converges to the solution of quasi-stationary
Maxwell's equations.

<id>
math/9804160v1
<category>
math.AP
<abstract>
We study impact of a forced symmetry-breaking in boundary conditions on the
bifurcation scenario of a semilinear elliptic partial differential equation. We
show that for the square domain the orthogonality of eigenfunctions of the
Laplacian may compensate partially the loss of symmetries in the boundary
conditions and allows some solution to have more symmetries than the imposed
boundary conditions.

<id>
math/9804161v1
<category>
math.AP
<abstract>
Monge-Amp\`ere equations of the form, $u_{xx}u_{yy}-u_{xy}^2=F(u,u_x,u_y)$
arise in many areas of fluid and solid mechanics. Here it is shown that in the
special case $F=u_y^4f(u, u_x/u_y)$, where $f$ denotes an arbitrary function,
the Monge-Amp\`ere equation can be linearized by using a sequence of Amp\`ere,
point, Legendre and rotation transformations. This linearization is a
generalization of three examples from finite elasticity, involving plane strain
and plane stress deformations of the incompressible perfectly elastic Varga
material and also relates to a previous linearization of this equation due to
Khabirov [7].

<id>
math/9804162v1
<category>
math.AP
<abstract>
A nonlinear transformation of the dispersive long wave equations in (2+1)
dimensions is derived by using the homogeneous balance method. With the aid of
the transformation given here, exact solutions of the equations are obtained.

<id>
math/9807167v2
<category>
math.AP
<abstract>
We establish rigorously the existence of a three-parameter family of
self-similar,globally bounded, and continuous weak solutions in two space
dimensions to the compressible Euler equations with axisymmetry for gamma-law
polytropic gases with gamma between 1 and 2, including 1. The initial data of
these solutions have constant densities and outward-swirling velocities. We use
the axisymmetry and self-similarity assumptions to reduce the equations to a
system of three ordinary differential equations, from which we obtain detailed
structures of solutions besides their existence. These solutions exhibit
familiar structures seen in hurricanes and tornadoes. They all have finite
local energy and vorticity with well-defined initial and boundary values.

<id>
math/9807171v2
<category>
math.AP
<abstract>
We prove local and global existence from large, rough initial data for a wave
map between 1+1 dimensional Minkowski space and an analytic manifold. Included
here is global existence for large data in the scale-invariant norm $\dot
L^{1,1}$, and in the Sobolev spaces $H^s$ for $s > 3/4$. This builds on
previous work in 1+1 dimensions of Pohlmeyer, Gu, Ginibre-Velo and Shatah.

<id>
math/9807190v1
<category>
math.AP
<abstract>
The concept of the theory of continuous groups of transformations has
attracted the attention of applied mathematicians and engineers to solve many
physical problems in the engineering sciences. Three applications are presented
in this paper. The first one is the problem of time-dependent vertical
temperature distribution in a stagnant lake. Two cases have been considered for
the forms of the water parameters, namely water density and thermal
conductivity. The second application is the unsteady free-convective
boundary-layer flow on a non-isothermal vertical flat plate. The third
application is the study of the dispersion of gaseous pollutants in the
presence of a temperature inversion. The results are found in closed form and
the effect of parameters are discussed.

<id>
math/9807191v1
<category>
math.AP
<abstract>
This paper deals with the limit behaviour of the solutions of quasi-linear
equations of the form \ $\ds -\limfunc{div}\left(a\left(x, x/{\varepsilon
_h},Du_h\right)\right)=f_h$ on $\Omega $ with Dirichlet boundary conditions.
The sequence $(\varepsilon _h)$ tends to $0$ and the map $a(x,y,\xi )$ is
periodic in $y$, monotone in $\xi $ and satisfies suitable continuity
conditions. It is proved that $u_h\rightarrow u$ weakly in $H_0^{1,2}(\Omega
)$, where $u$ is the solution of a homogenized problem \
$-\limfunc{div}(b(x,Du))=f$ on $\Omega $. We also prove some corrector results,
i.e. we find $(P_h)$ such that $Du_h-P_h(Du)\rightarrow 0$ in $L^2(\Omega
,R^n)$.

<id>
math/9807192v1
<category>
math.AP
<abstract>
Similarity reductions and new exact solutions are obtained for a nonlinear
diffusion equation. These are obtained by using the classical symmetry group
and reducing the partial differential equation to various ordinary differential
equations. For the equations so obtained, first integrals are deduced which
consequently give rise to explicit solutions. Potential symmetries, which are
realized as local symmetries of a related auxiliary system, are obtained. For
some special nonlinearities new symmetry reductions and exact solutions are
derived by using the nonclassical method.

<id>
math/9809186v1
<category>
math.AP
<abstract>
Let $L$ be an infinitely degenerate second-order linear operator defined on a
bounded smooth Euclidean domain. Under weaker conditions than those of
H\"ormander, we show that the Dirichlet problem associated with $L$ has a
unique smooth classical solution. The proof uses the Malliavin calculus. At
present, there appears to be no proof of this result using classical analytic
techniques.

<id>
math/9810164v1
<category>
math.AP
<abstract>
A method for proving symmetrization inequalities for some elliptic p.d.e.'s
on manifolds equipped with appropriate isoperimetric inequalities is outlined.
The method is based on a modification of an approach of Baernstein. The
question of what is the most general result that can be proved in this way is
still open, and the contributor can be consulted if the reader is interested in this
question.

<id>
math/9810205v1
<category>
math.AP
<abstract>
N-fold B\"acklund transformation for the Davey-Stewartson equation is
constructed by using the analytic structure of the Lax eigenfunction in the
complex eigenvalue plane. Explicit formulae can be obtained for a specified
value of N. Lastly it is shown how generalized soliton solutions are generated
from the trivial ones.

<id>
math/9810206v1
<category>
math.AP
<abstract>
The characteristic representation, or Goursat problem, for the
Klein-Fock-Gordon equation with Volkov interaction [1] is regarded. It is shown
that in this representation the explicit form of the Volkov propagator can be
obtained. Using the characteristic representation technique, the Schwinger
integral [2] in the Volkov problem can be calculated.

<id>
math/9810017v1
<category>
math.CT
<abstract>
A concise guide to very basic bicategory theory, from the definition of a
bicategory to the coherence theorem.

<id>
math/9812097v2
<category>
math.CT
<abstract>
This thesis concentrates on the development and application of rewriting and
Groebner basis methods to a range of combinatorial problems.
  Chapter Two contains the most important result, which is the application of
Knuth-Bendix procedures to Kan extensions, showing how rewriting provides a
useful method for attempting to solve a variety of combinatorial problems which
can be phrased in terms of Kan extensions.
  Chapter Three shows that the standard Knuth-Bendix algorithm is step-for-step
a special case of Buchberger's algorithm. The one-sided cases and higher
dimensions are considered.
  Chapter Four relates rewrite systems, Groebner bases and automata. Automata
which only accept irreducibles, and automata which output reduced forms are
discussed for presentations of Kan extensions. Reduction machines for rewrite
systems are identified with standard output automata and the reduction machines
devised for algebras are expressed as Petri nets.
  Chapter Five uses the completion of a group rewriting system to
algorithmically determine a contracting homotopy necessary in order to compute
the set of generators for the module of identities among relations using the
covering groupoid methods devised by Brown and Razak Salleh. Reducing the
resulting set of submodule generators is identified as a Groebner basis
problem. Algorithms are implemented in GAP3.

<id>
math/9901091v1
<category>
math.CT
<abstract>
This is the fourth installment of a series. The main point of the entire
series is the following: given a triangulated category T, it is possible to
attach to it a K-theory space.

<id>
math/9903004v1
<category>
math.CT
<abstract>
fc-multicategories are a very general kind of two-dimensional structure,
encompassing bicategories, monoidal categories, double categories and ordinary
multicategories. We define them and explain how they provide a natural setting
for two familiar categorical ideas. The first is the bimodules construction,
traditionally carried out on suitably cocomplete bicategories but perhaps more
naturally carried out on fc-multicategories. The second is enrichment: there is
a theory of categories enriched in an fc-multicategory, extending the usual
theory of enrichment in a monoidal category. We finish by indicating how this
work is just the simplest case of a much larger phenomenon.

<id>
math/9906039v2
<category>
math.CT
<abstract>
Ideals are used to define homological functors for additive categories. In
abelian categories the ideals corresponding to the usual universal objects are
principal, and the construction reduces, in a choice dependent way, to homology
groups.
  Applications are considered: derived categories and functors.

<id>
math/9909030v1
<category>
math.CT
<abstract>
The general theory of Grothendieck categories is presented. We systemize the
principle methods and results of the theory, showing how these results can be
used for studying rings and modules.

<id>
math/0002025v1
<category>
math.CT
<abstract>
For an arbitrary partially ordered set $P$ its {\em dual} $P^*$ is built as
the collection of all monotone mappings $P\to\2$ where $\2=\{0,1\}$ with $0<1$.
The set of mappings $P^*$ is proved to be a complete lattice with respect to
the pointwise partial order. The {\em second dual} $P^{**}$ is built as the
collection of all morphisms of complete lattices $P^*\to\2$ preserving
universal bounds. Then it is proved that the partially ordered sets $P$ and
$P^{**}$ are isomorphic.

<id>
math/0006061v1
<category>
math.CT
<abstract>
It is proved that MacLane's coherence results for monoidal and symmetric
monoidal categories can be extended to some other categories with
multiplication; namely, to relevant, affine and cartesian categories. All
results are formulated in terms of natural transformations equipped with
``graphs'' (g-natural transformations), and corresponding morphism theorems are
given as consequences. Using these results, some basic relations between the
free categories of these classes are obtained.

<id>
math/0006161v1
<category>
math.CT
<abstract>
Given a 2-category $\twocat{K}$ admitting a calculus of bimodules, and a
2-monad T on it compatible with such calculus, we construct a 2-category
$\twocat{L}$ with a 2-monad S on it such that: (1)S has the
adjoint-pseudo-algebra property. (2)The 2-categories of pseudo-algebras of S
and T are equivalent. Thus, coherent structures (pseudo-T-algebras) are
transformed into universally characterised ones (adjoint-pseudo-S-algebras).
The 2-category $\twocat{L}$ consists of lax algebras for the pseudo-monad
induced by T on the bicategory of bimodules of $\twocat{K}$. We give an
intrinsic characterisation of pseudo-S-algebras in terms of representability.
Two major consequences of the above transformation are the classifications of
lax and strong morphisms, with the attendant coherence result for
pseudo-algebras. We apply the theory in the context of internal categories and
examine monoidal and monoidal globular categories (including their monoid
classifiers) as well as pseudo-functors into $\Cat$.

<id>
math/0009145v1
<category>
math.CT
<abstract>
In this paper we deal with Grothendieck's interpretation of Artin's
interpretation of Galois's Galois Theory (and its natural relation with the
fundamental group and the theory of coverings) as he developed it in Expose V,
section 4, ``Conditions axiomatiques d'une theorie de Galois'' in the SGA1
1960/61.
  This is a beautiful piece of mathematics very rich in categorical concepts,
and goes much beyond the original Galois's scope (just as Galois went much
further than the non resubility of the quintic equation). We show explicitly
how Grothendieck's abstraction corresponds to Galois work.
  We introduce some axioms and prove a theorem of characterization of the
category (topos) of actions of a discrete group. This theorem corresponds
exactly to Galois fundamental result. The theorem of Grothendieck characterizes
the category (topos) of continuous actions of a profinite topological group. We
develop a proof of this result as a "passage into the limit'' (in an inverse
limit of topoi) of our theorem of characterization of the topos of actions of a
discrete group. We deal with the inverse limit of topoi just working with an
ordinary filtered colimit (or union) of the small categories which are their
(respective) sites of definition.
  We do not consider generalizations of Grothendieck's work, except by
commenting briefly in the last section how to deal with the prodiscrete (not
profinite) case. We also mention the work of Joyal-Tierney, which falls
naturally in our discussion.
  There is no need of advanced knowledge of category theory to read this paper,
exept for the comments in the last section.

<id>
math/0010253v2
<category>
math.CT
<abstract>
We construct set-valued right Kan-extensions via a relative Yoneda Lemma.

<id>
math/0012173v2
<category>
math.CT
<abstract>
In Proposition I of "Memoire sur les conditions de resolubilite des equations
par radicaux", Galois established that any intermediate extension of the
splitting field of a polynomial with rational coefficients is the fixed field
of its galois group.
  We first state and prove the (dual) categorical interpretation of of this
statement, which is a theorem about atomic sites with a representable point. In
the general case, the point determines a proobject and it becomes
(tautologically) prorepresentable. We state and prove the, mutatus mutatis,
prorepresentable version of Galois theorem. In this case the classical group of
automorphisms has to be replaced by the localic group of automorphisms. These
developments form the content of a theory that we call "Localic Galois Theory".
  An straightforward corollary of this theory is the theorem: "A topos with a
point is connected atomic if and only if it is the classifying topos of a
localic group, and this group can be taken to be the locale of automorphisms of
the point". This theorem was first proved in print in Joyal A, Tierney M. "An
extension of the Galois Theory of Grothendieck", Mem. AMS 151, Theorem 1,
Section 3, Chapter VIII. Our proof is completely independent of descent theory
and of any other result in that paper.

<id>
math/0101154v1
<category>
math.CT
<abstract>
It is known that factorisation systems in categories can be viewed as unitary
pseudo algebras for the "squaring" monad in Cat.
  We show in this note that an analogous fact holds for proper (i.e., epi-mono)
factorisation systems and a suitable quotient of the former monad, deriving
from a construct introduced by P. Freyd for stable homotopy.
  Structural similarities of the previous monad with the path endofunctor of
topological spaces are considered.

<id>
math/0103193v2
<category>
math.CT
<abstract>
The work is devoted to the extension groups in the category of functors from
a small category to an additive category with an Abelian structure in the sense
of Heller. It is constructed a spectral sequence which converges to the
extension group. Example for diagrams of locally convex spaces is given.

<id>
math/0104136v1
<category>
math.CT
<abstract>
The concept of n-categories and related subject is considered. An n-category
is described as an n-graph with a composition. A new definition of operad is
presented. Some illustrative examples are given.

<id>
math/0106094v1
<category>
math.CT
<abstract>
We present some constructions of limits and colimits in pro-categories. These
are critical tools in several applications. In particular, certain technical
arguments concerning strict pro-maps are essential for a theorem about \'etale
homotopy types. Also, we show that cofiltered limits in pro-categories commute
with finite colimits.

<id>
math/0106146v1
<category>
math.CT
<abstract>
We introduce the notion of a generalized flow on a graph with coefficients in
a R-representation and show that the module of flows is isomorphic to the first
derived functor of the colimit. We generalize Kirchhoff's laws and build an
exact sequence for calculating the module of flows on the union of graphs.

<id>
math/0108017v1
<category>
math.CT
<abstract>
In this paper we propose a higher non abelian cohomology theory without using
the notion of n-category. We use this to study compositions series of affine
manifolds and cohomology of manifolds.

<id>
math/0109021v1
<category>
math.CT
<abstract>
This paper, written in 1998, aims to clarify various higher categorical
structures, mostly through the theory of generalized operads and
multicategories. Chapters I and II, which cover this theory and its application
to give a definition of weak n-category, are largely superseded by my thesis
(math.CT/0011106), but Chapters III and IV have not appeared elsewhere. The
main result of Chapter III is that small Gray-categories can be characterized
as the sub-tricategories of the tricategory of 2-categories, homomorphisms,
strong transformations and modifications; there is also a conjecture on
coherence in higher dimensions. Chapter IV defines opetopes and a category of
n-pasting diagrams for each n, which in the case n=2 is a definition of the
category of trees.

<id>
math/0110273v1
<category>
math.CT
<abstract>
Let $L_n$ denote the Dwyer-Kan localization of the category of weak
n-categories divided by the n-equivalences. We propose a list of properties
that this simplicial category is likely to have, and conjecture that these
properties characterize $L_n$ up to equivalence. We show, using these
properties, how to obtain the morphism $n-1$-categories between two points in
an object of $L_n$ and how to obtain the composition map between the morphism
objects.

<id>
math/0204140v1
<category>
math.CT
<abstract>
In several familiar subcategories of the category ${\mathbb T}$ of
topological spaces and continuous maps, embeddings are not pushout-stable. But,
an interesting feature, capturable in many categories, namely in categories
$\mathcal{B}$ of topological spaces, is the following: For $\mathcal{M}$ the
class of all embeddings, the subclass of all pushout-stable
$\mathcal{M}$-morphisms (that is, of those $\mathcal{M}$-morphisms whose
pushout along an arbitrary morphism always belongs to $\mathcal{M}$) is of the
form $A^{Inj}$ for some space $A$, where $A^{Inj}$ consists of all morphisms
$m:X \to Y$ such that the map $Hom(m,A): Hom(Y,A) \to Hom(X,A)$ is surjective.
We study this phenomenon. We show that, under mild assumptions, the reflective
hull of such a space $A$ is the smallest $\mathcal{M}$-reflective subcategory
of $\mathcal{B}$; furthermore, the opposite category of this reflective hull is
equivalent to a reflective subcategory of the Eilenberg-Moore category
$Set^{\mathbb T}, where ${\mathbb T}$ is the monad induced by the right adjoint
$Hom(-,A): {\mathbb T}^{op} \to Set$. We also find conditions on a category
$\mathcal{B}$ under which the pushout-stable $\mathcal{M}$-morphisms are of the
form $\mathcal{A}^{Inj}$ for some category $\mathcal{A}$.

<id>
math/0204279v1
<category>
math.CT
<abstract>
We define the phrase `category enriched in an fc-multicategory' and explore
some examples. An fc-multicategory is a very general kind of 2-dimensional
structure, special cases of which are double categories, bicategories, monoidal
categories and ordinary multicategories. Enrichment in an fc-multicategory
extends the (more or less well-known) theories of enrichment in a monoidal
category, in a bicategory, and in a multicategory. Moreover, fc-multicategories
provide a natural setting for the bimodules construction, traditionally
performed on suitably cocomplete bicategories. Although this paper is
elementary and self-contained, we also explain why, from one point of view,
fc-multicategories are the natural structures in which to enrich categories.

<id>
math/0206124v2
<category>
math.CT
<abstract>
Many Properties of a category X, as for instance the existence of an adjoint
or a factorization system, are a consequence of the cowellpoweredness of X. In
the absence of cowellpoweredness, for general results, fairly strong assumption
on the category are needed. This paper provides a number of novel and useful
observations to tackle the cowellpoweredness problem of subcategories by means
of regular closure operators. Our exposition focusses on the question when two
subcategories A and B induce the same regular closure operators, then
information about (non)-cowellpoweredness of A may be gained from corresponding
property of B, and vice versa.

<id>
math/0207145v1
<category>
math.CT
<abstract>
This thesis studies the omega-categories associated with products of
infinite-dimensional globes.

<id>
math/0208222v3
<category>
math.CT
<abstract>
We elaborate on the representation theorems of topoi as topoi of discrete
actions of various kinds of localic groups and groupoids. We introduce the
concept of "proessential point" and use it to give a new characterization of
pointed Galois topoi. We establish a hierarchy of connected topoi:
  [1. essentially pointed Atomic = locally simply connected],
  [2. proessentially pointed Atomic = pointed Galois],
  [3. pointed Atomic].
  These topoi are the classifying topos of, respectively: 1. discrete groups,
2. prodiscrete localic groups, and 3. general localic groups.
  We analyze also the unpoited version, and show that for a Galois topos, may
be pointless, the corresponding groupoid can also be considered, in a sense,
the groupoid of "points". In the unpointed theories, these topoi classify,
respectively: 1. connected discrete groupoids, 2. connected (may be pointless)
prodiscrete localic groupoids, and 3. connected groupoids with discrete space
of objects and general localic spaces of hom-sets, when the topos has points
(we do not know the class of localic groupoids that correspond to pointless
connected atomic topoi).
  We comment and develop on Grothendieck's galois theory and its generalization
by Joyal-Tierney, and work by other contributors on these theories.

<id>
math/0209035v1
<category>
math.CT
<abstract>
For a given $\omega$-operad $A$ on globular sets we introduce a sequence of
symmetric operads on $Set$ called slices of $A$ and show how the connected
limit preserving properties of slices are related to the property of the
category of $n$-computads of $A$ being a presheaf topos.

<id>
math/0209093v2
<category>
math.CT
<abstract>
We show that the contributor's notion of Galois extensions of braided tensor
categories [22], see also [3], gives rise to braided crossed G-categories,
recently introduced for the purposes of 3-manifold topology [31]. The Galois
extensions C \rtimes S are studied in detail, and we determine for which g in G
non-trivial objects of grade g exist in C \rtimes S.

<id>
math/0212065v1
<category>
math.CT
<abstract>
Algebraic structures such as monoids, groups, and categories can be
formulated within a category using commutative diagrams. In many common
categories these reduce to familiar cases. In particular, group objects in Grp
are abelian groups, while internal categories in Grp are equivalent both to
group objects in Cat and to crossed modules of groups. In this exposition we
give an elementary introduction to some of the key concepts in this area.

<id>
math/0212219v1
<category>
math.CT
<abstract>
A 2-group is a `categorified' version of a group, in which the underlying set
G has been replaced by a category and the multiplication map m: G x G -> G has
been replaced by a functor. A number of precise definitions of this notion have
already been explored, but a full treatment of their relationships is difficult
to extract from the literature. Here we describe the relation between two of
the most important versions of this notion, which we call `weak' and `coherent'
2-groups. A weak 2-group is a weak monoidal category in which every morphism
has an inverse and every object x has a `weak inverse': an object y such that x
tensor y and y tensor x are isomorphic to 1. A coherent 2-group is a weak
2-group in which every object x is equipped with a specified weak inverse x*
and isomorphisms i_x: 1 -> x tensor x*, e_x: x* tensor x -> 1 forming an
adjunction. We define 2-categories of weak and coherent 2-groups and construct
an `improvement' 2-functor which turns weak 2-groups into coherent ones; using
this one can show that these 2-categories are biequivalent. We also internalize
the concept of a coherent 2-group. This gives a way of defining topological
2-groups, Lie 2-groups, and the like.

<id>
math/0301271v6
<category>
math.CT
<abstract>
In this paper we define a notion of gerbed tower, and use this notion to give
a geometric representation of cohomological classes.

<id>
math/9204236v1
<category>
math.CA
<abstract>
We announce a higher-dimensional generalization of the Bailey Transform,
Bailey Lemma, and iterative ``Bailey chain'' concept in the setting of basic
hypergeometric series very well-poised on unitary $A_{\ell}$ or symplectic
$C_{\ell}$ groups. The classical case, corresponding to $A_1$ or equivalently
$\roman U(2)$, contains an immense amount of the theory and application of
one-variable basic hypergeometric series, including elegant proofs of the
Rogers-Ramanujan-Schur identities. In particular, our program extends much of
the classical work of Rogers, Bailey, Slater, Andrews, and Bressoud.

<id>
math/9207221v1
<category>
math.CA
<abstract>
The polynomials that arise as coefficients when a power series is raised to
the power $x$ include many important special cases, which have surprising
properties that are not widely known. This paper explains how to recognize and
use such properties, and it closes with a general result about approximating
such polynomials asymptotically.

<id>
math/9207222v1
<category>
math.CA
<abstract>
Early 17th-century mathematical publications of Johann Faulhaber contain some
remarkable theorems, such as the fact that the $r$-fold summation of
$1^m,2^m,...,n^m$ is a polynomial in $n(n+r)$ when $m$ is a positive odd
number. The present paper explores a computation-based approach by which
Faulhaber may well have discovered such results, and solves a 360-year-old
riddle that Faulhaber presented to his readers. It also shows that similar
results hold when we express the sums in terms of central factorial powers
instead of ordinary powers. Faulhaber's coefficients can moreover be
generalized to factorial powers of noninteger exponents, obtaining asymptotic
series for $1^{\alpha}+2^{\alpha}+...+n^{\alpha}$ in powers of
$n^{-1}(n+1)^{-1}$.

<id>
math/9301215v1
<category>
math.CA
<abstract>
Singularities of the Radon transform of a piecewise smooth function $f(x)$,
$x\in R^n$, $n\geq 2$, are calculated. If the singularities of the Radon
transform are known, then the equations of the surfaces of discontinuity of
$f(x)$ are calculated by applying the Legendre transform to the functions,
which appear in the equations of the discontinuity surfaces of the Radon
transform of $f(x)$; examples are given. Numerical aspects of the problem of
finding discontinuities of $f(x)$, given the discontinuities of its Radon
transform, are discussed.

<id>
math/9301217v1
<category>
math.CA
<abstract>
A strong error estimate for the uniform rational approximation of $x^\alpha$
on $[0,1]$ is given, and its proof is sketched. Let $E_{nn}(x^\alpha,[0,1])$
denote the minimal approximation error in the uniform norm. Then it is shown
that $$\lim_{n\to\infty}e^{2\pi\sqrt{\alpha n}}E_{nn}(x^\alpha,[0,1]) =
4^{1+\alpha}|\sin\pi\alpha|$$ holds true for each $\alpha>0$.

<id>
math/9307203v1
<category>
math.CA
<abstract>
Using the standard square--function method (based on the Poisson semigroup),
multiplier conditions of H\"ormander type are derived for Laguerre expansions
in $L^p$--spaces with power weights in the $A_p$-range; this result can be
interpreted as an ``upper end point'' multiplier criterion which is fairly good
for $p$ near $1$ or near $\infty $. A weighted generalization of Kanjin's
\cite{kan} transplantation theorem allows to obtain a ``lower end point''
multiplier criterion whence by interpolation nearly ``optimal'' multiplier
criteria (in dependance of $p$, the order of the Laguerre polynomial, the
weight).

<id>
math/9307204v1
<category>
math.CA
<abstract>
The generating function of Stieltjes-Carlitz polynomials is a solution of
Heun's differential equation and using this relation Carlitz was the first to
get exact closed forms for some Heun functions. Similarly the associated
Stieltjes-Carlitz polynomials lead to a new differential equation which we call
associated Heun. Thanks to the link with orthogonal polynomials we are able to
deduce two integral relations connecting associated Heun functions with
different parameters and to exhibit the set of associated Heun functions which
generalize Carlitz's. Part of these results were used by the contributor to derive
the Stieltjes transform of the measure of orthogonality for the associated
Stieltjes-Carlitz polynomials using asymptotic analysis; here we present a new
derivation of this result.

<id>
math/9307209v1
<category>
math.CA
<abstract>
Weinstein's[2] brilliant short proof of de Branges'[1] theorem can be made
yet much shorter(modulo routine calculations), completely elementary (modulo
L\"owner theory), self contained(no need for the esoteric Legendre polynomials'
addition theorem), and motivated(ditto), as follows.

<id>
math/9307210v1
<category>
math.CA
<abstract>
It is shown how sums of squares of real valued functions can be used to give
new proofs of the reality of the zeros of the Bessel functions $J_\alpha (z)$
when $\alpha \ge -1,$ confluent hypergeometric functions ${}_0F_1(c\/; z)$ when
$c>0$ or $0>c>-1$, Laguerre polynomials $L_n^\alpha(z)$ when $\alpha \ge -2,$
and Jacobi polynomials $P_n^{(\alpha,\beta)}(z)$ when $\alpha \ge -1$ and $
\beta \ge -1.$ Besides yielding new inequalities for $|F(z)|^2,$ where $F(z)$
is one of these functions, the derived identities lead to inequalities for
$\partial |F(z)|^2/\partial y$ and $\partial ^2 |F(z)|^2/\partial y^2,$ which
also give new proofs of the reality of the zeros.

<id>
math/9307211v1
<category>
math.CA
<abstract>
The necessary multiplier conditions for Laguerre expansions derived in Gasper
and Trebels \cite{laguerre} are supplemented and modified. This allows us to
place Markett's Cohen type inequality \cite{cohen} (up to the $\log $--case) in
the general framework of necessary conditions.

<id>
math/9307213v1
<category>
math.CA
<abstract>
A number of new definite integrals involving Bessel functions are presented.
These have been derived by finding new integral representations for the product
of two Bessel functions of different order and argument in terms of the
generalized hypergeometric function with subsequent reduction to special cases.
Connection is made with Weber's second exponential integral and Laplace
transforms of products of three Bessel functions.

<id>
math/9307216v1
<category>
math.CA
<abstract>
This is an extended abstract of a lecture held at the Conference ``Fourier
and Radon transformations on symmetric spaces'' in honor of Professor S.
Helgason's 65th birthday, Roskilde, Denmark, Sept. 10--12, 1992.

<id>
math/9307217v1
<category>
math.CA
<abstract>
We present results on co-recursive associated Laguerre and Jacobi polynomials
which are of interest for the solution of the Chapman-Kolmogorov equations of
some birth and death processes with or without absorption. Explicit forms,
generating functions, and absolutely continuous part of the spectral measures
are given. We derive fourth-order differential equations satisfied by the
polynomials with a special attention to some simple limiting cases.

<id>
math/9307218v1
<category>
math.CA
<abstract>
Recurrence coefficients of semi-classical orthogonal polynomials (orthogonal
polynomials related to a weight function $w$ such that $w'/w$ is a rational
function) are shown to be solutions of non linear differential equations with
respect to a well-chosen parameter, according to principles established by D.
G. Chudnovsky. Examples are given. For instance, the recurrence coefficients in
$a_{n+1}p_{n+1}(x)=xp_n(x) -a_np_{n-1}(x)$ of the orthogonal polynomials
related to the weight $\exp(-x^4/4-tx^2)$ on {\blackb R\/} satisfy $4a_n^3\ddot
a_n = (3a_n^4+2ta_n^2-n)(a_n^4+2ta_n^2+n)$, and $a_n^2$ satisfies a Painlev\'e
${\rm P}_{\rm IV}$ equation.

<id>
math/9307220v1
<category>
math.CA
<abstract>
Stieltjes' work on continued fractions and the orthogonal polynomials related
to continued fraction expansions is summarized and an attempt is made to
describe the influence of Stieltjes' ideas and work in research done after his
death, with an emphasis on the theory of orthogonal polynomials.

<id>
math/9307224v1
<category>
math.CA
<abstract>
This paper studies a suitably normalized set of generalized Hermite
polynomials and sets down a relevant Mehler formula, Rodrigues formula, and
generalized translation operator. Weighted generalized Hermite polynomials are
the eigenfunctions of a generalized Fourier transform which satisfies an F. and
M. Riesz theorem on the absolute continuity of analytic measures. The Bose-like
oscillator calculus, which generalizes the calculus associated with the quantum
mechanical simple harmonic oscillator, is studied in terms of these
polynomials.

<id>
math/9309213v1
<category>
math.CA
<abstract>
Extended abstract for the Proceedings of the Conference ``Modern developments
in complex analysis and related topics'' (on the occasion of the 70th birthday
of prof.\ dr.\ J. Korevaar), University of Amsterdam, January 27--29, 1993.

<id>
math/9310219v1
<category>
math.CA
<abstract>
We establish an integral representation of a right inverse of the
Askey-Wilson finite difference operator on $L^2$ with weight $(1-x^2)^{-1/2}$.
The kernel of this integral operator is $\vartheta'_4/\vartheta_4$ and is the
Riemann mapping function that maps the open unit disc conformally onto the
interior of an ellipse.

<id>
math/9310220v1
<category>
math.CA
<abstract>
It is well-known that orthogonal polynomials on the real line satisfy a
three-term recurrence relation and conversely every system of polynomials
satisfying a three-term recurrence relation is orthogonal with respect to some
positive Borel measure on the real line. In this paper we extend this result
and show that every system of polynomials satisfying some $(2N+1)$-term
recurrence relation can be expressed in terms of orthonormal matrix polynomials
for which the coefficients are $N\times N$ matrices. We apply this result to
polynomials orthogonal with respect to a discrete Sobolev inner product and
other inner products in the linear space of polynomials. As an application we
give a short proof of Krein's characterization of orthogonal polynomials with a
spectrum having a finite number of accumulation points.

<id>
math/9310221v1
<category>
math.CA
<abstract>
We establish an integral representations of a right inverses of the
Askey-Wilson finite difference operator in an $L^2$ space weighted by the
weight function of the continuous $q$-Jacobi polynomials. We characterize the
eigenvalues of this integral operator and prove a $q$-analog of the expansion
of $e^{ixy}$ in Jacobi polynomials of argument $x$. We also outline a general
procedure of finding integral representations for inverses of linear operators.

<id>
math/9310223v1
<category>
math.CA
<abstract>
Symmetric elliptic integrals, which have been used as replacements for
Legendre's integrals in recent integral tables and computer codes, are
homogeneous functions of three or four variables. When some of the variables
are much larger than the others, asymptotic approximations with error bounds
are presented. In most cases they are derived from a uniform approximation to
the integrand. As an application the symmetric elliptic integrals of the first,
second, and third kinds are proved to be linearly independent with respect to
coefficients that are rational functions.

<id>
math/9311209v1
<category>
math.CA
<abstract>
By splitting the real line into intervals of unit length a doubly infinite
integral of the form $\Int F(q^x)\,dx,\; 0<q<1$, can clearly be expressed as
$\Integ \Sum F(q^{x+n})\,dx$, provided $F$ satisfies the appropriate
conditions. This simple idea is used to prove Ramanujan's integral analogues of
his \ph{1}{1} sum and give a new proof of Askey and Roy's extention of it.
Integral analogues of the well-poised \ph{2}{2} sum as well as the
very-well-poised \ph{6}{6} sum are also found in a straightforward manner. An
extension to a very-well-poised and balanced \ph{8}{8} series is also given. A
direct proof of a recent q-beta integral of Ismail and Masson is given.

<id>
math/9311210v1
<category>
math.CA
<abstract>
The main difference between certain spectral problems for linear
Schr\"odinger operators, e.g. the almost Mathieu equation, and three-term
recurrence relations for orthogonal polynomials is that in the former the index
ranges across $\ZZ$ and in the latter only across $\Zp$. We present a technique
that, by a mixture of Dirichlet and Taylor expansions, translates the almost
Mathieu equation and its generalizations to three term recurrence relations.
This opens up the possibility of exploiting the full power of the theory of
orthogonal polynomials in the analysis of Schr\"odinger spectra.
  Aforementioned three-term recurrence relations share the property that their
coefficients are almost periodic. We generalize a method of proof, due
originally to Jeff Geronimo and Walter van Assche, to investigate essential
support of the Borel measure of associated orthogonal polynomials, thereby
deriving information on the underlying absolutely continuous spectra of
Schr\"odinger operators.

<id>
math/9312210v1
<category>
math.CA
<abstract>
A $\tphin$ contiguous relation is used to derive contiguous relations for a
very-well-poised $\ephis$. These in turn yield solutions to the associated
$q$-Askey-Wilson polynomial recurrence relation, expressions for the associated
continued fraction, the weight function and a $q$-analogue of a generalized
Dougall's theorem.

<id>
math/9312211v1
<category>
math.CA
<abstract>
We generalize Watson's $ q $-analogue of Ramanujan's Entry 40 continued
fraction by deriving solutions to a $ {}_{10} \phi_9 $ series contiguous
relation and applying Pincherle's theorem. Watson's result is recovered as a
special terminating case, while a limit case yields a new continued fraction
associated with an $ \ephis $ series contiguous relation.

<id>
math/9401208v1
<category>
math.CA
<abstract>
We study nonsymmetric tridiagonal operators acting in the Hilbert space
$\ell^2$ and describe the spectrum and the resolvent set of such operators in
terms of a continued fraction related to the resolvent. In this way we
establish a connection between Pad\'e approximants and spectral properties of
nonsymmetric tridiagonal operators.

<id>
math/9401209v1
<category>
math.CA
<abstract>
The weak convergence of orthogonal polynomials is given under conditions on
the asymptotic behaviour of the coefficients in the three-term recurrence
relation. The results generalize known results and are applied to several
systems of orthogonal polynomials, including orthogonal polynomials on a finite
set of points.

<id>
math/9402212v1
<category>
math.CA
<abstract>
In this paper we characterize the Rogers q-Hermite polynomials as the only
orthogonal polynomial set which is also ${\cal D}_q$-Appell where ${\cal D}_q $
is the Askey-Wilson finite difference operator.

<id>
math/9402216v1
<category>
math.CA
<abstract>
When $G(z)$ is a power series in $z$, many contributors now write `$[z^n] G(z)$'
for the coefficient of $z^n$ in $G(z)$, using a notation introduced by Goulden
and Jackson in [\GJ, p. 1]. More controversial, however, is the proposal of the
same contributors [\GJ, p. 160] to let `$[z^n/n!] G(z)$' denote the coefficient of
$z^n/n!$, i.e., $n!$ times the coefficient of $z^n$. An alternative
generalization of $[z^n] G(z)$, in which we define $[F(z)] G(z)$ to be a linear
function of both $F$ and $G$, seems to be more useful because it facilitates
algebraic manipulations. The purpose of this paper is to explore some of the
properties of such a definition. The remarks are dedicated to Tony Hoare
because of his lifelong interest in the improvement of notations that
facilitate manipulation.

<id>
math/9403213v1
<category>
math.CA
<abstract>
We investigate the asymptotic properties of orthogonal polynomials for a
class of inner products including the discrete Sobolev inner products $\langle
h,g \rangle = \int hg\, d\mu + \sum_{j=1}^m \sum_{i=0}^{N_j} M_{j,i}
h^{(i)}(c_j) g^{(i)}(c_j)$, where $\mu$ is a certain type of complex measure on
the real line, and $c_j$ are complex numbers in the complement of $\supp(\mu)$.
The Sobolev orthogonal polynomials are compared with the orthogonal polynomials
corresponding to the measure $\mu$.

<id>
math/9201305v1
<category>
math.CO
<abstract>
We introduce a family of planar regions, called Aztec diamonds, and study the
ways in which these regions can be tiled by dominoes. Our main result is a
generating function that not only gives the number of domino tilings of the
Aztec diamond of order $n$ but also provides information about the orientation
of the dominoes (vertical versus horizontal) and the accessibility of one
tiling from another by means of local modifications. Several proofs of the
formula are given. The problem turns out to have connections with the
alternating sign matrices of Mills, Robbins, and Rumsey, as well as the square
ice model studied by Lieb.

<id>
math/9206203v1
<category>
math.CO
<abstract>
A short and elementary proof, and a finite-form generalization, are given of
Jacobi's formula for the number of ways of writing an integer as a sum of four
squares (that implies Lagrange's famous 1777 theorem.)

<id>
math/9301202v1
<category>
math.CO
<abstract>
The future of mathematics is described, by using the WZ algorithmic proof
theory as a parable.

<id>
math/9306213v1
<category>
math.CO
<abstract>
Ramanujan's series for Pi, that appeared in his famous letter to Hardy, is
given a one-line WZ proof.

<id>
math/9307202v1
<category>
math.CO
<abstract>
The Vandermonde-Chu Binomial Coefficients Identity is shown to imply
Bombieri's deep norm inequalities, via identities of Beauzamy-D\'egot, and
Reznick.

<id>
math/9309212v1
<category>
math.CO
<abstract>
Capelli's and Turnbull's classical identities are given elegant combinatorial
proofs.

<id>
math/9310232v1
<category>
math.CO
<abstract>
The Dinitz conjecture states that, for each $n$ and for every collection of
$n$-element sets $S_{ij}$, an $n\times n$ partial latin square can be found
with the $(i,j)$\<th entry taken from $S_{ij}$. The analogous statement for
$(n-1)\times n$ rectangles is proven here. The proof uses a recent result by
Alon and Tarsi and is given in terms of even and odd orientations of graphs.

<id>
math/9312214v1
<category>
math.CO
<abstract>
This report contains expository notes about a function $\vartheta(G)$ that is
popularly known as the Lov\'asz number of a graph~$G$. There are many ways to
define $\vartheta(G)$, and the surprising variety of different
characterizations indicates in itself that $\vartheta(G)$ should be
interesting. But the most interesting property of $\vartheta(G)$ is probably
the fact that it can be computed efficiently, although it lies ``sandwiched''
between other classic graph numbers whose computation is NP-hard. I~have tried
to make these notes self-contained so that they might serve as an elementary
introduction to the growing literature on Lov\'asz's fascinating function.

<id>
math/9405212v1
<category>
math.CO
<abstract>
How Enumerative Combinatorics met Special Functions, thanks to Joe Gillis

<id>
math/9406220v1
<category>
math.CO
<abstract>
A generalization of the classical statistics ``maj'' and ``inv'' (the major
index and number of inversions) on words is introduced, parameterized by
arbitrary graphs on the underlying alphabet. The question of characterizing
those graphs that lead to equi-distributed "inv" and "maj" is posed and
answered.

<id>
math/9407211v1
<category>
math.CO
<abstract>
The number of $n \times n$ matrices whose entries are either -1, 0, or 1,
whose row- and column- sums are all 1, and such that in every row and every
column the non-zero entries alternate in sign, is proved to be $[1!4! >...
(3n-2)!]/[n!(n+1)! ... (2n-1)!]$, as conjectured by Mills, Robbins, and Rumsey.

<id>
math/9407212v1
<category>
math.CO
<abstract>
L. Weinstein's brilliant short proof of de Branges's Theorem is made even
shorter by using computer algebra.

<id>
math/9409212v1
<category>
math.CO
<abstract>
On an $r\times (n-r)$ lattice rectangle, we first consider walks that begin
at the SW corner, proceed with unit steps in either of the directions E or N,
and terminate at the NE corner of the rectangle. For each integer $k$ we ask
for $N_k^{n,r}$, the number of {\em ordered\/} pairs of these walks that
intersect in exactly $k$ points. The number of points in the intersection of
two such walks is defined as the cardinality of the intersection of their two
sets of vertices, excluding the initial and terminal vertices. We find two
explicit formulas for the numbers $N_k^{n,r}$. Next we note that $N_1^{n,r}= 2
N_0^{n,r}$, i.e., that {\em exactly twice as many pairs of walks have a single
intersection as have no intersection\/}. Such a relationship clearly merits a
bijective proof, and we supply one. We discuss a number of related results for
different assumptions on the two walks. We find the probability that two
independent walkers on a given lattice rectangle do not meet. In this
situation, the walkers start at the two points $(a,b+x+1)$ and (a+x+1,b)$ in
the first quadrant, and walk West or South at each step, except that when a
walker reaches the $x$-axis (resp. the $y$-axis) then all future steps are
constrained to be South (resp. West) until the origin is reached. We find that
if the probability $p(i,j)$ that a step from $(i,j)$ will go West depends only
on $i+j$, then the probabilty that the two walkers do not meet until they reach
the origin is the same as the probability that a single (unconstrained) walker
who starts at $(a, b+x+1)$ and and takes $a+b+x$ steps, finishes at one of the
points $(0,1), (-1,2), \ldots, (-x,1+x)$.

<id>
math/9409213v1
<category>
math.CO
<abstract>
Given a set $V$, a subset $S$, and a permutation $\pi$ of $V$, we say that
$\pi$ permutes $S$ if $\pi (S) \cap S = \emptyset$. Given a collection $\cS =
\{V; S_1,\ldots , S_m\}$, where $S_i \subseteq V ~~(i=1,\ldots ,m)$, we say
that $\cS$ is invertible if there is a permutation $\pi$ of $V$ such that $\pi
(S_i) \subseteq V-S_i$. In this paper, we present necessary and sufficient
conditions for the invertibility of a collection and construct a polynomial
algorithm which determines whether a given collection is invertible. For an
arbitrary collection, we give a lower bound for the maximum number of sets that
can be inverted. Finally, we consider the problem of constructing a collection
of sets such that no sub-collection of size three is invertible. Our
constructions of such collections come from solutions to the packing problem
with unbounded block sizes. We prove several new lower and upper bounds for the
packing problem and present a new explicit construction of packing.

<id>
math/9409214v1
<category>
math.CO
<abstract>
Let H = (H,V) be a hypergraph with edge set H and vertex set V. Then
hypergraph H is invertible iff there exists a permutation pi of V such that for
all E belongs to H(edges) intersection of(pi(E) and E)=0. H is invertibility
critical if H is not invertible but every hypergraph obtained by removing an
edge from H is invertible. The degree of H is d if |{E belongs to H(edges)|x
belongs to E}| =< d for each x belongs to V Let i(d) be the maximum number of
edges of an invertibility critical hypergraph of degree d. Theorem: i(d) =<
(d-1) {2d-1 choose d} + 1. The proof of this result leads to the following
covering problem on graphs: Let G be a graph. A family H is subset of (2^{V(G)}
is an edge cover of G iff for every edge e of G, there is an E belongs to
H(edge set) which includes e. H(edge set) is a minimal edge cover of G iff for
H' subset of H, H' is not an edge cover of G. Let b(d) (c(d)) be the maximum
cardinality of a minimal edge cover H(edge set) of a complete bipartite graph
(complete graph) where H(edge set) has degree d. Theorem: c(d)=< i(d)=<b(d)=<
c(d+1) and 3. 2^{d-1} - 2 =< b(d)=< (d-1) {2d-1choose d} +1. The proof of this
result uses Sperner theory. The bounds b(d) also arise as bounds on the maximum
number of elements in the union of minimal covers of families of sets.

<id>
math/9409215v1
<category>
math.CO
<abstract>
Let G be a graph with vertices V and edges E. Let F be the union-closed
family of sets generated by E. Then F is the family of subsets of V without
isolated points. Theorem: There is an edge e belongs to E such that |{U belongs
to F | e belongs to U}| =< 1/2|F|. This is equivalent to the following
assertion: If H is a union-closed family generated by a family of sets of
maximum degree two, then there is an $x$ such that |{U belongs to H | x belongs
to U}| > 1/2|H|. This is a special case of the union-closed sets conjecture. To
put this result in perspective, a brief overview of research on the
union-closed sets conjecture is given. A proof of a strong version of the
theorem on graph-generated families of sets is presented. This proof depends on
an analysis of the local properties of F and an application of Kleitman's
lemma. Much of the proof applies to arbitrary union-closed families and can be
used to obtain bounds on |{U belongs to F | e belongs to U}|/|F|.

<id>
math/9409216v1
<category>
math.CO
<abstract>
An expression in the exterior algebra of a Peano space yielding Pappus'
Theorem was originally given by Doubilet, Rota, and Stein. Motivated by an
identity of Rota, we give an identity in a Grassmann-Cayley algebra of step 3,
involving joins and meets alone, which expresses the Theorem of Pappus.

<id>
math/9409217v1
<category>
math.CO
<abstract>
The cycle prefix network is a Cayley coset digraph based on sequences over an
alphabet which has been proposed as a vertex symmetric communication network.
This network has been shown to have many remarkable communication properties
such as a large number of vertices for a given degree and diameter, simple
shortest path routing, Hamiltonicity, optimal connectivity, and others. These
considerations for designing symmetric and directed interconnection networks
are well justified in practice and have been widely recognized in the research
community. Among the important properties of a good network, efficient routing
is probably one of the most important. In this paper, we further study routing
schemes in the cycle prefix network. We confirm an observation first made from
computer experiments regarding the diameter change when certain links are
removed in the original network, and we completely determine the wide diameter
of the network. The wide diameter of a network is now perceived to be even more
important than the diameter. We show by construction that the wide diameter of
the cycle prefix network is very close to the ordinary diameter. This means
that routing in parallel in this network costs little extra time compared to
ordinary single path routing.

<id>
math/9409218v1
<category>
math.CO
<abstract>
In this paper we show that the set of closure relations on a finite poset P
forms a supersolvable lattice, as suggested by Rota. Furthermore this lattice
is dually isomorphic to the lattice of closed sets in a convex geometry (in the
sense of Edelman and Jamison). We also characterize the modular elements of
this lattice and compute its characteristic polynomial.

<id>
math/9409222v1
<category>
math.CO
<abstract>
We study the problem of finding small trees. Classical network design
problems are considered with the additional constraint that only a specified
number $k$ of nodes are required to be connected in the solution. A
prototypical example is the $k$MST problem in which we require a tree of
minimum weight spanning at least $k$ nodes in an edge-weighted graph. We show
that the $k$MST problem is NP-hard even for points in the Euclidean plane. We
provide approximation algorithms with performance ratio $2\sqrt{k}$ for the
general edge-weighted case and $O(k^{1/4})$ for the case of points in the
plane. Polynomial-time exact solutions are also presented for the class of
decomposable graphs which includes trees, series-parallel graphs, and bounded
bandwidth graphs, and for points on the boundary of a convex region in the
Euclidean plane. We also investigate the problem of finding short trees, and
more generally, that of finding networks with minimum diameter. A simple
technique is used to provide a polynomial-time solution for finding $k$-trees
of minimum diameter. We identify easy and hard problems arising in finding
short networks using a framework due to T. C. Hu.

<id>
math/9410210v1
<category>
math.CO
<abstract>
Consider a collection of objects, some of which may be `bad', and a test
which determines whether or not a given sub-collection contains no bad objects.
The non-adaptive pooling (or group testing) problem involves identifying the
bad objects using the least number of tests applied in parallel. The
`hypergeometric' case occurs when an upper bound on the number of bad objects
is known {\em a priori}. Here, practical considerations lead us to impose the
additional requirement of {\em a posteriori} confirmation that the bound is
satisfied. A generalization of the problem in which occasional errors in the
test outcomes can occur is also considered. Optimal solutions to the general
problem are shown to be equivalent to maximum-size collections of subsets of a
finite set satisfying a union condition which generalizes that considered by
Erd\"os \etal \cite{erd}. Lower bounds on the number of tests required are
derived when the number of bad objects is believed to be either 1 or 2. Steiner
systems are shown to be optimal solutions in some cases.

<id>
math/9410211v1
<category>
math.CO
<abstract>
We described a simple algorithm running in linear time for each fixed
constant $k$, that either establishes that the pathwidth of a graph $G$ is
greater than $k$, or finds a path-decomposition of $G$ of width at most
$O(2^{k})$. This provides a simple proof of the result by Bodlaender that many
families of graphs of bounded pathwidth can be recognized in linear time.

<id>
math/9410224v1
<category>
math.CO
<abstract>
In the paper [J. Combin. Theory Ser. A 43 (1986), 103--113], Stanley gives
formulas for the number of plane partitions in each of ten symmetry classes.
This paper together with results by Andrews [J. Combin. Theory Ser. A 66
(1994), 28-39] and Stembridge [Adv. Math 111 (1995), 227-243] completes the
project of proving all ten formulas.
  We enumerate cyclically symmetric, self-complementary plane partitions. We
first convert plane partitions to tilings of a hexagon in the plane by
rhombuses, or equivalently to matchings in a certain planar graph. We can then
use the permanent-determinant method or a variant, the Hafnian-Pfaffian method,
to obtain the answer as the determinant or Pfaffian of a matrix in each of the
ten cases. We row-reduce the resulting matrix in the case under consideration
to prove the formula. A similar row-reduction process can be carried out in
many of the other cases, and we analyze three other symmetry classes of plane
partitions for comparison.

<id>
math/9411218v1
<category>
math.CO
<abstract>
In this paper we give graphs with the largest known order for a given degree
$\Delta$ and diameter $D$. The graphs are constructed from Moore bipartite
graphs by replacement of some vertices by adequate structures. The paper also
contains the latest version of the $(\Delta, D)$ table for graphs.

<id>
math/9411220v1
<category>
math.CO
<abstract>
Let F be a family of subsets of {1,2,...,n}. The width-degree of an element x
in at least one member of F is the width of the family {U in F | x in U}. If F
has maximum width-degree at most k, then F is locally k-wide. Bounds on the
size of locally k-wide families of sets are established. If F is locally k-wide
and centered (every U in F has an element which does not belong to any member
of F incomparable to U), then |F| <= (k+1)(n-k/2); this bound is best possible.
Nearly exact bounds, linear in n and k, on the size of locally k-wide families
of arcs or segments are determined. If F is any locally k-wide family of sets,
then |F| is linearly bounded in n. The proof of this result involves an
analysis of the combinatorics of antichains. Let P be a poset and L a
semilattice (or an intersection-closed family of sets). The P-size of L is
|L^P|. For u in L, the P-density of u is the ratio |[u)^P|/|L^P|. The density
of u is given by the [1]-density of u. Let p be the number of filters of P. L
has the P-density property iff there is a join-irreducible a in L such that the
P-density of a is at most 1/p Which non-trivial semilattices have the P-density
property? For P=[1], it has been conjectured that the answer is: "all" (the
union-closed sets conjecture). Certain subdirect products of lower-semimodular
lattices and, for P=[n], of geometric lattices have the P-density property in a
strong sense. This generalizes some previously known results. A fixed lattice
has the [n]-density property if n is large enough. The density of a generator U
of a union-closed family of sets L containing the empty set is estimated. The
estimate depends only on the local properties of L at U. If L is generated by
sets of size at most two, then there is a generator U of L with estimated
density at most 1/2.

<id>
math/9411221v1
<category>
math.CO
<abstract>
Hamidoune's connectivity results for hierarchical Cayley digraphs are
extended to Cayley coset digraphs and thus to arbitrary vertex transitive
digraphs. It is shown that if a Cayley coset digraph can be hierarchically
decomposed in a certain way, then it is optimally vertex connected. The results
are obtained by extending the methods used by Hamidoune. They are used to show
that cycle-prefix graphs are optimally vertex connected. This implies that
cycle-prefix graphs have good fault tolerance properties.

<id>
math/9411223v1
<category>
math.CO
<abstract>
The learning complexity of special sets of vertices in graphs is studied in
the model(s) of exact learning by (extended) equivalence and membership
queries. Polynomial-time learning algorithms are described for vertex covers,
independent sets, and dominating sets. The complexity of learning vertex sets
of fixed size is also investigated, and it is shown that the k-element vertex
covers in a graph can be learned in a number of rounds of interaction that is
independent of the size of the graph. Apart from the elegance of these
algorithmic problems, the chief motivation is the surprising recently
established connection between the important unsolved problem of the learning
complexity of CNF (or DNF) formulas and the learning complexity of dominating
sets. The complexity of teaching sets of vertices in graphs is also considered.

<id>
math/9411239v1
<category>
math.CO
<abstract>
A method of Proctor [European J. Combin. 5 (1984), no. 4, 331-350] realizes
the set of arbitrary plane partitions in a box and the set of symmetric plane
partitions as bases of linear representations of Lie groups. We extend this
method by realizing transposition and complementation of plane partitions as
natural linear transformations of the representations, thereby enumerating
symmetric plane partitions, self-complementary plane partitions, and
transpose-complement plane partitions in a new way.

<id>
math/9411240v1
<category>
math.CO
<abstract>
An $\{r,s\}$-leaper is a generalized knight that can jump from $(x,y)$ to
$(x\pm r,y\pm s)$ or $(x\pm s,y\pm r)$ on a rectangular grid. The graph of an
$\{r,s\}$-leaper on an $m\times n$ board is the set of $mn$~vertices $(x,y)$
for $0\leq x<m$ and $0\leq y<n$, with an edge between vertices that are one
$\{r,s\}$-leaper move apart. We call $x$ the {\it rank} and $y$ the {\it file}
of board position $(x,y)$. George~P. Jelliss raised several interesting
questions about these graphs, and established some of their fundamental
properties. The purpose of this paper is to characterize when the graphs are
connected, for arbitrary~$r$ and~$s$, and to determine the smallest boards with
Hamiltonian circuits when $s=r+1$ or $r=1$.

<id>
math/9412223v2
<category>
math.CO
<abstract>
We address the degree-diameter problem for Cayley graphs of Abelian groups
(Abelian graphs), both directed and undirected. The problem turns out to be
closely related to the problem of finding efficient lattice coverings of
Euclidean space by shapes such as octahedra and tetrahedra; we exploit this
relationship in both directions. In particular, we find the largest Abelian
graphs with 2 generators (dimensions) and a given diameter. (The results for 2
generators are not new; they are given in the literature of distributed loop
networks.) We find an undirected Abelian graph with 3 generators and a given
diameter which we conjecture to be as large as possible; for the directed case,
we obtain partial results, which lead to unusual lattice coverings of 3-space.
We discuss the asymptotic behavior of the problem for large numbers of
generators. The graphs obtained here are substantially better than traditional
toroidal meshes, but, in the simpler undirected cases, retain certain desirable
features such as good routing algorithms, easy constructibility, and the
ability to host mesh-connected numerical algorithms without any increase in
communication times.

<id>
math/9307225v1
<category>
math.AC
<abstract>
An example is constructed of a local ring and a module of finite type and
finite projective dimension over that ring such that the module is not rigid.
This shows that the rigidity conjecture is false.

<id>
math/9403204v1
<category>
math.AC
<abstract>
Let $\u_{1\times n}$, $\X_{n\times n}$, and $\v_{n\times 1}$ be matrices of
indeterminates, $\Adj \X$ be the classical adjoint of $\X$, and $H(n)$ be the
ideal $I_1(\u\X)+I_1(\X\v)+I_1(\v\u-\Adj \X)$. Vasconcelos has conjectured that
$H(n)$ is a perfect Gorenstein ideal of grade $2n$. In this paper, we obtain
the minimal free resolution of $H(n)$; and thereby establish Vasconcelos'
conjecture.

<id>
math/9406208v1
<category>
math.AC
<abstract>
Assume $R$ is a polynomial ring over a field and $I$ is a homogeneous
Gorenstein ideal of codimension $g\ge3$ and initial degree $p\ge2$. We prove
that the number of minimal generators $\nu(I_p)$ of $I$ that are in degree $p$
is bounded above by $\nu_0={p+g-1\choose g-1}-{p+g-3\choose g-1}$, which is the
number of minimal generators of the defining ideal of the extremal Gorenstein
algebra of codimension $g$ and initial degree $p$. Further, $I$ is itself
extremal if $\nu(I_p)=\nu_0$.

<id>
math/9409208v1
<category>
math.AC
<abstract>
Let $R=\bigoplus_{n\ges0}R_n$ be a graded commutative ring generated over a
field $K=R_0$ by homogeneous elements $x_1,\dots,x_e$ of positive degrees
$d_1,\dots,d_e$. The Hilbert-Serre Theorem shows that for each finite graded
$R$--module $M=\bigoplus_{n\in\BZ}M_n$ the {\it Hilbert series\/}
$\sum_{n\in\BZ}(\rank_K M_n)t^n$ is the Laurent expansion around $0$ of a
rational function
  $$ H_M(t)=\frac{q_M(t)}{\prod_{i=1}^e(1-t^{d_i})} $$
  with $q_M(t)\in\BZ[t,\ti]$. We demonstrate that Laurent expansions
$\left[M\right]_z$ of $H_M(t)$ around other points $z$ of the extended complex
plane $\overline\BC$ also carry important structural information.

<id>
math/9411209v1
<category>
math.AC
<abstract>
In this paper we will define analogs of Gr\"obner bases for $R$-subalgebras
and their ideals in a polynomial ring $R[x_1,\ldots,x_n]$ where $R$ is a
noetherian integral domain with multiplicative identity and in which we can
determine ideal membership and compute syzygies. The main goal is to present
and verify algorithms for constructing these Gr\"obner basis counterparts. As
an application, we will produce a method for computing generators for the first
syzygy module of a subset of an $R$-subalgebra of $R[x_1,\ldots,x_n]$ where
each coordinate of each syzygy must be an element of the subalgebra.

<id>
math/9412210v1
<category>
math.AC
<abstract>
In a previous paper we exhibited the somewhat surprising property that most
direct links of prime ideals in Gorenstein rings are equimultiple ideals with
reduction number $1$. This led to the construction of large families of
Cohen--Macaulay Rees algebras. The first goal of this paper is to extend this
result to arbitrary Cohen--Macaulay rings. The means of the proof are changed
since one cannot depend so heavily on linkage theory. We then study the
structure of the Rees algebra of these links, more specifically we describe
their canonical module in sufficient detail to be able to characterize
self--linked prime ideals. In the last section multiplicity estimates for
classes of such ideals are established.

<id>
math/9504203v1
<category>
math.AC
<abstract>
In this paper we give an effective characterization of Hilbert functions and
polynomials of standard algebras over an Artinian equicharacteristic local
ring; the cohomological properties of such algebras are also studied. We
describe algorithms to check the admissibility of a given function or
polynomial as a Hilbert function or polynomial, and to produce a standard
algebra with a given Hilbert function.

<id>
math/9804052v2
<category>
math.AC
<abstract>
In this short note we introduce a notion of extremality for Betti numbers of
a minimal free resolution, which can be seen as a refinement of the notion of
Mumford-Castelnuovo regularity. We show that extremal Betti numbers of an
arbitrary submodule of a free S-module are preserved when taking the generic
initial module. We relate extremal multigraded Betti numbers in the minimal
resolution of a square free monomial ideal with those of the monomial ideal
corresponding to the Alexander dual simplicial complex and generalize theorems
of Eagon-Reiner and Terai. As an application we give easy (alternative) proofs
of classical criteria due to Hochster, Reisner, and Stanley.

<id>
math/9812112v1
<category>
math.AC
<abstract>
The principal result is a primary decomposition of ideals generated by the
(2x2)-subpermanents of a generic matrix. These permanental ideals almost always
have embedded components and their minimal primes are of three distinct
heights. Thus the permanental ideals are almost never Cohen-Macaulay, in
contrast with determinantal ideals.

<id>
math/9901119v1
<category>
math.AC
<abstract>
Let G be a finite group acting by automorphism on a lattice A, and hence on
the group algebra S=k[A]. The algebra of G-invariants in S is called an algebra
of multiplicative invariants.
  We investigate when algebras of multiplicative invariants are semigroup
algebras. In particular, we present an explicit version of a result of Farkas
stating that multiplicative invariants of finite reflection groups are indeed
semigroup algebras. On the other hand, multiplicative invariants arising from
fixed point free actions are shown to never be semigroup algebras. In
particular, this holds whenever G has odd prime order.

<id>
math/9904143v2
<category>
math.AC
<abstract>
We study the ring of all functions from the positive integers to some field.
This ring, which we call \emph{the ring of number-theoretic functions}, is an
inverse limit of the ``truncations'' \Gamma_n consisting of all functions f for
which f(m)=0 whenever m > n.
  Each \Gamma_n is a zero-dimensional, finitely generated (K)-algebra, which
may be expressed as the quotient of a finitely generated polynomial ring with a
\emph{reversely stable} monomial ideal. Using the description of the free
minimal resolution of stable ideals, given by Eliahou-Kervaire, and some
additional arguments by Aramova-Herzog and Peeva, we give the Poincar\'e-Betti
series for \Gamma_n.

<id>
math/9905125v1
<category>
math.AC
<abstract>
We give a necessary and sufficient condition on a homogeneous polynomial
ideal for its Taylor complex to be exact. Then we give a combinatorial
construction of a minimal resolution for ideals satisfying the above condition
(in particular for monomial ideals).

<id>
math/0003097v1
<category>
math.AC
<abstract>
We associate to each $r$-multigraded, locally finitely generated ideal in the
"large polynomial ring" on countably many indeterminates a power series in $r$
variables; this power series is the limit in the adic topology of the
numerators of the rational functions which give the Hilbert series of the
truncations of the ideal. We characterise the set of all power series so
obtained.
  Our main technical tools are an approximation result which asserts that
truncation and the forming of initial ideals commute in a filtered sense, and
standard inclusion/exclusion, M\"obius inversion, and LCM-lattice homology
methods generalised to monomial ideals in countably many variables.

<id>
math/0007089v3
<category>
math.AC
<abstract>
We give conjectures on the "asymptotic" behaviour of the Hilbert series of
(quotients by) generic ideals in the exterior algebra, as the number of
variables tend to infinity. Our conjectures are supported by extensive computer
calculations.

<id>
math/0008041v2
<category>
math.AC
<abstract>
In this paper we prove parts of a conjecture of Herzog giving lower bounds on
the rank of the free modules appearing in the linear strand of a graded $k$-th
syzygy module over the polynomial ring. If in addition the module is
$\mathbb{Z}^n$-graded we show that the conjecture holds in full generality.
Furthermore, we give lower and upper bounds for the graded Betti numbers of
graded ideals with a linear resolution and a fixed number of generators.

<id>
math/0010117v2
<category>
math.AC
<abstract>
In the article "Non-commutative Grobner bases for commutative algebras",
Eisenbud-Peeva-Sturmfels proved a number of results regarding Grobner bases and
initial ideals of those ideals in the free associative algebra which contain
the commutator ideal. We prove similar results for ideals which contains the
anti-commutator ideal (the defining ideal of the exterior algebra). We define
one notion of generic initial ideals in the free assoicative algebra, and show
that gin's of ideals containing the commutator ideal, or the anti-commutator
ideal, are finitely generated.

<id>
math/0010313v2
<category>
math.AC
<abstract>
In this paper we study the rank one discrete valuations of $k((X_1,...
,X_n))$ whose center in $k\lcor\X\rcor$ is the maximal ideal $(\X)$. In
sections 2 to 6 we give a construction of a system of parametric equations
describing such valuations. This amounts to finding a parameter and a field of
coefficients. We devote section 2 to finding an element of value 1, that is, a
parameter. The field of coefficients is the residue field of the valuation, and
it is given in section 5.
  The constructions given in these sections are not effective in the general
case, because we need either to use the Zorn's lemma or to know explicitly a
section $\sigma$ of the natural homomorphism $R_v\to\d$ between the ring and
the residue field of the valuation $v$.
  However, as a consequence of this construction, in section 7, we prove that
$k((\X))$ can be embedded into a field $L((\Y))$, where the {\em ``extended
valuation'' is as close as possible to the usual order function}.

<id>
math/0011096v1
<category>
math.AC
<abstract>
Let $v$ be a rank-one discrete valuation of the field $k((\X))$. We know,
after \cite{Bri2}, that if $n=2$ then the dimension of $v$ is 1 and if $v$ is
the usual order function over $k((\X))$ its dimension is $n-1$. In this paper
we prove that, in the general case, the dimension of a rank-one discrete
valuation can be any number between 1 and $n-1$.

<id>
math/0011211v2
<category>
math.AC
<abstract>
We study the x- and y-regularity of a bigraded K-algebra R. These notions are
used to study asymptotic properties of certain finitely generated bigraded
modules. As an application we get for any equigenerated graded ideal I upper
bounds for the number j_0 for which reg(I^j) is a linear function for j >= j_0.
Finally we give upper bounds for the x- and y-regularity of generalized
Veronese algebras.

<id>
math/0011251v2
<category>
math.AC
<abstract>
We show that diagonal subalgebras and generalized Veronese subrings of a
bigraded Koszul algebra are Koszul. We give upper bounds for the regularity of
sidediagonal and relative Veronese modules and apply the results to symmetric
algebras and Rees rings.

<id>
math/0012201v1
<category>
math.AC
<abstract>
We investigate the transfer of the Cohen-Macaulay property from a commutative
ring to a subring of invariants under the action of a finite group. Our point
of view is ring theoretic and not a priori tailored to a particular type of
group action. As an illustration, we briefly discuss the special case of
multiplicative actions, that is, actions on group algebras $k[\bbZ^n]$ via an
action on $\bbZ^n$.

<id>
math/0101081v1
<category>
math.AC
<abstract>
In this paper we study resolutions which arise as iterated mapping cones.

<id>
math/0101082v1
<category>
math.AC
<abstract>
The main result of the paper states that for a graded ideal I in a polynomial
ring R over a field of characteristic 0, the Hilbert functions of the local
cohomology modules of R/I and of R/Gin(I) coincide if and only if R/I is
sequentially Cohen-Macaulay.

<id>
math/0101086v1
<category>
math.AC
<abstract>
We prove asymptotic linear bounds for the Castelnuovo-Mumford regularity of
certain filtrations of homogeneous ideals whose Rees algebras need not to be
Noetherian.

<id>
math/0101122v1
<category>
math.AC
<abstract>
In this paper we study homological properties of the Rees ring R of the
graded maximal ideal of a standard graded k-algebra A. In particular we are
interested the comparison of the depth and regularity of A and R.

<id>
math/0103099v1
<category>
math.AC
<abstract>
Let $k$ be a perfect field of characteristic $p>0$, $k(t)_{per}$ the perfect
closure of $k(t)$ and $A$ a $k$-algebra. We characterize whether the ring
$A\otimes_k k(t)_{per}$ is noetherian or not. As a consequence, we prove that
the ring $A\otimes_k k(t)_{per}$ is noetherian when $A$ is the ring of formal
power series in $n$ indeterminates over $k$.

<id>
math/0104175v1
<category>
math.AC
<abstract>
Let (R,m) be a local ring with prime ideals p and q such that p+q is an
m-primary ideal. If R is regular and contains a field, and
dim(R/p)+dim(R/q)=dim(R), we prove that p^{(r)}\cap q^{(n)}\subseteq m^{m+n}
for all positive integers r and s. This is proved using a generalization of
Serre's Intersection Theorem which we apply to a hypersurface R/fR. The
generalization gives conditions that guarantee that Serre's bound on the
intersection dimension dim(R/p)+dim(R/q) \leq dim(R) holds when R is
nonregular.

<id>
math/0106226v2
<category>
math.AC
<abstract>
For a commutative ring $R$ of characteristic $p$, let $\phi : R \to R$ be the
Frobenius homomorphism and let $^{\phi^r}R$ denote the $R$-module structure on
$R$ defined via the $r$-th power of the Frobenius. We show that the Tor functor
against the Frobenius module, $\Tor^R_*(-, {^{\phi^r}}R)$, is rigid for a
certain class of depth zero rings which includes rings that are not complete
intersection. We also show that $\Tor^R_*(-, {^{\phi^r}}R)$ is not rigid
(non-vacuously) when $\depth (R) >0$ and $r$ is large enough. This answers a
question of Avramov and Miller: does rigidity of $\Tor^R_*(-, {^{\phi^r}}R)$
hold for non-complete intersections?

<id>
math/0109207v2
<category>
math.AC
<abstract>
In this paper we study the Kummer extensions of the power series field
$K=k((X_1,...,X_n)$, where $k$ is an algebraically closed field of arbitrary
characteristic.

<id>
math/0110089v1
<category>
math.AC
<abstract>
A theorem of Christol states that a power series over a finite field is
algebraic over the polynomial ring if and only if its coefficients can be
generated by a finite automaton. Using Christol's result, we prove that the
same assertion holds for generalized power series (whose index sets may be
arbitrary well-ordered sets of nonnegative rationals).

<id>
math/9201201v1
<category>
math.CV
<abstract>
If E is a nonempty closed subset of the locally finite Hausdorff
(2n-2)-measure on an n-dimensional complex manifold M and all points of E are
nonremovable for a meromorphic mapping of M \ E into a compact K\"ahler
manifold, then E is a pure (n-1)-dimensional complex analytic subset of M.

<id>
math/9202201v1
<category>
math.CV
<abstract>
No abstract available.

<id>
math/9203201v1
<category>
math.CV
<abstract>
No abstract available.

<id>
math/9204201v1
<category>
math.CV
<abstract>
A method of constructing an entire function with given zeros and estimates of
growth is suggested. It gives a possibility to describe zero sets of certain
classes of entire functions of one and several variables in terms of growth of
volume of these sets in certain polycylinders.

<id>
math/9204232v1
<category>
math.CV
<abstract>
We associate to any germ of an analytic variety a Lie algebra of tangent
vector fields, the {\it tangent algebra}. Conversely, to any Lie algebra of
vector fields an analytic germ can be associated, the {\it integral variety}.
The paper investigates properties of this correspondence: The set of all
tangent algebras is characterized in purely Lie algebra theoretic terms. And it
is shown that the tangent algebra determines the analytic type of the variety.

<id>
math/9207201v1
<category>
math.CV
<abstract>
In his famous 1981 paper, Lempert proved that given a point in a strongly
convex domain the complex geodesics (i.e., the extremal disks) for the
Kobayashi metric passing through that point provide a very useful fibration of
the domain. In this paper we address the question whether, given a smooth
complex Finsler metric on a complex manifold, it is possible to give purely
differential geometric properties of the metric ensuring the existence of such
a fibration in complex geodesics of the manifold. We first discuss at some
length the notion of holomorphic sectional curvature for a complex Finsler
metric; then, using the differential equation of complex geodesics we obtained
in a previous paper, we show that for every pair (point, tangent vector) there
is a (only a segment if the metric is not complete) complex geodesic passing
through the point tangent to the given vector iff the Finsler metric is
K\"ahler, has constant holomorphic sectional curvature -4 and satisfies a
simmetry condition on the curvature tensor. Finally, we show that a complex
Finsler metric of constant holomorphic sectional curvature -4 satisfying the
given simmetry condition on the curvature is necessarily the Kobayashi metric.

<id>
math/9207202v1
<category>
math.CV
<abstract>
The subject considered in this paper has, at least, three points of interest.
Suppose that we have a sequence of one-dimensional analytic varieties in a
domain in $\Bbb C^n$. The cluster of this sequence consists from all points in
the domains such that every neighbourhood of such points intersects with
infinitely many different varieties. The first question is: what analytic
properties does the cluster inherit from varieties? We give a sufficient
criterion when the cluster contains an analytic disk, but it follows from
examples of Stolzenberg and Wermer that, in general, clusters can contain no
analytic disks. So we study algebras of continuous function on clusters, which
can be approximated by holomorphic functions or polynomials, and show that this
algebras possess some analytic properties in all but explicitly pathological
and uninteresting cases. Secondly, we apply and results about clusters to
polynomial hulls and maximal functions, finding remnants of analytic structures
there too. And, finally, due to more and more frequent appearances of analytic
disks as tools in complex analysis, it seems to be interesting to look at their
sequences to establish terminology, basic notation and properties.

<id>
math/9207214v1
<category>
math.CV
<abstract>
A ``self--similar'' example is constructed that shows that a conjecture of N.
U. Arakelyan on the order of decrease of deficiencies of an entire function of
finite order is not true.

<id>
math/9207216v1
<category>
math.CV
<abstract>
We describe briefly a new approach to some problems related to Teichm\"uller
spaces, invariant metrics, and extremal quasiconformal maps. This approach is
based on the properties of plurisubharmonic functions, especially of the
plurisubharmonic Green function. The main theorem gives an explicit
representation of the Green function for Teichm\"uller spaces by the
Kobayashi-Teichm\"uller metric of these spaces. This leads to various
applications. In particular, this gives a new characterization of extremal
quasiconformal maps.

<id>
math/9210201v1
<category>
math.CV
<abstract>
The following theorem is proved:
  Let M be a locally Lipschitz hypersurface in C^n with one-sided extension
property at each point (e.g., without analytic discs). Let S be a closed subset
of M and f : M \ S ---> C^m \ E is a CR-mapping of class L^{\infty} such that
the cluster set of f on S along of Lebesque points of f is contained in a
closed complete pluripolar set E. Then there is a CR-mapping \~f : M ---> C^m
of class L^{\infty}(M) such that \~f |M\S = f. It follows also that S is
removable for CR \cap L^{\infty} (M \ S).

<id>
math/9309201v1
<category>
math.CV
<abstract>
We show that the Bergman, Szego, and Poisson kernels associated to a finitely
connected domain in the plane are all composed of finitely many easily computed
functions of one variable. The new formulas give rise to new methods for
computing the Bergman and Szeg\H o kernels in which all integrals used in the
computations are line integrals; at no point is an integral with respect to
area measure required. The results mentioned so far can be interpreted as
saying that the kernel functions are simpler than one might expect. However, we
also prove that the kernels cannot be too simple by showing that the only
finitely connected domains in the plane whose Bergman or Szeg\H o kernels are
rational functions are the obvious ones. This leads to a proof that the
classical Green's function associated to a finitely connected domain in the
plane is the logarithm of a rational function if and only if the domain is
simply connected and rationally equivalent to the unit disc.

<id>
math/9309202v1
<category>
math.CV
<abstract>
We prove that there is a continuous non-negative function $g$ on the unit
sphere in $\cd$, $d \geq 2$, whose logarithm is integrable with respect to
Lebesgue measure, and which vanishes at only one point, but such that no
non-zero bounded analytic function $m$ in the unit ball, with boundary values
$m^\star$, has $|m^\star| \leq g$ almost everywhere. The proof analyzes the
common range of co-analytic Toeplitz operators in the Hardy space of the ball.

<id>
math/9310201v1
<category>
math.CV
<abstract>
In this paper we describe an approach to complex Finsler metrics suitable to
deal with global questions, and stressing the similarities between hermitian
and complex Finsler metrics. Let $F$ be a smooth complex Finsler metric on a
complex manifold $M$, and assume that the indicatrices of $F$ are strongly
pseudoconvex -- we shall say that $F$ itself is strongly pseudoconvex.
  The vertical bundle $\cal V$ is the kernel of the differential of the
canonical projection of the holomorphic tangent bundle of $M$. Using $F$, it is
possible to endow $\cal V$ with a hermitian metric; let $D$ be the Chern
connection associated to this metric. It turns out that there is a canonical
way to build starting from $D$ a horizontal bundle $\cal H$, as well as a
bundle isomorphism $\Theta\colon{\cal V}\to\cal H$. Using $\Theta$ we may
transfer both the metric and the connection on $\cal H$; furthermore, there is
a canonical isometric embedding $\chi$ of the holomorphic tangent bundle of $M$
into $\cal H$.
  Our idea is that the Finsler geometry of $M$ can be studied applying standard
hermitian techniques to $\cal H$ using $\chi$ to transfer back and forth
problems and solutions. To support this claim, in this paper we discuss Bianchi
identities, K\"ahler conditions, the first and second variation formulas,
geodesics and holomorphic curvature. Furthermore, we provide a sound geometric
interpretation to our previous work on the existence of complex geodesic
curves. Finally, we prove that in complex K\"ahler Finsler manifolds with
constant nonpositive holomorphic curvature (and satisfying an additional
symmetry property on the curvature) the complex geodesic curves define a nice
fibration of the manifold, completely analogous to the one described by Lempert
in strongly convex domains.

<id>
math/9312201v1
<category>
math.CV
<abstract>
An extremal quasiconformal homeomorphisms in a class of homeomorphisms
between two CR 3-manifolds is an one which has the least conformal distortion
among this class. This paper studies extremal quasiconformal homeomorphisms
between CR 3-manifolds which admit transversal CR circle actions. Equivariant
$K$-quasiconformal homeomorphisms are characterized by an area-preserving
property and the $K$-quasiconformality of their quotient maps on the spaces of
$S^1$-orbits. A large family of invariant CR structures on $S^3$ is constructed
so that the extremal quasiconformal homeomorphisms among the equivariant
mappings between them and the standard structure are completely determined.
These homeomorphisms also serve as examples showing that the extremal
quasiconformal homeomorphisms between two invariant CR manifolds are not
necessarily equivariant.

<id>
math/9312202v1
<category>
math.CV
<abstract>
This paper first studies the regularity of conformal homeomorphisms on smooth
locally embeddable strongly pseudoconvex CR manifolds. Then moduli of curve
families are used to estimate the maximal dilatations of quasiconformal
homeomorphisms. On certain CR 3-manifolds, namely, CR circle bundles over flat
tori, extremal quasiconformal homeomorphisms in some homotopy classes are
constructed. These extremal mappings have similar behaviors to Teichm\"uller
mappings on Riemann surfaces.

<id>
math/9401223v1
<category>
math.CV
<abstract>
We will announce two theorems. The first theorem will classify all
topological types of degenerate fibers appearing in one-parameter families of
Riemann surfaces, in terms of ``pseudoperiodic'' surface homeomorphisms. The
second theorem will give a complete set of conjugacy invariants for the mapping
classes of such homeomorphisms. This latter result implies that Nielsen's set
of invariants [{\it Surface transformation classes of algebraically finite
type}, Collected Papers 2, Birkh\"auser (1986)] is not complete.

<id>
math/9402201v1
<category>
math.CV
<abstract>
Cartwright-type and Bernstein-type theorems, previously known only for
functions of exponential type in $\C^n$, are extended to the case of functions
of arbitrary order in a cone.

<id>
math/9402202v1
<category>
math.CV
<abstract>
We give a complete description of divisors of entire periodic functions in
$\C^n$ with plane zeros.

<id>
math/9404201v1
<category>
math.CV
<abstract>
The main result of this paper is that the identity component of the
automorphism group of a compact, connected, strictly pseudoconvex CR manifold
is compact unless the manifold is CR equivalent to the standard sphere. In
dimensions greater than 3, it has been pointed out by D. Burns that this result
follows from known results on biholomorphism groups of complex manifolds with
boundary and the fact that any such CR manifold M can be realized as the
boundary of an analytic variety. When M is 3-dimensional, Burns's proof breaks
down because abstract CR 3-manifolds are generically not realizable as
boundaries. This paper provides an intrinsic proof of compactness that works in
any dimension.

<id>
math/9406201v1
<category>
math.CV
<abstract>
The complex Monge-Amp\`ere operator $(dd^c)^n$ is an important tool in
complex analysis. It would be interesting to find the right notion of
convergence $u_j\to u$ such that $(dd^cu_j)^n\to (dd^cu)^n$ in the weak
topology. In this paper, using the $C_{n-1}$-capacity, we give a sufficient
condition of the weak convergence $(dd^cu_j)^n\to (dd^cu)^n$. We also show that
our condition is quite sharp in some case.

<id>
math/9407201v1
<category>
math.CV
<abstract>
In the paper we give some necessary conditions for a mapping to be a
$\kappa$-geodesic in non-convex complex ellipsoids. Using these results we
calculate explicitly the Kobayashi metric in the ellipsoids
$\{|z_1|^2+|z_2|^{2m}<1\}\subset\bold C^2$, where $m<\frac12$.

<id>
math/9411201v1
<category>
math.CV
<abstract>
We prove results on the propagation of Gevrey and analytic wave front sets
for a class of $C^\infty$ hypoelliptic equations with double characteristics.

<id>
math/9411202v1
<category>
math.CV
<abstract>
We prove global analytic hypoellipticity on a product of tori for partial
differential operators which are constructed as rigid (variable coefficient)
quadratic polynomials in real vector fields satisfying the H\"ormander
condition and where $P$ satisfies a `maximal' estimate. We also prove an
analyticity result that is local in some variables and global in others for
operators whose prototype is
  $$ P= \left({\partial \over {\partial x_1}}\right)^2 + \left({\partial \over
{\partial x_2}}\right)^2 + \left(a(x_1,x_2){\partial \over {\partial
t}}\right)^2.$$
  (with analytic $a(x), a(0)=0,$ naturally, but not identically zero). The
results, because of the flexibility of the methods, generalize recent work of
Cordaro and Himonas in \cite{Cordaro-Himonas 1994} and Himonas in \cite{Himonas
199X} which showed that certain operators known not to be locally analytic
hypoelliptic (those of Baouendi and Goulaouic \cite{Baouendi-Goulaouic 1971},
Hanges and Himonas \cite{Hanges-Himonas 1991}, and Christ \cite{Christ 1991a})
were {\it globally} analytic hypoelliptic on products of tori.

<id>
math/9412201v1
<category>
math.CV
<abstract>
The bounded domains of holomorphy in~$\mathbf{C}^n$ whose Bergman kernel
functions are zero-free form a nowhere dense subset (with respect to a variant
of the Hausdorff distance) of all bounded domains of holomorphy.

<id>
math/9412202v1
<category>
math.CV
<abstract>
We propose a reflection principle for holomorphic objects in ${\Bbb C}^n$.
Our construction generalizes the classical principle of H.Lewy, S.Pinchuk and
S.Webster.

<id>
math/9501201v1
<category>
math.CV
<abstract>
We define the domain of a linear fractional transformation in a space of
operators and show that both the affine automorphisms and the compositions of
symmetries act transitively on these domains. Further, we show that Liouville's
theorem holds for domains of linear fractional transformations, and, with an
additional trace class condition, so does the Riemann removable singularities
theorem. We also show that every biholomorphic mapping of the operator domain
$I < Z^*Z$ is a linear isometry when the space of operators is a complex Jordan
subalgebra of ${\cal L}(H)$ with the removable singularity property and that
every biholomorphic mapping of the operator domain $I + Z_1^*Z_1 < Z_2^*Z_2$ is
a linear map obtained by multiplication on the left and right by J-unitary and
unitary operators, respectively.
  Readers interested only in the finite dimensional case may identify our
spaces of operators with spaces of square and rectangular matrices.

<id>
math/9503201v1
<category>
math.CV
<abstract>
In the paper we generalize the notion of problem (P) introduced by Poletsky.
We introduce the notion of (P_m) extremals. For example, geodesics are (P_1)
extremals. Using obtained results we present a description of (P_m) extremals
in arbitrary complex ellipsoids. It is a generalization of the result obtained
by Jarnicki-Pflug-Zeinstra. We also have a proof of conjecture put forward by
Pflug-Zwonek concerning the formulas for geodesics in non-convex complex
ellipsoids.

<id>
math/9505201v1
<category>
math.CV
<abstract>
No abstract available.

<id>
math/9505202v1
<category>
math.CV
<abstract>
We prove that if $M$ and $M'$ are algebraic hypersurfaces in $ C^ N$, i.e.
both defined by the vanishing of real polynomials, then any sufficiently smooth
CR mapping with Jacobian not identically zero extends holomorphically provided
the hypersurfaces are holomorphically nondegenerate . Conversely, we prove that
holomorphic nondegeneracy is necessary for this property of CR mappings to
hold. For the case of unequal dimensions, we also prove that if $M$ is an
algebraic hypersurface in $ C^N$ which does not contain any complex variety of
positive codimension and $M'$ is the sphere in $ C^{N+1 }$ , then extendability
holds for all CR mappings with certain minimal a priori regularity.
  Theorem A. Let $M$ and $M'$ be two algebraic hypersurfaces in $C^N$ and
assume that $M$ is connected and holomorphically nondegenerate. If $H$ is a
smooth CR mapping from $M$ to $M'$ with $ Jac H \not\equiv 0$, where $Jac H$ is
the Jacobian determinant of $H$, then $H$ extends holomorphically in an open
neighborhood of $M$ in $ C^N$.
  A recent example given by Ebenfelt shows that the conclusion of Theorem A
need not hold if $M$ is real analytic, but not algebraic.
  Theorem B. Let $M$ be a connected real analytic hypersurface in $C^N$ which
is holomorphically degenerate at some point $p_1$. Let $p_0 \in M$ and suppose
there exists a germ at $p_0$ of a smooth CR function on $M$ which does not
extend holomorphically to any full neighborhood of $p_0$ in $C^N$. Then there
exists a germ at $p_0$ of a smooth CR diffeomorphism from $M$ into itself,
fixing $p_0$, which does not extend holomorphically to any neighborhood of
$p_0$ in $C^N$.
  Theorem C. Let $M\subset C^N$ be an algebraic hypersurface. Assume that there
is no nontrivial complex analytic variety contained in $M$ through $p_0 \in M$,
and let $m=m_{p_0}$ be the D'Angelo type. If $H: M \to S^{2N+1}\subset C^{N+1}$
is a CR map of class $C^m$, where $S^{2N+1}$ denotes the boundary of the unit
ball in $C^{N+1 }$, then $H$ admits a holomorphic extension in a neighborhood
of $p_0$.

<id>
math/9506201v1
<category>
math.CV
<abstract>
We show that for a certain family of integrable reversible transformations,
the curves of periodic points of a general transformation cross the level
curves of its integrals. This leads to the divergence of the normal form for a
general reversible transformation with integrals. We also study the integrable
holomorphic reversible transformations coming from real analytic surfaces in
C^2 with non-degenerate complex tangents. We show the existence of real
analytic surfaces with hyperbolic complex tangents, which are contained in a
real hyperplane, but cannot be transformed into the Moser-Webster normal form
through any holomorphic transformation.

<id>
math/9201255v1
<category>
math.DG
<abstract>
A rather simple natural outer derivation of the graded Lie algebra of all
vector valued differential forms with the Fr\"olicher-Nijenhuis bracket turns
out to be a differential and gives rise to a cohomology of the manifold, which
is functorial under local diffeomorphisms. This cohomology is determined as the
direct product of the de Rham cohomology space and the graded Lie algebra of
"traceless" vector valued differential forms, equipped with a new natural
differential concomitant as graded Lie bracket. We find two graded Lie algebra
structures on the space of differential forms. Some consequences and related
results are also discussed.

<id>
math/9201270v1
<category>
math.DG
<abstract>
Shoen and Uhlenbeck showed that ``tangent maps'' can be defined at singular
points of energy minimizing maps. Unfortunately these are not unique, even for
generic boundary conditions. Examples are discussed which have isolated
singularities with a continuum of distinct tangent maps.

<id>
math/9202207v1
<category>
math.DG
<abstract>
In the main part of this paper a connection is just a fiber projection onto a
(not necessarily integrable) distribution or sub vector bundle of the tangent
bundle. Here curvature is computed via the Froelicher-Nijenhuis bracket, and it
is complemented by cocurvature and the Bianchi identity still holds. In this
situation we determine the graded Lie algebra of all graded derivations over
the horizontal projection of a connection and we determine their commutation
relations. Finally, for a principal connection on a principal bundle and the
induced connection on an associated bundle we show how one may pass from one to
the other. The final results relate derivations on vector bundle valued forms
and derivations over the horizontal projection of the algebra of forms on the
principal bundle with values in the standard vector space.

<id>
math/9202208v1
<category>
math.DG
<abstract>
We study the action of the diffeomorphism group $\Diff(M)$ on the space of
proper immersions $\Imm_{\text{prop}}(M,N)$ by composition from the right. We
show that smooth transversal slices exist through each orbit, that the quotient
space is Hausdorff and is stratified into smooth manifolds, one for each
conjugacy class of isotropy groups.

<id>
math/9203202v1
<category>
math.DG
<abstract>
It is shown that a strong system of vector fields on a fiber bundle in the
sense of [Modugno, M. Systems of connections and invariant lagrangians. In:
Differential geometric methods in theoretical physics, Proc. 15th Int. Conf.,
DGM, Clausthal/FRG 1986, 518-534 World Scientific Publishing Co. (1987)] is
induced from a principal fiber bundle if and only if each vertical vector field
of the system is complete.

<id>
math/9204221v1
<category>
math.DG
<abstract>
The well known formula $[X,Y]=\tfrac12\tfrac{\partial^2}{\partial t^2}|_0
(\Fl^Y_{-t}\o\Fl^X_{-t}\o\Fl^Y_t\o\Fl^X_t)$ for vector fields $X$, $Y$ is
generalized to arbitrary bracket expressions and arbitrary curves of local
diffeomorphisms.

<id>
math/9204223v1
<category>
math.DG
<abstract>
A natural metric on the space of all almost hermitian structures on a given
manifold is investigated.

<id>
math/9207215v1
<category>
math.DG
<abstract>
We use an extension of Sunada's theorem to construct a nonisometric pair of
isospectral simply connected domains in the Euclidean plane, thus answering
negatively Kac's question, ``can one hear the shape of a drum?'' In order to
construct simply connected examples, we exploit the observation that an
orbifold whose underlying space is a simply connected manifold with boundary
need not be simply connected as an orbifold.

<id>
math/9209219v1
<category>
math.DG
<abstract>
Let $G\subset GL(V)$ be a linear Lie group with Lie algebra $\frak g$ and let
$A(\frak g)^G$ be the subalgebra of $G$-invariant elements of the associative
supercommutative algebra $A(\frak g)= S(\frak g^*)\otimes \La(V^*)$. To any
$G$-structure $\pi:P\to M$ with a connection $\omega$ we associate a
homomorphism $\mu_\omega:A(\frak g)^G\to \Omega(M)$. The differential forms
$\mu_\omega(f)$ for $f\in A(\frak g)^G$ which are associated to the
$G$-structure $\pi$ can be used to construct Lagrangians. If $\omega$ has no
torsion the differential forms $\mu_\omega(f)$ are closed and define
characteristic classes of a $G$-structure. The induced homomorphism
$\mu'_\omega:A(\g)^G\to H^*(M)$ does not depend on the choice of the
torsionfree connection $\omega$ and it is the natural generalization of the
Chern Weil homomorphism.

<id>
math/9307226v1
<category>
math.DG
<abstract>
There exist two new embedded minimal surfaces, asymptotic to the helicoid.
One is periodic, with quotient (by orientation-preserving translations) of
genus one. The other is nonperiodic of genus one.

<id>
math/9309214v1
<category>
math.DG
<abstract>
An action of a Lie algebra $\frak g$ on a manifold $M$ is just a Lie algebra
homomorphism $\zeta:\frak g\to \frak X(M)$. We define orbits for such an
action. In general the space of orbits $M/\frak g$ is not a manifold and even
has a bad topology. Nevertheless for a $\frak g$-manifold with equidimensional
orbits we treat such notions as connection, curvature, covariant
differentiation, Bianchi identity, parallel transport, basic differential
forms, basic cohomology, and characteristic classes, which generalize the
corresponding notions for principal $G$-bundles. As one of the applications, we
derive a sufficient condition for the projection $M\to M/\frak g$ to be a
bundle associated to a principal bundle.

<id>
math/9407216v1
<category>
math.DG
<abstract>
This note announces a general construction of characteristic currents for
singular connections on a vector bundle. It develops, in particular, a
Chern-Weil-Simons theory for smooth bundle maps $\alpha : E \rightarrow F$
which, for smooth connections on $E$ and $F$, establishes formulas of the type
$$ \phi \ = \ \text{\rm Res}_{\phi}\Sigma_{\alpha} + dT. $$ Here $\phi$ is a
standard charactersitic form, $\text{Res}_{\phi}$ is an associated smooth
``residue'' form computed canonically in terms of curvature, $\Sigma_{\alpha}$
is a rectifiable current depending only on the singular structure of $\alpha$,
and $T$ is a canonical, functorial transgression form with coefficients in
$\loc$. The theory encompasses such classical topics as: Poincar\'e-Lelong
Theory, Bott-Chern Theory, Chern-Weil Theory, and formulas of Hopf.
Applications include:\ \ a new proof of the Riemann-Roch Theorem for vector
bundles over algebraic curves, a $C^{\infty}$-generalization of the
Poincar\'e-Lelong Formula, universal formulas for the Thom class as an
equivariant characteristic form (i.e., canonical formulas for a de Rham
representative of the Thom class of a bundle with connection), and a
Differentiable Riemann-Roch-Grothendieck Theorem at the level of forms and
currents. A variety of formulas relating geometry and characteristic classes
are deduced as direct consequences of the theory.

<id>
math/9412221v1
<category>
math.DG
<abstract>
We develop an asymptotic expansion of the spectral measures on a degenerating
family of hyperbolic Riemann surfaces of finite volume. As an application of
our results, we study the asymptotic behavior of weighted counting functions,
which, if $M$ is compact, is defined for $w \geq 0$ and $T > 0$ by $$N_{M,w}(T)
= \sum\limits_{\lambda_n \leq T}(T-\lambda_n)^w $$ where $\{\lambda_n\}$ is the
set of eigenvalues of the Laplacian which acts on the space of smooth functions
on $M$. If $M$ is non-compact, then the weighted counting function is defined
via the inverse Laplace transform. Now let $M_{\ell}$ denote a degenerating
family of compact or non-compact hyperbolic Riemann surfaces of finite volume
which converges to the non-compact hyperbolic surface $M_{0}$. As an example of
our results, we have the following theorem: There is an explicitly defined
function $G_{\ell,w}(T)$ which depends solely on $\ell$, $w$, and $T$ such that
for $w > 3/2$ and $T>0$, we have $$N_{M_{\ell},w}(T) = G_{\ell,w}(T)
+N_{M_{0},w}(T) +o(1)$$ for $\ell \to 0$. We also consider the setting when $w
< 3/2$, and we obtain a new proof of the continuity of small eigenvalues on
degenerating hyperbolic Riemann surfaces of finite volume.

<id>
math/9412232v1
<category>
math.DG
<abstract>
For a more general notion of Cartan connection we define characteristic
classes, we investigate their relation to usual characteristic classes.

<id>
math/9502208v1
<category>
math.DG
<abstract>
We present a new construction for obtaining pairs of higher-step isospectral
Riemannian nilmanifolds and compare several resulting new examples. In
particular, we present new examples of manifolds that are isospectral on
functions, but not isospectral on one-forms.

<id>
math/9503216v1
<category>
math.DG
<abstract>
We define geometric zeta functions for locally symmetric spaces as
generalizations of the zeta functions of Ruelle and Selberg. As a special value
at zero we obtain the Reidemeister torsion of the manifold. For hermitian
spaces these zeta functions have as special value the quotient of the
holomorphic torsion of Ray and Singer and the holomorphic $L^2$-torsion, where
the latter is defined via the $L^2$-theory of Atiyah. For higher fundamental
rank twisted torsion numbers appear.

<id>
math/9503220v1
<category>
math.DG
<abstract>
Two Riemannian manifolds are said to have $C^k$-conjugate geodesic flows if
there exist an $C^k$ diffeomorphism between their unit tangent bundles which
intertwines the geodesic flows. We obtain a number of rigidity results for the
geodesic flows on compact 2-step Riemannian nilmanifolds: For generic 2-step
nilmanifolds the geodesic flow is $C^2$ rigid. For special classes of 2-step
nilmanifolds, we show that the geodesic flow is $C^0$ or $C^2$ rigid. In
particular, there exist continuous families of 2-step nilmanifolds whose
Laplacians are isospectral but whose geodesic flows are not $C^0$ conjugate.

<id>
math/9504207v1
<category>
math.DG
<abstract>
In this paper we construct and study isoperimetric functions at infinity for
Hadamard manifolds. These quasi-isometry invariants give a measure of the
spread of geodesics in such a manifold.

<id>
math/9504208v1
<category>
math.DG
<abstract>
We give an arithmetic criterion which is sufficient to imply the discreteness
of various two-generator subgroups of $PSL(2,{\bold C})$. We then examine
certain two-generator groups which arise as extremals in various geometric
problems in the theory of Kleinian groups, in particular those encountered in
efforts to determine the smallest co-volume, the Margulis constant and the
minimal distance between elliptic axes. We establish the discreteness and
arithmeticity of a number of these extremal groups, the associated minimal
volume arithmetic group in the commensurability class and we study whether or
not the axis of a generator is simple.

<id>
math/9504209v1
<category>
math.DG
<abstract>
The Margulis constant for Kleinian groups is the smallest constant $c$ such
that for each discrete group $G$ and each point $x$ in the upper half space
${\bold H}^3$, the group generated by the elements in $G$ which move $x$ less
than distance c is elementary. We take a first step towards determining this
constant by proving that if $\langle f,g \rangle$ is nonelementary and discrete
with $f$ parabolic or elliptic of order $n \geq 3$, then every point $x$ in
${\bold H}^3$ is moved at least distance $c$ by $f$ or $g$ where
$c=.1829\ldots$. This bound is sharp.

<id>
math/9505217v1
<category>
math.DG
<abstract>
We show that strictly abnormal geodesics arise in graded nilpotent Lie
groups. We construct such a group, for which some Carnot geodesics are strictly
abnormal; in fact, they are not normal in any subgroup. In the step-2 case we
also prove that these geodesics are always smooth. Our main technique is based
on the equations for the normal and abnormal curves, that we derive (for any
Lie group) explicitly in terms of the structure constants.

<id>
math/9508213v1
<category>
math.DG
<abstract>
We survey what is known about minimal surfaces in $\bold R^3 $ that are
complete, embedded, and have finite total curvature. The only classically known
examples of such surfaces were the plane and the catenoid. The discovery by
Costa, early in the last decade, of a new example that proved to be embedded
sparked a great deal of research in this area. Many new examples have been
found, even families of them, as will be described below. The central question
has been transformed from whether or not there are any examples except surfaces
of rotation to one of understanding the structure of the space of examples.

<id>
math/9508216v1
<category>
math.DG
<abstract>
Grafting is a surgery on Riemann surfaces introduced by Thurston which
connects hyperbolic geometry and the theory of projective structures on
surfaces. We will discuss the space of projective structures in terms of the
Thurston's geometric parametrization given by grafting. From this approach we
will prove that on any compact Riemann surface with genus greater than $1$
there exist infinitely many projective structures with Fuchsian holonomy
representations. In course of the proof it will turn out that grafting is
closely related to harmonic maps between surfaces.

<id>
math/9510207v1
<category>
math.DG
<abstract>
The subject of this paper is the relationship among the marked length
spectrum, the length spectrum, the Laplace spectrum on functions, and the
Laplace spectrum on forms on Riemannian nilmanifolds. In particular, we show
that for a large class of three-step nilmanifolds, if a pair of nilmanifolds in
this class has the same marked length spectrum, they necessarily share the same
Laplace spectrum on functions. In contrast, we present the first example of a
pair of isospectral Riemannian manifolds with the same marked length spectrum
but not the same spectrum on one-forms. Outside of the standard spheres vs. the
Zoll spheres, which are not even isospectral, this is the only example of a
pair of Riemannian manifolds with the same marked length spectrum, but not the
same spectrum on forms. This partially extends and partially contrasts the work
of Eberlein, who showed that on two-step nilmanifolds, the same marked length
spectrum implies the same Laplace spectrum both on functions and on forms.

<id>
math/9601208v1
<category>
math.DG
<abstract>
The contributors study the Hodge theory of the exterior differential operator $d$
acting on $q$-forms on a smoothly bounded domain in $\RR^{N+1}$, and on the
half space $\rnp$. The novelty is that the topology used is not an $L^2$
topology but a Sobolev topology. This strikingly alters the problem as compared
to the classical setup. It gives rise to a boundary-value problem belonging to
a class of problems first introduced by Vi\v{s}ik and Eskin, and by Boutet de
Monvel.

<id>
math/9602213v1
<category>
math.DG
<abstract>
Motivated by the physical concept of special geometry two mathematical
constructions are studied, which relate real hypersurfaces to tube domains and
complex Lagrangean cones respectively. Me\-thods are developed for the
classification of homogeneous Riemannian hypersurfaces and for the
classification of linear transitive reductive algebraic group actions on pseudo
Riemannian hypersurfaces. The theory is applied to the case of cubic
hypersurfaces, which is the case most relevant to special geometry, obtaining
the solution of the two classification problems and the description of the
corresponding homogeneous special K\"ahler manifolds.

<id>
math/9603214v1
<category>
math.DG
<abstract>
The paper sketches a recent progress and formulates several open problems in
studying equivariant quasiconformal and quasisymmetric homeomorphisms in
negatively curved spaces as well as geometry and topology of noncompact
geometrically finite negatively curved manifolds and their boundaries at
infinity having Carnot--Carath\'eodory structures. Especially, the most
interesting are complex hyperbolic manifolds with Cauchy--Riemannian structure
at infinity, which occupy a distinguished niche and whose properties make them
surprisingly different from real hyperbolic ones.

<id>
math/9604225v1
<category>
math.DG
<abstract>
We give an estimate of the Gauss curvature for minimal surfaces in ${\mathbb
R}^m$ whose Gauss map omits more than $m(m+1)/2$ hyperplanes in ${\mathbb
P}^{m-1}({\mathbb C})$.

<id>
math/9605222v1
<category>
math.DG
<abstract>
We prove the existence of a complete, embedded, singly periodic minimal
surface, whose quotient by vertical translations has genus one and two ends.
The existence of this surface was announced in our paper in {\it Bulletin of
the AMS}, 29(1):77--84, 1993. Its ends in the quotient are asymptotic to one
full turn of the helicoid, and, like the helicoid, it contains a vertical line.
Modulo vertical translations, it has two parallel horizontal lines crossing the
vertical axis. The nontrivial symmetries of the surface, modulo vertical
translations, consist of: $180^\circ$ rotation about the vertical line;
$180^\circ$ rotation about the horizontal lines (the same symmetry); and their
composition.

<id>
math/9609210v1
<category>
math.DG
<abstract>
An intrinsic definition in terms of conformal capacity is proposed for the
conformal type of a Carnot--Carath\'eodory space (parabolic or hyperbolic).
Geometric criteria of conformal type are presented. They are closely related to
the asymptotic geometry of the space at infinity and expressed in terms of the
isoperimetric function and the growth of the area of geodesic spheres. In
particular, it is proved that a sub-Riemannian manifold admits a conformal
change of metric that makes it into a complete manifold of finite volume if and
only if the manifold is of conformally parabolic type. Further applications are
discussed, such as the relation between local and global invertibility
properties of quasiconformal immersions (the global homeomorphism theorem).

<id>
math/9201271v1
<category>
math.DS
<abstract>
This is a list of unsolved problems given at the Conformal Dynamics
Conference which was held at SUNY Stony Brook in November 1989. Problems were
contributed by the editor and the other contributors.

<id>
math/9201273v1
<category>
math.DS
<abstract>
This note will discuss the dynamics of iterated cubic maps from the real or
complex line to itself, and will describe the geography of the parameter space
for such maps. It is a rough survey with few precise statements or proofs, and
depends strongly on work by Douady, Hubbard, Branner and Rees.

<id>
math/9201274v1
<category>
math.DS
<abstract>
Invertible compositions of one-dimensional maps are studied which are assumed
to include maps with non-positive Schwarzian derivative and others whose sum of
distortions is bounded. If the assumptions of the Koebe principle hold, we show
that the joint distortion of the composition is bounded. On the other hand, if
all maps with possibly non-negative Schwarzian derivative are almost
linear-fractional and their nonlinearities tend to cancel leaving only a small
total, then they can all be replaced with affine maps with the same domains and
images and the resulting composition is a very good approximation of the
original one. These technical tools are then applied to prove a theorem about
critical circle maps.

<id>
math/9201277v1
<category>
math.DS
<abstract>
We prove a technical lemma, the $C^{1+\alpha }$-Denjoy-Koebe distortion
lemma, estimating the distortion of a long composition of a $C^{1+\alpha }$
one-dimensional mapping $f:M\mapsto M$ with finitely many, non-recurrent, power
law critical points. The proof of this lemma combines the ideas of the
distortion lemmas of Denjoy and Koebe.

<id>
math/9201278v1
<category>
math.DS
<abstract>
We study geometrically finite one-dimensional mappings. These are a subspace
of $C^{1+\alpha}$ one-dimensional mappings with finitely many, critically
finite critical points. We study some geometric properties of a mapping in this
subspace. We prove that this subspace is closed under quasisymmetrical
conjugacy. We also prove that if two mappings in this subspace are
topologically conjugate, they are then quasisymmetrically conjugate. We show
some examples of geometrically finite one-dimensional mappings.

<id>
math/9201279v1
<category>
math.DS
<abstract>
This continues the investigation of a combinatorial model for the variation
of dynamics in the family of rational maps of degree two, by concentrating on
those varieties in which one critical point is periodic. We prove some general
results about nonrational critically finite degree two branched coverings, and
finally identify the boundary of the rational maps in the combinatorial model,
thus completing the proofs of results announced in Part 1.

<id>
math/9201281v1
<category>
math.DS
<abstract>
We prove that the period doubling operator has an expanding direction at the
fixed point. We use the induced operator, a ``Perron-Frobenius type operator'',
to study the linearization of the period doubling operator at its fixed point.
We then use a sequence of linear operators with finite ranks to study this
induced operator. The proof is constructive. One can calculate the expanding
direction and the rate of expansion of the period doubling operator at the
fixed point.

<id>
math/9201282v1
<category>
math.DS
<abstract>
It is shown that the boundary of the Mandelbrot set $M$ has Hausdorff
dimension two and that for a generic $c \in \bM$, the Julia set of $z \mapsto
z^2+c$ also has Hausdorff dimension two. The proof is based on the study of the
bifurcation of parabolic periodic points.

<id>
math/9201283v1
<category>
math.DS
<abstract>
We estimate harmonic scalings in the parameter space of a one-parameter
family of critical circle maps. These estimates lead to the conclusion that the
Hausdorff dimension of the complement of the frequency-locking set is less than
$1$ but not less than $1/3$. Moreover, the rotation number is a H\"{o}lder
continuous function of the parameter.

<id>
math/9201284v1
<category>
math.DS
<abstract>
In this paper we consider the space of smooth conjugacy classes of an Anosov
diffeomorphism of the two-torus. The only 2-manifold that supports an Anosov
diffeomorphism is the 2-torus, and Franks and Manning showed that every such
diffeomorphism is topologically conjugate to a linear example, and furthermore,
the eigenvalues at periodic points are a complete smooth invariant. The
question arises: what sets of eigenvalues occur as the Anosov diffeomorphism
ranges over a topological conjugacy class? This question can be reformulated:
what pairs of cohomology classes (one determined by the expanding eigenvalues,
and one by the contracting eigenvalues) occur as the diffeomorphism ranges over
a topological conjugacy class? The purpose of this paper is to answer this
question: all pairs of H\"{o}lder reduced cohomology classes occur.

<id>
math/9201285v1
<category>
math.DS
<abstract>
The goal of this note is to prove the following theorem: Let $p_a(z) = z^2+a$
be a quadratic polynomial which has no irrational indifferent periodic points,
and is not infinitely renormalizable. Then the Lebesgue measure of the Julia
set $J(p_a)$ is equal to zero.
  As part of the proof we discuss a property of the critical point to be {\it
persistently recurrent}, and relate our results to corresponding ones for real
one dimensional maps. In particular, we show that in the persistently recurrent
case the restriction $p_a|\omega(0)$ is topologically minimal and has zero
topological entropy. The Douady-Hubbard-Yoccoz rigidity theorem follows this
result.

<id>
math/9201286v1
<category>
math.DS
<abstract>
In this paper we study measurable dynamics for the widest reasonable class of
smooth one dimensional maps. Three principle decompositions are described in
this class : decomposition of the global measure-theoretical attractor into
primitive ones, ergodic decomposition and Hopf decomposition. For maps with
negative Schwarzian derivative this was done in the series of papers [BL1-BL5],
but the approach to the general smooth case must be different.

<id>
math/9201287v1
<category>
math.DS
<abstract>
We study scaling function geometry. We show the existence of the scaling
function of a geometrically finite one-dimensional mapping. This scaling
function is discontinuous. We prove that the scaling function and the
asymmetries at the critical points of a geometrically finite one-dimensional
mapping form a complete set of $C^{1}$-invariants within a topological
conjugacy class.

<id>
math/9201288v1
<category>
math.DS
<abstract>
We study hyperbolic mappings depending on a parameter $\varepsilon $. Each of
them has an invariant Cantor set. As $\varepsilon $ tends to zero, the mapping
approaches the boundary of hyperbolicity. We analyze the asymptotics of the gap
geometry and the scaling function geometry of the invariant Cantor set as
$\varepsilon $ goes to zero. For example, in the quadratic case, we show that
all the gaps close uniformly with speed $\sqrt {\varepsilon}$. There is a
limiting scaling function of the limiting mapping and this scaling function has
dense jump discontinuities because the limiting mapping is not expanding.
Removing these discontinuities by continuous extension, we show that we obtain
the scaling function of the limiting mapping with respect to the Ulam-von
Neumann type metric.

<id>
math/9201289v1
<category>
math.DS
<abstract>
Let $X$ be a compact tree, $f$ be a continuous map from $X$ to itself,
$End(X)$ be the number of endpoints and $Edg(X)$ be the number of edges of $X$.
We show that if $n>1$ has no prime divisors less than $End(X)+1$ and $f$ has a
cycle of period $n$, then $f$ has cycles of all periods greater than
$2End(X)(n-1)$ and topological entropy $h(f)>0$; so if $p$ is the least prime
number greater than $End(X)$ and $f$ has cycles of all periods from 1 to
$2End(X)(p-1)$, then $f$ has cycles of all periods (this verifies a conjecture
of Misiurewicz for tree maps). Together with the spectral decomposition theorem
for graph maps it implies that $h(f)>0$ iff there exists $n$ such that $f$ has
a cycle of period $mn$ for any $m$. We also define {\it snowflakes} for tree
maps and show that $h(f)=0$ iff every cycle of $f$ is a snowflake or iff the
period of every cycle of $f$ is of form $2^lm$ where $m\le Edg(X)$ is an odd
integer with prime divisors less than $End(X)+1$.

<id>
math/9201290v1
<category>
math.DS
<abstract>
We construct the "spectral" decomposition of the sets $\bar{Per\,f}$,
$\omega(f)=\cup\omega(x)$ and $\Omega(f)$ for a continuous map $f$ of the
interval to itself. Several corollaries are obtained; the main ones describe
the generic properties of $f$-invariant measures, the structure of the set
$\Omega(f)\setminus \bar{Per\,f}$ and the generic limit behavior of an orbit
for maps without wandering intervals. The "spectral" decomposition for
piecewise-monotone maps is deduced from the Decomposition Theorem. Finally we
explain how to extend the results of the present paper for a continuous map of
a one-dimensional branched manifold into itself.

<id>
math/9201291v1
<category>
math.DS
<abstract>
This paper will study topological, geometrical and measure-theoretical
properties of the real Fibonacci map. Our goal was to figure out if this type
of recurrence really gives any pathological examples and to compare it with the
infinitely renormalizable patterns of recurrence studied by Sullivan. It turns
out that the situation can be understood completely and is of quite regular
nature. In particular, any Fibonacci map (with negative Schwarzian and
non-degenerate critical point) has an absolutely continuous invariant measure
(so, we deal with a ``regular'' type of chaotic dynamics). It turns out also
that geometrical properties of the closure of the critical orbit are quite
different from those of the Feigenbaum map: its Hausdorff dimension is equal to
zero and its geometry is not rigid but depends on one parameter.

<id>
math/9201292v1
<category>
math.DS
<abstract>
It is shown that some topological equivalency classes of S-unimodal maps are
equal to quasisymmetric conjugacy classes. This includes some infinitely
renormalizable polynomials of unbounded type.

<id>
math/9201293v1
<category>
math.DS
<abstract>
In this paper we consider maps on the plane which are similar to quadratic
maps in that they are degree 2 branched covers of the plane. In fact, consider
for $\alpha$ fixed, maps $f_c$ which have the following form (in polar
coordinates):
  $$f_c(r\,e^{i\theta})\;=\;r^{2\alpha}\,e^{2i\theta}\,+\,c$$
  When $\alpha=1$, these maps are quadratic ($z \maps z^2 + c$), and their
dynamics and bifurcation theory are to some degree understood. When $\alpha$ is
different from one, the dynamics is no longer conformal. In particular, the
dynamics is not completely determined by the orbit of the critical point.
Nevertheless, for many values of the parameter c, the dynamics has strong
similarities to that of the quadratic family. For other parameter values the
dynamics is dominated by 2 dimensional behavior: saddles and the like.
  The objects of study are Julia sets, filled-in Julia sets and the
connectedness locus. These are defined in analogy to the conformal case. The
main drive in this study is to see to what extent the results in the conformal
case generalize to that of maps which are topologically like quadratic maps
(and when $\alpha$ is close to one, close to being quadratic).

<id>
math/9201294v1
<category>
math.DS
<abstract>
A semigroup (dynamical system) generated by $C^{1+\alpha}$-contracting
mappings is considered. We call a such semigroup regular if the maximum $K$ of
the conformal dilatations of generators, the maximum $l$ of the norms of the
derivatives of generators and the smoothness $\alpha$ of the generators satisfy
a compatibility condition $K< 1/l^{\alpha}$. We prove the {\em geometric
distortion lemma} for a regular semigroup generated by
$C^{1+\alpha}$-contracting mappings.

<id>
math/9201295v1
<category>
math.DS
<abstract>
We use the upper and lower potential functions and Bowen's formula estimating
the Hausdorff dimension of the limit set of a regular semigroup generated by
finitely many $C^{1+\alpha}$-contracting mappings. This result is an
application of the geometric distortion lemma in the first paper at this
series.

<id>
math/9201296v1
<category>
math.DS
<abstract>
We establish that every formal critical portrait (as defined by Goldberg and
Milnor), can be realized by a postcritically finite polynomial.

<id>
math/9201297v1
<category>
math.DS
<abstract>
We prove the existence of at least $cl(M)$ periodic orbits for certain time
dependant Hamiltonian systems on the cotangent bundle of an arbitrary compact
manifold $M$. These Hamiltonians are not necessarily convex but they satisfy a
certain boundary condition given by a Riemannian metric on $M$. We discretize
the variational problem by decomposing the time 1 map into a product of
``symplectic twist maps''. A second theorem deals with homotopically non
trivial orbits in manifolds of negative curvature.

<id>
math/9201298v1
<category>
math.DS
<abstract>
Let $K$ be a compact subset of $\bar{\bold C} ={\bold R}^2$ and let $K^c$
denote its complement. We say $K\in HR$, $K$ is holomorphically removable, if
whenever $F:\bar{\bold C} \to\bar{\bold C}$ is a homeomorphism and $F$ is
holomorphic off $K$, then $F$ is a M\"obius transformation. By composing with a
M\"obius transform, we may assume $F(\infty )=\infty$. The contribution of this
paper is to show that a large class of sets are $HR$. Our motivation for these
results is that these sets occur naturally (e.g. as certain Julia sets) in
dynamical systems, and the property of being $HR$ plays an important role in
the Douady-Hubbard description of their structure.

<id>
math/9201300v1
<category>
math.DS
<abstract>
A general construction for $\sigma-$finite absolutely continuous invariant
measure will be presented. It will be shown that the local bounded distortion
of the Radon-Nykodym derivatives of $f^n_*(\lambda)$ will imply the existence
of a $\sigma-$finite invariant measure for the map $f$ which is absolutely
continuous with respect to $\lambda$, a measure on the phase space describing
the sets of measure zero. Furthermore we will discuss sufficient conditions for
the existence of $\sigma-$finite invariant absolutely continuous measures for
real 1-dimensional dynamical systems.

<id>
math/9202209v1
<category>
math.DS
<abstract>
Circle maps with a flat spot are studied which are differentiable, even on
the boundary of the flat spot. Estimates on the Lebesgue measure and the
Hausdorff dimension of the non-wandering set are obtained. Also, a sharp
transition is found from degenerate geometry similar to what was found earlier
for non-differentiable maps with a flat spot to bounded geometry as in critical
maps without a flat spot.

<id>
math/9202210v1
<category>
math.DS
<abstract>
We consider polynomial maps $f:\C\to\C$ of degree $d\ge 2$, or more generally
polynomial maps from a finite union of copies of $\C$ to itself which have
degree two or more on each copy. In any space $\p^{S}$ of suitably normalized
maps of this type, the post-critically bounded maps form a compact subset
$\cl^{S}$ called the connectedness locus, and the hyperbolic maps in $\cl^{S}$
form an open set $\hl^{S}$ called the hyperbolic connectedness locus. The
various connected components $H_\alpha\subset \hl^{S}$ are called hyperbolic
components. It is shown that each hyperbolic component is a topological cell,
containing a unique post-critically finite map which is called its center
point. These hyperbolic components can be separated into finitely many distinct
``types'', each of which is characterized by a suitable reduced mapping schema
$\bar S(f)$. This is a rather crude invariant, which depends only on the
topology of $f$ restricted to the complement of the Julia set. Any two
components with the same reduced mapping schema are canonically biholomorphic
to each other. There are similar statements for real polynomial maps, or for
maps with marked critical points.

<id>
math/9203203v1
<category>
math.DS
<abstract>
The group $SL(n,{\bf Z})$ acts linearly on $\R^n$, preserving the integer
lattice $\Z^{n} \subset \R^{n}$. The induced (left) action on the n-torus
$\T^{n} = \R^{n}/\Z^{n}$ will be referred to as the ``standard action''.
  It has recently been shown that the standard action of $SL(n,\Z)$ on $\T^n$,
for $n \geq 3$, is both topologically and smoothly rigid. That is, nearby
actions in the space of representations of $SL(n,\Z)$ into ${\rm
Diff}^{+}(\T^{n})$ are smoothly conjugate to the standard action. In fact, this
rigidity persists for the standard action of a subgroup of finite index. On the
other hand, while the $\Z$ action on $\T^{n}$ defined by a single hyperbolic
element of $SL(n,\Z)$ is topologically rigid, an infinite dimensional space of
smooth conjugacy classes occur in a neighborhood of the linear action.
  The standard action of $SL(2, \Z)$ on $\T^2$ forms an intermediate case, with
different rigidity properties from either extreme. One can construct continuous
deformations of the standard action to obtain an (arbritrarily near) action to
which it is not topologically conjugate. The purpose of the present paper is to
show that if a nearby action, or more generally, an action with some mild
Anosov properties, is conjugate to the standard action of $SL(2, \Z)$ on $\T^2$
by a homeomorphism $h$, then $h$ is smooth. In fact, it will be shown that this
rigidity holds for any non-cyclic subgroup of $SL(2, \Z)$.

<id>
math/9204240v1
<category>
math.DS
<abstract>
A semigroup generated by two dimensional $C^{1+\alpha}$ contracting maps is
considered. We call a such semigroup regular if the maximum $K$ of the
conformal dilatations of generators, the maximum $l$ of the norms of the
derivatives of generators and the smoothness $\alpha$ of the generators satisfy
a compatibility condition $K< 1/l^{\alpha}$. We prove that the shape of the
image of the core of a ball under any element of a regular semigroup is good
(bounded geometric distortion like the Koebe $1/4$-lemma \cite{a}). And we use
it to show a lower and a upper bounds of the Hausdorff dimension of the limit
set of a regular semigroup. We also consider a semigroup generated by higher
dimensional maps.

<id>
math/9204241v1
<category>
math.DS
<abstract>
Consider $d$ disjoint closed subintervals of the unit interval and consider
an orientation preserving expanding map which maps each of these subintervals
to the whole unit interval. The set of points where all iterates of this
expanding map are defined is a Cantor set. Associated to the construction of
this Cantor set is the scaling function which records the infinitely deep
geometry of this Cantor set. This scaling function is an invariant of $C^1$
conjugation. We solve the inverse problem posed by Dennis Sullivan: given a
scaling function, determine the maximal possible smoothness of any expanding
map which produces it.

<id>
math/9201202v1
<category>
math.FA
<abstract>
This is a continuation of the paper [FJS] with a similar title. Several
results from there are strengthened, in particular:
  1. If T is a "natural" embedding of l_2^n into L_1 then, for any well-bounded
factorization of T through an L_1 space in the form T=uv with v of norm one, u
well-preserves a copy of l_1^k with k exponential in n.
  2. Any norm one operator from a C(K) space which well-preserves a copy of
l_2^n also well-preserves a copy of l_{\infty}^k with k exponential in n.
  As an application of these and other results we show the existence, for any
n, of an n-dimensional space which well-embeds into a space with an
unconditional basis only if the latter contains a copy of l_{\infty}^k with k
exponential in n.

<id>
math/9201207v2
<category>
math.FA
<abstract>
We show that for any operator $T:l_\infty^N\to Y$, where $Y$ is a Banach
space, that its cotype 2 constant, $K_2(T)$, is related to its $(2,1)$-summing
norm, $\pi_{2,1}(T)$, by $K_2(T) \le c \log\log N \pi_{2,1}(T) $. Thus, we can
show that there is an operator $T:C(K)\to Y$ that has cotype 2, but is not
2-summing.

<id>
math/9201209v1
<category>
math.FA
<abstract>
In this supplement to [GJ1], [GJ3], we give an intrinsic characterization of
(bounded, linear) operators on Banach lattices which factor through Banach
lattices not containing a copy of $c_0$ which complements the characterization
of [GJ1], [GJ3] that an operator admits such a factorization if and only if it
can be written as the product of two operators neither of which preserves a
copy of $c_0$. The intrinsic characterization is that the restriction of the
second adjoint of the operator to the ideal generated by the lattice in its
bidual does not preserve a copy of $c_0$. This property of an operator was
introduced by C. Niculescu [N2] under the name ``strong type B".

<id>
math/9201210v1
<category>
math.FA
<abstract>
Let $X$ be a compact Hausdorff space, let $E$ be a Banach space, and let
$C(X,E)$ stand for the Banach space of $E$-valued continuous functions on $X$
under the uniform norm. In this paper we characterize Integral operators (in
the sense of Grothendieck) on $C(X,E)$ spaces in term of their representing
vector measures. This is then used to give some applications to Nuclear
operators on $C(X,E)$ spaces.

<id>
math/9201211v1
<category>
math.FA
<abstract>
Let $\Omega$ be a compact Hausdorff space, let $E$ be a Banach space, and let
$C(\Omega, E)$ stand for the Banach space of all $E$-valued continuous
functions on $\Omega$ under supnorm. In this paper we study when nuclear
operators on $C(\Omega, E)$ spaces can be completely characterized in terms of
properties of their representing vector measures. We also show that if $F$ is a
Banach space and if $T:\ C(\Omega, E)\rightarrow F$ is a nuclear operator, then
$T$ induces a bounded linear operator $T^\#$ from the space $C(\Omega)$ of
scalar valued continuous functions on $\Omega$ into $\slN(E,F)$ the space of
nuclear operators from $E$ to $F$, in this case we show that $E^*$ has the
Radon-Nikodym property if and only if $T^\#$ is nuclear whenever $T$ is
nuclear.

<id>
math/9201212v2
<category>
math.FA
<abstract>
If Z is a quotient of a subspace of a separable Banach space X, and V is any
separable Banach space, then there is a Banach couple (A_0,A_1) such that A_0
and A_1 are isometric to $X\oplus V$, and any intermediate space obtained using
the real or complex interpolation method contains a complemented subspace
isomorphic to Z. Thus many properties of Banach spaces, including having
non-trivial cotype, having the Radon-Nikodym property, and having the analytic
unconditional martingale difference sequence property, do not pass to
intermediate spaces.

<id>
math/9201213v1
<category>
math.FA
<abstract>
General permutations acting on the Haar system are investigated. We give a
necessary and sufficient condition for permutations to induce an isomorphism on
dyadic BMO. Extensions of this characterization to Lipschitz spaces $\lip,
(0<p\leq1)$ are obtained. When specialized to permutations which act on one
level of the Haar system only, our approach leads to a short straightforward
proof of a result due to E.M.Semyonov and B.Stoeckert.

<id>
math/9201214v1
<category>
math.FA
<abstract>
In this paper we prove some results related to the problem of isomorphically
classifying the complemented subspaces of $X_{p}$. We characterize the
complemented subspaces of $X_{p}$ which are isomorphic to $X_{p}$ by showing
that such a space must contain a canonical complemented subspace isomorphic to
$X_{p}.$ We also give some characterizations of complemented subspaces of
$X_{p}$ isomorphic to $\ell_{p}\oplus \ell_{2}.$

<id>
math/9201215v2
<category>
math.FA
<abstract>
Let $X,Y$ and $Z$ be Banach spaces, and let $\prod_p(Y,Z) (1\leq p<\infty)$
denote the space of $p$-summing operators from $Y$ to $Z$. We show that, if $X$
is a {\it \$}$_\infty$-space, then a bounded linear operator $T: X\hat
\otimes_\epsilon Y\longrightarrow Z$ is 1-summing if and only if a naturally
associated operator $T^#: X\longrightarrow \prod_1(Y,Z)$ is 1-summing. This
result need not be true if $X$ is not a {\it \$}$_\infty$-space. For $p>1$,
several examples are given with $X=C[0,1]$ to show that $T^#$ can be
$p$-summing without $T$ being $p$-summing. Indeed, there is an operator $T$ on
$C[0,1]\hat \otimes_\epsilon \ell_1$ whose associated operator $T^#$ is
2-summing, but for all $N\in \N$, there exists an $N$-dimensional subspace $U$
of $C[0,1]\hat \otimes_\epsilon \ell_1$ such that $T$ restricted to $U$ is
equivalent to the identity operator on $\ell^N_\infty$. Finally, we show that
there is a compact Hausdorff space $K$ and a bounded linear operator $T:\
C(K)\hat \otimes_\epsilon \ell_1\longrightarrow \ell_2$ for which $T^#:\
C(K)\longrightarrow \prod_1(\ell_1, \ell_2)$ is not 2-summing.

<id>
math/9201216v1
<category>
math.FA
<abstract>
We introduce a concentration property for probability measures on
$\scriptstyle{R^n}$, which we call Property~($\scriptstyle\tau$); we show that
this property has an interesting stability under products and contractions
(Lemmas 1,~2,~3). Using property~($\scriptstyle\tau$), we give a short proof
for a recent deviation inequality due to Talagrand. In a third section, we also
recover known concentration results for Gaussian measures using our approach.}

<id>
math/9201219v1
<category>
math.FA
<abstract>
It is proved that if a Banach space $Y$ is a quotient of a Banach space
having a shrinking unconditional basis, then every normalized weakly null
sequence in $Y$ has an unconditional subsequence. The proof yields the
corollary that every quotient of Schreier's space is $c_o$-saturated.

<id>
math/9201220v1
<category>
math.FA
<abstract>
We prove that a Banach space has the uniform approximation property with
proportional growth of the uniformity function iff it is a weak Hilbert space.

<id>
math/9201221v2
<category>
math.FA
<abstract>
Orlicz-Lorentz spaces provide a common generalization of Orlicz spaces and
Lorentz spaces. They have been studied by many contributors, including Masty\l o,
Maligranda, and Kami\'nska. In this paper, we consider the problem of comparing
the Orlicz-Lorentz norms, and establish necessary and sufficient conditions for
them to be equivalent. As a corollary, we give necessary and sufficient
conditions for a Lorentz-Sharpley space to be equivalent to an Orlicz space,
extending results of Lorentz and Raynaud. We also give an example of a
rearrangement invariant space that is not an Orlicz-Lorentz space.

<id>
math/9201222v1
<category>
math.FA
<abstract>
A non RNP Banach space E is constructed such that $E^{*}$ is separable and
RNP is equivalent to PCP on the subsets of E.

<id>
math/9201223v1
<category>
math.FA
<abstract>
A result of Nymann is extended to show that a positive $\sigma$-finite
measure with range an interval is determined by its level sets. An example is
given of two finite positive measures with range the same finite union of
intervals but with the property that one is determined by its level sets and
the other is not.

<id>
math/9201224v1
<category>
math.FA
<abstract>
Let $(x_n)$ be a normalized weakly null sequence in a Banach space and let
$\varep>0$. We show that there exists a subsequence $(y_n)$ with the following
property: $$\hbox{ if }\ (a_i)\subseteq \IR\ \hbox{ and }\ F\subseteq \nat$$
satisfies $\min F\le |F|$ then $$\big\|\sum_{i\in F} a_i y_i\big\| \le
(2+\varep) \big\| \sum a_iy_i\big\|\ . $$

<id>
math/9201225v1
<category>
math.FA
<abstract>
In this work we construct a ``Tsirelson like Banach space'' which is
arbitrarily distortable.

<id>
math/9201226v1
<category>
math.FA
<abstract>
In this paper, equivalence between interpolation properties of linear
operators and monotonicity conditions are studied, for a pair $(X_0,X_1)$ of
rearrangement invariant quasi Banach spaces, when the extreme spaces of the
interpolation are $L^\infty$ and a pair $(A_0,A_1)$ under some assumptions.
Weak and restricted weak intermediate spaces fall in our context. Applications
to classical Lorentz and Lorentz-Orlicz spaces are given.

<id>
math/9201228v1
<category>
math.FA
<abstract>
We give a simple proof of Bourgain's disc algebra version of Grothendieck's
theorem, i.e. that every operator on the disc algebra with values in $L_1$ or
$L_2$ is 2-absolutely summing and hence extends to an operator defined on the
whole of $C$. This implies Bourgain's result that $L_1/H^1$ is of cotype 2. We
also prove more generally that $L_r/H^r$ is of cotype 2 for $0<r< 1$.

<id>
math/9201229v1
<category>
math.FA
<abstract>
We give an elementary proof that the $H^p$ spaces over the unit disc (or the
upper half plane) are the interpolation spaces for the real method of
interpolation between $H^1$ and $H^\infty$. This was originally proved by Peter
Jones. The proof uses only the boundedness of the Hilbert transform and the
classical factorisation of a function in $H^p$ as a product of two functions in
$H^q$ and $H^r$ with $1/q+1/r=1/p$. This proof extends without any real extra
difficulty to the non-commutative setting and to several Banach space valued
extensions of $H^p$ spaces. In particular, this proof easily extends to the
couple $H^{p_0}(\ell_{q_0}),H^{p_1}(\ell_{q_1})$, with $1\leq p_0, p_1, q_0,
q_1 \leq \infty$. In that situation, we prove that the real interpolation
spaces and the K-functional are induced ( up to equivalence of norms ) by the
same objects for the couple $L_{p_0}(\ell_{q_0}), L_{p_1}(\ell_{q_1})$. In
another direction, let us denote by $C_p$ the space of all compact operators
$x$ on Hilbert space such that $tr(|x|^p) <\infty$. Let $T_p$ be the subspace
of all upper triangular matrices relative to the canonical basis. If
$p=\infty$, $C_p$ is just the space of all compact operators. Our proof allows
us to show for instance that the space $H^p(C_p)$ (resp. $T_p$) is the
interpolation space of parameter $(1/p,p)$ between $H^1(C_1)$ (resp. $T_1$) and
$H^\infty(C_\infty)$ (resp. $T_\i$). We also prove a similar result for the
complex interpolation method. Moreover, extending a recent result of
Kaftal-Larson and Weiss, we prove that the distance to the subspace of upper
triangular matrices in $C_1$ and $C_\infty$ can be essentially realized
simultaneously by the same element.

<id>
math/9201230v1
<category>
math.FA
<abstract>
A Banach space E is said to have Property (w) if every (bounded linear)
operator from E into E' is weakly compact. We give some interesting examples of
James type Banach spaces with Property (w). We also consider the passing of
Property (w) from E to C(K,E).

<id>
math/9201231v1
<category>
math.FA
<abstract>
We prove a new inequality for Gaussian processes, this inequality implies the
Gordon-Chevet inequality. Some remarks on Gaussian proofs of Dvoretzky's
theorem are given.

<id>
math/9201232v1
<category>
math.FA
<abstract>
Let (A_0,A_1) be a compatible couple of Banach spaces in the interpolation
theory sense. We give a formula for the K_t-functional of the interpolation
couples (l_1(A_0),c_0(A_1)) or (l_1(A_0),l_infinity(A_1)) and
(L_1(A_0),L_infinity(A_1)).

<id>
math/9201233v1
<category>
math.FA
<abstract>
A Banach space $X$ is reflexive if the Mackey topology $\tau(X^*,X)$ on $X^*$
agrees with the norm topology on $X^*$. Borwein [B] calls a Banach space $X$
{\it sequentially reflexive\/} provided that every $\tau(X^*,X)$ convergent
{\it sequence\/} in $X^*$ is norm convergent. The main result in [B] is that
$X$ is sequentially reflexive if every separable subspace of $X$ has separable
dual, and Borwein asks for a characterization of sequentially reflexive spaces.
Here we answer that question by proving
  \proclaim Theorem. {\sl A Banach space $X$ is sequentially reflexive if and
only if $\ell_1$ is not isomorphic to a subspace of $X$.}

<id>
math/9201234v1
<category>
math.FA
<abstract>
We study biorthogonal sequences with special properties, such as weak or
weak-star convergence to 0, and obtain an extension of the Josefson-Nissenzweig
theorem. This result is applied to embed analytic disks in the fiber over 0 of
the spectrum of H^infinity (B), the algebra of bounded analytic functions on
the unit ball B of an arbitrary infinite dimensional Banach space. Various
other embedding theorems are obtained. For instance, if the Banach space is
superreflexive, then the unit ball of a Hilbert space of uncountable dimension
can be embedded analytically in the fiber over 0 via an embedding which is
uniformly bicontinuous with respect to the Gleason metric.

<id>
math/9201235v2
<category>
math.FA
<abstract>
Let B denote an arbitrary Banach space, G a compact abelian group with Haar
measure $\mu$ and dual group $\Gamma$. Let E be a Sidon subset of $\Gamma$ with
Sidon constant S(E). Let r_n denote the n-th Rademacher function on [0, 1]. We
show that there is a constant c, depending only on S(E), such that, for all
$\alpha > 0$: c^{-1}P[| \sum_{n=1}^Na_nr_n| >= c \alpha ] <= \mu[|
\sum_{n=1}^Na_n\gamma_n| >= \alpha ] <= cP [|\sum_{n=1}^Na_nr_n| >= c^{-1}
\alpha ]

<id>
math/9201236v1
<category>
math.FA
<abstract>
Certain subclasses of $B_1(K)$, the Baire-1 functions on a compact metric
space $K$, are defined and characterized. Some applications to Banach spaces
are given.

<id>
math/9201237v1
<category>
math.FA
<abstract>
It is shown that the weak $L^p$ spaces $\ell^{p,\infty}, L^{p,\infty}[0,1]$,
and $L^{p,\infty}[0,\infty)$ are isomorphic as Banach spaces.

<id>
math/9202202v1
<category>
math.FA
<abstract>
We discuss relationships between the McShane, Pettis, Talagrand and Bochner
integrals. A large number of different methods of integration of
Banach-space-valued functions have been introduced, based on the various
possible constructions of the Lebesgue integral. They commonly run fairly
closely together when the range space is separable (or has w^*-separable dual)
and diverge more or less sharply for general range spaces. The McShane integral
as described by [Go] is derived from the `gauge-limit' integral of [McS]. Here
we give both positive and negative results concerning it and the other three
integrals listed above.

<id>
math/9202203v1
<category>
math.FA
<abstract>
For a Banach space X we define RUMD_n(X) to be the infimum of all c>0 such
that (AVE_{\epsilon_k =\pm 1} || \sum_1^n epsilon_k (M_k - M_{k-1}
)||_{L_2^X}^2 )^{1/2} <= c || M_n ||_{L_2^X} holds for all Walsh-Paley
martingales {M_k}_0^n subset L_2^X with M_0 =0. We relate the asymptotic
behaviour of the sequence {RUMD(X)}_{n=1}^{infinity} to geometrical properties
of the Banach space X such as K-convexity and superreflexivity.

<id>
math/9808044v1
<category>
math.GM
<abstract>
Here is present short proofing of Jordan's theorem about dividing of flat on
two disjoint subsets by one closed curve.

<id>
math/9810027v5
<category>
math.GM
<abstract>
The recently developed proof of Fermat's Last Theorem is very lengthy and
difficult, so much so as to be beyond all but a small body of specialists.
While certainly of value in the developments that resulted, that proof could
not be, nor was offered as being, possibly the proof Fermat had in mind. The
present proof being brief, direct and concise is a candidate for being what
Fermat had in mind. It is also completely accessible to any one trained in
common algebra. That critical suggestions offered by significant mathematics
contributorities have been unable to invalidate this concise and direct proof would
tend to be major confirmation that: The proof stands, valid and not validly
challenged.

<id>
math/9903081v12
<category>
math.GM
<abstract>
As of the date of this version, this monograph (parts I and II) contains all
of the known technical results relative to the Robinson-styled nonstandard
modeling of natural languages and certain associated linguistic processes such
as deduction via consequence operators among other concepts. These results have
direct application to the construction of the GGU-model, the GID-model, the
GD-model, the MA-model and also apply to philosophy, psychology, properton
theory and other aspects of the Nonstandard Physical World (NSP-world).

<id>
math/9903082v11
<category>
math.GM
<abstract>
As of this date of this version, this monograph (part I and II) contains most
of the technical results relative to the Robinson-styled nonstandard modeling
of natural languages and certain associated linguistic processes such as
deduction via consequence operators among other concepts. These results have
direct application to the construction of the GGU-model, GID-model, D-world
model, MA-model and also apply to philosophy, psychology, properton theory and
other aspects of the Nonstandard Physical World (NSP-world).

<id>
math/9909153v1
<category>
math.GM
<abstract>
Through an equivalent condition on the Farey series set forth by Franel and
Landau, we prove Riemann Hypothesis for the Riemann zeta-function and the
Dirichlet L-function.

<id>
math/9909154v1
<category>
math.GM
<abstract>
We solve Landau's four unattackable problems, including Goldbach Conjecture
and Twin Prime Conjecture through sieve method.

<id>
math/9911147v1
<category>
math.GM
<abstract>
The interactive game theoretical approach to tactics and behavioral
self-organization is developed. Though it uses the interactive game theoretical
formalization of dialogues as psycholinguistic phenomena, the crucial role is
played by the essentially new concept of a tactical game. Applications to the
perception processes and related subjects (memory, recollection, image
understanding, imagination) are discussed together with relations to the
computer vision and pattern recognition (the dynamical formation of patterns
and perception models during perception as a result of its self-organization)
and computer games (modelling of the tactical behavior and self-organization,
tactical RPG and elaboration of new tactical game techniques). The appendix is
devoted to the operative computer games and the user programming of operative
units in a multi-user online operative computer game.

<id>
math/9912090v1
<category>
math.GM
<abstract>
It is shown that for finding rational approximates to m'th root of any
integer to any accuracy one only needs the ability to count and to distinguish
between m different classes of objects. To every integer N can be associated a
'replacement rule' that generates a word W* from another word W consisting of
symbols belonging to a finite 'alphabet' of size m. This rule applied
iteratively on almost any initial word W0, yields a sequence of words {Wi} such
that the relative frequency of different symbols in the word Wi approaches
powers of the m'th root of N as i tends to infinity

<id>
math/9912230v1
<category>
math.GM
<abstract>
To every integer monic polynomial of degree m can be associated a
`replacement rule' that generates a word W* from another word W consisting of
symbols belonging to a finite `alphabet' of size 2m. This rule applied
iteratively on almost any initial word Wo, yields a sequence of words {Wi}.
  From acount of different symbols in the word Wi, one can obtain a rational
approximate to the largest real root of the polynomial.

<id>
math/0001012v1
<category>
math.GM
<abstract>
Simple divisibility rules are given for the 1st 1000 prime numbers.

<id>
math/0001032v1
<category>
math.GM
<abstract>
This article is devoted to the tactical game theoretical interpretation of
dialectics. Dialectical games are considered as abstractly as well as models of
the internal dialogue and reflection. The models related to the representation
theory (representative dynamics) are specially investigated in detail, they
correlate with the hypothesis on the dialectical features of human thinking in
general and mathematical thought (the constructing of a solution of
mathematical problem) in particular.

<id>
math/0001112v1
<category>
math.GM
<abstract>
To every integer monic polynomial of degree m can be associated m integer
sequences having interesting properties to the roots of the polynomial. These
sequences can be used to find the real roots of any integer monic polynomial by
using recursion relation involving integers only. This method is faster than
the conventional methods using floating point arithmetic.

<id>
math/0001144v1
<category>
math.GM
<abstract>
The roots of any polynomial of degree m with integer coefficients, can be
computed by manipulation of sequences made from 2m distinct symbols and
counting the different symbols in the sequences. This method requires only
'primitive' operations like replacement of sequences and counting of symbols.
No calculations using 'advanced' operations like multiplication, division,
logarithms etc. are needed. The method can be implemented as a geometric
construction of roots of polynomials to arbitrary accuracy using only a
straight edge, a compass, and pencils of 2m different colors. In particular,
the ancient problem of the "doubling of cube" is soluble asymptotically by the
above-mentioned construction. This method, by which a cube can be doubled,
albeit, in infinite steps, is probably the closest to the original problem of
construction using only a straight edge and compass in a finite number of
steps.
  Moreover, to every polynomial of degree m over the field of rationals, can be
associated an m-term recurrence relation for generating integer sequences. A
set of m such sequences, which together exhibit interesting properties related
to the roots of the polynomial, can be obtained if the m initial terms of each
of these m sequences is chosen in a special way using a matrix associated with
the polynomial. Only two of these integer sequences need to be computed to
obtain the real root having the largest absolute value. Since this method
involves only integers, it is faster than the conventional methods using
floating-point arithmetic.

<id>
math/0001174v1
<category>
math.GM
<abstract>
The roots of any polynomial of degree m with complex integer coefficients can
be computed by manipulation of sequences made from distinct symbols and
counting the different symbols in the sequences. This method requires only
primitive operations like replacement of sequences and counting of symbols. No
calculations using advanced operations like multiplication, division,
logarithms etc. are needed. The method can be implemented as a geometric
construction using only a ruler and a compass.

<id>
math/0002059v3
<category>
math.GM
<abstract>
We present a multi-parameter non-constant-invariant class of Abel ordinary
differential equations with the following remarkable features. This one class
is shown to unify, that is, contain as particular cases, all the integrable
classes presented by Abel, Liouville and Appell, as well as all those shown in
Kamke's book and various other references. In addition, the class being
presented includes other new and fully integrable subclasses, as well as the
most general parameterized class of which we know whose members can
systematically be mapped into Riccati equations. Finally, many integrable
members of this class can be systematically mapped into an integrable member of
a different class. We thus find new integrable classes from previously known
ones.

<id>
math/0002134v1
<category>
math.GM
<abstract>
Classical problem of random triangle in square is solved by simple and
transparent geometrical method.

<id>
math/0002227v1
<category>
math.GM
<abstract>
The notion of 'bifurcating continued fractions' is introduced. Two coupled
sequences of non-negative integers are obtained from an ordered pair of
positive real numbers in a manner that generalizes the notion of continued
fractions. These sequences enable simple representations of roots of cubic
equations. In particular, remarkably simple and elegant 'bifurcating continued
fraction' representations of Tribonacci and Moore numbers, the cubic variations
of the 'golden mean', are obtained. This is further generalized to associate m
non-negative integer sequences with a set of m given real numbers so as to
provide simple 'bifurcating continued fraction' representation of roots of
polynomial equations of degree m+1.

<id>
math/0003001v1
<category>
math.GM
<abstract>
The article is devoted to mathematical methods of experimental detection of
interactive phenomena in complex systems and their analysis.

<id>
math/0003112v2
<category>
math.GM
<abstract>
We give a formula for matrix exponentials and partial fraction
decompositions.

<id>
math/0003122v3
<category>
math.GM
<abstract>
I try to find natural statement and proof of the de Rham Theorem and of other
cohomology theorems.

<id>
math/0005026v1
<category>
math.GM
<abstract>
The motivation behind this note, is due to the non success in finding the
complete solution to the General Quintic Equation. The hope was to have a
solution with all the parameters precisely calculated in a straight forward
manner. This paper gives the closed form solution for the five roots of the
General Quintic Equation. They can be generated on Maple V, or on the new
version Maple VI. On the new version of maple, Maple VI, it may be possible to
insert all the substitutions calculated in this paper, into one another, and
construct one large equation for the Tschirnhausian Transformation. The
solution also uses the Generalized Hypergeometric Function which Maple V can
calculate, robustly.

<id>
math/0005141v1
<category>
math.GM
<abstract>
Stated lemma contains the assertions about isomorphism of exact m-forms and
exterior differentials of regular m-maps, of linearly harmonic m-forms and
exterior differentials of regular harmonic m-maps, of global minimal
(n-m)-surfaces and level (n-m)-surfaces of regular minimal m-maps. It hold in
n-dimensional Euclidean space.

<id>
math/0005185v2
<category>
math.GM
<abstract>
"Goldbach's Conjecture" proven by analysis of how all combinations of the odd
primes, summed in pairs, generates all of the even numbers.

<id>
math/0005188v1
<category>
math.GM
<abstract>
It is investigated a possibility of physical interpretation of vector fields
(dynamic flows) in Euclidean spaces of higher dimension. There are analyzed the
methods of measurements of dynamic flows, the characteristics of dynamic flow
and the connection between its differential and integral characteristics. It is
obtained the criterion of local minimality of (n-1)-surfaces that is not
connected with interior geometry of surface. It is analyzed some analogy
between harmonicity of dynamic flows and dynamic principle of nature.

<id>
math/0005189v1
<category>
math.GM
<abstract>
It is shown that application of dynamic flows concept in 4-dimensional
Euclidean space makes possible to form Minkowski space and to formulate the
generalized variational problem of electrodynamics and gravi- dynamics. It is
shown that 1-dimensional (cylindrical) factorization of 4-dimensional Euclidean
space provides a quantization of ths model.

<id>
math/0005191v1
<category>
math.GM
<abstract>
Goldbach`s Conjecture, "every even number greater than 2 can be expressed as
the sum of two primes" is renamed Goldbach`s Rule for it can not be otherwise.
The conjecture is proven by showing that the existence of prime pairs adding to
any even number greater than 2 is a natural by-product of the existence of the
prime sequence less than that even number. First it is shown that the remainder
of cancellations process which identifies primes less than an even number also
remainders prime pairs adding to that even number as a natural part of the
process. Then a minimum limit for the remaindered number of prime pairs adding
to an even number is expressed in terms of that even number and shown to exist
for every even number greater than 2. Furthermore, the reasonings and
formulations used in the proof are demonstrated to hold against observations.

<id>
math/0005213v1
<category>
math.GM
<abstract>
It is described the group of arrowy permutations (that is extension of
symmetric group) and the consequent process of generation of GL(n) and some its
subgroups by this combinatoric group and its subgroups.

<id>
math/0005214v1
<category>
math.GM
<abstract>
It is shown that the groups of automorphisms of Euclidean spaces are
isomorphic to the groups of topologic automorphisms of respectively factored
arithmetic spaces. In particular, the geometry of Euclidean n-space with
positive signature is associated with factorization of n-dimensional arithmetic
space into n-dimensional sphere.

<id>
math/0005215v1
<category>
math.GM
<abstract>
It is shown that classical Clifford algebras are group algebras of cyclic
subgroups of arrowy rermutations. It is established that Euclidean 3-space,
Pauli and Dirac algebras and groups of global guage transformations are
corollary from the geometry of 8-dimensional vacuum and 9-dimensional cosmos.

<id>
math/0005216v1
<category>
math.GM
<abstract>
It is constructed the functor from category of product linear space to
category of skew-symmetric tensor space. It is defined and described the bound
bundle as analog of a symplex and as basis element of new constructive homology
theory.

<id>
math/9712206v1
<category>
math.GN
<abstract>
K. Kuperberg found a locally connected, finite-dimensional continuum which is
homogeneous but not bihomogeneous. We give a similar but simpler example. Like
previous constructions, the example is locally a Cartesian product of Menger
spaces. The new idea is to choose a fundamental group in which not every
element is conjugate to its inverse.

<id>
math/9806092v1
<category>
math.GN
<abstract>
A hereditarily indecomposable tree-like continuum without the fixed point
property is constructed. The example answers a question of Knaster and Bellamy.

<id>
math/9809068v1
<category>
math.GN
<abstract>
The aim of this paper is to continue the study of sg-compact spaces, a
topological notion much stronger than hereditary compactness. We investigate
the relations between sg-compact and $C_2$-spaces and the interrelations to
hereditarily sg-closed sets.

<id>
math/9810056v1
<category>
math.GN
<abstract>
We discuss the problem of finding an analogue of the concept of a topological
space in supergeometry, motivated by a search for a procedure to compactify a
supermanifold along odd coordinates. In particular, we examine the topologies
naturally arising on the sets of points of locally ringed superspaces, and show
that in the presence of a nontrivial odd sector such topologies are never
compact. The main outcome of our discussion is that not only the usual
framework of supergeometry (the theory of locally ringed spaces), but the more
general approach of the functor of points, need to be further enlarged.

<id>
math/9810072v1
<category>
math.GN
<abstract>
Recently, Mr\v{s}evi\'{c} and Reilly discussed some covering properties of a
topological space and its associated $\alpha$-topology in both topological and
bitopological ways. The main aim of this paper is to investigate some common
and controversial covering properties of $\cal T$ and ${\cal T}^{\alpha}$.

<id>
math/9810074v1
<category>
math.GN
<abstract>
The aim of this paper is to introduce a new weak separation axiom that
generalizes the separation properties between $T_1$ and completely Hausdorff.
We call a topological space $(X,\tau)$ a $T_{\kappa,\xi}$-space if every
compact subset of $X$ with cardinality $\leq \kappa$ is $\xi$-closed, where
$\xi$ is a general closure operator. We concentrate our attention mostly on two
new concepts: kd-spaces and $T_{1/3}$-spaces.

<id>
math/9810075v1
<category>
math.GN
<abstract>
An ideal is a nonempty collection of subsets closed under heredity and finite
additivity. The aim of this paper is to unify some weak separation properties
via topological ideals. We concentrate our attention on the separation axioms
between $T_0$ and $T_2$. We prove that if $(X,\tau,{\cal I})$ is a
semi-Alexandroff $T_{\cal I}$-space and $\cal I$ is a $\tau$-boundary, then
$\cal I$ is completely codense.

<id>
math/9810076v1
<category>
math.GN
<abstract>
The aim of this note is to show that every subset of a given topological
space is the intersection of a preopen and a preclosed set, therefore
$\beta$-locally closed, and that every topological space is $\beta$-submaximal.

<id>
math/9810077v1
<category>
math.GN
<abstract>
The aim of this paper is to show that every scattered subset of a
dense-in-itself semi-$T_D$-space is nowhere dense. We are thus able to answer a
recent question of Coleman in the affirmative. In terms of Digital Topology, we
prove that in semi-$T_D$-spaces with no open screen, trace spaces have no
consolidations.

<id>
math/9810078v1
<category>
math.GN
<abstract>
In this paper we will continue the study of p-closed spaces. This class of
spaces is strictly placed between the class of strongly compact spaces and the
class of quasi-H-closed spaces. We will provide new characterizations of
p-closed spaces and investigate their relationships with some other classes of
topological spaces.

<id>
math/9810079v1
<category>
math.GN
<abstract>
The aim of this paper is to introduce and study the concept of a
contra-semicontinuous function and further investigate the class of strongly
$S$-closed spaces. We obtain some new decompositions of generalized continuous
functions.

<id>
math/9810080v1
<category>
math.GN
<abstract>
In this paper we define the concepts of $g.\Lambda_s$-sets and $g.V_s$-sets
and we use them in order to obtain new characterizations of semi-T_1-,
semi-R_0- and semi-T_{1/2}-spaces.

<id>
math/9810128v1
<category>
math.GN
<abstract>
This paper considers the question of which continua are 2-to-1 retracts of
continua.

<id>
math/9810129v1
<category>
math.GN
<abstract>
Many mathematicians encounter k-to-1 maps only in the study of covering maps.
But, of course, k-to-1 maps do not have to be open. This paper touches on
covering maps, and simple maps, but concentrates on ordinary k-to-1 functions
(both continuous and finitely discontinuous) from one metric continuum to
another. New results, old results and ideas for further research are given; and
a baker's dozen of questions are raised.

<id>
math/9810174v1
<category>
math.GN
<abstract>
The aim of this paper is to continue the study of sg-compact spaces. The
class of sg-compact spaces is a proper subclass of the class of hereditarily
compact spaces. In our paper we shall consider sg-compactness in product
spaces. Our main result says that if a product space is sg-compact, then either
all factor spaces are finite, or exactly one factor space is infinite and
sg-compact and the remaining ones are finite and locally indiscrete.

<id>
math/9810175v1
<category>
math.GN
<abstract>
Recently, Saleh claimed to have solved `a long standing open question' in
Topology; namely, he proved that every almost continuous function is closure
continuous (= $\theta$-continuous). Unfortunately, this problem was settled
long time ago and even a better result is known.

<id>
math/9810176v2
<category>
math.GN
<abstract>
We answer a recent question of Pyrih by proving that a topological space
$(X,\tau)$ is open-normal if and only if it is extremally disconnected.

<id>
math/9810177v1
<category>
math.GN
<abstract>
The aim of this survey article is to cover most of the recent research on
preopen sets. I try to present majority of the results on preopen sets that I
am aware of.

<id>
math/9811002v1
<category>
math.GN
<abstract>
The aim of this paper is to introduce the class of ${\cal A}{\cal B}$-sets as
the sets that are the intersection of an open and a semi-regular set. Several
classes of well-known topological spaces are characterized via the new concept.
A new decomposition of continuity is provided.

<id>
math/9811003v1
<category>
math.GN
<abstract>
This paper covers some recent progress in the study of sg-open sets,
sg-compact spaces, N-scattered spaces and some related concepts. A subset $A$
of a topological space $(X,\tau)$ is called sg-closed if the semi-closure of
$A$ is included in every semi-open superset of $A$. Complements of sg-closed
sets are called sg-open. A topological space $(X,\tau)$ is called sg-compact if
every cover of $X$ by sg-open sets has a finite subcover. N-scattered space is
a topological spaces in which every nowhere dense subset is scattered.

<id>
math/9812037v1
<category>
math.GN
<abstract>
The aim of this paper is introduce and initiate the study of extremally
$T_1$-spaces, i.e., the spaces where all hereditarily compact $C_2$-subspaces
are closed. A $C_2$-space is a space whose nowhere dense sets are finite.

<id>
math/9812038v1
<category>
math.GN
<abstract>
A topological space $(X,\tau)$ is called a locally LC-space if every point of
$X$ has a neighborhood $U$ such that every Lindel\"{o}f subset of $(U,\tau|U)$
is a closed subset of $(U,\tau|U)$. The aim of this paper is to continue the
study of locally LC-spaces.

<id>
math/9901017v1
<category>
math.GN
<abstract>
In 1990, Ganster and Reilly proved that a function is continuous if and only
if it is precontinuous and LC-continuous. In this paper we extend their
decomposition of continuity in terms of ideals. We show that a function $f
\colon (X,\tau,{\cal I}) \to (Y,\sigma)$ is continuous if and only if it is
pre-I-continuous and I-LC-continuous. We also provide a decomposition of
I-continuity.

<id>
math/9902120v1
<category>
math.GN
<abstract>
The natural duality between "topological" and "regular," both considered as
convergence space properties, extends naturally to p-regular convergence
spaces, resulting in the new concept of a p-topological convergence space.
Taking advantage of this duality, the behavior of p-topological and p-regular
convergence spaces is explored, with particular emphasis on the former, since
they have not been previously studied. Their study leads to the new notion of a
neighborhood operator for filters, which in turn leads to an especially simple
characterization of a topology in terms of convergence criteria. Applications
include the topological and regularity series of a convergence space.

<id>
math/9904162v1
<category>
math.GN
<abstract>
By a Knaster continuum we understand the inverse limit of copies of [0,1]
with open bonding maps. We prove that for any two Knaster continua K_1 and K_2,
there are 2^\aleph_0 distinct homotopy types of maps of K_1 onto K_2 that map
the endpoint of K_1 to the endpoint of K_2.

<id>
math/9906162v1
<category>
math.GN
<abstract>
An example is given of a compact absolute retract that is not a Hilbert cube
manifold but whose second symmetric porduct is the Hilbert cube. A factor
theorem is given for nth symmetric product of the cartesian product of any
absolute neighborhood retract with the Hilbert cube. A short proof is included
of the known fact that symmetric products preserve the property of being a
compact Hilbert cube manifold (the theorem is proved here for all Hilbert cube
manifolds).

<id>
math/9907192v2
<category>
math.GN
<abstract>
We establish some basic theorems in dimension theory and absolute extensor
theory in the coarse category of metric spaces. Some of the statements in this
category can be translated in general topology language by applying the Higson
corona functor. The relation of problems and results of this `Asymptotic
Topology' to Novikov and similar conjectures is discussed.

<id>
math/9908072v1
<category>
math.GN
<abstract>
We characterize AE(0)-spaces that are Baire isomorphic to the powers of the
real line.

<id>
math/9908073v1
<category>
math.GN
<abstract>
We show that for each countable simplicial complex P the following conditions
are equivalent: (1) $P \in AE(X)$ iff $P \in AE(\beta X)$ for any space X; (2)
There exists a P-invertible map of a metrizable compactum X with $P \in AE(X)$
onto the Hilbert cube.

<id>
math/9908074v1
<category>
math.GN
<abstract>
It is shown, under the assumption of Jensen's principle $\lozenge$, that if
for a complex L with $[L] \geq [S^{4}]$ there exists a metrizable compactum
whose extension dimension is L, then there exists a differentiable, countably
compact, perfectly normal and hereditarily separable 4-manifold whose extension
dimension is also [L].

<id>
math/9301212v1
<category>
math.GT
<abstract>
A physically natural potential energy for simple closed curves in $\bold R^3$
is shown to be invariant under M\"obius transformations. This leads to the
rapid resolution of several open problems: round circles are precisely the
absolute minima for energy; there is a minimum energy threshold below which
knotting cannot occur; minimizers within prime knot types exist and are
regular. Finally, the number of knot types with energy less than any constant
$M$ is estimated.

<id>
math/9304209v1
<category>
math.GT
<abstract>
In this article we shall give an account of certain developments in knot
theory which followed upon the discovery of the Jones polynomial in 1984. The
focus of our account will be recent glimmerings of understanding of the
topological meaning of the new invariants. A second theme will be the central
role that braid theory has played in the subject. A third will be the unifying
principles provided by representations of simple Lie algebras and their
universal enveloping algebras. These choices in emphasis are our own. They
represent, at best, particular aspects of the far-reaching ramifications that
followed the discovery of the Jones polynomial.

<id>
math/9304210v1
<category>
math.GT
<abstract>
We construct examples of nonresolvable generalized $n$-manifolds, $n\geq 6$,
with arbitrary resolution obstruction, homotopy equivalent to any simply
connected, closed $n$-manifold. We further investigate the structure of
generalized manifolds and present a program for understanding their topology.

<id>
math/9307233v2
<category>
math.GT
<abstract>
For an oriented link $L \subset S^3 = \Bd\!D^4$, let $\chi_s(L)$ be the
greatest Euler characteristic $\chi(F)$ of an oriented 2-manifold $F$ (without
closed components) smoothly embedded in $D^4$ with boundary $L$. A knot $K$ is
{\it slice} if $\chi_s(K)=1$. Realize $D^4$ in $\C^2$ as
$\{(z,w):|z|^2+|w|^2\le1\}$. It has been conjectured that, if $V$ is a
nonsingular complex plane curve transverse to $S^3$, then $\chi_s(V\cap
S^3)=\chi(V\cap D^4)$. Kronheimer and Mrowka have proved this conjecture in the
case that $V\cap D^4$ is the Milnor fiber of a singularity. I explain how this
seemingly special case implies both the general case and the ``slice-Bennequin
inequality'' for braids. As applications, I show that various knots are not
slice (e.g., pretzel knots like $\Pscr(-3,5,7)$; all knots obtained from a
positive trefoil $O\{2,3\}$ by iterated untwisted positive doubling). As a
sidelight, I give an optimal counterexample to the ``topologically locally-flat
Thom conjecture''.

<id>
math/9407217v1
<category>
math.GT
<abstract>
Alexander's and Markov's theorems state that any link type in $R^3$ is
represented by a closed braid and that such representations are related by some
elementary operations called Markov moves. We generalize the notion of a braid
to that in 4-dimensional space and establish an analogue of these theorems.

<id>
math/9407222v1
<category>
math.GT
<abstract>
We study the Teichm\"uller metric on the Teichm\"uller space of a surface of
finite type, in regions where the injectivity radius of the surface is small.
The main result is that in such regions the Teichm\"uller metric is
approximated up to bounded additive distortion by the sup metric on a product
of lower dimensional spaces. The main technical tool in the proof is the use of
estimates of extremal lengths of curves in a surface based on the geometry of
their hyperbolic geodesic representatives.

<id>
math/9409209v1
<category>
math.GT
<abstract>
This document is a practical guide to computations using an automatic
structure for the mapping class group of a once-punctured, oriented surface
$S$. We describe a quadratic time algorithm for the word problem in this group,
which can be implemented efficiently with pencil and paper. The input of the
algorithm is a word, consisting of ``chord diagrams'' of ideal triangulations
and elementary moves, which represents an element of the mapping class group.
The output is a word called a ``normal form'' that uniquely represents the same
group element.

<id>
math/9410215v1
<category>
math.GT
<abstract>
Let $M$ be an irreducible, compact, connected, orientable 3-manifold whose
boundary is a torus. We show that if $M$ is hyperbolic, then it admits at most
six finite/cyclic fillings of maximal distance 5. Further, the distance of a
finite/cyclic filling to a cyclic filling is at most 2. If $M$ has a
non-boundary-parallel, incompressible torus and is not a generalized 1-iterated
torus knot complement, then there are at most three finite/cyclic fillings of
maximal distance 1. Further, if $M$ has a non-boundary-parallel, incompressible
torus and is not a generalized 1- or 2-iterated torus knot complement and if
$M$ admits a cyclic filling of odd order, then $M$ does not admit any other
finite/cyclic filling. Relations between finite/cyclic fillings and other
exceptional fillings are also discussed.

<id>
math/9410218v1
<category>
math.GT
<abstract>
A homotopy equivalence between a hyperbolic 3-manifold and a closed
irreducible 3-manifold is homotopic to a homeomorphsim provided the hyperbolic
manifold satisfies a purely geometric condition. There are no known examples of
hyperbolic 3-manifolds which do not satisfy this condition.

<id>
math/9411211v1
<category>
math.GT
<abstract>
We define a decomposition of link projections whose pieces we call atoroidal
graphs. We describe a surgery operation on these graphs and show that all
atoroidal graphs can be generated by performing surgery repeatedly on a family
of well known link projections. This gives a method of enumerating atoroidal
graphs and hence link projections by recomposing the pieces of the
decomposition.

<id>
math/9506209v1
<category>
math.GT
<abstract>
In this paper we investigate how the volume of hyperbolic manifolds increases
under the process of removing a curve, that is, Dehn drilling. If the curve we
remove is a geodesic we are able to show that for a certain family of manifolds
the volume increase is bounded above by $\pi \cdot l$ where $l$ is the length
of the geodesic drilled. Also we construct examples to show that there is no
lower bound to the volume increase in terms of a linear function of a positive
power of length and in particular volume increase is not bounded linearly in
length.

<id>
math/9506221v1
<category>
math.GT
<abstract>
In 1985 lectures at MSRI, A. Casson introduced an interesting integer valued
invariant for any oriented integral homology 3-sphere Y via beautiful
constructions on representation spaces (see [1] for an exposition). The Casson
invariant \lambda(Y) is roughly defined by measuring the oriented number of
irreducible representations of the fundamental group \pi_1(Y) in SU(2). Such an
invariant generalized the Rohlin invariant and gives surprising corollaries in
low dimensional topology.

<id>
math/9507216v1
<category>
math.GT
<abstract>
Any closed, oriented, hyperbolic three-manifold with nontrivial second
homology has many quasigeodesic flows, where quasigeodesic means that flow
lines are uniformly efficient in measuring distance in relative homotopy
classes. The flows are pseudo-Anosov flows which are almost transverse to
finite depth foliations in the manifold. The main tool is the use of a sutured
manifold hierarchy which has good geometric properties.

<id>
math/9605220v1
<category>
math.GT
<abstract>
An end sum is a non-compact analogue of a connected sum. Suppose we are given
two connected, oriented $n$-manifolds $M_1$ and $M_2$. Recall that to form
their connected sum one chooses an $n$-ball in each $M_i$, removes its
interior, and then glues together the two $S^{n-1}$ boundary components thus
created by an orientation reversing homeomorphism. Now suppose that $M_1$ and
$M_2$ are also open, i.e. non-compact with empty boundary. To form an end sum
of $M_1$ and $M_2$ one chooses a halfspace $H_i$ (a manifold \homeo\ to ${\bold
R}^{n-1} \times [0, \infty)$) embedded in $M_i$, removes its interior, and then
glues together the two resulting ${\bold R}^{n-1}$ boundary components by an
orientation reversing homeomorphism. In order for this space $M$ to be an
$n$-manifold one requires that each $H_i$ be {\bf end-proper} in $M_i$ in the
sense that its intersection with each compact subset of $M_i$ is compact. Note
that one can regard $H_i$ as a regular neighborhood of an end-proper ray (a
1-manifold \homeo\ to $[0,\infty)$) $\ga_i$ in $M_i$.

<id>
math/9605232v1
<category>
math.GT
<abstract>
Given any connected, open 3-manifold $U$ having finitely many ends, a
non-compact 3-manifold $M$ is constructed having the following properties: the
interior of $M$ is homeomorphic to $U$; the boundary of $M$ is the disjoint
union of finitely many planes; $M$ is not almost compact; $M$ is eventually
end-irreducible; there are no proper, incompressible embeddings of $S^1 \times
\bold R$ in $M$; every compact subset of $M$ is contained in a larger compact
subset whose complement is anannular; there is a compact subset of $M$ whose
complement is $\bold P^2$-irreducible.
  If $U$ is irreducible it also has the following two properties: every proper,
non-trivial plane in $M$ is boundary-parallel; every proper surface in $M$ each
component of which has non-empty boundary and is non-compact and simply
connected lies in a collar on $\partial M$.
  This construction can be chosen so that $M$ admits no homeomorphisms which
take one boundary plane to another or reverse orientation. For the given $U$
there are uncountably many non-homeomorphic such $M$.
  Two auxiliary results may be of independent interest. First, general
conditions are given under which infinitely many ``trivial'' compact components
of the intersection of two proper, non-compact surfaces in an irreducible
3-manifold can be removed by an ambient isotopy. Second, $n$ component tangles
in a 3-ball are constructed such that every non-empty union of components of
the tangle has hyperbolic exterior.

<id>
math/9606225v1
<category>
math.GT
<abstract>
Suppose $M$ is a closed, connected, orientable, \irr\ \3m\ such that
$G=\pi_1(M)$ is infinite. One consequence of Thurston's geometrization
conjecture is that the universal covering space $\widetilde{M}$ of $M$ must be
\homeo\ to $\RRR$. This has been verified directly under several different
additional assumptions on $G$. (See, for example, \cite{2}, \cite{3}, \cite{6},
\cite{19}.)

<id>
math/9608211v1
<category>
math.GT
<abstract>
In this paper we present our results on the homology cobordism group $\Th$ of
the oriented integral homology 3-spheres. We specially emphasize the role
played in the subject by the gauge theory including Floer homology and
invariants by Donaldson and Seiberg -- Witten.

<id>
math/9609207v1
<category>
math.GT
<abstract>
This paper introduces a rigorous computer-assisted procedure for analyzing
hyperbolic 3-manifolds. This technique is used to complete the proof of several
long-standing rigidity conjectures in 3-manifold theory as well as to provide a
new lower bound for the volume of a closed orientable hyperbolic 3-manifold.
  We prove the following result:
  \it\noindent Let $N$ be a closed hyperbolic 3-manifold. Then
\begin{enumerate} \item[(1)] If $f\colon M \to N$ is a homotopy equivalence
where $M$ is a closed irreducible 3-manifold, then $f$ is homotopic to a
homeomorphism. \item[(2)] If $f,g\colon M\to N$ are homotopic homeomorphisms,
then $f$ is isotopic to $g$. \item[(3)] The space of hyperbolic metrics on $N$
is path connected. \end{enumerate}

<id>
math/9609208v1
<category>
math.GT
<abstract>
Let $\F$ be a compact surface and let $I$ be the unit interval. This paper
gives a standard form for all 2-sided incompressible surfaces in the 3-manifold
$\F \times I$. Since $\F \times I$ is a handlebody when $\F$ has boundary, this
standard form applies to incompressible surfaces in a handlebody.

<id>
math/9609209v1
<category>
math.GT
<abstract>
Let (X,d) be a tree (T) of hyperbolic metric spaces satisfying the
quasi-isometrically embedded condition. Let $v$ be a vertex of $T$. Let
$({X_v},d_v)$ denote the hyperbolic metric space corresponding to $v$. Then $i
: X_v \rightarrow X$ extends continuously to a map $\hat{i} : \widehat{X_v}
\rightarrow \widehat{X}$. This generalizes a Theorem of Cannon and Thurston.
The techniques are used to give a new proof of a result of Minsky: Thurston's
ending lamination conjecture for certain Kleinian groups. Applications to
graphs of hyperbolic groups and local connectivity of limit sets of Kleinian
groups are also given.

<id>
math/9610220v1
<category>
math.GT
<abstract>
The Nielsen Conjecture for Homeomorphisms asserts that any homeomorphism $f$
of a closed manifold is isotopic to a map realizing the Nielsen number of $f$,
which is a lower bound for the number of fixed points among all maps homotopic
to $f$. The main theorem of this paper proves this conjecture for all
orientation preserving maps on geometric or Haken 3-manifolds. It will also be
shown that on many manifolds all maps are isotopic to fixed point free maps.
  The proof is based on the understanding of homeomorphisms on 2-orbifolds and
3-manifolds. Thurston's classification of surface homeomorphisms will be
generalized to 2-dimensional orbifolds, which is used to study fiber preserving
maps of Seifert fiber spaces. Maps on most Seifert fiber spaces are indeed
isotopic to fiber preserving maps, with the exception of four manifolds and
orientation reversing maps on lens spaces or $S^3$. It will also be determined
exactly which manifolds have a unique Seifert fibration up to isotopy. These
informations will be used to deform a map to certain standard map on each piece
of the JSJ decomposition, as well as on the neighborhood of the decomposition
tori, which will make it possible to shrink each fixed point class to a single
point, and remove inessential fixed point classes.

<id>
math/9612214v1
<category>
math.GT
<abstract>
This paper concerns the class of contractible open 3-manifolds which are
``locally finite strong end sums'' of eventually end-irreducible Whitehead
manifolds. It is shown that whenever a 3-manifold in this class is a covering
space of another 3-manifold the group of covering translations must be a free
group. It follows that such a 3-manifold cannot cover a closed 3-manifold. For
each countable free group a specific uncountable family of irreducible open
3-manifolds is constructed whose fundamental groups are isomorphic to the given
group and whose universal covering spaces are in this class and are pairwise
non-homeomorphic.

<id>
math/9612215v1
<category>
math.GT
<abstract>
An irreducible open 3-manifold $W$ is {\bf R}$^2$-irreducible if every proper
plane in $W$ splits off a halfspace. In this paper it is shown that if such a
$W$ is the universal cover of a connected, {\bf P}$^2$-irreducible open
3-manifold $M$ with finitely generated fundamental group, then either $W$ is
homeomorphic to {\bf R}$^3$ or the group is a free product of infinite cyclic
groups and infinite closed surface groups. Given any such finitely generated
group uncountably many $M$ are constructed with that fundamental group such
that their universal covers are {\bf R}$^2$-irreducible, are not homeomorphic
to {\bf R}$^3$, and are pairwise non-homeomorphic. These results are related to
the conjecture that closed, orientable, irreducible, aspherical 3-manifolds are
covered by {\bf R}$^3$.

<id>
math/9612216v1
<category>
math.GT
<abstract>
We show that if the lower central series of the fundamental group of a closed
oriented $3$-manifold stabilizes then the maximal nilpotent quotient is a
cyclic group, a quaternion $2$-group cross an odd order cyclic group, or a
Heisenberg group. These groups are well known to be precisely the nilpotent
fundamental groups of closed oriented $3$-manifolds.

<id>
math/9701211v1
<category>
math.GT
<abstract>
In this paper we answer the question posed by M.~Atiyah and give an explicit
formula for Floer homology of Brieskorn homology spheres in terms of their
branching sets over the 3--sphere. We further show how Floer homology is
related to other invariants of knots and 3--manifolds, among which are the
$\bar\mu$--invariant of W.~Neumann and L.~Siebenmann and the Jones polynomial.
Essential progress is made in proving the homology cobordism invariance of our
own $\nu$--invariant.

<id>
math/9703206v1
<category>
math.GT
<abstract>
Techniques are introduced which determine the geometric structure of
non-simple two-generator $3$-manifolds from purely algebraic data. As an
application, the satellite knots in the $3$-sphere with a two-generator
presentation in which at least one generator is represented by a meridian for
the knot are classified.

<id>
math/9703211v1
<category>
math.GT
<abstract>
We describe theoretical backgrounds for a computer program that recognizes
all closed orientable 3-manifolds up to complexity 8. The program can treat
also not necessarily closed 3-manifolds of bigger complexities, but here some
unrecognizable (by the program) 3-manifolds may occur.

<id>
math/9704221v1
<category>
math.GT
<abstract>
This article is solicited by C.\ Adams for a special issue of {\it Chaos,
Solitons and Fractals\/} devoted to knot theory and its applications. We
present some recent results about Dehn surgeries on arborescent knots and
links.

<id>
math/9704222v1
<category>
math.GT
<abstract>
We show that on any hyperbolic knot in $S^3$ there is at most one
non-integral Dehn surgery which yields a manifold containing an incompressible
torus.

<id>
math/9704223v1
<category>
math.GT
<abstract>
This is an expository paper, in which we give a summary of some of the joint
work of John Luecke and the contributor on Dehn surgery. We consider the situation
where we have two Dehn fillings $M(\alpha)$ and $M(\beta)$ on a given
3-manifold $M$, each containing a surface that is either essential or a
Heegaard surface. We show how a combinatorial analysis of the graphs of
intersection of the two corresponding punctured surfaces in $M$ enables one to
find faces of these graphs which give useful topological information about
$M(\alpha)$ and $M(\beta)$, and hence, in certain cases, good upper bounds on
the intersection number $\Delta(\alpha, \beta)$ of the two filling slopes.

<id>
math/9201304v1
<category>
math.GR
<abstract>
This note presents an elementary version of Sims's algorithm for computing
strong generators of a given perm group, together with a proof of correctness
and some notes about appropriate low-level data structures. Upper and lower
bounds on the running time are also obtained. (Following a suggestion of
Vaughan Pratt, we adopt the convention that perm $=$ permutation, perhaps
thereby saving millions of syllables in future research.)

<id>
math/9210219v1
<category>
math.GR
<abstract>
A set of invariants for a finite group is described. These arise naturally
from Frobenius' early work on the group determinant and provide an answer to a
question of Brauer. Whereas it is well known that the ordinary character table
of a group does not determine the group uniquely, it is a consequence of the
results presented here that a group is determined uniquely by its
``3-character'' table.

<id>
math/9210221v1
<category>
math.GR
<abstract>
It is proved that the free $m$-generated Burnside groups $\Bbb{B}(m,n)$ of
exponent $n$ are infinite provided that $m>1$, $n\ge2^{48}$.

<id>
math/9305201v1
<category>
math.GR
<abstract>
The object of this paper is to describe a simple method for proving that
certain groups are residually torsion-free nilpotent, to describe some new
parafree groups and to raise some new problems in honour of the memory of
Wilhelm Magnus.

<id>
math/9306201v1
<category>
math.GR
<abstract>
No abstract is available

<id>
math/9306202v1
<category>
math.GR
<abstract>
If $M$ is a closed Nil geometry 3-manifold then $\pi_1(M)$ is almost convex
with respect to a fairly simple ``geometric'' generating set. If $G$ is a
central extension or a ${\Bbb Z}$-extension of a word hyperbolic group, then
$G$ is also almost convex with respect to some generating set. Combining these
with previously known results shows that if $M$ is a closed 3-manifold with one
of Thurston's eight geometries, $\pi_1(M)$ is almost convex with respect to
some generating set if and only if the geometry in question is not Sol.

<id>
math/9306203v1
<category>
math.GR
<abstract>
In ``A remark about the description of free products of groups'', Proc.
Cambgridge Philos. Soc 62(1966), io ha studite lo que occurre in le
circumstantia que un gruppo $G$ ha un subensemble $P$ tal que tote elemento de
$G$ es representabile unicamente per un verbo reducite in $P$. Il eveni que tal
$P$ es multo como un producto libere. Que occurre quando le representation per
verbo reducite es unic solmente modulo le sorta de equivalentia que interveni
in le theoria del productos libere amalgamate? In iste articulo, io determina
le structura internal del subensemble $P$ (io los appella ``pregruppos''), e
prova, sequente le methodo de van der Waerden, que su gruppo universal ha le
proprietate desiderate. Multe interessante exemplos pote esser trovate; tote
semble simile aliquanto al productos libere amalgamate; sed il es nulle simple
maniera de construer los omne ex ordinari tal productos.

<id>
math/9306204v1
<category>
math.GR
<abstract>
Anisimov and Seifert show that a group has a regular word problem ifand only
if it is finite. Muller and Schupp (together with Dunwoody's accessibility
result) show that a group has context free word problem if and only if it is
virtually free. In this note, we exhibit a class of groups where the word
problem is as close as possible to being a context sensitive language. This
class includes the automatic groups and is closed under passing to finitely
generated subgroups. Consequently, it is quite large, including many groups
which are not finitely presented.

<id>
math/9306205v1
<category>
math.GR
<abstract>
We study the synchronous and asynchronous automatic structures on the
fundamental group of a graph of groups in which each edge group is finite. Up
to a natural equivalence relation, the set of biautomatic structures on such a
graph product bijects to the product of the sets of biautomatic structures on
the vertex groups. The set of automatic structures is much richer. Indeed, it
is dense in the infinite product of the sets of automatic structures of all
conjugates of the vertex groups. We classify these structures by a class of
labelled graphs which ``mimic" the underlying graph of the graph of groups.
Analogous statements hold for asynchronous automatic structures. We also
discuss the boundaries of these structures.

<id>
math/9310202v1
<category>
math.GR
<abstract>
Given a finite simplicial graph ${\cal G}$, the graph group $G{\cal G}$" is
the group with generators in one-to-one correspondence with the vertices of
${\cal G}$ and with relations stating two generators commute if their
associated vertices are adjacent in ${\cal G}$. The Bieri-Neumann-Strebel
invariant can be explicitly described in terms of the original graph ${\cal G}$
and hence there is an explicit description of the distribution of finitely
generated normal subgroups of $G{\cal G}$ with abelian quotient. We construct
Eilenberg-MacLane spaces for graph groups and find partial extensions of this
work to the higher dimensional invariants.

<id>
math/9310203v1
<category>
math.GR
<abstract>
A combinatorial group-theoretic hypothesis is presented that serves as a
necessary and sufficient condition for a union of connected Cockcroft
two-complexes to be Cockcroft. This hypothesis has a component that can be
expressed in terms of the second homology of groups. The hypothesis is applied
to the study of the third homology of groups given by generators and relators.

<id>
math/9310204v1
<category>
math.GR
<abstract>
The cogrowth of a subgroup is defined as the growth of a set of coset
representatives which are of minimal length. A subgroup is essential if it
intersects non-trivially every non-trivial subgroup. The main result of this
paper is that every function $f:{\Bbb N}\cup \{0\}\rightarrow {\Bbb N}$ which
is strictly increasing, but at most exponential, is equivalent to a cogrowth
function of an essential subgroup of infinite index of the free group of rank
two. This class of functions properly contains the class of growth functions of
groups. The notions of growth and cogrowth of right ideals in algebras are
introduced. We show that when the algebra is without zero divisors then every
right ideal, whose cogrowth is less than that of the algebra, is essential.

<id>
math/9310205v1
<category>
math.GR
<abstract>
The ways in which a nontrivial commutator can be a proper power in a free
product of groups are identified.

<id>
math/9310206v1
<category>
math.GR
<abstract>
A classification of the ways in which an element of a free group can be
expressed as a product of commutators or as a product of squares is given. This
is then applied to some particular classes of elements. Finally, a question
about expressing a commutator as a product of squares is addressed.

<id>
math/9310207v1
<category>
math.GR
<abstract>
Let $\Gamma$ be a finite graph together with a group $G_v$ at each vertex
$v$. The graph product $G(\Gamma)$ is obtained from the free product of all
$G_v$ by factoring out by the normal subgroup generated by $\{g^{-1}h^{-1}gh;
g\in G_v, h\in G_w\}$ for all adjacent $v,w$. In this note we construct a
projective resolution for $G(\Gamma)$ given projective resolutions for each
$G_v$, and obtain some applications.

<id>
math/9310208v1
<category>
math.GR
<abstract>
Let $\Gamma$ be a finite graph, and for each vertex $i$ let $G_i$ be a
finitely presented group. Let $G$ be the graph product of the $G_i$. That is,
$G$ is the group obtained from the free product of the $G_i$ by factoring out
by the smallest normal subgroup containing all $[g,h]$ where $g\in G_i$ and
$h\in G_j$ and there is an edge joining i and j . We show that $G$ has an
isoperimetric function of degree $k\ge 2$ (or an exponential isoperimetric
function) if each vertex group has such an isoperimetric function.

<id>
math/9310209v1
<category>
math.GR
<abstract>
The idea of applying isoperimetric functions to group theory is due to
M.Gromov. We introduce the concept of a ``bicombing of narrow shape'' which
generalizes the usual notion of bicombing. Our bicombing is related to but
different from the combings defined by M. Bridson. If the Cayley graph of a
group with respect to a given set of generators admits a bicombing of narrow
shape then the group is finitely presented and satisfies a sub-exponential
isoperimetric inequality, as well as a polynomial isodiametric inequality. We
give an infinite class of examples which are not bicombable in the usual sense
but admit bicombings of narrow shape.

<id>
math/9311201v1
<category>
math.GR
<abstract>
A study of triangulations of cycles in the Cayley diagrams of finitely
generated groups leads to a new geometric characterization of hyperbolic
groups.

<id>
math/9401201v1
<category>
math.GR
<abstract>
We show that the set $SA(G)$ of equivalence classes of synchronously
automatic structures on a geometrically finite hyperbolic group $G$ is dense in
the product of the sets $SA(P)$ over all maximal parabolic subgroups $P$. The
set $BSA(G)$ of equivalence classes of biautomatic structures on $G$ is
isomorphic to the product of the sets $BSA(P)$ over the cusps (conjugacy
classes of maximal parabolic subgroups) of $G$. Each maximal parabolic $P$ is a
virtually abelian group, so $SA(P)$ and $BSA(P)$ were computed in ``Equivalent
automatic structures and their boundaries'' by M.Shapiro and W.Neumann, Intern.
J. of Alg. Comp. 2 (1992) We show that any geometrically finite hyperbolic
group has a generating set for which the full language of geodesics for $G$ is
regular. Moreover, the growth function of $G$ with respect to this generating
set is rational. We also determine which automatic structures on such a group
are equivalent to geodesic ones. Not all are, though all biautomatic structures
are.

<id>
math/9404202v1
<category>
math.GR
<abstract>
We see that a building whose Coxeter group is hyperbolic is itself
hyperbolic. Thus any finitely generated group acting co-compactly on such a
building is hyperbolic, hence automatic. We turn our attention to affine
buildings and consider a group $\Gamma$ which acts simply transitively and in a
``type-rotating'' way on the vertices of a locally finite thick building of
type $\tilde A_n$. We show that $\Gamma$ is biautomatic, using a presentation
of $\Gamma$ and unique normal form for each element of $\Gamma$, as described
in ``Groups acting simply transitively on the vertices of a building of type
$\tilde A_n$'' by D.I. Cartwright, to appear, Proceedings of the 1993 Como
conference ``Groups of Lie type and their geometries''.

<id>
math/9404203v1
<category>
math.GR
<abstract>
The quotient of a biautomatic group by a subgroup of the center is shown to
be biautomatic. The main tool used is the Neumann-Shapiro triangulation of
$S^{n-1}$, associated to a biautomatic structure on ${\Bbb Z}^n$. As an
application, direct factors of biautomatic groups are shown to be biautomatic.

<id>
math/9406202v1
<category>
math.GR
<abstract>
A primary reference on computer implementation of coset enumeration
procedures is a 1973 paper of Cannon, Dimino, Havas and Watson. Programs and
techniques described there are updated in this paper. Improved coset definition
strategies, space saving techniques and advice for obtaining improved
performance are included. New coset definition strategies for Felsch-type
methods give substantial reductions in total cosets defined for some
pathological enumerations. Significant time savings are achieved for coset
enumeration procedures in general. Statistics on performance are presented,
both in terms of time and in terms of maximum and total cosets defined for
selected enumerations.

<id>
math/9406203v1
<category>
math.GR
<abstract>
Group theory is a particularly fertile field for the design of practical
algorithms. Algorithms have been developed across the various branches of the
subject and they find wide application. Because of its relative maturity,
computational group theory may be used to gain insight into the general
structure of algebraic algorithms. This paper examines the basic ideas behind
some of the more important algorithms for finitely presented groups and
permutation groups, and surveys recent developments in these fields.

<id>
math/9406204v1
<category>
math.GR
<abstract>
An important way for describing groups is by finite presentations. Large
presentations arise in practice which are poorly suited for either human or
computer use. Presentation simplification processes which take bad
presentations and produce good presentations have been developed. Substantial
use is made of substring searching and appropriate techniques for this context
are described. Effective use is made of signatures and change flags. Change
flags are shown to be the most beneficial of the methods tested here, with very
significant performance improvement. Experimental performance figures are
given.

<id>
math/9406205v1
<category>
math.GR
<abstract>
Finitely generated Z-modules have canonical decompositions. When such modules
are given in a finitely presented form there is a classical algorithm for
computing a canonical decomposition. This is the algorithm for computing the
Smith normal form of an integer matrix. We discuss algorithms for Smith normal
form computation, and present practical algorithms which give excellent
performance for modules arising from badly presented abelian groups. We
investigate such issues as congruential techniques, sparsity considerations,
pivoting strategies for Gauss-Jordan elimination, lattice basis reduction and
computational complexity. Our results, which are primarily empirical, show
dramatically improved performance on previous methods.

<id>
math/9406206v1
<category>
math.GR
<abstract>
We describe a substring search problem that arises in group presentation
simplification processes. We suggest a two-level searching model: skip and
match levels. We give two timestamp algorithms which skip searching parts of
the text where there are no matches at all and prove their correctness. At the
match level, we consider Harrison signature, Karp-Rabin fingerprint, Bloom
filter and automata based matching algorithms and present experimental
performance figures.

<id>
math/9406207v1
<category>
math.GR
<abstract>
Computer based techniques for recognizing finitely presented groups are quite
powerful. Tools available for this purpose are outlined. They are available
both in stand-alone programs and in more comprehensive systems. A general
computational approach for investigating finitely presented groups by way of
quotients and subgroups is described and examples are presented. The techniques
can provide detailed information about group structure. Under suitable
circumstances a finitely presented group can be shown to be soluble and its
complete derived series can be determined, using what is in effect a soluble
quotient algorithm.

<id>
math/9410220v1
<category>
math.GR
<abstract>
We announce the classification of two related classes of flag-transitive
geometries. There is an infinite family of such geometries, related to the
nonsplit extensions $3^{[{n\atop 2}]_{_2}}\cdot \SP_{2n}(2)$, and twelve
sporadic examples coming from the simple groups $M_{22}$, $M_{23}$, $M_{24}$,
$He$, $Co_1$, $Co_2$, $J_4$, $BM$, $M$ and the nonsplit extensions $3\cdot
M_{22}$, $3^{23}\cdot Co_2$, and $3^{4371}\cdot BM$.

<id>
math/9411203v1
<category>
math.GR
<abstract>
Let $E$ be a virtually central extension of the group $G$ by a finitely
generated abelian group $A$. We show that $E$ carries a biautomatic structure
if and only if $G$ has a biautomatic structure $L$ for which the cohomology
class of the extension is represented by an $L$-regular cocycle. Moreover, a
cohomology class is $L$-regular if some multiple of it is or if its restriction
to some finite index subgroup is.
  We also show that the entire second cohomology of a Fuchsian group is
regular, so any virtually central extension is biautomatic. In particular, if
the fundamental group of a Seifert fibered 3-manifold is not virtually
nilpotent then it is biautomatic. ECHLPT had shown automaticity in this case
and in an unpublished 1992 preprint Gersten constructed a biautomatic structure
for circle bundles over hyperbolic surfaces and asked if the same could be done
for these Seifert fibered 3-manifolds.

<id>
math/9412203v1
<category>
math.GR
<abstract>
Schreier formula for the rank of a subgroup of finite index of a finitely
generated free group $F$ is generalized to an arbitrary (even infinitely
generated) subgroup $H$ through the Schreier transversals of $H$ in $F$. The
rank formula may also be expressed in terms of the cogrowth of $H$. We
introduce the rank-growth function $rk_H(i)$ of a subgroup $H$ of a finitely
generated free group $F$. $rk_H(i)$ is defined to be the rank of the subgroup
of $H$ generated by elements of length less than or equal to $i$ (with respect
to the generators of $F$), and it equals the rank of the fundamental group of
the subgraph of the cosets graph of $H$, which consists of the paths starting
at $1$ that are of length $\leq i$. When $H$ is supnormal, i.e. contains a
non-trivial normal subgroup of $F$, we show that its rank-growth is equivalent
to the cogrowth of $H$. A special case of this is the known result that a
supnormal subgroup of $F$ is of finite index if and only if it is finitely
generated. In particular, when $H$ is normal then the growth of the group
$G=F/H$ is equivalent to the rank-growth of $H$. A Schreier transversal forms a
spanning tree of the cosets graph of $H$, and thus its topological structure is
of a contractible spanning subcomplex of a simplicial complex. The
$d$-dimensional simplicial complexes that contain contractible spanning
subcomplexes have the homotopy type of a bouquet of $r$ $d$-spheres. When these
complexes are also $n$-regular then $r$ can be computed by generalizing the
rank formula (which applies to Schreier transversals) to higher dimensions.

<id>
math/9201262v1
<category>
math.HO
<abstract>
The contributors discuss the role of controversy in mathematics as a preface to
two opposing articles on computational complexity theory: "Some basic
information on information-based complexity theory" by Beresford Parlett
[math.NA/9201266] and "Perspectives on information-based complexity" by J. F.
Traub and Henryk Wo\'zniakowski [math.NA/9201269].

<id>
math/9205211v1
<category>
math.HO
<abstract>
The contributor advocates two specific mathematical notations from his popular
course and joint textbook, "Concrete Mathematics". The first of these,
extending an idea of Iverson, is the notation "[P]" for the function which is 1
when the Boolean condition P is true and 0 otherwise. This notation can
encourage and clarify the use of characteristic functions and Kronecker deltas
in sums and integrals.
  The second notation puts Stirling numbers on the same footing as binomial
coefficients. Since binomial coefficients are written on two lines in
parentheses and read "n choose k", Stirling numbers of the first kind should be
written on two lines in brackets and read "n cycle k", while Stirling numbers
of the second kind should be written in braces and read "n subset k". (I might
say "n partition k".) The written form was first suggested by Imanuel Marx. The
virtues of this notation are that Stirling partition numbers frequently appear
in combinatorics, and that it more clearly presents functional relations
similar to those satisfied by binomial coefficients.

<id>
math/9307227v1
<category>
math.HO
<abstract>
Is speculative mathematics dangerous? Recent interactions between physics and
mathematics pose the question with some force: traditional mathematical norms
discourage speculation, but it is the fabric of theoretical physics. In
practice there can be benefits, but there can also be unpleasant and
destructive consequences. Serious caution is required, and the issue should be
considered before, rather than after, obvious damage occurs. With the hazards
carefully in mind, we propose a framework that should allow a healthy and
positive role for speculation.

<id>
math/9404229v1
<category>
math.HO
<abstract>
This article is a collection of letters solicited by the editors of the
Bulletin in response to a previous article by Jaffe and Quinn
[math.HO/9307227]. The contributors discuss the role of rigor in mathematics and the
relation between mathematics and theoretical physics.

<id>
math/9404231v1
<category>
math.HO
<abstract>
The contributors discuss various objections and rejoinders in the collected
responses [math.HO/9404229,math.HO/9404236] to their original article on the
relationship between mathematics and theoretical physics [math.HO/9307227].

<id>
math/9404233v1
<category>
math.HO
<abstract>
This note is a preface to various responses [math.HO/9404229,math.HO/9404236]
to an opinion piece by Jaffe and Quinn [math.HO/9307227] on the relationship
between mathematics and theoretical physics.

<id>
math/9404236v1
<category>
math.HO
<abstract>
In response to Jaffe and Quinn [math.HO/9307227], the contributor discusses forms
of progress in mathematics that are not captured by formal proofs of theorems,
especially in his own work in the theory of foliations and geometrization of
3-manifolds and dynamical systems.

<id>
math/9708203v1
<category>
math.HO
<abstract>
In the Forum section of the November, 1993 Notices of the American
Mathematical Society, John Franks discussed the electronic journal of the
future. Since then, the New York Journal of Mathematics, the first electronic
general mathematics journal, has begun publication. In this article, we explore
the issues of electronic journal publishing in the context of this new project.
We also discuss future developments.

<id>
math/9801013v1
<category>
math.HO
<abstract>
Many important journal functions would be lost if the mathematical community
replaced all paper journals with electronic media. Electronic media are useful
for some purposes, but they will not be the basis for a publishing revolution
in the near future.

<id>
math/9903001v1
<category>
math.HO
<abstract>
The note is devoted to an interactive game theoretic formalization of
dialogues as psycholinguistic phenomena and the unraveling of a hidden dialogue
structure of 2-person differential interactive games. In the field-theoretic
description of interactive games the dialogues are defined naively as
interactive games of discrete time with intention fields of continuous time;
the correct mathematical formulation is proposed. The states and the controls
of a dialogue correspond to the speech whereas the intention fields describe
the understanding. In the case of dialogues the main inverse problem is to
describe geometrical and algebraical properties of the understanding. On the
other hand, a precise mathematical definition of dialogues allows to formulate
a problem of the unraveling of a hidden dialogue structure of any 2-person
differential interactive game. Such procedure is called the verbalization. It
means that the states of a differential interactive game are interpreted as
intention fields of a hidden dialogue and the problem is to describe such
dialogue completely. If a 2-person differential interactive game is
verbalizable one is able to consider many linguistic (e.g. the formal grammar
of a related hidden dialogue) or psycholinguistic (e.g. the dynamical
correlation of various implications) aspects of it.

<id>
math/9904021v1
<category>
math.HO
<abstract>
A problem from Democritus is used to illustrate the building, and use, of
infinitesimal covectors from its regularized, finite, counterpart.

<id>
math/9905180v2
<category>
math.HO
<abstract>
Kaleidoscope-roulettes, a proper class of perception games, is described.
Kaleidoscope-roulette is defined as a perception and, hence, verbalizable
interactive game, whose hidden dialogue consists of quasirandom sequences of
``words''. The resonance phenomena in such games and their controlling are
discussed.

<id>
math/9905186v1
<category>
math.HO
<abstract>
We discuss the babylonian method of extracting the root square of a number,
from the point of view of modern mathematics. We also speculate that the
babylonian mathematics was rich enough for a generalization of this method,
despite the lack of general statements and justified procedures in their
mathematics.

<id>
math/9908139v2
<category>
math.HO
<abstract>
A methodical analysis of the research related to the article, ``Sur les
groupes continus'', of Henri Poincar\'{e} reveals many historical
misconceptions and inaccuracies regarding his contribution to Lie theory. A
thorough reading of this article confirms the precedence of his discovery of
many important concepts, especially that of the \textit{universal enveloping
algebra} of a Lie algebra over a field of characteristic zero, the \textit{%
canonical map} (\textit{symmetrization}) of the symmetric algebra onto the
universal enveloping algebra. The essential part of this article consists of a
detailed discussion of his rigorous, complete, and enlightening proof of the
so-called Birkhoff-Witt theorem.

<id>
math/9911040v1
<category>
math.HO
<abstract>
This short note is devoted to the representative dynamics, which realizes a
link between the theory of controlled systems and representation theory.
Dynamical inverse problem of representation theory for controlled systems is
considered: to solve it means to correspond a representative dynamics to the
controlled system.

<id>
math/9912039v1
<category>
math.HO
<abstract>
We give a hierarchial set of axioms for mathematical origami. The hierachy
gives the fields of Pythagorean numbers, first discussed by Hilbert, the field
of Euclidean constructible numbers which are obtained by the usual
constructions of straightedge and compass, and the Origami numbers, which is
also the field generated from the intersections of conics or equivalently the
marked ruler.

<id>
math/0004187v1
<category>
math.HO
<abstract>
A counter-intuitive result of Gauss (formulae (1.6), (1.7) below) is made
less mysterious by virtue of being generalized through the introduction of an
additional parameter.

<id>
math/0010281v1
<category>
math.HO
<abstract>
The Pythagorean triples have the structure of a ternary rooted tree; the tree
is based on the Cayley graph of a free subgroup of the modular group

<id>
math/0103051v1
<category>
math.HO
<abstract>
A suggestion is put forward regarding a partial proof of FLT(case1), which is
elegant and simple enough to have caused Fermat's enthusiastic remark in the
margin of his Bachet edition of Diophantus' "Arithmetica". It is based on an
extension of Fermat's Small Theorem (FST) to mod p^k for any k>0, and the cubic
roots of 1 mod p^k for primes p=1 mod 6. For this solution in residues the
exponent p distributes over a sum, which blocks extension to equality for
integers, providing a partial proof of FLT case1 for all p=1 mod 6. This simple
solution begs the question why it was not found earlier. Some mathematical,
historical and psychological reasons are presented. . . . . In a companion
paper, on the triplet structure of Arithmetic mod p^k, this cubic root solution
is extended to the general rootform of FLT (mod p^k) (case1), called "triplet".
While the cubic root solution (a^3=1 mod p^k) involves one inverse pair:
a+a^{-1} = -1 mod p^k, a triplet has three inverse pairs in a 3-loop: a+b^{-1}
= b+c^{-1} = c+a^{-1} = -1 (mod p^k) where abc = 1 (mod p^k), which reduces to
the cubic root form if a=b=c (\neq 1) mod p^k. The triplet structure is not
restricted to p-th power residues (for some p \geq 59) but applies to all
residues in the group G_k(.) of units in the semigroup of multiplication mod
p^k.

<id>
math/0108072v1
<category>
math.HO
<abstract>
We give a description of the growth of research on Knot Theory in Japan. We
place our report in a general historical context. In particular, we compare the
development of research on mathematical topology in Japan with that in Poland
and USA, observing several similarities. Toward the end of XIX century and at
the beginning of XX century several young mathematicians, educated in Germany,
France or England were returning to their native countries and building, almost
from scratch, schools of modern mathematics. After a general description of the
growth of topology in Japan between the World Wars, we describe the beginning
of Knot Theory in Japan. Gaisi Takeuti, later a famous logician, conducted the
first Knot Theory seminar in Japan in 1952 or 1953. Kunio Murasugi (later a
prominent knot theoretist) was the only student who attended it. In Osaka, a
Knot Theory seminar started in 1955, initiated by Hidetaka Terasaka and his
students Shin'ichi Kinoshita and Takeshi Yajima. We complete the paper by
listing 70 Japanese topologists born before 1946, and by sketching the
biography of Fox.

<id>
math/0110197v1
<category>
math.HO
<abstract>
The nineteenth century Russian contributor Leo Tolstoy based his egalitarian views
on sociology and history on mathematical and probabilistic views, and he also
proposed a mathematical theory of waging war.

<id>
math/0110263v2
<category>
math.HO
<abstract>
This paper has been withdrawn by the contributor.

<id>
math/0111229v2
<category>
math.HO
<abstract>
The original tables of body cuboids by Maurice Kraitchik are corrected,
restoring 159 missing cuboids. His table range is then extended for all odd
sides less than 1,000,000 to a new limit of 4,294,967,295. Over this new range,
12,517 unique body cuboids are listed, from the original 416.

<id>
math/0112048v1
<category>
math.HO
<abstract>
During the past 25 years there has been a controversy regarding the adequacy
of Newton's proof of Prop. 1 in Book 1 of the {\it Principia}. This proposition
is of central importance because its proof of Kepler's area law allowed Newton
to introduce a geometric measure for time to solve problems in orbital dynamics
in the {\it Principia}. It is shown here that the critics of Prop. 1 have
misunderstood Newton's fundamental limit argument by neglecting to consider the
justification for this limit which he gave in Lemma 3. We clarify the proof of
Prop. 1 by filling in some details left out by Newton which show that his proof
of this proposition was adequate and well grounded.

<id>
math/0202308v1
<category>
math.HO
<abstract>
We give a short biographical sketch of Karl Weierstrass.

<id>
math/0203002v2
<category>
math.HO
<abstract>
This article discusses what can be proved about the foundations of
mathematics using the notions of algorithm and information. The first part is
retrospective, and presents a beautiful antique, Godel's proof, the first
modern incompleteness theorem, Turing's halting problem, and a piece of
postmodern metamathematics, the halting probability Omega. The second part
looks forward to the new century and discusses the convergence of theoretical
physics and theoretical computer science and hopes for a theoretical biology,
in which the notions of algorithm and information are again crucial.

<id>
math/0206043v1
<category>
math.HO
<abstract>
We give a short biographical sketch of Emmy Noether.

<id>
math/0206259v1
<category>
math.HO
<abstract>
This essay contains three parts. The first part of essay focuses on the
hypothesis of the functional semantic constructions (FSC-Hypothesis). This
hypothesis explains that a language, a number, a money are the functional
semantic constructions. In the second part the contributor considers the Mathematics
with respect to the FSC-Hypothesis. Author turns in the solution for the
following problems: Ontology of Mathematics, Objects of Mathematics, Number,
Classification of the numbers. Last part contains the critical remarks to the
axiomatic allocation of the real numbers to the linear point continuum and to
the countability / uncountability of the set of rational and of the set of real
numbers.

<id>
math/0210035v1
<category>
math.HO
<abstract>
We discuss views about whether the universe can be rationally comprehended,
starting with Plato, then Leibniz, and then the views of some distinguished
scientists of the previous century. Based on this, we defend the thesis that
comprehension is compression, i.e., explaining many facts using few theoretical
assumptions, and that a theory may be viewed as a computer program for
calculating observations. This provides motivation for defining the complexity
of something to be the size of the simplest theory for it, in other words, the
size of the smallest program for calculating it. This is the central idea of
algorithmic information theory (AIT), a field of theoretical computer science.
Using the mathematical concept of program-size complexity, we exhibit
irreducible mathematical facts, mathematical facts that cannot be demonstrated
using any mathematical theory simpler than they are. It follows that the world
of mathematical ideas has infinite complexity and is therefore not fully
comprehensible, at least not in a static fashion. Whether the physical world
has finite or infinite complexity remains to be seen. Current science believes
that the world contains randomness, and is therefore also infinitely complex,
but a deterministic universe that simulates randomness via pseudo-randomness is
also a possibility, at least according to recent highly speculative work of S.
Wolfram.

<id>
math/0210144v1
<category>
math.HO
<abstract>
This essay was invited for publication in Nieuw Archief voor Wiskunde; it
will also appear in translation in the SMF Gazette and in the DMV Mitteilungen.
  I discuss the recent trends in scholarly communication in mathematics, the
current state and intentions of the arXiv, and a proposal to reform peer review
with the arXiv as a foundation.

<id>
cs/9906001v1
<category>
cs.IT
<abstract>
This paper computationally obtains optimal bounded-weight, binary,
error-correcting codes for a variety of distance bounds and dimensions. We
compare the sizes of our codes to the sizes of optimal constant-weight, binary,
error-correcting codes, and evaluate the differences.

<id>
cs/0406039v3
<category>
cs.IT
<abstract>
Let A(q,n,d) denote the maximum size of a q-ary code of length n and distance
d. We study the minimum asymptotic redundancy \rho(q,n,d)=n-log_q A(q,n,d) as n
grows while q and d are fixed. For any d and q<=d-1, long algebraic codes are
designed that improve on the BCH codes and have the lowest asymptotic
redundancy \rho(q,n,d) <= ((d-3)+1/(d-2)) log_q n known to date. Prior to this
work, codes of fixed distance that asymptotically surpass BCH codes and the
Gilbert-Varshamov bound were designed only for distances 4,5 and 6.

<id>
cs/0406048v1
<category>
cs.IT
<abstract>
We give a new lower bound on the expansion coefficient of an edge-vertex
graph of a $d$-regular graph. As a consequence, we obtain an improvement on the
lower bound on relative minimum distance of the expander codes constructed by
Sipser and Spielman. We also derive some improved results on the vertex
expansion of graphs that help us in improving the parameters of the expander
codes of Alon, Bruck, Naor, Naor, and Roth.

<id>
cs/0407010v1
<category>
cs.IT
<abstract>
We derive improved bounds on the error and erasure rate for spherical codes
and for binary linear codes under Forney's erasure/list decoding scheme and
prove some related results.

<id>
cs/0407011v3
<category>
cs.IT
<abstract>
We address the problem of bounding below the probability of error under
maximum likelihood decoding of a binary code with a known distance distribution
used on a binary symmetric channel. An improved upper bound is given for the
maximum attainable exponent of this probability (the reliability function of
the channel). In particular, we prove that the ``random coding exponent'' is
the true value of the channel reliability for code rate $R$ in some interval
immediately below the critical rate of the channel. An analogous result is
obtained for the Gaussian channel.

<id>
cs/0408008v1
<category>
cs.IT
<abstract>
We study codes on graphs combined with an iterative message passing algorithm
for quantization. Specifically, we consider the binary erasure quantization
(BEQ) problem which is the dual of the binary erasure channel (BEC) coding
problem. We show that duals of capacity achieving codes for the BEC yield codes
which approach the minimum possible rate for the BEQ. In contrast, low density
parity check codes cannot achieve the minimum rate unless their density grows
at least logarithmically with block length. Furthermore, we show that duals of
efficient iterative decoding algorithms for the BEC yield efficient encoding
algorithms for the BEQ. Hence our results suggest that graphical models may
yield near optimal codes in source coding as well as in channel coding and that
duality plays a key role in such constructions.

<id>
cs/0408017v1
<category>
cs.IT
<abstract>
A variable-length code is a fix-free code if no codeword is a prefix or a
suffix of any other codeword. In a fix-free code any finite sequence of
codewords can be decoded in both directions, which can improve the robustness
to channel noise and speed up the decoding process. In this paper we prove a
new sufficient condition of the existence of fix-free codes and improve the
upper bound on the redundancy of optimal fix-free codes.

<id>
cs/0408038v1
<category>
cs.IT
<abstract>
Fundamental results concerning the dynamics of abelian group codes
(behaviors) and their duals are developed. Duals of sequence spaces over
locally compact abelian groups may be defined via Pontryagin duality; dual
group codes are orthogonal subgroups of dual sequence spaces. The dual of a
complete code or system is finite, and the dual of a Laurent code or system is
(anti-)Laurent. If C and C^\perp are dual codes, then the state spaces of C act
as the character groups of the state spaces of C^\perp. The controllability
properties of C are the observability properties of C^\perp. In particular, C
is (strongly) controllable if and only if C^\perp is (strongly) observable, and
the controller memory of C is the observer memory of C^\perp. The controller
granules of C act as the character groups of the observer granules of C^\perp.
Examples of minimal observer-form encoder and syndrome-former constructions are
given. Finally, every observer granule of C is an "end-around" controller
granule of C.

<id>
cs/0408062v1
<category>
cs.IT
<abstract>
We consider lossy source coding when side information affecting the
distortion measure may be available at the encoder, decoder, both, or neither.
For example, such distortion side information can model reliabilities for noisy
measurements, sensor calibration information, or perceptual effects like
masking and sensitivity to context. When the distortion side information is
statistically independent of the source, we show that in many cases (e.g, for
additive or multiplicative distortion side information) there is no penalty for
knowing the side information only at the encoder, and there is no advantage to
knowing it at the decoder. Furthermore, for quadratic distortion measures
scaled by the distortion side information, we evaluate the penalty for lack of
encoder knowledge and show that it can be arbitrarily large. In this scenario,
we also sketch transform based quantizers constructions which efficiently
exploit encoder side information in the high-resolution limit.

<id>
cs/0409011v2
<category>
cs.IT
<abstract>
We continue to discuss why MMSE estimation arises in coding schemes that
approach the capacity of linear Gaussian channels. Here we consider schemes
that involve successive decoding, such as decision-feedback equalization or
successive cancellation.

<id>
cs/0409026v1
<category>
cs.IT
<abstract>
We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel and is refined for the BEC.

<id>
cs/0409027v1
<category>
cs.IT
<abstract>
We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel, and is refined for the BEC.

<id>
cs/0409053v1
<category>
cs.IT
<abstract>
We discuss why MMSE estimation arises in lattice-based schemes for
approaching the capacity of linear Gaussian channels, and comment on its
properties.

<id>
cs/0410002v1
<category>
cs.IT
<abstract>
We compare the elementary theories of Shannon information and Kolmogorov
complexity, the extent to which they have a common purpose, and where they are
fundamentally different. We discuss and relate the basic notions of both
theories: Shannon entropy versus Kolmogorov complexity, the relation of both to
universal coding, Shannon mutual information versus Kolmogorov (`algorithmic')
mutual information, probabilistic sufficient statistic versus algorithmic
sufficient statistic (related to lossy compression in the Shannon theory versus
meaningful information in the Kolmogorov theory), and rate distortion theory
versus Kolmogorov's structure function. Part of the material has appeared in
print before, scattered through various publications, but this is the first
comprehensive systematic comparison. The last mentioned relations are new.

<id>
cs/0410003v2
<category>
cs.IT
<abstract>
Capacity formulas and random-coding exponents are derived for a generalized
family of Gel'fand-Pinsker coding problems. These exponents yield asymptotic
upper bounds on the achievable log probability of error. In our model,
information is to be reliably transmitted through a noisy channel with finite
input and output alphabets and random state sequence, and the channel is
selected by a hypothetical adversary. Partial information about the state
sequence is available to the encoder, adversary, and decoder. The design of the
transmitter is subject to a cost constraint. Two families of channels are
considered: 1) compound discrete memoryless channels (CDMC), and 2) channels
with arbitrary memory, subject to an additive cost constraint, or more
generally to a hard constraint on the conditional type of the channel output
given the input. Both problems are closely connected. The random-coding
exponent is achieved using a stacked binning scheme and a maximum penalized
mutual information decoder, which may be thought of as an empirical generalized
Maximum a Posteriori decoder. For channels with arbitrary memory, the
random-coding exponents are larger than their CDMC counterparts. Applications
of this study include watermarking, data hiding, communication in presence of
partially known interferers, and problems such as broadcast channels, all of
which involve the fundamental idea of binning.

<id>
cs/0410008v1
<category>
cs.IT
<abstract>
We consider source coding with fixed lag side information at the decoder. We
focus on the special case of perfect side information with unit lag
corresponding to source coding with feedforward (the dual of channel coding
with feedback) introduced by Pradhan. We use this duality to develop a linear
complexity algorithm which achieves the rate-distortion bound for any
memoryless finite alphabet source and distortion measure.

<id>
cs/0410040v1
<category>
cs.IT
<abstract>
We propose use of QR factorization with sort and Dijkstra's algorithm for
decreasing the computational complexity of the sphere decoder that is used for
ML detection of signals on the multi-antenna fading channel. QR factorization
with sort decreases the complexity of searching part of the decoder with small
increase in the complexity required for preprocessing part of the decoder.
Dijkstra's algorithm decreases the complexity of searching part of the decoder
with increase in the storage complexity. The computer simulation demonstrates
that the complexity of the decoder is reduced by the proposed methods
significantly.

<id>
cs/0410041v1
<category>
cs.IT
<abstract>
In this paper, we analyze the performance of space-time block codes which
enable symbolwise maximum likelihood decoding. We derive an upper bound of
maximum mutual information (MMI) on space-time block codes that enable
symbolwise maximum likelihood decoding for a frequency non-selective
quasi-static fading channel. MMI is an upper bound on how much one can send
information with vanishing error probability by using the target code.

<id>
cs/0411006v1
<category>
cs.IT
<abstract>
In this paper, we present two low complexity algorithms that achieve capacity
for the noiseless (d,k) constrained channel when k=2d+1, or when k-d+1 is not
prime. The first algorithm, called symbol sliding, is a generalized version of
the bit flipping algorithm introduced by Aviran et al. [1]. In addition to
achieving capacity for (d,2d+1) constraints, it comes close to capacity in
other cases. The second algorithm is based on interleaving, and is a
generalized version of the bit stuffing algorithm introduced by Bender and Wolf
[2]. This method uses fewer than k-d biased bit streams to achieve capacity for
(d,k) constraints with k-d+1 not prime. In particular, the encoder for
(d,d+2^m-1) constraints, 1\le m<\infty, requires only m biased bit streams.

<id>
cs/0411011v1
<category>
cs.IT
<abstract>
Capacity analysis for channels with side information at the receiver has been
an active area of interest. This problem is well investigated for the case of
finite alphabet channels. However, the results are not easily generalizable to
the case of continuous alphabet channels due to analytic difficulties inherent
with continuous alphabets. In the first part of this two-part paper, we address
an analytical framework for capacity analysis of continuous alphabet channels
with side information at the receiver. For this purpose, we establish novel
necessary and sufficient conditions for weak* continuity and strict concavity
of the mutual information. These conditions are used in investigating the
existence and uniqueness of the capacity-achieving measures. Furthermore, we
derive necessary and sufficient conditions that characterize the capacity value
and the capacity-achieving measure for continuous alphabet channels with side
information at the receiver.

<id>
cs/0411012v1
<category>
cs.IT
<abstract>
In this part, we consider the capacity analysis for wireless mobile systems
with multiple antenna architectures. We apply the results of the first part to
a commonly known baseband, discrete-time multiple antenna system where both the
transmitter and receiver know the channel's statistical law. We analyze the
capacity for additive white Gaussian noise (AWGN) channels, fading channels
with full channel state information (CSI) at the receiver, fading channels with
no CSI, and fading channels with partial CSI at the receiver. For each type of
channels, we study the capacity value as well as issues such as the existence,
uniqueness, and characterization of the capacity-achieving measures for
different types of moment constraints. The results are applicable to both
Rayleigh and Rician fading channels in the presence of arbitrary line-of-sight
and correlation profiles.

<id>
cs/0411014v4
<category>
cs.IT
<abstract>
We examine the structure of families of distortion balls from the perspective
of Kolmogorov complexity. Special attention is paid to the canonical
rate-distortion function of a source word which returns the minimal Kolmogorov
complexity of all distortion balls containing that word subject to a bound on
their cardinality. This canonical rate-distortion function is related to the
more standard algorithmic rate-distortion function for the given distortion
measure. Examples are given of list distortion, Hamming distortion, and
Euclidean distortion. The algorithmic rate-distortion function can behave
differently from Shannon's rate-distortion function. To this end, we show that
the canonical rate-distortion function can and does assume a wide class of
shapes (unlike Shannon's); we relate low algorithmic mutual information to low
Kolmogorov complexity (and consequently suggest that certain aspects of the
mutual information formulation of Shannon's rate-distortion function behave
differently than would an analogous formulation using algorithmic mutual
information); we explore the notion that low Kolmogorov complexity distortion
balls containing a given word capture the interesting properties of that word
(which is hard to formalize in Shannon's theory) and this suggests an approach
to denoising; and, finally, we show that the different behavior of the
rate-distortion curves of individual source words to some extent disappears
after averaging over the source words.

<id>
cs/0411036v2
<category>
cs.IT
<abstract>
The feedback capacity of the stationary Gaussian additive noise channel has
been open, except for the case where the noise is white. Here we find the
feedback capacity of the stationary first-order moving average additive
Gaussian noise channel in closed form. Specifically, the channel is given by
$Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\{X_i\}$ satisfies a power
constraint and the noise $\{Z_i\}$ is a first-order moving average Gaussian
process defined by $Z_i = \alpha U_{i-1} + U_i,$ $|\alpha| \le 1,$ with white
Gaussian innovations $U_i,$ $i = 0,1,....$
  We show that the feedback capacity of this channel is $-\log x_0,$ where
$x_0$ is the unique positive root of the equation $ \rho x^2 = (1-x^2) (1 -
|\alpha|x)^2,$ and $\rho$ is the ratio of the average input power per
transmission to the variance of the noise innovation $U_i$. The optimal coding
scheme parallels the simple linear signalling scheme by Schalkwijk and Kailath
for the additive white Gaussian noise channel -- the transmitter sends a
real-valued information-bearing signal at the beginning of communication and
subsequently refines the receiver's error by processing the feedback noise
signal through a linear stationary first-order autoregressive filter. The
resulting error probability of the maximum likelihood decoding decays
doubly-exponentially in the duration of the communication. This feedback
capacity of the first-order moving average Gaussian channel is very similar in
form to the best known achievable rate for the first-order
\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, and
Tiernan, although the optimality of the latter is yet to be established.

<id>
cs/0411073v1
<category>
cs.IT
<abstract>
Geographic routing with greedy relaying strategies have been widely studied
as a routing scheme in sensor networks. These schemes assume that the nodes
have perfect information about the location of the destination. When the
distance between the source and destination is normalized to unity, the
asymptotic routing delays in these schemes are $\Theta(\frac{1}{M(n)}),$ where
M(n) is the maximum distance traveled in a single hop (transmission range of a
radio). In this paper, we consider routing scenarios where nodes have location
errors (imprecise GPS), or where only coarse geographic information about the
destination is available, and only a fraction of the nodes have routing
information. We show that even with such imprecise or limited
destination-location information, the routing delays are
$\Theta(\frac{1}{M(n)})$. We also consider the throughput-capacity of networks
with progressive routing strategies that take packets closer to the destination
in every step, but not necessarily along a straight-line. We show that the
throughput-capacity with progressive routing is order-wise the same as the
maximum achievable throughput-capacity.

<id>
cs/0411098v1
<category>
cs.IT
<abstract>
We obtain the first term in the high signal-to-noise ratio (SNR) expansion of
the capacity of fading networks where the transmitters and receivers--while
fully cognizant of the fading \emph{law}--have no access to the fading
\emph{realization}. This term is an integer multiple of $\log \log
\textnormal{SNR}$ with the coefficient having a simple combinatorial
characterization.

<id>
cs/0412060v2
<category>
cs.IT
<abstract>
The dependence of the Gaussian input information rate on the line-of-sight
(LOS) matrix in multiple-input multiple-output coherent Rician fading channels
is explored. It is proved that the outage probability and the mutual
information induced by a multivariate circularly symmetric Gaussian input with
any covariance matrix are monotonic in the LOS matrix D, or more precisely,
monotonic in D'D in the sense of the Loewner partial order. Conversely, it is
also demonstrated that this ordering on the LOS matrices is a necessary
condition for the uniform monotonicity over all input covariance matrices. This
result is subsequently applied to prove the monotonicity of the isotropic
Gaussian input information rate and channel capacity in the singular values of
the LOS matrix. Extensions to multiple-access channels are also discussed.

<id>
cs/0412067v1
<category>
cs.IT
<abstract>
Recently, a quasi-orthogonal space-time block code (QSTBC) capable of
achieving a significant fraction of the outage mutual information of a
multiple-input-multiple output (MIMO) wireless communication system for the
case of four transmit and one receive antennas was proposed. We generalize
these results to $n_T=2^n$ transmit and an arbitrary number of receive antennas
$n_R$. Furthermore, we completely characterize the structure of the equivalent
channel for the general case and show that for all $n_T=2^n$ and $n_R$ the
eigenvectors of the equivalent channel are fixed and independent from the
channel realization. Furthermore, the eigenvalues of the equivalent channel are
independent identically distributed random variables each following a
noncentral chi-square distribution with $4n_R$ degrees of freedom.
  Based on these important insights into the structure of the QSTBC, we derive
an analytical lower bound for the fraction of outage probability achieved with
QSTBC and show that this bound is tight for low signal-to-noise-ratios (SNR)
values and also for increasing number of receive antennas. We also present an
upper bound, which is tight for high SNR values and derive analytical
expressions for the case of four transmit antennas. Finally, by utilizing the
special structure of the QSTBC we propose a new transmit strategy, which
decouples the signals transmitted from different antennas in order to detect
the symbols separately with a linear ML-detector rather than joint detection,
an up to now only known advantage of orthogonal space-time block codes (OSTBC).

<id>
cs/0412108v1
<category>
cs.IT
<abstract>
This paper deals with arbitrarily distributed finite-power input signals
observed through an additive Gaussian noise channel. It shows a new formula
that connects the input-output mutual information and the minimum mean-square
error (MMSE) achievable by optimal estimation of the input given the output.
That is, the derivative of the mutual information (nats) with respect to the
signal-to-noise ratio (SNR) is equal to half the MMSE, regardless of the input
statistics. This relationship holds for both scalar and vector signals, as well
as for discrete-time and continuous-time noncausal MMSE estimation. This
fundamental information-theoretic result has an unexpected consequence in
continuous-time nonlinear estimation: For any input signal with finite power,
the causal filtering MMSE achieved at SNR is equal to the average value of the
noncausal smoothing MMSE achieved with a channel whose signal-to-noise ratio is
chosen uniformly distributed between 0 and SNR.

<id>
cs/0412111v2
<category>
cs.IT
<abstract>
A new lower bound on the error probability of maximum likelihood decoding of
a binary code on a binary symmetric channel was proved in Barg and McGregor
(2004, cs.IT/0407011). It was observed in that paper that this bound leads to a
new region of code rates in which the random coding exponent is asymptotically
tight, giving a new region in which the reliability of the BSC is known
exactly. The present paper explains the relation of these results to the union
bound on the error probability.

<id>
cs/0412112v1
<category>
cs.IT
<abstract>
We introduce the idea of distortion side information, which does not directly
depend on the source but instead affects the distortion measure. We show that
such distortion side information is not only useful at the encoder, but that
under certain conditions, knowing it at only the encoder is as good as knowing
it at both encoder and decoder, and knowing it at only the decoder is useless.
Thus distortion side information is a natural complement to the signal side
information studied by Wyner and Ziv, which depends on the source but does not
involve the distortion measure. Furthermore, when both types of side
information are present, we characterize the penalty for deviating from the
configuration of encoder-only distortion side information and decoder-only
signal side information, which in many cases is as good as full side
information knowledge.

<id>
math/9409211v1
<category>
math.KT
<abstract>
We define an unreduced version of the e-controlled lower $K$-theoretic groups
of Ranicki and Yamasaki, and Quinn. We show that the reduced versions of our
groups coincide (in the inverse limit and its first derived, $\lim^1$) with
those of Ranicki and Yamasaki. We also relate the controlled groups to the
continuously controlled groups of Anderson and Munkholm, and to the Quinn
homology groups of Quinn.

<id>
math/9901144v1
<category>
math.KT
<abstract>
Studies the cohomology of p-central, powerful, p-groups with a certain
extension property. These groups are naturally associated to Lie algebras. The
paper develops a machinery that calculates the first few terms of the Bockstein
spectral sequence in terms of the associated Lie algebras. This is then used to
obtain results on the integral cohomology of these groups.

<id>
math/9901145v1
<category>
math.KT
<abstract>
Studies among other things, the question of whether a Lie algebra over
Z/(p^k)Z lifts to one over Z/(p^(k+1))Z. An obstruction theory is developed and
examples of Fp-Lie algebras which don't lift to Lie algebras over Z/p^2Z are
discussed. An example of an application of the result: A Fp-Lie algebra L with
H^3(L, ad)=0 will lift to a p-adic Lie algebra.

<id>
math/9906205v1
<category>
math.KT
<abstract>
We prove excision in entire and periodic cyclic cohomology and construct a
Chern-Connes character for Fredholm modules over a C*-algebra without
summability restrictions, taking values in a variant of Connes's entire cyclic
cohomology.
  Before these results can be obtained, we have to sort out some fundamental
questions about the class of algebras on which to define entire cyclic
cohomology. The right domain of definition for entire cyclic cohomology is the
category of complete bornological algebras. For these algebras, we define a
bivariant cohomology theory, called analytic cyclic cohomology, that contains
Connes's entire cyclic cohomology as a special case.
  The definition of analytic cyclic cohomology is based on the Cuntz-Quillen
approach to cyclic cohomology theories using tensor algebras and X-complexes.
The appropriate completion of the tensor algebra that yields analytic cyclic
cohomology can be understood using an appropriate notion of analytic
nilpotence.
  In addition, we develop the elementary theory of analytic cyclic cohomology
(smooth homotopy invariance, stability, Chern character in K-theory).

<id>
math/9910186v2
<category>
math.KT
<abstract>
The contributors establish a connection between the Quillen K-theory of certain
local fields and the de Rham-Witt complex of their rings of integers with
logarithmic poles at the maximal ideal. They consider fields K that are
complete discrete valuation fields of characteristic zero with perfect residue
fields k of characteristic p > 2. They evaluate the K-theory with
Z/p^v-coefficients of K, and verify the Lichtenbaum-Quillen conjecture for K.

<id>
math/0001138v1
<category>
math.KT
<abstract>
In this paper we study the fiber F of the rational Jones-Goodwillie character
$$ F:=\hofiber(ch:K^\rat(A)@>>>HN^\rat(A)) $$ going from K-theory to negative
cyclic homology of associative rings. We describe this fiber F in terms of
sheaf cohomology. We prove that, for $n\ge 1$, there is an isomorphism: $$
\pi_n(F)\cong H^{-n}_{inf}(A,K^\rat) $$ between the homotopy of the fiber and
the hypercohomology groups of $K^\rat$ on a non-commutative version of
Grothendieck's infinitesimal site.

<id>
math/0001145v1
<category>
math.KT
<abstract>
We consider commutative algebras and chain DG algebras over a fixed
commutative ground ring $k$ as in the title. We are concerned with the problem
of computing the cyclic (and Hochschild) homology of such algebras via free
DG-resolutions $\Lambda V @>>> A$. We find spectral sequences
$$E^2_{p,q}=H_p(\Lambda V\otimes\Gamma^q(dV))\Rightarrow HH_{p+q}(\Lambda V)$$
and $${E'}^2_{\pq}=H_p(\Lambda V\otimes\Gamma^{\le q}(dV)) \Rightarrow
HC_{p+q}(\Lambda V)$$ The algebra $\Lambda V\otimes\Gamma(dV)$ is a divided
power version of the de Rham algebra; in the particular case when $k$ is a
field of characteristic zero, the spectral sequences above agree with those
found by Burghelea and Vigu\'e (Cyclic homology of commutative algebras I,
Lecture Notes in Math. {\bf 1318} (1988) 51-72), where it is shown they
degenerate at the $E^2$ term. For arbitrary ground rings we prove here (Theorem
2.3) that if $V_n=0$ for $n\ge 2$ then $E^2=E^\infty$. From this we derive a
formula for the Hochschild homology of flat complete intersections in terms of
a filtration of the complex for crystalline cohomology, and find a description
of ${E'}^2$ also in terms of crystalline cohomology (theorem 3.0). The latter
spectral sequence degenerates for complete intersections of embedding dimension
$\le 2$ (Corollary 3.1). Without flatness assumptions, our results can be
viewed as the computation Shukla (cyclic) homology (T. Pirashvili, F.
Waldhausen; Mac Lane homology and topological Hochschild homology, J. Pure
Appl. Algebra{\bf 82} (1992) 81-98).

<id>
math/0001179v1
<category>
math.KT
<abstract>
Cuntz and Quillen have shown that for algebras over a field $k$ with
$char(k)=0$, periodic cyclic homology may be regarded, in some sense, as the
derived functor of (non-commutative) de Rham (co-)homology. The purpose of this
paper is to formalize this derived functor analogy. We show that the
localization ${Def}^{-1}\Cal{PA}$ of the category $\Cal{PA}$ of countable
pro-algebras at the class of (infinitesimal) deformations exists (in any
characteristic) (Theorem 3.2) and that, in characteristic zero, periodic cyclic
homology is the derived functor of de Rham cohomology with respect to this
localization (Corollary 5.4). We also compute the derived functor of rational
$K$-theory for algebras over $\Bbb Q$, which we show is essentially the fiber
of the Chern character to negative cyclic homology (Theorem 6.2).

<id>
math/0005126v3
<category>
math.KT
<abstract>
I. M. Gelfand and D. B. Fuks have studied the cohomology of the Lie algebra
of vector fields on a manifold. In this article, we generalize their main tools
to compute the Leibniz cohomology, by extending the two spectral sequences
associated to the diagonal and the order filtration. In particular, we
determine some new generators for the diagonal Leibniz cohomology of the Lie
algebra of vector fields on the circle.

<id>
math/0005271v1
<category>
math.KT
<abstract>
We calculate the R(G)-algebra structure on the reduced equivariant K-groups
of two-dimensional spheres on which a compact Lie group G acts as involutions.
In particular, the reduced equivariant K-groups are trivial if G is abelian,
which shows that the previous Y. Yang's calculation in [Yan95] is not true.

<id>
math/0009236v3
<category>
math.KT
<abstract>
We define an equivariant $K_0$-theory for \textit{Yetter-Drinfeld} algebras
over a Hopf algebra with an invertible antipode. We then show that this
definition can be generalized to all Hopf-module algebras. We show that there
exists a pairing, generalizing Connes' pairing, between this theory and a
suitably defined Hopf algebra equivariant cyclic cohomology theory.

<id>
math/0010153v3
<category>
math.KT
<abstract>
We define a new cyclic module, dual to the Connes-Moscovici cyclic module,
for Hopf algebras, and give a characteristric map for the coaction of Hopf
algebras. We also compute the resulting cyclic homology for cocommutative Hopf
algebras, and some quantum groups.

<id>
math/0010226v1
<category>
math.KT
<abstract>
Bhatwadekar and Raja Sridharan have constructed a homomorphism of abelian
groups from an orbit set Um(n,A)/E(n,A) of unimodular rows to an Euler class
group. We suggest that this is the last map in a longer exact sequence of
abelian groups. The hypothetical group G that precedes Um(n,A)/E(n,A) in the
sequence is an orbit set of unimodular two by n matrices over the ring A. If n
is at least four we describe a partially defined operation on two by n
matrices. We conjecture that this operation describes a group structure on G if
A has Krull dimension at most 2n-6. We prove that G is mapped onto a subgroup
of Um(n,A)/E(n,A) if A has Krull dimension at most 2n-5.

<id>
math/0011248v6
<category>
math.KT
<abstract>
We introduce the cylindrical module $A \natural \mathcal{H}$, where
$\mathcal{H}$ is a Hopf algebra and $A$ is a Hopf module algebra over
$\mathcal{H}$. We show that there exists an isomorphism between
$\mathsf{C}_{\bullet}(A^{op} \rtimes \mathcal{H}^{cop})$ the cyclic module of
the crossed product algebra $A^{op} \rtimes \mathcal{H}^{cop} $, and $\Delta(A
\natural \mathcal{H}) $, the cyclic module related to the diagonal of $A
\natural \mathcal{H}$. If $S$, the antipode of $\mathcal{H}$, is invertible it
follows that $\mathsf{C}_{\bullet}(A \rtimes \mathcal{H}) \simeq \Delta(A^{op}
\natural \mathcal{H}^{cop})$. When $S$ is invertible, we approximate
$HC_{\bullet}(A \rtimes \mathcal{H})$ by a spectral sequence and give an
interpretation of $ \mathsf{E}^0, \mathsf{E}^1$ and $\mathsf{E}^2 $ terms of
this spectral sequence.

<id>
math/0012107v1
<category>
math.KT
<abstract>
Let F be a global field, A its ring of adeles, G a reductive group over F. We
prove the Baum-Connes conjecture for the adelic group G(A).

<id>
math/0012213v1
<category>
math.KT
<abstract>
A brief account of K-theory written in honour of Friedrich Hirzebruch

<id>
math/0105105v7
<category>
math.KT
<abstract>
We introduce the concept of {\it para-Hopf algebroid} and define their cyclic
cohomology in the spirit of Connes-Moscovici cyclic cohomology for Hopf
algebras. Para-Hopf algebroids are closely related to, but different from, Hopf
algebroids. Their definition is motivated by attempting to define a cyclic
cohomology theory for Hopf algebroids in general. We show that many of Hopf
algebraic structures, including the Connes-Moscovici algebra
$\mathcal{H}_{FM}$, are para-Hopf algebroids.

<id>
math/0107166v3
<category>
math.KT
<abstract>
We extend our work in~\cite{rm01} to the case of Hopf comodule coalgebras. We
introduce the cocylindrical module $C \natural^{} \mathcal{H}$, where
$\mathcal{H}$ is a Hopf algebra with bijective antipode and $C$ is a Hopf
comodule coalgebra over $\mathcal{H}$. We show that there exists an isomorphism
between the cocyclic module of the crossed coproduct coalgebra $C >
\blacktriangleleft \mathcal{H} $ and $\Delta(C \natural^{}\mathcal{H}) $, the
cocyclic module related to the diagonal of $C \natural^{} \mathcal{H}$. We
approximate $HC^{\bullet}(C > \blacktriangleleft \mathcal{H}) $ by a spectral
sequence and we give an interpretation for $ \mathsf{E}^0, \mathsf{E}^1$ and
$\mathsf{E}^2 $ terms of this spectral sequence.

<id>
math/0107200v3
<category>
math.KT
<abstract>
We obtain a decomposition for the Hochschild cochain complex of a split
algebra and we study some properties of the cohomology of each term of this
decomposition. Then, we consider the case of trivial extensions, specially of
Frobenius algebras. In particular, we determine completely the cohomology of
the trivial extension of a finite dimensional Hopf algebra. Finally, as an
application, we obtain a result about the Hochschild cohomology of Frobenius
algebras.

<id>
math/0108126v3
<category>
math.KT
<abstract>
In this paper we construct a cylindrical module $A \natural \mathcal{H}$ for
an $\mathcal{H}$-comodule algebra $A$, where the antipode of the Hopf algebra
$\mathcal{H}$ is bijective. We show that the cyclic module associated to the
diagonal of $A \natural \mathcal{H}$ is isomorphic with the cyclic module of
the crossed product algebra $A \rtimes \mathcal{H}$. This enables us to derive
a spectral sequence for the cyclic homology of the crossed product algebra. We
also construct a cocylindrical module for Hopf module coalgebras and establish
a similar spectral sequence to compute the cyclic cohomology of crossed product
coalgebras.

<id>
math/0109025v1
<category>
math.KT
<abstract>
We compute Hochschild homology and cohomology of a class of generalized Weyl
algebras (for short GWA, defined by Bavula in St.Petersbourg Math. Journal 1999
4(1) pp. 71-90). Examples of such algebras are the n-th Weyl algebras, U(sl_2),
primitive quotients of U(sl_2), and subalgebras of invariants of these algebras
under finite cyclic groups of automorphisms. We answer a question of Bavula -
Jordan (Trans. A.M.S. 353 (2) 2001 pp. 769 -794) concerning the generator of
the group of automorphism of a GWA. We also explain previous results on the
invariants of Weyl algebras and of primitive quotients.

<id>
math/0109068v1
<category>
math.KT
<abstract>
Let $A_n$ be the $n$-th Weyl algebra, and let
$G\subset\Sp_{2n}(\C)\subset\Aut(A_n)$ be a finite group of linear
automorphisms of $A_n$. In this paper we compute the multiplicative structure
on the Hochschild cohomology $\HH^*(A_n^G)$ of the algebra of invariants of
$G$. We prove that, as a graded algebra, $\HH^*(A_n^G)$ is isomorphic to the
graded algebra associated to the center of the group algebra $\C G$ with
respect to a filtration defined in terms of the defining representation of $G$.

<id>
math/0110163v2
<category>
math.KT
<abstract>
In this paper the homology stability for symplectic groups over a ring with
finite stable rank is established. First we develop a `nerve theorem' on the
homotopy type of a poset in terms of a cover by subposets, where the cover is
itself indexed by a poset. We use the nerve theorem to show that a poset of
sequences of isotropic vectors is highly connected, as conjectured by Charney
in the eighties.

<id>
math/0111096v5
<category>
math.KT
<abstract>
Let $f:A \to B$ be a ring homomorphism of not necessarily unital rings and
$I\triangleleft A$ an ideal which is mapped by f isomorphically to an ideal of
B. The obstruction to excision in K-theory is the failure of the map between
relative K-groups $K_*(A:I) \to K_*(B:f(I))$ to be an isomorphism; it is
measured by the birelative groups $K_*(A,B:I)$. We show that these are
rationally isomorphic to the corresponding birelative groups for cyclic
homology up to a dimension shift. In the particular case when A and B are
$\Q$-algebras we obtain an integral isomorphism.

<id>
math/0111117v1
<category>
math.KT
<abstract>
In this paper homology stability for unitary groups over a ring with finite
unitary stable rank is established. Homology stability of symplectic groups and
orthogonal groups appears as a special case of our results.

<id>
math/0202037v2
<category>
math.KT
<abstract>
We define KK-theory spectra associated to C*-categories and look at certain
instances of the Kasparov product at this level. This machinery is used to give
a description of the analytic assembly map as a natural map of spectra.

<id>
math/0205276v2
<category>
math.KT
<abstract>
We compute periodic, analytic and local cyclic cohomology for convolution
algebras of compact Lie groups in order to exhibit differences between these
theories. A surprising result is that the periodic and analytic cyclic
cohomology of the smooth convolution algebras differ, although these algebras
have finite homological dimension.

<id>
math/0206231v1
<category>
math.KT
<abstract>
Nous generalisons la theorie de la K-moyennabilite au cas d'un unitaire
multiplicatif regulier V. Nous montrons que si (H,V,U) est un systeme de Kac
K-moyennable, alors pour toute S-algebre A, les algebres $ A\times_{m}\hat S$
(produit croise maximal) et $ A\times \hat S$ (produit croise reduit) sont
KK-equivalentes ou S est la $C^*$-algebre de Hopf reduite associee a V.

<id>
math/0207138v1
<category>
math.KT
<abstract>
We first prove that the Whitehead group of a torsion-free virtually solvable
linear group vanishes. Next we make a reduction of the fibered isomorphism
conjecture from virtually solvable groups to a class of virtually solvable
Q-linear groups. Finally we prove an L-theory analogue for elementary amenable
groups.

<id>
math/0207154v2
<category>
math.KT
<abstract>
We prove that the category of Hopf bimodules over any Hopf algebra has enough
injectives, which enables us to extend some results on the unification of Hopf
bimodule cohomologies of [T1,T2] to the infinite dimensional case. We also
prove that the cup-product defined on these cohomologies is graded-commutative.

<id>
math/9201239v1
<category>
math.LO
<abstract>
We construct a generic extension in which the aleph_2 nd canonical function
on aleph_1 exists.

<id>
math/9201240v1
<category>
math.LO
<abstract>
Suppose L is a relational language and P in L is a unary predicate. If M is
an L-structure then P(M) is the L-structure formed as the substructure of M
with domain {a: M models P(a)}. Now suppose T is a complete first order theory
in L with infinite models. Following Hodges, we say that T is relatively
lambda-categorical if whenever M, N models T, P(M)=P(N), |P(M)|= lambda then
there is an isomorphism i:M-> N which is the identity on P(M). T is relatively
categorical if it is relatively lambda-categorical for every lambda. The
question arises whether the relative lambda-categoricity of T for some lambda
>|T| implies that T is relatively categorical.
  In this paper, we provide an example, for every k>0, of a theory T_k and an
L_{omega_1 omega} sentence varphi_k so that T_k is relatively
aleph_n-categorical for n < k and varphi_k is aleph_n-categorical for n<k but
T_k is not relatively beth_k-categorical and varphi_k is not
beth_k-categorical.

<id>
math/9201241v1
<category>
math.LO
<abstract>
This the first of a series of articles dealing with abstract classification
theory. The apparatus to assign systems of cardinal invariants to models of a
first order theory (or determine its impossibility) is developed in [Sh:a]. It
is natural to try to extend this theory to classes of models which are
described in other ways. Work on the classification theory for nonelementary
classes [Sh:88] and for universal classes [Sh:300] led to the conclusion that
an axiomatic approach provided the best setting for developing a theory of
wider application. In the first chapter we describe the axioms on which the
remainder of the article depends and give some examples and context to justify
this level of generality. The study of universal classes takes as a primitive
the notion of closing a subset under functions to obtain a model. We replace
that concept by the notion of a prime model. We begin the detailed discussion
of this idea in Chapter II. One of the important contributions of
classification theory is the recognition that large models can often be
analyzed by means of a family of small models indexed by a tree of height at
most omega. More precisely, the analyzed model is prime over such a tree.
Chapter III provides sufficient conditions for prime models over such trees to
exist.

<id>
math/9201242v1
<category>
math.LO
<abstract>
It is consistent that for every n >= 2, every stationary subset of omega_n
consisting of ordinals of cofinality omega_k where k = 0 or k <= n-3 reflects
fully in the set of ordinals of cofinality omega_{n-1}. We also show that this
result is best possible.

<id>
math/9201243v1
<category>
math.LO
<abstract>
We show that the ordering of the Hanf number of L_{omega, omega}(wo) (well
ordering), L^c_{omega, omega} (quantification on countable sets), L_{omega,
omega}(aa) (stationary logic) and second order logic, have no more restraints
provable in ZFC than previously known (those independence proofs assume
CON(ZFC) only). We also get results on corresponding logics for L_{lambda, mu} .

<id>
math/9201244v1
<category>
math.LO
<abstract>
We continue here [Sh276] but we do not relay on it. The motivation was a
conjecture of Galvin stating that 2^{omega} >= omega_2 + omega_2->
[omega_1]^{n}_{h(n)} is consistent for a suitable h: omega-> omega. In section
5 we disprove this and give similar negative results. In section 3 we prove the
consistency of the conjecture replacing omega_2 by 2^omega, which is quite
large, starting with an Erd\H{o}s cardinal. In section 1 we present iteration
lemmas which are needed when we replace omega by a larger lambda and in section
4 we generalize a theorem of Halpern and Lauchli replacing omega by a larger
lambda .

<id>
math/9201245v1
<category>
math.LO
<abstract>
We show that it is not provable in ZFC that any two countable elementarily
equivalent structures have isomorphic ultrapowers relative to some ultrafilter
on omega .

<id>
math/9201246v1
<category>
math.LO
<abstract>
This is the second in a series of articles developing abstract classification
theory for classes that have a notion of prime models over independent pairs
and over chains. It deals with the problem of smoothness and establishing the
existence and uniqueness of a `monster model'. We work here with a predicate
for a canonically prime model.

<id>
math/9201247v1
<category>
math.LO
<abstract>
We look at an old conjecture of A. Tarski on cardinal arithmetic and show
that if a counterexample exists, then there exists one of length omega_1 +
omega .

<id>
math/9201248v1
<category>
math.LO
<abstract>
Every partition of [[omega_1]^{< omega}]^2 into finitely many pieces has a
cofinal homogeneous set. Furthermore, it is consistent that every directed
partially ordered set satisfies the partition property if and only if it has
finite character.

<id>
math/9201249v1
<category>
math.LO
<abstract>
Assuming 0^sharp does not exist, kappa is an uncountable cardinal and for all
cardinals lambda with kappa <= lambda < kappa^{+ omega}, 2^lambda = lambda^+,
we present a ``mini-coding'' between kappa and kappa^{+ omega}. This allows us
to prove that any subset of kappa^{+ omega} can be coded into a subset, W of
kappa^+ which, further, ``reshapes'' the interval [kappa, kappa^+), i.e., for
all kappa < delta < kappa^+, kappa = (card delta)^{L[W cap delta]}. We sketch
two applications of this result, assuming 0^sharp does not exist. First, we
point out that this shows that any set can be coded by a real, via a set
forcing. The second application involves a notion of abstract condensation, due
to Woodin. Our methods can be used to show that for any cardinal mu,
condensation for mu holds in a generic extension by a set forcing.

<id>
math/9201251v1
<category>
math.LO
<abstract>
We present a survey of some results of the pcf-theory and their applications
to cardinal arithmetic. We review basics notions (in section 1), briefly look
at history in section 2 (and some personal history in section 3). We present
main results on pcf in section 5 and describe applications to cardinal
arithmetic in section 6. The limitations on independence proofs are discussed
in section 7, and in section 8 we discuss the status of two axioms that arise
in the new setting. Applications to other areas are found in section 9.

<id>
math/9201253v1
<category>
math.LO
<abstract>
It is shown that if T is stable unsuperstable, and aleph_1< lambda
=cf(lambda)< 2^{aleph_0}, or 2^{aleph_0} < mu^+< lambda =cf(lambda)<
mu^{aleph_0} then T has no universal model in cardinality lambda, and if e.g.
aleph_omega < 2^{aleph_0} then T has no universal model in aleph_omega. These
results are generalized to kappa =cf(kappa) < kappa (T) in the place of
aleph_0. Also: if there is a universal model in lambda >|T|, T stable and kappa
< kappa (T) then there is a universal tree of height kappa +1 in cardinality
lambda .

<id>
math/9202205v1
<category>
math.LO
<abstract>
We study how equivalent nonisomorphic models of unsuperstable theories can
be. We measure the equivalence by Ehrenfeucht-Fraisse games. This paper
continues [HySh:474].

<id>
math/9204202v1
<category>
math.LO
<abstract>
We use the core model for sequences of measures to prove a new lower bound
for the consistency strength of the failure of the SCH:
  THEOREM
  (i) If there is a singular strong limit cardinal $\kappa$ such that $2^\kappa
> kappa^+$ then there is an inner model with a cardinal $\kappa$ such that for
all ordinals $\alpha<\kappa$ there is an ordinal $\nu < \kappa$ with $o(\nu) >
\alpha$.
  (ii) If there is a singular strong limit cardinal $\kappa$ of uncountable
cofinality such that $2^\kappa > \kappa^+$ then there is an inner model with
$o(\kappa) = \kappa^{++}$.
  Since this paper was originally submitted, Gitik has improved this result to
give exact lower bounds.

<id>
math/9204203v1
<category>
math.LO
<abstract>
The normal form theorem, proved in R. Laver, On the left distributive law and
the freeness of an algebra of elementary embeddings, Advances in Mathematics 91
(1992), 209-231, for the free algebra $\Cal A$ on one generator $x$ satisfying
the left distributive law $a(bc) = (ab)(ac)$ is extended by showing that
members of $\Cal A$ can be put into a "division form."

<id>
math/9204204v1
<category>
math.LO
<abstract>
Let $j:V_\lambda---> V_\lambda$ be an elementary embedding, with critical
point $\kappa$, and let $f(n)$ be the number of critical points of embeddings
in the algebra generated by $j$ which lie between $j^n(\kappa)$ and
$j^{n+1}(\kappa)$. It is shown that $f(n)$ is finite for all $n$.

<id>
math/9204205v1
<category>
math.LO
<abstract>
Various questions posed by P. Nyikos concerning ultrafilters on $\omega$ and
chains in the partial order $(\omega,<^*)$ are answered. The main tool is the
oracle chain condition and variations of it.

<id>
math/9204206v1
<category>
math.LO
<abstract>
Gives a short proof of Dehornoy's latest result. The same simple argument
(and more) was discovered by Laver's student Larue.

<id>
math/9204207v1
<category>
math.LO
<abstract>
A very short proof of G\"odel's second incompleteness theorem (for set
theory, second order arithmetic etc.)

<id>
math/9204208v1
<category>
math.LO
<abstract>
The purpose of this note is to prove irreflexivity, and hence the linear
ordering, in ZFC, without some of the machinery used by Dehornoy.

<id>
math/9204209v1
<category>
math.LO
<abstract>
This paper, dating from May 1991, contains preliminary (and unpublishable)
notes on investigations about iteration trees. They will be of interest only to
the specialist.
  In the first two sections I define notions of support and embeddings for tree
iterations, proving for example that every tree iteration is a direct limit of
finite tree iterations. This is a generalization to models with extenders of
basic ideas of iterated ultrapowers using only ultrapowers.
  In the final section (which is most of the paper) I sketch a proof that any
tree iteration can be embedded into a normal iteration, that is, a tree
iteration with the extenders in nondecreasing order of strength and with
strictly increasing critical points.

<id>
math/9204210v1
<category>
math.LO
<abstract>
A subset $A$ of a Boolean algebra $B$ is said to be $(n,m)$-reaped if there
is a partition of unity $P \subset B$ of size $n$ such that the cardinality of
$\{b \in P: b \wedge a \neq \emptyset\}$ is greater than or equal to $m$ for
all $a\in A$. The reaping number $r_{n,m}(B)$ of a Boolean algebra $B$ is the
minimum cardinality of a set $A \subset B\setminus \{0\}$ such which cannot be
$(n,m)$-reaped. It is shown that, for each $n \in \omega$, there is a Boolean
algebra $B$ such that $r_{n+1,2}(B) \neq r_{n,2}(B)$. Also, $\{r_{n,m}(B) :
\{n,m\}\subseteq\omega\}$ consists of at most two consecutive integers. The
existence of a Boolean algebra $B$ such that $r_{n,m}(B) \neq r_{n',m'}(B)$ is
equivalent to a statement in finite combinatorics which is also discussed.

<id>
math/9204218v1
<category>
math.LO
<abstract>
A stationary subset S of a regular uncountable cardinal kappa reflects fully
at regular cardinals if for every stationary set T subseteq kappa of higher
order consisting of regular cardinals there exists an alpha in T such that S
cap alpha is a stationary subset of alpha. We prove that the Axiom of Full
Reflection which states that every stationary set reflects fully at regular
cardinals, together with the existence of n-Mahlo cardinals is equiconsistent
with the existence of Pi^1_n-indescribable cardinals. We also state the
appropriate generalization for greatly Mahlo cardinals.

<id>
math/9205201v1
<category>
math.LO
<abstract>
When the second uniform indiscernible is $\aleph_{2}$, the Martin-Solovay
tree only constructs countably many reals; this resolves a number of open
questions in descriptive set theory.

<id>
math/9205202v1
<category>
math.LO
<abstract>
Given two elementary embeddings from the collection of sets of rank less than
$\lambda$ to itself, one can combine them to obtain another such embedding in
two ways: by composition, and by applying one to (initial segments of) the
other. Hence, a single such nontrivial embedding $j$ generates an algebra of
embeddings via these two operations, which satisfies certain laws (for example,
application distributes over both composition and application). Laver has
shown, among other things, that this algebra is free on one generator with
respect to these laws.
  The set of critical points of members of this algebra is the subject of this
paper. This set contains the critical point $\kappa_0$ of $j$, as well as all
of the other ordinals $\kappa_n$ in the critical sequence of $j$ (defined by
$\kappa_{n+1} = j(\kappa_n)$). But the set includes many other ordinals as
well. The main result of this paper is that the number of critical points below
$\kappa_n$ (which has been shown to be finite by Laver and Steel) grows so
quickly with $n$ that it dominates any primitive recursive function. In fact,
it grows faster than the Ackermann function, and even faster than a slow
iterate of the Ackermann function. Further results show that, even just below
$\kappa_4$, one can find so many critical points that the number is only
expressible using fast-growing hierarchies of iterated functions (six levels of
iteration beyond exponentials).

<id>
math/9205208v1
<category>
math.LO
<abstract>
For g < f in omega^omega we define c(f,g) be the least number of uniform
trees with g-splitting needed to cover a uniform tree with f-splitting. We show
that we can simultaneously force aleph_1 many different values for different
functions (f,g). In the language of Blass: There may be aleph_1 many distinct
uniform Pi^0_1 characteristics.

<id>
math/9207203v1
<category>
math.LO
<abstract>
Given a free ideal J of subsets of a set X, we consider games where player
ONE plays an increasing sequence of elements of the sigma completion of J, and
TWO tries to cover the union of this sequence by playing one set at a time from
J. We describe various conditions under which player TWO has has a winning
strategy that uses only information about the most recent k moves of ONE, and
apply some of these results to the Banach-Mazur game.

<id>
math/9207204v1
<category>
math.LO
<abstract>
Shelah introduced the revised countable support (RCS) iteration to iterate
semiproperness. This was an endpoint in the search for an iteration of a weak
condition, still implying that aleph1 is preserved.
  Dieter Donder found a better manageable approach to this iteration, which is
presented here.

<id>
math/9207205v1
<category>
math.LO
<abstract>
Let m be the least cardinal k such that MA(k) fails. The only known model for
"m is singular" was constructed by Kunen. In Kunen's model cof(m)=omega_1. It
is unknown whether "omega_1 < cof(m) < m" is consistent. The purpose of this
paper is to present a proof of Kunen's result and to identify the difficulties
of generalizing this result to an arbitrary uncountable cofinality.

<id>
math/9401222v1
<category>
math.MP
<abstract>
The immediate purpose of the paper was neither to review the basic
definitions of percolation theory nor to rehearse the general physical notions
of universality and renormalization (an important technique to be described in
Part Two). It was rather to describe as concretely as possible, although in
hypothetical form, the geometric aspects of universality, especially conformal
invariance, in the context of percolation, and to present the numerical results
that support the hypotheses. On the other hand, one ulterior purpose is to draw
the attention of mathematicians to the mathematical problems posed by the
physical notions. Some precise basic definitions are necessary simply to orient
the reader. Moreover a brief description of scaling and universality on the one
hand and of renormalization on the other is also essential in order to
establish their physical importance and to clarify their mathematical content.

<id>
math/9709219v1
<category>
math.MP
<abstract>
We establish the isomorphism between a nonlinear $\sigma$-model and the
abelian gauge theory on an arbitrary curved background, which allows us to
derive integrable models and the corresponding Lax representations from gauge
theoretical point of view. In our approach the spectral parameter is related to
the global degree of freedom associated with the conformal or Galileo
transformations of the spacetime. The B$\ddot{\rm a}$cklund transformations are
derived from Chern-Simons theory where the spectral parameter is defined in
terms of the extract compactified space dimension coordinate.

<id>
math/9712278v1
<category>
math.MP
<abstract>
The initial value problem for the Ginzburg-Landau-Schr\"odinger equation is
examined in the $\epsilon \rightarrow 0$ limit under two main assumptions on
the initial data $\phi^\epsilon$. The first assumption is that $\phi^\epsilon$
exhibits $m$ distinct vortices of degree $\pm 1$; these are described as points
of concentration of the Jacobian $[J\phi^\epsilon]$ of $\phi^\epsilon$. Second,
we assume energy bounds consistent with vortices at the points of
concentration. Under these assumptions, we identify ``vortex structures'' in
the $\epsilon \rightarrow 0$ limit of $\phi^\epsilon$ and show that these
structures persist in the solution $u^\epsilon(t)$ of $GLS_\epsilon$. We derive
ordinary differential equations which govern the motion of the vortices in the
$\epsilon \rightarrow 0$ limit. The limiting system of ordinary differential
equations is a Hamitonian flow governed by the renormalized energy of Bethuel,
Brezis and H\'elein. Our arguments rely on results about the structural
stability of vortices which are proved in a separate paper.

<id>
math/9801140v1
<category>
math.MP
<abstract>
We study the Cauchy problem for an $p$-Laplacian type of evolution system
${\mathbf H}_{t}+\g [ | \g {\mathbf H}|^{p-2} \g {\mathbf H}|]={\mathbf F}$.
This system governs the evolution of a magnetic field ${\bf H}$, where the
current displacement is neglected and the electrical resistivity is assumed to
be some power of the current density. The existence, uniqueness and regularity
of solutions to the system are established. Furthermore, it is shown that the
limit solution as the power $p\rightarrow \infty$ solves the problem of Bean's
model in the type-II superconductivity theory. The result provides us
information about how the superconductor material under the external force to
become the normal conductor and vice visa. It also provides an effective method
to find numerical solutions to Bean's model.

<id>
math-ph/9801201v1
<category>
math.MP
<abstract>
We study symmetry properties of the Schr\"odinger equation with the potential
as a new dependent variable, i.e., the transformations which do not change the
form of the class of equations. We also consider systems of the Schr\"odinger
equations with certain conditions on the potential. In addition we investigate
symmetry properties of the equation with convection term. The contact
transformations of the Schr\"odinger equation with potential are obtained.

<id>
math-ph/9801202v1
<category>
math.MP
<abstract>
We study the differential forms over the frame bundle of the based loop
space. They are stochastics in the sense that we put over this frame bundle a
probability measure. In order to understand the curvatures phenomena which
appear when we look at the Lie bracket of two horizontal vector fields, we
impose some regularity assumptions over the kernels of the differential forms.
This allows us to define an exterior stochastic differential derivative over
these forms.

<id>
math-ph/9801203v1
<category>
math.MP
<abstract>
We investigate closed ideals in the Grassmann algebra serving as bases of
Lie-invariant geometric objects studied before by E. Cartan. Especially, the E.
Cartan theory is enlarged for Lax integrable nonlinear dynamical systems to be
treated in the frame work of the Wahlquist Estabrook prolongation structures on
jet-manifolds and Cartan-Ehresmann connection theory on fibered spaces. General
structure of integrable one-forms augmenting the two-forms associated with a
closed ideal in the Grassmann algebra is studied in great detail. An effective
Maurer-Cartan one-forms construction is suggested that is very useful for
applications. As an example of application the developed Lie-invariant
geometric object theory for the Burgers nonlinear dynamical system is
considered having given rise to finding an explicit form of the associated Lax
type representation.

<id>
math-ph/9801204v1
<category>
math.MP
<abstract>
We investigate Lie symmetries of Einstein's vacuum equations in N dimensions,
with a cosmological term. For this purpose, we first write down the second
prolongation of the symmetry generating vector fields, and compute its action
on Einstein's equations. Instead of setting to zero the coefficients of all
independent partial derivatives (which involves a very complicated substitution
of Einstein's equations), we set to zero the coefficients of derivatives that
do not appear in Einstein's equations. This considerably constrains the
coefficients of symmetry generating vector fields. Using the Lie algebra
property of generators of symmetries and the fact that general coordinate
transformations are symmetries of Einstein's equations, we are then able to
obtain all the Lie symmetries. The method we have used can likely be applied to
other types of equations.

<id>
math-ph/9801205v1
<category>
math.MP
<abstract>
Quantization of BKP type equations are done through the Moyal bracket and the
formalism of pseudo-differential operators. It is shown that a variant of the
dressing operator can also be constructed for such quantized systems.

<id>
math-ph/9804010v1
<category>
math.MP
<abstract>
This paper offers a solution method that allows one to find exact values for
a large class of convergent series of rational terms. Sums of this form arise
often in problems dealing with Quantum Field Theory.

<id>
math-ph/9804012v2
<category>
math.MP
<abstract>
The quantum derivatives of $e^{-A}, A^{-1}$ and $\log A$, which play a basic
role in quantum statistical physics, are derived and their convergence is
proven for an unbounded positive operator $A$ in a Hilbert space. Using the
quantum analysis based on these quantum derivatives, a basic equation for the
entropy operator in nonequilibrium systems is derived, and Zubarev's theory is
extended to infinite order with respect to a perturbation. Using the
first-order term of this general perturbational expansion of the entropy
operator, Kubo's linear response is rederived and expressed in terms of the
inner derivation $\delta_{{\cal H}}$ for the relevant Hamiltonian ${\cal H}$.
Some remarks on the conductivity $\sigma (\omega)$ are given.

<id>
math-ph/9804013v2
<category>
math.MP
<abstract>
We introduce the fuzzy supersphere as sequence of finite-dimensional,
noncommutative $Z_{2}$-graded algebras tending in a suitable limit to a dense
subalgebra of the $Z_{2}$-graded algebra of ${\cal H}^{\infty}$-functions on
the $(2| 2)$-dimensional supersphere. Noncommutative analogues of the body map
(to the (fuzzy) sphere) and the super-deRham complex are introduced. In
particular we reproduce the equality of the super-deRham cohomology of the
supersphere and the ordinary deRham cohomology of its body on the "fuzzy
level".

<id>
math-ph/9804015v3
<category>
math.MP
<abstract>
We find the covariant deformed Heisenberg algebra and the Laplace-Beltrami
operator on the extended $h$-deformed quantum plane and solve the Schr\"odinger
equations explicitly for some physical systems on the quantum plane. In the
commutative limit the behaviour of a quantum particle on the quantum plane
becomes that of the quantum particle on the Poincar\'e half-plane, a surface of
constant negative Gaussian curvature. We show the bound state energy spectra
for particles under specific potentials depend explicitly on the deformation
parameter $h$. Moreover, it is shown that bound states can survive on the
quantum plane in a limiting case where bound states on the Poincar\'e
half-plane disappear.

<id>
math-ph/9804017v1
<category>
math.MP
<abstract>
In this paper we study Lie symmetries, Kac-Moody-Virasoro algebras,
similarity reductions and particular solutions of two different recently
introduced (2+1)-dimensional nonlinear evolution equations, namely (i)
(2+1)-dimensional breaking soliton equation and (ii) (2+1)-dimensional
nonlinear Schr\"odinger type equation introduced by Zakharov and studied later
by Strachan. Interestingly our studies show that not all integrable higher
dimensional systems admit Kac-Moody-Virasoro type sub-algebras. Particularly
the two integrable systems mentioned above do not admit Virasoro type
subalgebras, eventhough the other integrable higher dimensional systems do
admit such algebras which we have also reviewed in the Appendix. Further, we
bring out physically interesting solutions for special choices of the symmetry
parameters in both the systems.

<id>
math-ph/9804018v1
<category>
math.MP
<abstract>
In this paper we give a method to obtain Darboux transformations (DTs) of
integrable equations. As an example we give a DT of the dispersive water wave
equation. Using the Miura map, we also obtain the DT of the Jaulent-Miodek
equation. \end{abstract}

<id>
math-ph/9805001v1
<category>
math.MP
<abstract>
The motion of an incompressible fluid in Lagrangian coordinates involves
infinitely many symmetries generated by the left Lie algebra of group of volume
preserving diffeomorphisms of the three dimensional domain occupied by the
fluid. Utilizing a 1+3-dimensional Hamiltonian setting an explicit realization
of this symmetry algebra is constructed recursively. A dynamical connection is
used to split the symmetries into reparametrization of trajectories and
one-parameter family of volume preserving diffeomorphisms of fluid domain.
Algebraic structures of symmetries and Hamiltonian structures of their
generators are inherited from the same construction. A comparison with the
properties of 2D flows is included.

<id>
math-ph/9805002v1
<category>
math.MP
<abstract>
A Poisson structure on the time-extended space R x M is shown to be
appropriate for a Hamiltonian formalism in which time is no more a privileged
variable and no a priori geometry is assumed on the space M of motions.
Possible geometries induced on the spatial domain M are investigated. An
abstract representation space for sl(2,R) algebra with a concrete physical
realization by the Darboux-Halphen system is considered for demonstration. The
Poisson bi-vector on R x M is shown to possess two intrinsic infinitesimal
automorphisms one of which is known as the modular or curl vector field.
Anchored to these two, an infinite hierarchy of automorphisms can be generated.
Implications on the symmetry structure of Hamiltonian dynamical systems are
discussed. As a generalization of the isomorphism between contact flows and
their symplectifications, the relation between Hamiltonian flows on R x M and
infinitesimal motions on M preserving a geometric structure therein is
demonstrated for volume preserving diffeomorphisms in connection with
three-dimensional motion of an incompressible fluid.

<id>
math-ph/9805003v1
<category>
math.MP
<abstract>
We propose a method for the approximate computation of the Green function of
a scalar massless field subjected to potential barriers of given size and shape
in spacetime. The potential of the barriers has the form
V(phi)=xi(phi^2-phi_0^2)^2; xi is very large and phi_0 very close to zero, the
product (xi phi_0^2) being finite and small. This is equivalent to the
insertion of a suitable constraint in the functional integral for phi. The
Green function contains a double Fourier transform of the characteristic
function of the region where the potential has support.

<id>
math-ph/9805016v1
<category>
math.MP
<abstract>
We review several procedures of quantization formulated in the framework of
(classical) phase space M. These quantization methods consider Quantum
Mechanics as a "deformation" of Classical Mechanics by means of the
"transformation" of the commutative algebra of smooth functions on M in a new
non-commutative algebra. These ideas lead in a natural way to Quantum Groups as
deformation (or quantization, in a broad sense) of Poisson-Lie groups, which is
also analysed here.

<id>
math-ph/9805018v1
<category>
math.MP
<abstract>
Let ${\cal H}(x,\xi)$ be a holomorphic Hamiltonian of quadratic growth on $
R^{2n}$, $b$ a holomorphic exponentially localized observable, $H$, $B$ the
corresponding operators on $L^2(R^n)$ generated by Weyl quantization, and
$U(t)=\exp{iHt/\hbar}$. It is proved that the $L^2$ norm of the difference
between the Heisenberg observable $B_t=U(t)BU(-t)$ and its semiclassical
approximation of order ${N-1}$ is majorized by $K N^{(6n+1)N}(-\hbar
ln\hbar)^N$ for $t\in [0,T_N(\hbar)]$ where $T_N(\hbar)=-{2 ln\hbar\over
{N-1}}$. Choosing a suitable $N(\hbar)$ the error is majorized by
$C\hbar^{ln|ln\hbar|}$, $0\leq t\leq |ln\hbar|/ln|ln\hbar|$. (Here $K,C$ are
constants independent of $N,\hbar$).

<id>
math-ph/9805024v1
<category>
math.MP
<abstract>
It is shown that any dynamic equation on a configuration bundle $Q\to R$ of
non-relativistic time-dependent mechanics is associated with connections on the
affine jet bundle $J^1Q\to Q$ and on the tangent bundle $TQ\to Q$. As a
consequence, any non-relativistic dynamic equation can be seen as a geodesic
equation with respect to a (non-linear) connection on the tangent bundle $TQ\to
Q$. Using this fact, the relationship between relativistic and non-relativistic
equations of motion is studied. The geometric notions of reference frames and
relative accelerations in non-relativistic mechanics are introduced in the
terms of connections. The covariant form of non-relativistic dynamic equations
is written.

<id>
math-ph/9805027v1
<category>
math.MP
<abstract>
A formula is derived that provides generating functions for any
multi-j-symbol, such as the 3-j-symbol, the 6-j-symbol, the 9-j-symbol, etc.
The result is completely determined by geometrical objects (loops and curves)
in the graph of the the multi-j-symbol. A geometric-combinatorical
interpretation for multi-j-symbols is given.

<id>
math-ph/9806004v1
<category>
math.MP
<abstract>
The talk presented at ICMP 97 focused on the scaling limits of critical
percolation models, and some other systems whose salient features can be
described by collections of random lines. In the scaling limit we keep track of
features seen on the macroscopic scale, in situations where the short--distance
scale at which the system's basic variables are defined is taken to zero. Among
the challenging questions are the construction of the limit, and the
explanation of some of the emergent properties, in particular the behavior
under conformal maps as discussed in [LPS 94]. A descriptive account of the
project, and some related open problems, is found in ref. [A] and in [AB]
(joint work with A. Burchard) where tools are developed for establishing a
curve--regularity condition which plays a key role in the construction of the
limit. The formulation of the scaling limit as a random Web measure permits to
formulate the question of uniqueness of measure(s) describing systems of random
curves satisfying the conditions of independence, Euclidean invariance, and
regularity. The uniqueness question remains open; progress on it could shed
light on the purported universality of critical behavior and the apparent
conformal invariance of the critical measures. The random Web yields also
another perspective on some of the equations of conformal field theory which
have appeared in this context, such as the equation proposed by J. Cardy [C].

<id>
math-ph/9806009v1
<category>
math.MP
<abstract>
A typical result of the paper is the following. Let $H_\gamma=H_0 +\gamma V$
where $H_0$ is multiplication by $|x|^{2l}$ and $V$ is an integral operator
with kernel $\cos< x,y\rang le$ in the space $L_2(R^d)$. If $l=d/2+ 2k$ for
some $k= 0,1,...$, then the operator $H_\gamma$ has infinite number of negative
eigenvalues for any coupling constant $\gamma\neq 0$. For other values of $l$,
the negative spectrum of $H_\gamma$ is infinite for $|\gamma|> \sigma_l$ where
$\sigma_l$ is some explicit positive constant. In the case $\pm \gamma\in
(0,\sigma_l]$, the number $N^{(\pm)}_l$ of negative eigenvalues of $H_\gamma$
is finite and does not depend on $\gamma$. We calculate $N^{(\pm)}_l$.

<id>
math-ph/9806011v2
<category>
math.MP
<abstract>
In this work we introduce the Killing-Yano symmetry on the phase space and we
investigate the symplectic structure on the space of Killing-Yano tensors. We
perform the detailed analyze of the $n$-dimensional flat space and the
Riemaniann manifolds with constant scalar curvature. We investigate the form of
some multipole tensors, which arise in the expansion of a system of charges and
currents, in terms of second-order Killing-Yano tensors in the phase space of
classical mechanics.
  We find some relations between these tensors and the generators of dynamical
symmetries like the angular momentum, the mass-inertia tensor, the conformal
operator and the momentum conjugate Runge-Lenz vector.

<id>
math-ph/9806012v2
<category>
math.MP
<abstract>
We give a proof of the Lieb-Thirring inequality in the critical case $d=1$,
$\gamma= 1/2$, which yields the best possible constant.

<id>
math-ph/9807001v2
<category>
math.MP
<abstract>
We study charge transport driven by deformations in molecular rings and
chains. Level crossings and the associated Longuet-Higgins phase play a central
role in this theory. In molecular rings a vanishing cycle of shears pinching a
gap closure leads, generically, to diverging charge transport around the ring.
We call such behavior homeopathic. In an infinite chain such a cycle leads to
integral charge transport which is independent of the strength of deformation.
In the Jahn-Teller model of a planar molecular ring there is a distinguished
cycle in the space of uniform shears which keeps the molecule in its manifold
of ground states and pinches level crossing. The charge transport in this cycle
gives information on the derivative of the hopping amplitudes.

<id>
math-ph/9807002v2
<category>
math.MP
<abstract>
The linearized Vlasov equation for a plasma system in a uniform magnetic
field and the corresponding linear Vlasov operator are studied. The spectrum
and the corresponding eigenfunctions of the Vlasov operator are found. The
spectrum of this operator consists of two parts: one is continuous and real;
the other is discrete and complex. Interestingly, the real eigenvalues are
infinitely degenerate, which causes difficulty solving this initial value
problem by using the conventional eigenfunction expansion method. Finally, the
Vlasov equation is solved by the resolvent method.

<id>
math-ph/9807003v1
<category>
math.MP
<abstract>
In this paper all seven-vertex type solutions of the coloured Yang-Baxter
equation dependent on spectral as well as coloured parameters are given. It is
proved that they are composed of five groups of basic solutions, two groups of
their degenerate forms up to five solution transformations. Moreover, all
solutions can be claasified into two types called Baxter type and free-fermion
type.

<id>
math-ph/9807005v1
<category>
math.MP
<abstract>
The Gutzwiller semiclassical trace formula links the eigenvalues of the
Scrodinger operator ^H with the closed orbits of the corresponding classical
mechanical system, associated with the Hamiltonian H, when the Planck constant
is small ("semiclassical regime"). Gutzwiller gave a heuristic proof, using the
Feynman integral representation for the propagator of ^H. Later on
mathematicians gave rigorous proofs of this trace formula, under different
settings, using the theory of Fourier Integral Operators and Lagrangian
manifolds. Here we want to show how the use of coherent states (or gaussian
beams) allows us to give a simple and direct proof.

<id>
math/9210213v1
<category>
math.MG
<abstract>
A family of sets has the $(p,q)$ property if among any $p$ members of the
family some $q$ have a nonempty intersection. It is shown that for every $p\ge
q\ge d+1$ there is a $c=c(p,q,d)<\infty$ such that for every family $\scr F$ of
compact, convex sets in $R^d$ that has the $(p,q)$ property there is a set of
at most $c$ points in $R^d$ that intersects each member of $\scr F$. This
extends Helly's Theorem and settles an old problem of Hadwiger and Debrunner.

<id>
math/9210218v1
<category>
math.MG
<abstract>
We describe a characterization of convex polyhedra in $\h^3$ in terms of
their dihedral angles, developed by Rivin. We also describe some geometric and
combinatorial consequences of that theory. One of these consequences is a
combinatorial characterization of convex polyhedra in $\E^3$ all of whose
vertices lie on the unit sphere. That resolves a problem posed by Jakob Steiner
in 1832.

<id>
math/9404230v1
<category>
math.MG
<abstract>
We present a method which shows that in $\Eb$ the Busemann-Petty problem,
concerning central sections of centrally symmetric convex bodies, has a
positive answer. Together with other results, this settles the problem in each
dimension.

<id>
math/9405218v1
<category>
math.MG
<abstract>
The Koebe circle packing theorem states that every finite planar graph can be
realized as the nerve of a packing of (non-congruent) circles in R^3. We
investigate the average kissing number of finite packings of non-congruent
spheres in R^3 as a first restriction on the possible nerves of such packings.
We show that the supremum k of the average kissing number for all packings
satisfies
  12.566 ~ 666/53 <= k < 8 + 4*sqrt(3) ~ 14.928
  We obtain the upper bound by a resource exhaustion argument and the upper
bound by a construction involving packings of spherical caps in S^3. Our result
contradicts two naive conjectures about the average kissing number: That it is
unbounded, or that it is supremized by an infinite packing of congruent
spheres.

<id>
math/9505210v1
<category>
math.MG
<abstract>
We give a construction of a self-similar tiling of the plane with any
prescribed expansion coefficient $\lambda\in\C$ (satisfying the necessary
algebraic condition of being a complex Perron number).
  For any integer $m>1$ we show that there exists a self-similar tiling with
$2\pi/m$-rotational symmetry group and expansion $\lambda$ if and only if
either $\lambda$ or $\lambda e^{2\pi i/m}$ is a complex Perron number for which
$e^{2\pi i/m}$ is in $\Q[\lambda]$, respectively $Q[\lambda e^{2\pi i/m}]$.

<id>
math/9508209v3
<category>
math.MG
<abstract>
We give a formula for the number of interior intersection points made by the
diagonals of a regular $n$-gon. The answer is a polynomial on each residue
class modulo 2520. We also compute the number of regions formed by the
diagonals, by using Euler's formula $V-E+F=2$.

<id>
math/9510217v1
<category>
math.MG
<abstract>
Let $P\subset\R^d$ be a $d$-dimensional polytope. The {\em realization space}
of~$P$ is the space of all polytopes $P'\subset\R^d$ that are combinatorially
equivalent to~$P$, modulo affine transformations. We report on work by the
first contributor, which shows that realization spaces of \mbox{4-dimensional}
polytopes can be ``arbitrarily bad'': namely, for every primary semialgebraic
set~$V$ defined over~$\Z$, there is a $4$-polytope $P(V)$ whose realization
space is ``stably equivalent'' to~$V$. This implies that the realization space
of a $4$-polytope can have the homotopy type of an arbitrary finite simplicial
complex, and that all algebraic numbers are needed to realize all $4$-
polytopes. The proof is constructive. These results sharply contrast the
$3$-dimensional case, where realization spaces are contractible and all
polytopes are realizable with integral coordinates (Steinitz's Theorem). No
similar universality result was previously known in any fixed dimension.

<id>
math/9511225v1
<category>
math.MG
<abstract>
We introduce and study certain notions which might serve as substitutes for
maximum density packings and minimum density coverings. A body is a compact
connected set which is the closure of its interior. A packing $\cal P$ with
congruent replicas of a body $K$ is $n$-saturated if no $n-1$ members of it can
be replaced with $n$ replicas of $K$, and it is completely saturated if it is
$n$-saturated for each $n\ge 1$. Similarly, a covering $\cal C$ with congruent
replicas of a body $K$ is $n$-reduced if no $n$ members of it can be replaced
by $n-1$ replicas of $K$ without uncovering a portion of the space, and it is
completely reduced if it is $n$-reduced for each $n\ge 1$. We prove that every
body $K$ in $d$-dimensional Euclidean or hyperbolic space admits both an
$n$-saturated packing and an $n$-reduced covering with replicas of $K$. Under
some assumptions on $K\subset \mathbb{E}^d$ (somewhat weaker than convexity),
we prove the existence of completely saturated packings and completely reduced
coverings, but in general, the problem of existence of completely saturated
packings and completely reduced coverings remains unsolved. Also, we
investigate some problems related to the the densities of $n$-saturated
packings and $n$-reduced coverings. Among other things, we prove that there
exists an upper bound for the density of a $d+2$-reduced covering of
$\mathbb{E}^d$ with congruent balls, and we produce some density bounds for the
$n$-saturated packings and $n$-reduced coverings of the plane with congruent
circles.

<id>
math/9605221v1
<category>
math.MG
<abstract>
Let $d_1\leq d_2\leq\ldots\leq d_{n\choose 2}$ denote the distances
determined by $n$ points in the plane. It is shown that $\min\sum_i
(d_{i+1}-d_i)^2=O(n^{-6/7})$, where the minimum is taken over all point sets
with minimal distance $d_1 \geq 1$. This bound is asymptotically tight.

<id>
math/9701213v1
<category>
math.MG
<abstract>
For a (compact) subset $K$ of a metric space and $\varepsilon > 0$, the {\em
covering number} $N(K , \varepsilon )$ is defined as the smallest number of
balls of radius $\varepsilon$ whose union covers $K$. Knowledge of the {\em
metric entropy}, i.e., the asymptotic behaviour of covering numbers for
(families of) metric spaces is important in many areas of mathematics
(geometry, functional analysis, probability, coding theory, to name a few). In
this paper we give asymptotically correct estimates for covering numbers for a
large class of homogeneous spaces of unitary (or orthogonal) groups with
respect to some natural metrics, most notably the one induced by the operator
norm. This generalizes earlier contributor's results concerning covering numbers of
Grassmann manifolds; the generalization is motivated by applications to
noncommutative probability and operator algebras. In the process we give a
characterization of geodesics in $U(n)$ (or $SO(m)$) for a class of
non-Riemannian metric structures.

<id>
math/9804023v1
<category>
math.MG
<abstract>
We give a short argument that for some C > 0, every n-dimensional Banach ball
K admits a 256-round subquotient of dimension at least C n/(log n). This is a
weak version of Milman's quotient of subspace theorem, which lacks the
logarithmic factor.

<id>
math/9804040v1
<category>
math.MG
<abstract>
For any delta > 1 we construct a periodic and locally finite packing of the
plane with ellipses whose delta-enlargement covers the whole plane. This
answers a question of Imre B\'ar\'any. On the other hand, we show that if C is
a packing in the plane with circular discs of radius at most 1, then its
1.00001-enlargement covers no square with side length 4.

<id>
math/9807107v1
<category>
math.MG
<abstract>
A real valued function $f$ defined on a convex $K$ is anemconvex function iff
it satisfies $$ f((x+y)/2) \le (f(x)+f(y))/2 + 1. $$ A thorough study of
approximately convex functions is made. The principal results are a sharp
universal upper bound for lower semi-continuous approximately convex functions
that vanish on the vertices of a simplex and an explicit description of the
unique largest bounded approximately convex function~$E$ vanishing on the
vertices of a simplex.
  A set $A$ in a normed space is an approximately convex set iff for all
$a,b\in A$ the distance of the midpoint $(a+b)/2$ to $A$ is $\le 1$. The bounds
on approximately convex functions are used to show that in $\R^n$ with the
Euclidean norm, for any approximately convex set $A$, any point $z$ of the
convex hull of $A$ is at a distance of at most
$[\log_2(n-1)]+1+(n-1)/2^{[\log_2(n-1)]}$ from $A$. Examples are given to show
this is the sharp bound. Bounds for general norms on $R^n$ are also given.

<id>
math/9809165v3
<category>
math.MG
<abstract>
Makeev conjectured that every constant-width body is inscribed in the dual
difference body of a regular simplex. We prove that homologically, there are an
odd number of such circumscribing bodies in dimension 3, and therefore
geometrically there is at least one. We show that the homological answer is
zero in higher dimensions, a result which is inconclusive for the geometric
question. We also give a partial generalization involving affine
circumscription of strictly convex bodies.

<id>
math/9810023v1
<category>
math.MG
<abstract>
We describe decomposition formulas for rotations of $R^3$ and $R^4$ that have
special properties with respect to stereographic projection. We use the lower
dimensional decomposition to analyze stereographic projections of great circles
in $S^2 \subset R^3$. This analysis provides a pattern for our analysis of
stereographic projections of the Clifford torus ${\mathcal C}\subset S^3
\subset R^4$. We use the higher dimensional decomposition to prove a symmetry
assertion for stereographic projections of ${\mathcal C}$ which we believe we
are the first to observe and which can be used to characterize the Clifford
torus among embedded minimal tori in $S^3$---though this last assertion goes
beyond the scope of this paper. An effort is made to intuitively motivate all
necessary concepts including rotation, stereographic projection, and symmetry.

<id>
math/9811071v2
<category>
math.MG
<abstract>
This is the first in a series of papers giving a proof of the Kepler
conjecture, which asserts that the density of a packing of congruent spheres in
three dimensions is never greater than $\pi/\sqrt{18}\approx 0.74048...$. This
is the oldest problem in discrete geometry and is an important part of
Hilbert's 18th problem. An example of a packing achieving this density is the
face-centered cubic packing.
  This paper has a historical overview and a synopsis of the rest of the
series. The other papers in the series are math.MG/9811072, math.MG/9811073,
math.MG/9811074, math.MG/9811075, math.MG/9811076, math.MG/9811077, and
math.MG/9811078.

<id>
math/9811072v2
<category>
math.MG
<abstract>
This is the second in a series of papers giving a proof of the Kepler
conjecture, which asserts that the density of a packing of congruent spheres in
three dimensions is never greater than $\pi/\sqrt{18}\approx 0.74048...$. This
is the oldest problem in discrete geometry and is an important part of
Hilbert's 18th problem. An example of a packing achieving this density is the
face-centered cubic packing.
  This paper defines a local formulation of the conjecture which is used in the
proof.

<id>
math/9811073v1
<category>
math.MG
<abstract>
We describe a program to prove the Kepler conjecture on sphere packings. We
then carry out the first step of this program. Each packing determines a
decomposition of space into Delaunay simplices, which are grouped together into
finite configurations called Delaunay stars. A score, which is related to the
density of packings, is assigned to each Delaunay star. We conjecture that the
score of every Delaunay star is at most the score of the stars in the
face-centered cubic and hexagonal close packings. This conjecture implies the
Kepler conjecture. To complete the first step of the program, we show that
every Delaunay star that satisfies a certain regularity condition satisfies the
conjecture.

<id>
math/9811074v1
<category>
math.MG
<abstract>
An earlier paper describes a program to prove the Kepler conjecture on sphere
packings. This paper carries out the second step of that program. A sphere
packing leads to a decomposition of $R^3$ into polyhedra. The polyhedra are
divided into two classes. The first class of polyhedra, called quasi-regular
tetrahedra, have density at most that of a regular tetrahedron. The polyhedra
in the remaining class have density at most that of a regular octahedron (about
0.7209).

<id>
math/9811075v2
<category>
math.MG
<abstract>
This is the fifth in a series of papers giving a proof of the Kepler
conjecture, which asserts that the density of a packing of congruent spheres in
three dimensions is never greater than $\pi/\sqrt{18}\approx 0.74048...$. This
is the oldest problem in discrete geometry and is an important part of
Hilbert's 18th problem. An example of a packing achieving this density is the
face-centered cubic packing.
  This paper carries out the third step of the program outlined in
math.MG/9811073: A proof that if all of the standard regions are triangles or
quadrilaterals, then the total score is less than $8 \pt$ (excluding the case
of pentagonal prisms).

<id>
math/9811076v2
<category>
math.MG
<abstract>
This is the sixth in a series of papers giving a proof of the Kepler
conjecture, which asserts that the density of a packing of congruent spheres in
three dimensions is never greater than $\pi/\sqrt{18}\approx 0.74048...$. This
is the oldest problem in discrete geometry and is an important part of
Hilbert's 18th problem. An example of a packing achieving this density is the
face-centered cubic packing.
  This paper completes part of the fourth step of the program outlined in
math.MG/9811073: A proof that if some standard region has more than four sides,
then the star scores less than $8 \pt$.

<id>
math/9811077v1
<category>
math.MG
<abstract>
The Hales program to prove the Kepler conjecture on sphere packings consists
of five steps, which if completed, will jointly comprise a proof of the
conjecture. We carry out step five of the program [outlined in
math.MG/9811073], a proof that the local density of a certain combinatorial
arrangement, the pentahedral prism, is less than that of the face-centered
cubic lattice packing. We prove various relations on the local density using
computer-based interval arithmetic methods. Together, these relations imply the
local density bound.

<id>
math/9811078v2
<category>
math.MG
<abstract>
This is the eighth and final paper in a series giving a proof of the Kepler
conjecture, which asserts that the density of a packing of congruent spheres in
three dimensions is never greater than $\pi/\sqrt{18}\approx 0.74048...$. This
is the oldest problem in discrete geometry and is an important part of
Hilbert's 18th problem. An example of a packing achieving this density is the
face-centered cubic packing.
  This paper completes the fourth step of the program outlined in
math.MG/9811073: A proof that if some standard region has more than four sides,
then the star scores less than $8 \pt$.

<id>
math/9811079v3
<category>
math.MG
<abstract>
The dodecahedral conjecture states that the volume of the Voronoi polyhedron
of a sphere in a packing of equal spheres is at least the volume of a regular
dodecahedron with inradius 1. The contributors prove the conjecture following the
methodology of the proof the Kepler conjecture. (See math.MG/9811071.)

<id>
math/9902131v1
<category>
math.MG
<abstract>
In this article we construct a complete system of M\"obius-geometric
invariants for pairs $(S^m, S^l), l \leq m$, of spheres contained in the
M\"obius space $S^n$. It consists of n-m generalised stationary angles. We
interpret these invariants geometrically.

<id>
math/9902160v1
<category>
math.MG
<abstract>
We show how a $d$-stress on a piecewise-linear realization of an oriented
(non-simplicial, in general) $d$-manifold in \rd naturally induces stresses of
lower dimensions on this manifold, and discuss implications of this
construction to the analysis of self-stresses in spatial frameworks. The
constructed mappings are not linear, but polynomial. In 1860-70s J. C. Maxwell
described an interesting relationship between self-stresses in planar
frameworks and vertical projections of polyhedral 2-surfaces. We offer a
partial analog of Maxwell correspondence for self-stresses in spatial
frameworks and vertical projections of 3-dimensional surfaces based on our
construction of polynomial mappings. Applying this theorem we derive a class of
three-dimensional spider webs similar to the family of two-dimensional spider
webs described by Maxwell. In addition, we conjecture an important property of
our mappings which is supported by a heuristic count based on the lower bound
theorem ($g_2(d+1)=dim\:Stress_2 \ge 0$) for $d$-pseudomanifolds generically
realized in ${\R}^{d+1}$ (Fogelsanger).

<id>
math/9903200v1
<category>
math.MG
<abstract>
We derive a formula connecting the derivatives of parallel section functions
of an origin-symmetric star body in R^n with the Fourier transform of powers of
the radial function of the body. A parallel section function (or
(n-1)-dimensional X-ray) gives the ((n-1)-dimensional) volumes of all
hyperplane sections of the body orthogonal to a given direction. This formula
provides a new characterization of intersection bodies in R^n and leads to a
unified analytic solution to the Busemann-Petty problem: Suppose that K and L
are two origin-symmetric convex bodies in R^n such that the ((n-1)-dimensional)
volume of each central hyperplane section of K is smaller than the volume of
the corresponding section of L; is the (n-dimensional) volume of K smaller than
the volume of L? In conjunction with earlier established connections between
the Busemann-Petty problem, intersection bodies, and positive definite
distributions, our formula shows that the answer to the problem depends on the
behavior of the (n-2)-nd derivative of the parallel section functions. The
affirmative answer to the Busemann-Petty problem for n\le 4 and the negative
answer for n\ge 5 now follow from the fact that convexity controls the second
derivatives, but does not control the derivatives of higher orders.

<id>
math/9903205v1
<category>
math.MG
<abstract>
H. Busemann and C. M. Petty posed the following problem in 1956: If K and L
are origin-symmetric convex bodies in R^n and for each hyperplane H through the
origin the volumes of their central slices satisfy vol(K cap H) < vol(L cap H),
does it follow that the volumes of the bodies themselves satisfy vol(K) <
vol(L)?
  The problem is trivially positive in R^2. However, a surprising negative
answer for n <= 12 was given by Larman and Rogers in 1975. Subsequently, a
series of contributions were made to reduce the dimensions to n >= 5 by a
number of contributors. That is, the problem has a negative answer for n >= 5. It
was proved by Gardner that the problem has a positive answer for n=3. The case
of n=4 was considered in [Ann. of Math. (2) 140 (1994), 331-346], but the
answer given there is not correct. This paper presents the correct solution,
namely, the Busemann-Petty problem has a positive solution in R^4, which,
together with results of other cases, brings the Busemann-Petty problem to a
conclusion.

<id>
math/9904047v5
<category>
math.MG
<abstract>
TO APPEAR IN AEQUATIONES MATHEMATICAE - WITHOUT THEOREM 2. THEOREM 2 IS
CORRECTLY PROVED IN PREVIOUS VERSIONS 1 AND 2. AUTHOR'S VERSION 3 (WITH A NEW
FIGURE 6A) IS UNNECESSARY. Let F \subseteq R denote the field of numbers which
are constructible by means of ruler and compass. We prove that: (1) if x,y \in
R^n (n>1) and |x-y| is an algebraic number then there exists a finite set
S(x,y) \subseteq R^n containing x and y such that each map from S(x,y) to R^n
preserving all unit distances preserves the distance between x and y; if x,y
\in F^n then we can choose S(x,y) \subseteq F^n, (2) only algebraic distances
|x-y| have the property from item (1), (3) if X1,X2,...,Xm \in R^n (n>1) lie on
some affine hyperplane then there exists a finite set L(X1,X2,...,Xm) \subseteq
R^n containing X1,X2,...,Xm such that each map from L(X1,X2,...,Xm) to R^n
preserving all unit distances preserves the property that X1,X2,...,Xm lie on
some affine hyperplane, (4) if J,K,L,M \in R^n (n>1) and |JK|=|LM| (|JK|<|LM|)
then there exists a finite set C(J,K,L,M) \subseteq R^n containing J,K,L,M such
that any map f:C(J,K,L,M) \to R^n that preserves unit distance satisfies
|f(J)f(K)|=|f(L)f(M)| (|f(J)f(K)|<|f(L)f(M)|).

<id>
math/9905111v1
<category>
math.MG
<abstract>
A new method of metric space investigation, based on classification of its
finite subspaces, is suggested. It admits to derive information on metric space
properties which is encoded in metric. The method describes geometry in terms
of only metric. It admits to remove constraints imposed usually on metric (the
triangle axiom and nonnegativity of the squared metric), and to use the metric
space for description of the space-time and other geometries with indefinite
metric. Describing space-time and using this method, one can explain quantum
effects as geometric effects, i.e. as space-time properties

<id>
math/9204217v1
<category>
math.NT
<abstract>
In the study of Dirichlet series with arithmetic significance there has
appeared (through the study of known examples) certain expectations, namely (i)
if a functional equation and Euler product exists, then it is likely that a
type of Riemann hypothesis will hold, (ii) that if in addition the function has
a simple pole at the point s=1, then it must be a product of the Riemann
zeta-function and another Dirichlet series with similar properties, and (iii)
that a type of converse theorem holds, namely that all such Dirichlet series
can be obtained by considering Mellin transforms of automorphic forms
associated with arithmetic groups.

<id>
math/9204234v1
<category>
math.NT
<abstract>
In this paper we discuss the basic problems of algorithmic algebraic number
theory. The emphasis is on aspects that are of interest from a purely
mathematical point of view, and practical issues are largely disregarded. We
describe what has been done and, more importantly, what remains to be done in
the area. We hope to show that the study of algorithms not only increases our
understanding of algebraic number fields but also stimulates our curiosity
about them. The discussion is concentrated of three topics: the determination
of Galois groups, the determination of the ring of integers of an algebraic
number field, and the computation of the group of units and the class group of
that ring of integers.

<id>
math/9210214v1
<category>
math.NT
<abstract>
Best possible bounds are obtained for the concentration function of an
additive arithmetic function on sequences of shifted primes.

<id>
math/9407219v1
<category>
math.NT
<abstract>
The contributor reviews results and conjectures of Selberg on a class of Dirichlet
series functions which share properties with the Riemann zeta function, and he
relates this work to the theory of Artin L-functions.

<id>
math/9407220v1
<category>
math.NT
<abstract>
In lectures at the Newton Institute in June of 1993, Andrew Wiles announced a
proof of a large part of the Taniyama-Shimura Conjecture and, as a consequence,
Fermat's Last Theorem. This report for nonexperts discusses the mathematics
involved in Wiles' lectures, including the necessary background and the
mathematical history.

<id>
math/9410212v1
<category>
math.NT
<abstract>
For a positive integer k and an arbitrary integer h, the Dedekind sum s(h,k)
was first studied by Dedekind because of the prominent role it plays in the
transformation theory of the Dedekind eta-function, which is a modular form of
weight 1/2 for the full modular group SL_2(Z). There is an extensive literature
about the Dedekind sums. Rademacher [8] has written an introductory book on the
subject.

<id>
math/9410216v1
<category>
math.NT
<abstract>
We show that two number fields with the same zeta function, and even with
isomorphic adele rings, do not necessarily have the same class number.

<id>
math/9411212v1
<category>
math.NT
<abstract>
A new upper bound is given for the dimension of the space of holomorphic cusp
forms of weight one and prime level $q$: $$ \hbox{dim}\, S_1(q) << q^{11/12}
\log^4{q} $$ with an absolute implied constant.

<id>
math/9411213v1
<category>
math.NT
<abstract>
As we have shown several years ago [Y2], zeros of $L(s, \Delta )$ and
$L^(2)(s, \Delta )$ can be calculated quite efficiently by a certain
experimental method. Here $\Delta$ denotes the cusp form of weight 12 with
respect to SL$(2, Z)$ and $L(s, \Delta )$ (resp. $L^(2)(s, \Delta )$) denotes
the standard (resp. symmetric square) $L$-function attached to $\Delta$. The
purpose of this paper is to show that this method can be applied to a wide
class of $L$-functions so that we can obtain precise numerical values of their
zeros.

<id>
math/9411214v1
<category>
math.NT
<abstract>
We describe the construction of vector valued modular forms transforming
under a given congruence representation of the modular group SL$(\bold Z)$ in
terms of theta series. We apply this general setup to obtain closed and easily
computable formulas for conformal characters of rational models of
$W$-algebras.

<id>
math/9412220v1
<category>
math.NT
<abstract>
The GUE Hypothesis, which concerns the distribution of zeros of the Riemann
zeta-function, is used to evaluate some integrals involving the logarithmic
derivative of the zeta-function. Some connections are shown between the GUE
Hypothesis and other conjectures.

<id>
math/9503218v1
<category>
math.NT
<abstract>
We describe some new general constructions of $p$-adic $L$-functions attached
to certain arithmetically defined complex $L$-functions coming from motives
over $\bold Q$ with coefficiens in a number field $T$, with $[T:\bold
Q]<\infty$. These constructions are equivalent to proving some generalized
Kummer congruences for critical special values of these complex $L$-functions.

<id>
math/9503219v1
<category>
math.NT
<abstract>
In this article, I discuss material which is related to the recent proof of
Fermat's Last Theorem: elliptic curves, modular forms, Galois representations
and their deformations, Frey's construction, and the conjectures of Serre and
of Taniyama--Shimura.

<id>
math/9504222v1
<category>
math.NT
<abstract>
In this expository paper we show how one can, in a uniform way, calculate the
weight distributions of some well-known binary cyclic codes. The codes are
related to certain families of curves, and the weight distributions are related
to the distribution of the number of rational points on the curves.

<id>
math/9504229v1
<category>
math.NT
<abstract>
Some identities are presented that generalize the formula x^3 = 3x floor(x
floor(x)) - 3 floor(x) floor(x floor(x)) + floor(x)^3 + 3 frac(x) frac(x
floor(x)) + frac(x)^3 to a representation of the product x_0x_1 ... x_{n-1}.

<id>
math/9506210v1
<category>
math.NT
<abstract>
We study monodromy action on abelian varieties satisfying certain bad
reduction conditions. These conditions allow us to get some control over the
Galois image. As a consequence we verify the Mumford--Tate conjecture for such
abelian varieties.

<id>
math/9507217v1
<category>
math.NT
<abstract>
It is conjectured that for fixed $A$, $r \ge 1$, and $d \ge 1$, there is a
uniform bound on the size of the torsion submodule of a Drinfeld $A$-module of
rank $r$ over a degree $d$ extension $L$ of the fraction field $K$ of $A$. We
verify the conjecture for $r=1$, and more generally for Drinfeld modules having
potential good reduction at some prime above a specified prime of $K$.
Moreover, we show that within an $\Lbar$-isomorphism class, there are only
finitely many Drinfeld modules up to isomorphism over $L$ which have nonzero
torsion. For the case $A=\Fq[T]$, $r=1$, and $L=\Fq(T)$, we give an explicit
description of the possible torsion submodules. We present three methods for
proving these cases of the conjecture, and explain why they fail to prove the
conjecture in general. Finally, an application of the Mordell conjecture for
characteristic $p$ function fields proves the uniform boundedness for the
$\pp$-primary part of the torsion for rank~2 Drinfeld $\Fq[T]$-modules over a
fixed function field.

<id>
math/9507218v1
<category>
math.NT
<abstract>
We compute the central critical value of the triple product $L$-function
associated to three cusp forms $f_1,f_2,f_3$ with trivial character for groups
$\Gamma_0(N_i)$ with square free levels $N_i$ not all of which are $1$ and
weights $k_i$ satisfying $k_1\ge k_2\ge k_3$ and $k_1<k_2+k_3$. This
generalizes work of Gross and Kudla and gives an alternative classical proof of
their results in the case $N_1=N_2=N_3$ with $k_1=k_2=k_3=2$.

<id>
math/9508208v1
<category>
math.NT
<abstract>
We discuss the equation $a^p + 2^\a b^p + c^p =0$ in which $a$, $b$, and $c$
are non-zero relatively prime integers, $p$ is an odd prime number, and $\a$ is
a positive integer. The technique used to prove Fermat's Last Theorem shows
that the equation has no solutions with $\a>1$ or $b$ even. When $\a=1$ and $b$
is odd, there are the two trivial solutions $(\pm 1, \mp 1, \pm 1)$. In 1952,
D\'enes conjectured that these are the only ones. Using methods of Darmon, we
prove this conjecture for $p\equiv1$ mod~4. We link the case $p\equiv3$ mod~4
to conjectures of Frey and Darmon about elliptic curves over~$\Q$ with
isomorphic mod~$p$ Galois representations.

<id>
math/9508210v1
<category>
math.NT
<abstract>
Let $C$ be an algebraically closed field containing the finite field $F_q$
and complete with respect to an absolute value $|\;|$. We prove that under
suitable constraints on the coefficients, the series $f(z) = \sum_{n \in \Z}
a_n z^{q^n}$ converges to a surjective, open, continuous $F_q$-linear
homomorphism $C \rightarrow C$ whose kernel is locally compact. We characterize
the locally compact sub-$F_q$-vector spaces $G$ of $C$ which occur as kernels
of such series, and describe the extent to which $G$ determines the series.
  We develop a theory of Newton polygons for these series which lets us compute
the Haar measure of the set of zeros of $f$ of a given valuation, given the
valuations of the coefficients. The ``adjoint'' series $f^\ast(z) = \sum_{n \in
\Z} a_n^{1/q^n} z^{1/q^n}$ converges everywhere if and only if $f$ does, and in
this case there is a natural bilinear pairing $$ \ker f \times \ker f^\ast
\rightarrow F_q $$ which exhibits $\ker f^\ast$ as the Pontryagin dual of $\ker
f$. Many of these results extend to non-linear fractional power series. We
apply these results to construct a Drinfeld module analogue of the Weil
pairing, and to describe the topological module structure of the kernel of the
adjoint exponential of a Drinfeld module.

<id>
math/9508211v1
<category>
math.NT
<abstract>
It has been conjectured that for $N$ sufficiently large, there are no
quadratic polynomials in $\bold Q[z]$ with rational periodic points of period
$N$. Morton proved there were none with $N=4$, by showing that the genus~$2$
algebraic curve that classifies periodic points of period~4 is birational to
$X_1(16)$, whose rational points had been previously computed. We prove there
are none with $N=5$. Here the relevant curve has genus~$14$, but it has a
genus~$2$ quotient, whose rational points we compute by performing
a~$2$-descent on its Jacobian and applying a refinement of the method of
Chabauty and Coleman. We hope that our computation will serve as a model for
others who need to compute rational points on hyperelliptic curves. We also
describe the three possible Gal$_{\bold Q}$-stable $5$-cycles, and show that
there exist Gal$_{\bold Q}$-stable $N$-cycles for infinitely many $N$.
Furthermore, we answer a question of Morton by showing that the genus~$14$
curve and its quotient are not modular. Finally, we mention some partial
results for $N=6$.

<id>
math/9509224v1
<category>
math.NT
<abstract>
We give explicit formulae for all of the terms in the asymptotic expansion of
the mean fourth power of the Riemann zeta-function on the critical line.

<id>
math/9510208v1
<category>
math.NT
<abstract>
We continue our study of Yoshida's lifting, which associates to a pair of
automorphic forms on the adelic multiplicative group of a quaternion algebra a
Siegel modular form of degree 2. We consider here the case that the automorphic
forms on the quaternion algebra correspond to modular forms of arbitrary even
weights and square free levels; in particular we obtain a construction of
Siegel modular forms of weight 3 attached to a pair of elliptic modular forms
of weights 2 and 4.

<id>
math/9511209v1
<category>
math.NT
<abstract>
Consider the $m$-th roots of unity in {\bf C}, where $m>0$ is an integer. We
address the following question: For what values of $n$ can one find $n$ such
$m$-th roots of unity (with repetitions allowed) adding up to zero? We prove
that the answer is exactly the set of linear combinations with non-negative
integer coefficients of the prime factors of $m$.

<id>
math/9601217v2
<category>
math.NT
<abstract>
Let $G$ be a reductive algebraic group defined over $\bQ$, with anisotropic
centre. Given a rational action of $G$ on a finite-dimensional vector space
$V$, we analyze the truncated integral of the theta series corresponding to a
Schwartz-Bruhat function on $V(\bA)$. The Poisson summation formula then yields
an identity of distributions on $V(\bA)$. The truncation used is due to Arthur.

<id>
math/9602221v1
<category>
math.NT
<abstract>
This paper gives an expository account of our experiments concerning
relations between modular forms for congruence subgroups of SL(3,Z) and three
dimensional Galois representations. The main new result presented here is a
calculation of the variations of the Hodge structure corresponding to the
motives we consider in realizing the Galois representations. It turns out that
the period spaces for the Hodge structures are four dimensional, while the
geometric realizations of such Hodge structures can appear in subspaces of
dimension at most one.

<id>
math/9604222v1
<category>
math.NT
<abstract>
S. Lang conjectured in 1974 that a hyperbolic algebraic variety defined over
a number field has only finitely many rational points, and its analogue over
function fields. We discuss the Nevanlinna-Cartan theory over function fields
of arbitrary dimension and apply it for Diophantine property of hyperbolic
projective hypersurfaces (homogeneous Diophantine equations) constructed by
Masuda-Noguchi. We also deal with the finiteness property of $S$-units points
of those Diophantine equations over number fields.

<id>
math/9604223v1
<category>
math.NT
<abstract>
The algebraic degeneracy of holomorphic curves in a semi-Abelian variety
omitting a divisor is proved (Lang's conjecture generalized to semi-Abelian
varieties) by making use of the {\it jet-projection method} and the logarithmic
Wronskian jet differential after Siu-Yeung. We also prove a structure theorem
for the locus which contains all possible image of non-constant entire
holomorphic curves in a semi-Abelian variety omitting a divisor.

<id>
math/9605216v1
<category>
math.NT
<abstract>
In an earlier work, the contributors have determined all possible weights $n$ for
which there exists a vanishing sum $\zeta_1+\cdots +\zeta_n=0$ of $m$th roots
of unity $\zeta_i$ in characteristic 0. In this paper, the same problem is
studied in finite fields of characteristic $p$. For given $m$ and $p$, results
are obtained on integers $n_0$ such that all integers $n\geq n_0$ are in the
``weight set'' $W_p(m)$. The main result $(1.3)$ in this paper guarantees,
under suitable conditions, the existence of solutions of $x_1^d+\cdots+x_n^d=0$
with all coordinates not equal to zero over a finite field.

<id>
math/9609214v1
<category>
math.NT
<abstract>
For each prime $p$, we determine the distribution of the $p^{th}$ Fourier
coefficients of the Hecke eigenforms of large weight for the full modular
group. As $p\to\infty$, this distribution tends to the Sato--Tate distribution.

<id>
math/9201266v1
<category>
math.NA
<abstract>
Numerical analysts might be expected to pay close attention to a branch of
complexity theory called information-based complexity theory (IBCT), which
produces an abundance of impressive results about the quest for approximate
solutions to mathematical problems. Why then do most numerical analysts turn a
cold shoulder to IBCT? Close analysis of two representative papers reveals a
mixture of nice new observations, error bounds repackaged in new language,
misdirected examples, and misleading theorems.
  Some elements in the framework of IBCT, erected to support a rigorous yet
flexible theory, make it difficult to judge whether a model is off-target or
reasonably realistic. For instance, a sharp distinction is made between
information and algorithms restricted to this information. Yet the information
itself usually comes from an algorithm, so the distinction clouds the issues
and can lead to true but misleading inferences. Another troublesome aspect of
IBCT is a free parameter $F$, the class of admissible problem instances. By
overlooking $F$'s membership fee, the theory sometimes distorts the economics
of problem solving in a way reminiscent of agricultural subsidies.
  The current theory's surprising results pertain only to unnatural situations,
and its genuinely new insights might serve us better if expressed in the
conventional modes of error analysis and approximation theory.

<id>
math/9201269v1
<category>
math.NA
<abstract>
The contributors discuss information-based complexity theory, which is a model of
finite-precision computations with real numbers, and its applications to
numerical analysis.

<id>
math/9604245v1
<category>
math.NA
<abstract>
Numerous mathematical models have emerged in the medical literature over the
past two decades attempting to characterize the pressure and volume dynamics
the central nervous system compartment. These models have been used to study he
behavior of this compartment under such pathological clinical conditions s
hydrocephalus, head injury and brain edema. The number of different pproaches
has led to considerable confusion regarding the validity, accuracy or
appropriateness of the various models. In this paper we review the mathematical
basis for these models in a mplified fashion, leaving the mathematical details
to appendices. We show at most previous models are in fact particular cases of
a single basic differential equation describing the evolution in time of the
cerebrospinal fluid pressure (CFS). Central to this approach is the hypothsis
that the rate change of CSF volume with respect to pressure is a measure of the
compliance of the brain tissue which as a consequence leads to particular
models epending on the form of the compliance funtion. All such models in fact
give essentially no information on the behavior of the brain itself. More
recent models (solved numerically using the Finite Element Method) have begun
to address this issue but have difficulties due to the lack of information
about the mechanical properties of the brain. Suggestions are made on how
development of models which account for these chanical properties might be
developed.

<id>
math/9806061v1
<category>
math.NA
<abstract>
This paper presents numerical and analytical investigation of gas flow in
gas-dynamic filter - a device for cleaning gas from solid particles with
counter flow of large water particles in order to prevent their release to the
atmosphere.
  Ideal and viscous gas flows are considered. It is assumed, that gas flow is
stationary, incompressible and plane, thus in the case of ideal gas stream
function is considered, and in its terms boundary conditions are formulated. To
determine stream function Dirichlet problem for Laplace equation is solved.
Numerical solution is obtained using five-point scheme, and analytical - by
conformal mapping. It is demonstrated that numerical solution fits very
accurately with the analytical one. Then in the already known gas flow field
trajectories of particles of different size are calculated in Lagrange
formulation, taking into account dust particles as well as filtering water
drops.
  Trajectories of particles of several different sizes under different modes of
filter operation were analysed. The adequacy of real and computed flow is
demonstrated.
  Computation of the flow is done using full Navier-Stokes equations. The
possibility of formation of rotation and separation zones in the flow is
demonstrated.

<id>
math/9811125v1
<category>
math.NA
<abstract>
We consider problems of dynamic viscoelasticity taking into account the
coupling of elastic and thermal fields. Efficient approximate models are
developed and computational results on thermomechanical behaviour of
shape-memory-alloy structures are presented.

<id>
math/9901055v1
<category>
math.NA
<abstract>
The possibility of interaction between Maple and numeric compiled languages
in performing extensive numeric calculations is exemplified by the Ndynamics
package, a tool for studying the (chaotic) behavior of dynamical systems.
Programming hints concerning the construction of Ndynamics are presented. The
system command, together with the application of the black-box concept, is used
to implement a powerful cooperation between Maple code and some other numeric
language code.

<id>
math/9901121v1
<category>
math.NA
<abstract>
We study numerical methods for the solution of general linear moment
problems, where the solution belongs to a family of nested subspaces of a
Hilbert space. Multi-level algorithms, based on the conjugate gradient method
and the Landweber--Richardson method are proposed that determine the "optimal"
reconstruction level a posteriori from quantities that arise during the
numerical calculations. As an important example we discuss the reconstruction
of band-limited signals from irregularly spaced noisy samples, when the actual
bandwidth of the signal is not available. Numerical examples show the
usefulness of the proposed algorithms.

<id>
math/9901122v1
<category>
math.NA
<abstract>
A shift-invariant system is a collection of functions $\{g_{m,n}\}$ of the
form $g_{m,n}(k) = g_m(k-an)$. Such systems play an important role in
time-frequency analysis and digital signal processing. A principal problem is
to find a dual system $\gamma_{m,n}(k) = \gamma_m(k-an)$ such that each
function $f$ can be written as $f = \sum < f, \gamma_{m,n} > g_{m,n}$. The
mathematical theory usually addresses this problem in infinite dimensions
(typically in $L_2(R)$ or $l_2(Z)$), whereas numerical methods have to operate
with a finite-dimensional model. Exploiting the link between the frame operator
and Laurent operators with matrix-valued symbol, we apply the finite section
method to show that the dual functions obtained by solving a finite-dimensional
problem converge to the dual functions of the original infinite-dimensional
problem in $l_2(Z)$. For compactly supported $g_{m,n}$ (FIR filter banks) we
prove an exponential rate of convergence and derive explicit expressions for
the involved constants. Further we investigate under which conditions one can
replace the discrete model of the finite section method by the periodic
discrete model, which is used in many numerical procedures. Again we provide
explicit estimates for the speed of convergence. Some remarks on tight frames
complete the paper.

<id>
math/9901123v2
<category>
math.NA
<abstract>
Trigonometric polynomials are widely used for the approximation of a smooth
function $f$ from a set of nonuniformly spaced samples
$\{f(x_j)\}_{j=0}^{N-1}$. If the samples are perturbed by noise, controlling
the smoothness of the trigonometric approximation becomes an essential issue to
avoid overfitting and underfitting of the data. Using the polynomial degree as
regularization parameter we derive a multi-level algorithm that iteratively
adapts to the least squares solution of optimal smoothness. The proposed
algorithm computes the solution in at most $\cal{O}(NM + M^2)$ operations ($M$
being the polynomial degree of the approximation) by solving a family of nested
Toeplitz systems. It is shown how the presented method can be extended to
multivariate trigonometric approximation. We demonstrate the performance of the
algorithm by applying it in echocardiography to the recovery of the boundary of
the Left Ventricle.

<id>
math/9901152v1
<category>
math.NA
<abstract>
The two-dimensional unsteady coupled Burgers' equations with moderate to
severe gradients, are solved numerically using higher-order accurate finite
difference schemes; namely the fourth-order accurate compact ADI scheme, and
the fourth-order accurate Du Fort Frankel scheme. The question of numerical
stability and convergence are presented. Comparisons are made between the
present schemes in terms of accuracy and computational efficiency for solving
problems with severe internal and boundary gradients. The present study shows
that the fourth-order compact ADI scheme is stable and efficient.

<id>
math/9902136v1
<category>
math.NA
<abstract>
The spectrum of the evolution Operator associated with a nonlinear stochastic
flow with additive noise is evaluated by diagonalization in a polynomial basis.
The method works for arbitrary noise strength. In the weak noise limit we
formulate a new perturbative expansion for the spectrum of the stochastic
evolution Operator in terms of expansions around the classical periodic orbits.
The diagonalization of such operators is easier to implement than the standard
Feynman diagram perturbation theory. The result is a stochastic analog of the
Gutzwiller semiclassical spectral determinant with the ``$\hbar$'' corrections
computed to at least two orders more than what has so far been attainable in
stochastic and quantum-mechanical applications, supplemented by the estimate
for the late terms in the asymptotic saddlepoint expansions.

<id>
math/9904122v1
<category>
math.NA
<abstract>
Discretization methods for ordinary differential equations based on the use
of matrix exponentials have been known for decades. This set of ideas has come
off age and acquired greater urgency recently, within the context of geometric
integration and discretization methods on manifolds based on the use of
Lie-group actions.
  In the present paper we study the approximation of the matrix exponential in
a particular context: given a Lie group $G$ and its Lie algebra $g$, we seek
approximants $F(tB)$ of $\exp(tB)$ such that $F(tB)\in G$ if $B\in g$. Having
fixed a basis of the Lie algebra, we write $F(tB)$ as a composition of
exponentials of the basis elements pre-multiplied by suitable scalar functions.

<id>
math/9904128v1
<category>
math.NA
<abstract>
An apriori bound for the condition number associated to each of the following
problems is given: general linear equation solving, minimum squares,
non-symmetric eigenvalue problems, solving univariate polynomials, solving
systems of multivariate polynomials. It is assumed that the input has integer
coefficients and is not on the degenerate locus of the respective problem (i.e.
the condition number is finite). Then condition numbers are bounded in terms of
the dimension and of the bit-size of the input.
  In the same setting, bounds are given for the speed of convergence of the
following iterative algorithms: QR without shift for the symmetric eigenvalue
problem, and Graeffe iteration for univariate polynomials.

<id>
math/9904129v1
<category>
math.NA
<abstract>
Lower bounds for some explicit decision problems over the complex numbers are
given.

<id>
math/9904130v1
<category>
math.NA
<abstract>
The class $\mathcal{UP}$ of `ultimate polynomial time' problems over $\mathbb
C$ is introduced; it contains the class $\mathcal P$ of polynomial time
problems over $\mathbb C$.
  The $\tau$-Conjecture for polynomials implies that $\mathcal{UP}$ does not
contain the class of non-deterministic polynomial time problems definable
without constants over $\mathbb C$.
  This latest statement implies that $\mathcal P \ne \mathcal{NP}$ over
$\mathbb C$.
  A notion of `ultimate complexity' of a problem is suggested. It provides
lower bounds for the complexity of structured problems.

<id>
math/9905003v1
<category>
math.NA
<abstract>
One of strengths in the finite element (FE) and Galerkin methods is their
capability to apply weak formulations via integration by parts, which leads to
elements matching at lower degree of continuity and relaxes requirements of
choosing basis functions. However, when applied to nonlinear problems, the
methods of this type require a great amount of computing effort of repeated
numerical integration. It is well known that the method of weighted residual is
the very basis of various popular numerical techniques including the FE and
Galerkin methods. This paper presents a novel methodology of weighted residual
for nonlinear computation with objectives to avoid the above-mentioned
shortcomings. It is shown that the presented nonlinear formulations of the FE
and Galerkin methods can be expressed in the Hadamard product form as in the
collocation and finite difference methods. Therefore, the recently developed
SJT product approach can be applied in the evaluation of the Jacobian matrix of
these nonlinear formulations. This also provides possibility to introduce the
nonlinear uncoupling technique to the FE and Galerkin nonlinear computing.
Furthermore, the present scheme of weighted residuals also greatly eases the
use of the least square and boundary element methods to nonlinear problems

<id>
math/9905036v1
<category>
math.NA
<abstract>
We give an algorithm for efficient step size control in numerical integration
of non-stiff initial value problems, based on a formula tailormade to methods
where the numerical solution is compared with a solution of lower order.

<id>
math/9905042v2
<category>
math.NA
<abstract>
Based on the matrix expression of general nonlinear numerical analogues
presented by the present contributor, this paper proposes a novel philosophy of
nonlinear computation and analysis. The nonlinear problems are considered an
ill-posed linear system. In this way, all nonlinear algebraic terms are instead
expressed as Linearly independent variables. Therefore, a n-dimension nonlinear
system can be expanded as a linear system of n(n+1)/2 dimension space. This
introduces the possibility to applying generalized inverse of matrix to
computation of nonlinear systems. Also, singular value decomposition (SVD) can
be directly employed in nonlinear analysis by using such a methodology.

<id>
math/9905091v1
<category>
math.NA
<abstract>
Approximate $p$-point Leibniz derivation formulas as well as interpolatory
Simpson quadrature sums adapted to oscillatory functions are discussed. Both
theoretical considerations and numerical evidence concerning the dependence of
the discretization errors on the frequency parameter of the oscillatory
functions show that the accuracy gain of the present formulas over those based
on the exponential fitting approach [L. Ixaru, "Computer Physics
Communications", 105 (1997) 1--19] is overwhelming.

<id>
math/9906054v1
<category>
math.NA
<abstract>
This paper provides a general proof of a relationship theorem between
nonlinear analogue polynomial equations and the corresponding Jacobian matrix,
presented recently by the present contributor. This theorem is also verified
generally effective for all nonlinear polynomial algebraic system of equations.
As two particular applications of this theorem, we gave a Newton formula
without requiring the evaluation of nonlinear function vector as well as a
simple formula to estimate the relative error of the approximate Jacobian
matrix. Finally, some possible applications of this theorem in nonlinear system
analysis are discussed.

<id>
math/9906055v1
<category>
math.NA
<abstract>
In order to avoid the evaluation of the Jacobian matrix and its inverse, the
present contributor recently introduced the pseudo-Jacobian matrix with a general
applicability of any nonlinear systems of equations. By using this concept,
this paper proposes the pseudo-Newton method

<id>
math/9906176v1
<category>
math.NA
<abstract>
This article is concerned with the integration of the time-dependent
Ginzburg-Landau (TDGL) equations of superconductivity. Four algorithms, ranging
from fully explicit to fully implicit, are presented and evaluated for
stability, accuracy, and compute time. The benchmark problem for the evaluation
is the equilibration of a vortex configuration in a superconductor that is
embedded in a thin insulator and subject to an applied magnetic field.

<id>
math/9907024v1
<category>
math.NA
<abstract>
`Dual composition', a new method of constructing energy-preserving
discretizations of conservative PDEs, is introduced. It extends the
summation-by-parts approach to arbitrary differential operators and conserved
quantities. Links to pseudospectral, Galerkin, antialiasing, and Hamiltonian
methods are discussed.

<id>
math/9907094v1
<category>
math.NA
<abstract>
The quasi-Newton equation is the very basis of a variety of the quasi-Newton
methods. By using a relationship formula between nonlinear polynomial equations
and the corresponding Jacobian matrix. presented recently by the present
contributor, we established an exact alternative of the approximate quasi-Newton
equation and consequently derived an modified BFGS updating formulas.

<id>
math/9907161v1
<category>
math.NA
<abstract>
This note presents a new definition of nonlinear statistics mean and variance
to simplify the nonlinear statistics computations. These concepts aim to
provide a theoretical explanation of a novel nonlinear weighted residual
methodology presented recently by the present contributor.

<id>
math/9907190v1
<category>
math.NA
<abstract>
We solve Poisson's equation using new multigrid algorithms that converge
rapidly. The novel feature of the 2D and 3D algorithms are the use of extra
diagonal grids in the multigrid hierarchy for a much richer and effective
communication between the levels of the multigrid. Numerical experiments
solving Poisson's equation in the unit square and unit cube show simple
versions of the proposed algorithms are up to twice as fast as correspondingly
simple multigrid iterations on a conventional hierarchy of grids.

<id>
math/9908149v1
<category>
math.NA
<abstract>
A new version of the Graeffe algorithm for finding all the roots of
univariate complex polynomials is proposed. It is obtained from the classical
algorithm by a process analogous to renormalization of dynamical systems. This
iteration is called Renormalized Graeffe Iteration.
  It is globally convergent, with probability 1. All quantities involved in the
computation are bounded, once the initial polynomial is given (with probability
1). This implies remarkable stability properties for the new algorithm, thus
overcoming known limitations of the classical Graeffe algorithm. If we start
with a degree-$d$ polynomial, each renormalized Graeffe iteration costs
$O(d^2)$ arithmetic operations, with memory $O(d)$. A probabilistic global
complexity bound is given. The case of univariate real polynomials is briefly
discussed. A numerical implementation of the algorithm presented herein allowed
us to solve random polynomials of degree up to 1000.

<id>
math/9908150v1
<category>
math.NA
<abstract>
Graeffe iteration was the choice algorithm for solving univariate polynomials
in the XIX-th and early XX-th century. In this paper, a new variation of
Graeffe iteration is given, suitable to IEEE floating-point arithmetics of
modern digital computers. We prove that under a certain generic assumption the
proposed algorithm converges. We also estimate the error after N iterations and
the running cost. The main ideas from which this algorithm is built are:
classical Graeffe iteration and Newton Diagrams, changes of scale
(renormalization), and replacement of a difference technique by a
differentiation one. The algorithm was implemented successfully and a number of
numerical experiments are displayed.

<id>
math/9909006v1
<category>
math.NA
<abstract>
A new highly accurate numerical approximation scheme based on a Gauss type
Clenshaw-Curtis Quadrature for Fredholm integral equations of the second kind,
whose kernel is either discontinuous or not smooth along the main diagonal, is
presented. This scheme is of spectral accuracy when the kernel is infinitely
differentiable away from the main diagonal, and is also applicable when the
kernel is singular along the boundary, and at isolated points on the main
diagonal. The corresponding composite rule is described. Application to
integro-differential Schroedinger equations with non-local potentials is given.

<id>
math/9909102v1
<category>
math.NA
<abstract>
We present a theoretical framework and numerical methods for predicting the
large-scale properties of solutions of partial differential equations that are
too complex to be properly resolved. We assume that prior statistical
information about the distribution of the solutions is available, as is often
the case in practice. The quantities we can compute condition the prior
information and allow us to calculate mean properties of solutions in the
future. We derive approximate ways for computing the evolution of the
probabilities conditioned by what we can compute, and obtain ordinary
differential equations for the expected values of a set of large-scale
variables. Our methods are demonstrated on two simple but instructive examples,
where the prior information consists of invariant canonical distributions

<id>
math/9204228v1
<category>
math.OA
<abstract>
Let $A$ be a von Neumann algebra with no direct summand of Type $\roman I_2$,
and let $\scr P(A)$ be its lattice of projections. Let $X$ be a Banach space.
Let $m\:\scr P(A)\to X$ be a bounded function such that $m(p+q)=m(p)+m(q)$
whenever $p$ and $q$ are orthogonal projections. The main theorem states that
$m$ has a unique extension to a bounded linear operator from $A$ to $X$. In
particular, each bounded complex-valued finitely additive quantum measure on
$\scr P(A)$ has a unique extension to a bounded linear functional on $A$.

<id>
math/9210227v1
<category>
math.OA
<abstract>
We present the analytic foundation of a unified B-D-F extension functor
$\operatorname{Ext}_\tau$ on the category of noncommutative smooth algebras,
for any Fr\'echet operator ideal $\Cal K_\tau$. Combining the techniques
devised by Arveson and Voiculescu, we generalize Voiculescu's theorem to smooth
algebras and Fr\'echet operator ideals. A key notion involved is
$\tau$-smoothness, which is verified for the algebras of smooth functions, via
a noncommutative Sobolev lemma. The groups $\operatorname{Ext}_\tau$ are
computed for many examples.

<id>
math/9501228v1
<category>
math.OA
<abstract>
We prove a basic result about tensor products of a $\text{II}_1$ factor with
a finite von Neumann algebra and use it to answer, affirmatively, a question
asked by S. Popa about maximal injective factors.

<id>
math/9511217v1
<category>
math.OA
<abstract>
We explicitly compute certain Douglas algebras that are invariant under both
the Bourgain map and the minimal envelope map. We also compute the Bourgain
algebra and the minimal envelope of the maximal subalgebras of a certain singly
generated Douglas algebra.

<id>
math/9512210v1
<category>
math.OA
<abstract>
Let $A$ be a Banach algebra, not necessarily unital, and let $B$ be a closed
subalgebra of $A$. We establish a connection between the Banach cyclic
cohomology group $ {\cal{HC}}^n(A)$ of $A$ and the Banach $B$-relative cyclic
cohomology group $ {\cal{HC}}^n_B(A) $ of $A$. We prove that, for a Banach
algebra $A$ with a bounded approximate identity and an amenable closed
subalgebra $B$ of $A$, up to topological isomorphism, ${\cal{HC}}^n(A) =
{\cal{HC}}^n_B(A) $ for all $n \ge 0$. We also establish a connection between
the Banach simplicial or cyclic cohomology groups of $A$ and those of the
quotient algebra $A/I$ by an amenable closed bi-ideal $I$. The results are
applied to the calculation of these groups for certain operator algebras,
including von Neumann algebras.

<id>
math/9512214v1
<category>
math.OA
<abstract>
We give several conditions on certain families of Douglas algebras that imply
that the minimal envelope of the given algebra is the algebra itself. We also
prove that the minimal envelope of the intersection of two Douglas algebras is
the intersection of their minimal envelope.

<id>
math/9512220v1
<category>
math.OA
<abstract>
Let $\Omega$ and $\Omega_{\fin}$ be the sets of all interpolating Blaschke
products of type $G$ and of finite type $G$, respectively. Let $E$ and
$E_{\fin}$ be the Douglas algebras generated by $H^\infty$ together with the
complex conjugates of elements of $\Omega$ and $\Omega_{\fin}$, respectively.
We show that the set of all invertible inner functions in $E$ is the set of all
finite products of elements of $\Omega$ , which is also the closure of $\Omega$
among the Blaschke products. Consequently, finite convex combinations of finite
products of elements of $\Omega$ are dense in the closed unit ball of the
subalgebra of $H^\infty$ generated by $\Omega$. The same results hold when we
replace $\Omega$ by $\Omega_{\fin}$ and $E$ by $E_{\fin}$.

<id>
math/9602219v1
<category>
math.OA
<abstract>
This paper gives a first step toward extending the theory of
Fourier-Stieltjes algebras from groups to groupoids. If G is a locally compact
(second countable) groupoid, we show that B(G), the linear span of the Borel
positive definite functions on G, is a Banach algebra when represented as an
algebra of completely bounded maps on a C^*-algebra associated with G. This
necessarily involves identifying equivalent elements of B(G). An example shows
that the linear span of the continuous positive definite functions need not be
complete. For groups, B(G) is isometric to the Banach space dual of C^*(G). For
groupoids, the best analog of that fact is to be found in a representation of
B(G) as a Banach space of completely bounded maps from a C^*-algebra associated
with G to a C^*-algebra associated with the equivalence relation induced by G.
This paper adds weight to the clues in the earlier study of Fourier-Stieltjes
algebras that there is a much more general kind of duality for Banach algebras
waiting to be explored.

<id>
math/9605227v1
<category>
math.OA
<abstract>
Let $\M$ be a von Neumann algebra with a faithful normal trace $\T$, and let
$H^\infty$ be a finite, maximal, subdiagonal algebra of $\M$. Fundamental
theorems on conjugate functions for weak$^*$\!-Dirichlet algebras are shown to
be valid for non-commutative $H^\infty$. In particular the conjugation operator
is shown to be a bounded linear map from $L^p(\M, \T)$ into $L^p(\M, \T)$ for
$1 < p < \infty$, and to be a continuous map from $L^1(\M,\T)$ into $L^{1,
\infty}(\M,\T)$. We also obtain that if an operator $a$ is such that
$|a|\log^+|a| \in L^1(\M,\T)$ then its conjugate belongs to $L^1(\M,\T)$.
Finally, we present some partial extensions of the classical Szeg\"o's theorem
to the non-commutative setting.

<id>
math/9606212v1
<category>
math.OA
<abstract>
We prove that, for every extension of Banach algebras $ 0 \rightarrow B
\rightarrow A \rightarrow D \rightarrow 0 $ such that $B$ has a left or right
bounded approximate identity, the existence of an associated long exact
sequence of Banach simplicial or cyclic cohomology groups is equivalent to the
existence of one for homology groups. It follows from the continuous version of
a result of Wodzicki that associated long exact sequences exist. In particular,
they exist for every extension of $C^*$-algebras.

<id>
math/9607230v1
<category>
math.OA
<abstract>
The contributor provides some definitions and structural results about Fell
bundles, defined as C^*-algebra bundles over topological groupoids. Such
bundles are a mutual generalization of semi-direct products of groups with
C^*-algebras and C^*-algebra bundles over topological spaces. In particular a
Morita equivalence theorem with semi-direct products is established.

<id>
math/9707228v1
<category>
math.OA
<abstract>
Let Z denote the simple limit of prime dimension drop algebras that has a
unique tracial state. Let A != 0 be a unital C^*-algebra with A = A tensor Z.
Then the homotopy groups of the group U(A) of unitaries in A are stable
invariants, namely, \pi_i(U(A)) = K_{i-1}(A) for all integers i >= 0.
Furthermore, A has cancellation for full projections, and satisfies the
comparability question for full projections. Analogous results hold for
non-unital Z-stable C^*-algebras.

<id>
math/9801002v2
<category>
math.OA
<abstract>
Groupoid actions on C*-bundles and inverse semigroup actions on C*-algebras
are closely related when the groupoid is r-discrete.

<id>
math/9801014v1
<category>
math.OA
<abstract>
Every Heisenberg manifold has a natural "sub-Riemannian" metric with
interesting properties. We describe the corresponding noncommutative metric
structure for Rieffel's quantum Heisenberg manifolds.

<id>
math/9802067v1
<category>
math.OA
<abstract>
Pimsner introduced the C*-algebra O_X generated by a Hilbert bimodule X over
a C*-algebra A. We look for additional conditions that X should satisfy in
order to study simplicity and, more generally, the ideal structure of O_X when
X is finite projective. We introduce two conditions: `(I)-freeness' and
`(II)-freeness', stronger than the former, in analogy with [J. Cuntz, W.
Krieger, Invent. Math. 56, 251-268] and [J. Cuntz, Invent. Math. 63, 25-40]
respectively. (I)-freeness comprehend the case of the bimodules associated with
an inclusion of simple C*-algebras with finite index, real or pseudoreal
bimodules with finite dimension and the case of `Cuntz-Krieger bimodules'. If X
satisfies this condition the C*-algebra O_X does not depend on the choice of
the generators when A is faithfully represented. As a consequence, if X is
(I)-free and A is X-simple, then O_X is simple. In the case of Cuntz-Krieger
algebras, X-simplicity corresponds to irreducibility of the defining matrix. If
A is simple and p.i. then O_X is p.i., if A is nonnuclear then O_X is
nonnuclear. We therefore provide examples of (purely) infinite nonnuclear
simple C*-algebras. Furthermore if X is (II)-free, we determine the ideal
structure of O_X.

<id>
math/9803043v1
<category>
math.OA
<abstract>
First, we construct the Jones tower and tunnel of the central sequence
subfactor arising from a hyperfinite type II_1 subfactor with finite index and
finite depth, and prove each algebra has the double commutant property in the
ultraproduct of the enveloping II_1 factor. Next, we show the equivalence
between Popa's strong amenability and the double commutant property of the
central sequence factor for subfactors as above without assuming the finite
depth condition.

<id>
math/9803133v1
<category>
math.OA
<abstract>
In this paper we discuss some physical applications of topological *-algebras
of unbounded operators. Our first example is a simple system of free bosons.
Then we analyze different models which are related to this one. We also discuss
the time evolution of two interacting models of matter and bosons. We show that
for all these systems it is possible to build up a common framework where the
thermodynamical limit of the algebraic dynamics can be conveniently studied and
obtained.

<id>
math/9803134v1
<category>
math.OA
<abstract>
For each $\alpha \in (0,1)$, $A_\alpha$ denotes the universal $C^*$-algebra
generated by two unitaries $u$ and $v$, which fulfill the commutation relation
$uv=\exp (2\pi i\alpha)vu$. We consider the order four automorphism $\sigma$ of
$A_\alpha$ defined by $\sigma (u)=v$, $\sigma (v)=u^{-1}$ and describe a method
for constructing projections in the fixed point algebra $A_\alpha^\sigma$,
using Rieffel's imprimitivity bimodules and Jacobi's theta functions. In the
case $\alpha =q^{-1}$, $q\in {\bf Z}$, $q\geq 2$, we give explicit formulae for
such projections and find some lower bounds for $|u+u^* +v+v^*|$.

<id>
math/9803148v1
<category>
math.OA
<abstract>
We define for discrete finitely presented groups a new property related to
their asymptotic representations. Namely we say that a groups has the property
AGA if every almost representation generates an asymptotic representation. We
give examples of groups with and without this property. For our example of a
group $G$ without AGA the group $K^0(BG)$ cannot be covered by asymptotic
representations of $G$.

<id>
math/9804026v1
<category>
math.OA
<abstract>
We characterize injectivity of von Neumann algebras in terms of factoring
bilinear maps as products of linear maps.

<id>
math/9804090v3
<category>
math.OA
<abstract>
Let A be a commutative unital C*-algebra and let S denote its Gelfand
spectrum. We give some necessary and sufficient conditions for a nondegenerate
representation of A to be unitarily equivalent to a multiplicative
representation on a space L^2(S, m), where m is a positive measure on the Baire
sets of S. We also compare these conditions with the multiplicity-free property
of a representation.

<id>
math/9804101v1
<category>
math.OA
<abstract>
Every AF-algebra arises as a graph algebra in the sense of Kumjian, Pask,
Raeburn, and Renault. For AF-algebras, the diagonal subalgebra defined by
Stratila and Voiculescu is consistent with Kumjian's notion of diagonal, and
the groupoid arising from a well-chosen Bratteli diagram for A coincides with
Kumjian's twist groupoid constructed from a diagonal of A.

<id>
math/9806007v2
<category>
math.OA
<abstract>
Every invariant linear manifold for a CSL-algebra is a closed subspace if,
and only if, each non-zero projection in the projection lattice is generated by
finitely many atoms. In the case of a nest, this condition is equivalent to the
condition that every non-zero projection in the nest has an immediate
predecessor (the nest of orthogonal complements is well ordered).
  The invariant linear manifolds of a nest algebra are totally ordered by
inclusion if, and only if, every non-zero projection in the nest has an
immediate predecessor.

<id>
math/9806045v1
<category>
math.OA
<abstract>
We develop a theory of boundary functions for ideals in trivially analytic
subalgebras of simple AF C*-algebras with an injective 0-cocycle, a class which
includes all full nest algebras. Boundary functions are maps from the spectrum
of the diagonal of the analytic subalgebra to itself. The relation between
boundary functions and ideal sets is explored and a description is given of
meet and join irreducible boundary functions.

<id>
math/9806093v3
<category>
math.OA
<abstract>
Suppose a C*-algebra A acts by adjointable operators on a Hilbert A-module X.
Pimsner constructed a C*-algebra O_X which includes, for particular choices of
X, crossed products of A by Z, the Cuntz algebras O_n, and the Cuntz-Krieger
algebras O_B. Here we analyse the representations of the corresponding Toeplitz
algebra. One consequence is a uniqueness theorem for the Toeplitz-Cuntz-Krieger
algebras of directed graphs, which includes Cuntz's uniqueness theorem for
O_\infty.

<id>
math/9807054v1
<category>
math.OA
<abstract>
Given two C*-algebras A and B, abstract A-B bimodules that can be
isometrically represented as operator bimodules are characterised in terms of
their norm. Various properties of such bimodules are given. Their theory is
very similar to those of classical normed spaces.

<id>
math/9807084v2
<category>
math.OA
<abstract>
Let a compact Lie group act ergodically on a unital $C^*$-algebra $A$. We
consider several ways of using this structure to define metrics on the state
space of $A$. These ways involve length functions, norms on the Lie algebra,
and Dirac operators. The main thrust is to verify that the corresponding metric
topologies on the state space agree with the weak-$*$ topology.

<id>
math/9808100v1
<category>
math.OA
<abstract>
A notion of curvature is introduced in multivariable operator theory and an
analogue of the Gauss-Bonnet-Chern theorem is established for graded
(contractive) Hilbert modules over the complex polynomial algebra in d
variables, d=1,2,3,....
  The curvature invariant, Euler characteristic, and degree are computed for
some explicit examples based on varieties in (multidimensional) complex
projective space, and applications are given to the structure of graded ideals
in C[z_1,...,z_d] and to the existence of "inner sequences" for closed
submodules of the free Hilbert module H^2(C^d).

<id>
math/9808131v1
<category>
math.OA
<abstract>
A classification is given for (regular) positions of direct sums of two
matroid algebras (unital algebraic limits of matrix algebras) in a matroid
superalgebra, where the individual summands have index 2 in their associated
corner algebra. A similar classification is obtained for positions of direct
sums of 2-symmetric algebras and, in the odd case, for the positions of sums of
2-symmetric C*-algebras in matroid C*-algebras. The approach relies on an
analysis of intermediate non-self-adjoint operator algebras and the
classifications are given in terms of K0 invariants, partial isometry homology
and scales in the associated composite K0-homology group.

<id>
math/9809089v1
<category>
math.OA
<abstract>
The mid-seventies' works on C*-algebras of Brown-Douglas-Fillmore and Elliott
both contained uniqueness and existence results in a now standard sense. These
papers served as keystones for two separate theories -- KK-theory and the
classification program -- which for many years parted ways with only moderate
interaction. But recent years have seen a fruitful interaction which has been
one of the main engines behind rapid progress in the classification program.
  In the present paper we take this interaction even further. We prove general
existence and uniqueness results using KK-theory and a concept of
quasidiagonality for representations. These results are employed to obtain new
classification results for certain classes of quasidiagonal C*-algebras
introduced by H. Lin. An important novel feature of these classes is that they
are defined by a certain local approximation property, rather than by an
inductive limit construction.
  Our existence and uniqueness results are in the spirit of classical
Ext-theory. The main complication overcome in the paper is to control the
stabilization which is necessary when one works with finite C*-algebras. In the
infinite case, where programs of this type have already been successfully
carried out, stabilization is unnecessary. Yet, our methods are sufficiently
versatile to allow us to reprove, from a handful of basic results, the
classification of purely infinite nuclear C*-algebras of Kirchberg and
Phillips.
  Indeed, it is our hope that this can be the starting point of a unified
approach to classification of nuclear C*-algebras.

<id>
math/9503235v1
<category>
math.OC
<abstract>
Shapley and Scarf introduced a notion of stable allocation between traders
and indivisible goods, when each trader has rank-ordered each of the goods. The
purpose of this note is to prove that the distribution of ranks after
allocation is the same as the distribution of search distances in uniform
hashing, when the rank-orderings are independent and uniformly random.
Therefore the average sum of final ranks is just $(n+1)H_n-n$, and the standard
deviation is O(n). The proof involves a family of interesting one-to-one
correspondences between permutations of a special kind.

<id>
math/9504228v1
<category>
math.OC
<abstract>
Given $n$ real numbers $0\leq x_1,...,x_n<1$ and a permutation~$\sigma$ of
$\{1,...,n\}$, we can always find $\xbar_1,...,\xbar_n\in\{0,1\}$ so that the
partial sums $\xbar_1+... +\xbar_k$ and $\xbar_{\sigma 1}+... +\xbar_{\sigma
k}$ differ from the unrounded values $x_1+... + x_k$ and $x_{\sigma 1}+...
+x_{\sigma k}$ by at most $n/(n+1)$, for $1\leq k\leq n$. The latter bound is
best possible. The proof uses an elementary argument about flows in a certain
network, and leads to a simple algorithm that finds an optimum way to round.

<id>
math/9901140v1
<category>
math.OC
<abstract>
In this paper we introduce a new method to design control laws for non-linear
underactuated systems. Our method produces an infinite dimensional family of
control laws, whereas most control techniques only produce a finite dimensional
family. These control laws each come with a natural Lyapunov function. The
inverted pendulum cart is used as an example. In addition, we construct an
abstract system which is open loop unstable and cannot be stabilized using any
linear control law, and demonstrate that our method produces a stabilizing
control law.

<id>
math/9902023v1
<category>
math.OC
<abstract>
This paper studies controllability properties of recurrent neural networks.
The new contributions are:
  (1) an extension of the result in the previous paper "Complete
controllability of continuous-time recurrent neural networks" (Sontag and
Sussmann) to a slightly different model, where inputs appear in an affine form,
  (2) a formulation and proof of a necessary and sufficient condition, in terms
of local-local controllability, and
  (3) a complete analysis of the 2-dimensional case for which the hypotheses
made in previous work do not apply

<id>
math/9902124v2
<category>
math.OC
<abstract>
This paper is concerned with the coordinate-free approach to control systems.
The coordinate-free approach is a factorization approach but does not require
the coprime factorizations of the plant. We present two criteria for feedback
stabilizability for MIMO systems in which transfer functions belong to the
total rings of fractions of commutative rings. Both of them are generalizations
of Sule's results in [SIAM J. Control Optim., 32-6, 1675-1695(1994)]. The first
criterion is expressed in terms of modules generated from a causal plant and
does not require the plant to be strictly causal. It shows that if the plant is
stabilizable, the modules are projective. The other criterion is expressed in
terms of ideals called generalized elementary factors. This gives the
stabilizability of a causal plant in terms of the coprimeness of the
generalized elementary factors. As an example, a discrete finite-time delay
system is considered.

<id>
math/9903022v1
<category>
math.OC
<abstract>
We propose a tuner, suitable for adaptive control and (in its discrete-time
version) adaptive filtering applications, that sets the second derivative of
the parameter estimates rather than the first derivative as is done in the
overwhelming majority of the literature. Comparative stability and performance
analyses are presented.

<id>
math/9904056v1
<category>
math.OC
<abstract>
We describe a general-purpose method for finding high-quality solutions to
hard optimization problems, inspired by self-organized critical models of
co-evolution such as the Bak-Sneppen model. The method, called Extremal
Optimization, successively eliminates extremely undesirable components of
sub-optimal solutions, rather than ``breeding'' better components. In contrast
to Genetic Algorithms which operate on an entire ``gene-pool'' of possible
solutions, Extremal Optimization improves on a single candidate solution by
treating each of its components as species co-evolving according to Darwinian
principles. Unlike Simulated Annealing, its non-equilibrium approach effects an
algorithm requiring few parameters to tune. With only one adjustable parameter,
its performance proves competitive with, and often superior to, more elaborate
stochastic optimization procedures. We demonstrate it here on two classic hard
optimization problems: graph partitioning and the traveling salesman problem.

<id>
math/9906107v1
<category>
math.OC
<abstract>
This short note is devoted to the unraveling of the hidden interactivity of
ordinary games which is an artefact of predictions of the behaviour of other
players by the fixed player and describes deviations of their real behaviour
from such predictions. A method to improve the predictions is proposed.
Applications to the strategical analysis of interactive games are also briefly
specified.

<id>
math/9908087v2
<category>
math.OC
<abstract>
We present a parameterization of the stabilizing controllers over commutative
rings. In the classical case, that is, in the case where there exist the
right-/left-coprime factorizations of the given plant, the stabilizing
controllers can be parameterized by the method called
``Youla-Kucera-parameterization''. However, it is known that there exist models
in which some stabilizable transfer matrices do not have their
right-/left-coprime factorizations. In such models, we cannot employ the
Youla-Kucera-parameterization directly. Our method of this paper can be applied
to even such models.

<id>
math/9908088v1
<category>
math.OC
<abstract>
Anantharam showed in 1985 the existence of a model in which some stabilizable
plants do not have its right-/left-coprime factorizations. In this paper, we
give a condition of the nonexistence of the right-/left-coprime factorizations
of stabilizable plants as a generalization of Anantharam's result. As examples
of the models which satisfy the condition, we present two models. We illustrate
the construction of a stabilizing controller of stabilizable single-input
single-output plants of such models.

<id>
math/9908134v2
<category>
math.OC
<abstract>
In this paper, for continuous, linearly-controllable quadratic control
systems with a single input, an explicit, constructive method is proposed for
studying their Brunovsky forms, initially studied in [W. Kang and A. J. Krener,
Extended quadratic controller normal form and dynamic state feedback
linearization of nonlinear systems, SIAM Journal on Control and Optimization,
30:1319-1337, 1992]. In this approach, the computation of Brunovsky forms and
transformation matrices and the proof of their existence and uniqueness are
carried out simultaneously. In addition, it is shown that quadratic
transformations in the aforementioned paper can be simplified to prevent
multiplicity in Brunovsky forms. This method is extended for studying discrete
quadratic systems. Finally, computation algorithms for both continuous and
discrete systems are summarized, and examples demonstrated.

<id>
math/9910134v1
<category>
math.OC
<abstract>
In this paper we will discuss problems and techniques related to
underactuated systems. We give a mathematical formulation of several problems
arising from applications, review some standard and new techniques, and pose
some interesting and challenging open questions.

<id>
math/9911233v1
<category>
math.OC
<abstract>
This work explores Lyapunov characterizations of the input-output-to-state
stability (IOSS) property for nonlinear systems. The notion of IOSS is a
natural generalization of the standard zero-detectability property used in the
linear case. The main contribution of this work is to establish a complete
equivalence between the input-output-to-state stability property and the
existence of a certain type of smooth Lyapunov function. As corollaries, one
shows the existence of ``norm-estimators'', and obtains characterizations of
nonlinear detectability in terms of relative stability and of finite-energy
estimates.

<id>
math/9912007v1
<category>
math.OC
<abstract>
This paper deals with the regularity of solutions of the Hamilton-Jacobi
Inequality which arises in H-infinity control. It shows by explicit
counterexamples that there are gaps between existence of continuous and locally
Lipschitz (positive definite and proper) solutions, and between Lipschitz and
continuously differentiable ones. On the other hand, it is shown that it is
always possible to smooth-out solutions, provided that an infinitesimal
increase in gain is allowed.

<id>
math/9912035v1
<category>
math.OC
<abstract>
We study a specific convex maximization problem in n-dimensional space. The
conjectured solution is proved to be a vertex of the polyhedral feasible
region, but only a partial proof of local maximality is known. Integer
sequences with interesting patterns arise in the analysis, owing to the number
theoretic origin of the problem.

<id>
math/9912036v1
<category>
math.OC
<abstract>
We study a specific convex maximization problem in the space of continuous
functions defined on a semi-infinite interval. An unexplained connection to the
discrete version of this problem is investigated.

<id>
math/0003064v1
<category>
math.OC
<abstract>
In 1994, Sule presented the necessary and sufficient conditions of the
feedback stabilizability of systems over unique factorization domains in terms
of elementary factors and in terms of reduced minors. Recently, Mori and Abe
have generalized his theory over commutative rings. They have introduced the
notion of the generalized elementary factor, which is a~generalization of the
elementary factor, and have given the necessary and sufficient condition of the
feedback stabilizability. In this paper, we present two generalization of the
reduced minors. Using each of them, we state the necessary and sufficient
condition of the feedback stabilizability over commutative rings. Further we
present the relationship between the generalizations and the generalized
elementary factors.

<id>
math/0003177v1
<category>
math.OC
<abstract>
This note describes a method for generating an infinite-dimensional family of
nonlinear control laws for underactuated systems. For a ball and beam system,
the entire family is found explicitly.

<id>
math/0003196v1
<category>
math.OC
<abstract>
This note describes two problems related to the digital implementation of
control laws in the infinite dimensional family of matching control laws,
namely state estimation and sampled data induced error. The entire family of
control laws is written for an inverted pendulum cart. Numerical simulations
which include sampled data and a state estimator are presented for one of the
control laws in this family.

<id>
math/0004064v1
<category>
math.OC
<abstract>
This paper deals with fractional-order controllers. We outline mathematical
description of fractional controllers and methods of their synthesis and
application. Synthesis method is a modified root locus method for
fractional-order systems and fractional-order controllers. In the next section
we describe how to apply the fractional controller on control systems.

<id>
math/0006016v1
<category>
math.OC
<abstract>
The calibration method is used to identify some minimizers of the
Mumford-Shah functional. The method is then extended to more general free
discontinuity problems.

<id>
math/0006121v2
<category>
math.OC
<abstract>
A recent approach to the control of underactuated systems is to look for
control laws which will induce some specified structure on the closed loop
system. This basic idea is used in several papers already. In this paper, we
will describe one matching condition and an approach for finding all control
laws that fit the condition. After an analysis of the resulting control laws
for linear systems, we will present the results from an experiment on a ball
and beam system.

<id>
math/0006190v1
<category>
math.OC
<abstract>
This paper deals with fractional-order controlled systems and
fractional-order controllers in the discrete domain. The mathematical
description by the fractional difference equations and properties of these
systems are presented. A practical example for modelling the fractional-order
control loop is shown and obtained results are discussed in conclusion.

<id>
math/0007063v1
<category>
math.OC
<abstract>
This paper discusses the systematic design of an adaptive feedback
linearizing neurocontroller for a high-order model of the synchronous
machine/infinite bus power system. The power system is first modelled as an
input-output nonlinear discrete-time system approximated by two neural
networks. The approach allows a simple linear pole-placement controller (which
is itself not a neural network) to be designed. The control law is specified
such that the controller adaptively calculates an appropriate feedback
linearizing control law at each sampling instant by utilizing plant parameter
estimates provided by the neural system model. The control system also adapts
itself on-line. This avoids the requirement for exact knowledge of the power
system dynamics and full state measurement as well as other difficulties
associated with implementing analytical input-output feedback linearizing
control for a complex power system model. Furthermore, a departure is made from
the `ad hoc' manner in which many neural controllers have been designed for
power systems; the approach used here has foundations in control theoretic
concepts of adaptive feedback linearization and pole-placement control design.
  Simulation results demonstrate the performance of this controller for a
representative example of a single-machine/infinite bus power system
configuration under various operational conditions.

<id>
math/0007100v2
<category>
math.OC
<abstract>
This paper introduces a new approach for output feedback stabilization of
SISO systems which, unlike most of the techniques found in the literature, does
not use high-gain observers and control input saturation to achieve separation
between the state feedback and observer designs. Rather, we show that by using
nonlinear observers, together with a projection algorithm, the same kind of
separation principle is achieved for a larger class of systems, namely
stabilizable and incompletely observable plants. Furthermore, this new approach
avoids using knowledge of the inverse of the observability mapping, which is
needed by most techniques in the literature when controlling general
stabilizable systems.

<id>
math/0007101v1
<category>
math.OC
<abstract>
The problem of controlling surge and stall in jet engine compressors is of
fundamental importance in preventing damage and lengthening the life of these
components. In this paper, we use the Moore-Greitzer mathematical model to
develop an output feedback controller for these two instabilities (only one of
the three states is measurable). This problem is particularly challenging since
the system is not completely observable and, hence, none of the output feedback
control techniques found in the literature can be applied to recover the
performance of a full state feedback controller. However, we show how to
successfully solve it by using a novel output feedback approach for the
stabilization of general stabilizable and incompletely observable systems.

<id>
math/0007155v1
<category>
math.OC
<abstract>
In this paper we present the mathematical description and analysis of a
fractional-order regulated system in the state space. A little historical
background of our results in the analysis and synthesis of the fractional-order
dynamical regulated systems is given. The methods and results of simulations of
the fractional-order system described by a state space equation equivalent to
three-member fractional-order differential equation with a fractional-order
$PD^{\delta}$ regulator are then presented. The possibility of investigating
the stability of such systems is also considered.

<id>
math/0007168v1
<category>
math.OC
<abstract>
In this paper we present a direct adaptive control method for a class of
uncertain nonlinear systems with a time-varying structure. We view the
nonlinear systems as composed of a finite number of ``pieces,'' which are
interpolated by functions that depend on a possibly exogenous scheduling
variable. We assume that each piece is in strict feedback form, and show that
the method yields stability of all signals in the closed-loop, as well as
convergence of the state vector to a residual set around the equilibrium, whose
size can be set by the choice of several design parameters. The class of
systems considered here is a generalization of the class of strict feedback
systems traditionally considered in the backstepping literature. We also
provide design guidelines based on L-infinity bounds on the transient.

<id>
math/0008186v1
<category>
math.OC
<abstract>
This paper deals with fractional-order controlled systems and
fractional-order controllers in the frequency domain. The mathematical
description by fractional transfer functions and properties of these systems
are presented. The new ways for modelling of fractional-order systems are
illustrated with a numerical example and obtained results are discussed in
conclusion.

<id>
math/0010095v1
<category>
math.OC
<abstract>
In this paper we illustrate how non-stochastic (max,+) techniques can be used
to describe partial synchronization in a Discrete Event Dynamical System. Our
work uses results from the spectral theory of dioids and analyses (max,+)
equations describing various synchronization rules in a simple network. The
network in question is a transport network consisting of two routes joined at a
single point, and our Discrete Events are the departure times of transport
units along these routes. We calculate the maximum frequency of circulation of
these units as a function of the synchronization parameter. These functions
allow us further to determine the waiting times on various routes, and here we
find critical parameters (dependent on the fixed travel times on each route)
which dictate the overall behavoiur. We give explicit equations for these
parameters and state the rules which enable optimal performance in the network
(corresponding to minimum waiting time).

<id>
math/9310225v1
<category>
math.PR
<abstract>
Uniform Harnack inequalities for harmonic functions on the pre- and graphical
Sierpinski carpets are proved using a probabilistic coupling argument. Various
results follow from this, including the construction of Brownian motion on
Sierpinski carpets embedded in $\R^d$, $d\geq 3$, estimates on the fundamental
solution of the heat equation, and Sobolev and Poincar\'e inequalities.

<id>
math/9410222v1
<category>
math.PR
<abstract>
This announcement describes a probabilistic approach to cascades which, in
addition to providing an entirely probabilistic proof of the Kahane-Peyri\`ere
theorem for independent cascades, readily applies to general dependent
cascades. Moreover, this unifies various seemingly disparate cascade
decompositions, including Kahane's T-martingale decomposition and dimension
disintegration.

<id>
math/9508222v1
<category>
math.PR
<abstract>
Consider a planar Brownian motion run for finite time. The frontier or
``outer boundary'' of the path is the boundary of the unbounded component of
the complement. Burdzy (1989) showed that the frontier has infinite length. We
improve this by showing that the Hausdorff dimension of the frontier is
strictly greater than 1. (It has been conjectured that the Brownian frontier
has dimension $4/3$, but this is still open.) The proof uses Jones's Traveling
Salesman Theorem and a self-similar tiling of the plane by fractal tiles known
as Gosper Islands.

<id>
math/9701222v1
<category>
math.PR
<abstract>
We show that fractal (or "Mandelbrot") percolation in two dimensions produces
a set containing no directed paths, when the set produced has zero area. This
improves a similar result by the first contributor in the case of constant retention
probabilities to the case of retention probabilities approaching 1.

<id>
math/9701223v1
<category>
math.PR
<abstract>
A general criterion is given for when a Markov chain trapped with probability
p(x) in state x will be almost surely trapped. The quenched (state x is a trap
forever with probability p(x)) and annealed (state x traps with probability
p(x) on each visit) problems are shown to be equivalent.

<id>
math/9701224v1
<category>
math.PR
<abstract>
Vertex-reinforced random walk is defined in Pemantle's (1988) thesis; it is a
random walk that is biased to visit sites it has already visited a lot. We show
that this reinforcement scheme, in contrast to the scheme of
edge-reinforcement, causes random walk on a line to get trapped in a finite
set.

<id>
math/9701225v1
<category>
math.PR
<abstract>
Any fixed cylinder is hit almost surely by a 3-dimensional Brownian motion,
but is there a random cylinder that is in the complement? We answer this for
cylinders, and then replacing a cylinder with a more general set.

<id>
math/9701226v1
<category>
math.PR
<abstract>
We generalize Richardson's model by starting with two sites of different
colors and giving each new site the color of the site that spawned it. We show
that co-existence is possible.

<id>
math/9701227v1
<category>
math.PR
<abstract>
We show that oriented percolation occurs whenever a condition is satisfied
called "exponential intersection tails". This condition says that a measure on
paths exists for which the probability of two independent paths intersecting in
more than k sites is exponentially small in k.

<id>
math/9701228v1
<category>
math.PR
<abstract>
Lower and upper estimates are given for the probability that the
epsilon-enlargement of planar Brownian motion to time 1 (the epsilon sausage)
contains a unit line segment. The estimates imply that Brownian motion to time
1 itself contains no line segment.

<id>
math/9704231v1
<category>
math.PR
<abstract>
Does a minimal harmonic function $h$ remain minimal when it is viewed as a
parabolic function? The question is answered for a class of long thin
semi-infinite tubes $D\subset \R^d$ of variable width and minimal harmonic
functions $h$ corresponding to the boundary point of $D$ ``at infinity.''
Suppose $f(u)$ is the width of the tube $u$ units away from its endpoint and
$f$ is a Lipschitz function. The answer to the question is affirmative if and
only if $\int^\infty f^3(u)du = \infty$. If the test fails, there exist
parabolic $h$-transforms of space-time Brownian motion in $D$ with infinite
lifetime which are not time-homogenous.

<id>
math/9712279v1
<category>
math.PR
<abstract>
We give necessary and sufficient conditions for a multivariate stationary
stochastic process to be completely regular. We also give the answer to a
question of V.V. Peller concerning the spectral measure characterization of
such processes.

<id>
math/9801143v1
<category>
math.PR
<abstract>
The Dirichlet form associated with the intrinsic gradient on Poisson space is
known to be quasi-regular on the complete metric space $\ddot\Gamma=$
$\{Z_+$-valued Radon measures on $\IR^d\}$. We show that under mild conditions,
the set $\ddot\Gamma\setminus\Gamma$ is $\e$-exceptional, where $\Gamma$ is the
space of locally finite configurations in $\IR^d$, that is, measures
$\gamma\in\ddot\Gamma$ satisfying $\sup_{x\in\IR^d}\gamma(\{x\})\leq 1$. Thus,
the associated diffusion lives on the smaller space $\Gamma$. This result also
holds for Gibbs measures with superstable interactions.

<id>
math/9801144v1
<category>
math.PR
<abstract>
Strong and Markov uniqueness problems in $L^2$ for Dirichlet operators on
rigged Hilbert spaces are studied. An analytic approach based on a--priori
estimates is used. The extension of the problem to the $L^p$-setting is
discussed. As a direct application essential self--adjointness and strong
uniqueness in $L^p$ is proved for the generator (with initial domain the
bounded smooth cylinder functions) of the stochastic quantization process for
Euclidean quantum field theory in finite volume $\Lambda \subset \R^2$.

<id>
math/9801145v1
<category>
math.PR
<abstract>
Sufficient conditions are given for existence and uniqueness in
Smoluchowski's coagulation equation, for a wide class of coagulation kernels
and initial mass distributions. An example of non-uniqueness is constructed.
The stochastic coalescent is shown to converge weakly to the solution of
Smoluchowski's equation.

<id>
math/9802045v1
<category>
math.PR
<abstract>
We study an ordinary differential equation controlled by a stochastic
process. We present results on existence and uniqueness of solutions, on
associated local times (Trotter and Ray-Knight theorems), and on time and
direction of bifurcation. A relationship with Lipschitz approximations to
Brownian paths is also discussed.

<id>
math/9802068v1
<category>
math.PR
<abstract>
We study p-adic counterparts of stable distributions, that is limit
distributions for sequences of normalized sums of independent identically
distributed p-adic-valued random variables. In contrast to the classical case,
non-degenerate limit distributions can be obtained only under certain
assumptions on the asymptotic behaviour of the number of summands in the
approximating sums. This asymptotics determines the ``exponent of stability''.

<id>
math/9802130v1
<category>
math.PR
<abstract>
We extend the class of $(\xi,\psi,K)$-superprocesses known so far by applying
a simple transformation induced by a \lq\lq weight function\rq\rq\ for the
one-particle motion. These transformed superprocesses may exist under weak
conditions on the branching parameters, and their state space automatically
extends to a certain space of possibly infinite Radon measures. It turns out
that a number of superprocesses which were so far not included in the general
theory fall into this class. For instance, we are able to extend the hyperbolic
branching catalyst of Fleischmann and Mueller to the case of $\beta$-branching.
In the second part of this paper, we discuss regularity properties of our
processes. Under the assumption that the one-particle motion is a Hunt process,
we show that our superprocesses possess right versions having \cadlag\ paths
with respect to a natural topology on the state space. The proof uses an
approximation with branching particle systems on Skorohod space.

<id>
math/9802131v1
<category>
math.PR
<abstract>
We consider an $L^2$-Wasserstein type distance $\rho$ on the configuration
space $\Gamma_X$ over a Riemannian manifold $X$, and we prove that
$\rho$-Lipschitz functions are contained in a Dirichlet space associated with a
measure on $\Gamma_X$ satisfying some general assumptions. These assumptions
are in particular fulfilled by a large class of tempered grandcanonical Gibbs
measures with respect to a superstable lower regular pair potential. As an
application we prove a criterion in terms of $\rho$ for a set to be
exceptional. This result immediately implies, for instance, a quasi-sure
version of the spatial ergodic theorem. We also show that $\rho$ is optimal in
the sense that it is the intrinsic metric of our Dirichlet form.

<id>
math/9803034v1
<category>
math.PR
<abstract>
The growth exponent $\alpha$ for loop-erased or Laplacian random walk on the
integer lattice is defined by saying that the expected time to reach the sphere
of radius $n$ is of order $n^\alpha$. We prove that in two dimensions, the
growth exponent is strictly greater than one. The proof uses a known estimate
on the third moment of the escape probability and an improvement on the
discrete Beurling projection theorem.

<id>
math/9803035v1
<category>
math.PR
<abstract>
We study the fluctuations, in the large deviations regime, of the longest
increasing subsequence of a random i.i.d. sample on the unit square. In
particular, our results yield the precise upper and lower exponential tails for
the length of the longest increasing subsequence of a random permutation.

<id>
math/9803049v1
<category>
math.PR
<abstract>
Let X and Y be time-homogeneous Markov processes with common state space E,
and assume that the transition kernels of X and Y admit densities with respect
to suitable reference measures. We show that if there is a time t>0 such that,
for each x\in E, the conditional distribution of (X_s)_{0 < s < t}, given X_0 =
x = X_t, coincides with the conditional distribution of (Y_s)_{0 < s < t},
given Y_0 = x = Y_t, then the infinitesimal generators of X and Y are related
by [L^Y]f = \psi^{-1}[L^X](\psi f)-\lambda f, where \psi is an eigenfunction of
L^X with eigenvalue \lambda. Under an additional continuity hypothesis, the
same conclusion obtains assuming merely that X and Y share a ``bridge'' law for
one triple (x,t,y). Our work entends and clarifies a recent result of I.
Benjamini and S. Lee.

<id>
math/9803100v1
<category>
math.PR
<abstract>
We give a simple non-analytic proof of Biggins' theorem on martingale
convergence for branching random walks.

<id>
math/9803160v1
<category>
math.PR
<abstract>
We formulate and prove a {\it Local Stable Manifold Theorem\/} for stochastic
differential equations (sde's) that are driven by spatial Kunita-type
semimartingales with stationary ergodic increments. Both Stratonovich and
It\^o-type equations are treated. Starting with the existence of a stochastic
flow for a sde, we introduce the notion of a hyperbolic stationary trajectory.
We prove the existence of invariant random stable and unstable manifolds in the
neighborhood of the hyperbolic stationary solution. For Stratonovich sde's, the
stable and unstable manifolds are dynamically characterized using forward and
backward solutions of the anticipating sde. The proof of the stable manifold
theorem is based on Ruelle-Oseledec multiplicative ergodic theory.

<id>
math/9803162v1
<category>
math.PR
<abstract>
The purpose of this paper is to provide a both comprehensive and summarizing
account on recent results about analysis and geometry on configuration spaces
$\Gamma_X$ over Riemannian manifolds $X$. Particular emphasis is given to a
complete description of the so--called ``lifting--procedure'', Markov resp.
strong resp. $L^1$--uniqueness results, the non--conservative case, the
interpretation of the constructed diffusions as solutions of the respective
classical ``heuristic'' stochastic differential equations, and a
self--contained presentation of a general closability result for the
corresponding pre--Dirichlet forms. The latter is presented in the general case
of arbitrary (not necessarily pair) potentials describing the singular
interactions. A support property for the diffusions, the intrinsic metric, and
a Rademacher theorem on $\Gamma_X$, recently proved, are also discussed.

<id>
math/9804031v2
<category>
math.PR
<abstract>
We present a new approach to study measures on ensembles of contours,
polymers or other objects interacting by some sort of exclusion condition. For
concreteness we develop it here for the case of Peierls contours. Unlike
existing methods, which are based on cluster-expansion formalisms and/or
complex analysis, our method is strictly probabilistic and hence can be applied
even in the absence of analyticity properties. It involves a Harris graphical
construction of a loss network for which the measure of interest is invariant.
The existence of the process and its mixing properties depend on the absence of
infinite clusters for a dual (backwards) oriented percolation process which we
dominate by a multitype branching process. Within the region of subcriticality
of this branching process the approach yields: (i) exponential convergence to
the equilibrium (=contour) measures, (ii) standard clustering and finite-effect
properties of the contour measure, (iii) a particularly strong form of the
central limit theorem, and (iv) a Poisson approximation for the distribution of
contours at low temperature.

<id>
math/9804150v1
<category>
math.PR
<abstract>
In this paper, some new forms of the Cheeger's inequalities are established
for general (maybe unbounded) symmetric forms, the resulting estimates improve
and extend the ones obtained by Lawler and Sokal (1988) for bounded jump
processes. Furthermore, some existence criteria for spectral gap of general
symmetric forms or general reversible Markov processes are presented, based on
the Cheeger's inequalities and a relationship between the spectral gap and the
first Dirichlet and Neumann eigenvalues on local region.

<id>
math/9804158v1
<category>
math.PR
<abstract>
In this paper we present a martingale related to the exit measures of
super-Brownian motion. By changing measure with this martingale in the
canonical way we have a new process associated with the conditioned exit
measure. This measure is shown to be identical to a measure generated by a
non-homogeneous branching particle system with immigration of mass. An
application is given to the problem of conditioning the exit measure to hit a
number of specified points on the boundary of a domain. The results are similar
in flavor to the "immortal particle" picture of conditioned super-Brownian
motion but more general, as the change of measure is given by a martingale
which need not arise from a single harmonic function.

<id>
math/9806022v2
<category>
math.PR
<abstract>
Let (f_n) be a mean zero vector valued martingale sequence. Then there exist
vector valued functions (d_n) from [0,1]^n such that int_0^1 d_n(x_1,...,x_n)
dx_n = 0 for almost all x_1,...,x_{n-1}, and such that the law of (f_n) is the
same as the law of (sum_{k=1}^n d_k(x_1,...,x_k)) . Similar results for tangent
sequences and sequences satisfying condition (C.I.) are presented. We also
present a weaker version of a result of McConnell that provides a Skorohod like
representation for vector valued martingales. This paper may be found at
http://math.missouri.edu/~stephen/preprints

<id>
math/9806112v1
<category>
math.PR
<abstract>
Brownian motions in the infinite-dimensional group of all unitary operators
are studied under strong continuity assumption rather than norm continuity.
Every such motion can be described in terms of a countable collection of
independent one-dimensional Brownian motions. The proof involves continuous
tensor products and continuous quantum measurements. A by-product: a Brownian
motion in a separable F-space (not locally convex) is a Gaussian process.

<id>
math/9201257v1
<category>
math.QA
<abstract>
We define two $(n+1)$ graded Lie brackets on spaces of multilinear mappings.
The first one is able to recognize $n$-graded associative algebras and their
modules and gives immediately the correct differential for Hochschild
cohomology. The second one recognizes $n$-graded Lie algebra structures and
their modules and gives rise to the notion of Chevalley cohomology.

<id>
math/9208203v1
<category>
math.QA
<abstract>
In this short review article we sketch some developments which should
ultimately lead to the analogy of the Chern-Weil homomorphism for principal
bundles in the realm of non-commutative differential geometry. Principal
bundles there should have Hopf algebras as structure `cogroups'. Since the
usual machinery of Lie algebras, connection forms, etc\., just is not available
in this setting, we base our approach on the Fr\"olicher--Nijenhuis bracket.

<id>
math/9212209v1
<category>
math.QA
<abstract>
In this paper we show that in the case of noncommutative two-tori one gets in
a natural way simple structures which have analogous formal properties as Hopf
algebra structures but with a deformed multiplication on the tensor product.

<id>
math/9601221v1
<category>
math.QA
<abstract>
Jaeger [Geom. Dedicata 44 (1992), 23-52] discovered a remarkable checkerboard
state model based on the Higman-Sims graph that yields a value of the Kauffman
polynomial, which is a quantum invariant of links. We present a simple argument
that the state model has the desired properties using the combinatorial $B_2$
spider [Comm. Math. Phys. 180 (1996), 109-151].

<id>
math/9801020v1
<category>
math.QA
<abstract>
We develop a general theory of `quantum' diffeomorphism groups based on the
universal comeasuring quantum group $M(A)$ associated to an algebra $A$ and its
various quotients. Explicit formulae are introduced for this construction, as
well as dual quasitriangular and braided R-matrix versions. Among the examples,
we construct the $q$-diffeomorphisms of the quantum plane $yx=qxy$, and recover
the quantum matrices $M_q(2)$ as those respecting its braided group addition
law.

<id>
math/9801032v1
<category>
math.QA
<abstract>
In this paper we employ the construction of Dirac bracket for the remaining
current of $sl(2)_q$ deformed Kac-Moody algebra when constraints similar to
those connecting the $sl(2)$-WZW model and the Liouville theory are imposed and
show that it satisfy the q-Virasoro algebra proposed by Frenkel and
Reshetikhin. The crucial assumption considered in our calculation is the
existence of a classical Poisson bracket algebra induced, in a consistent
manner by the correspondence principle, mapping the quantum generators into
commuting objects of classical nature preserving their algebra.

<id>
math/9801035v1
<category>
math.QA
<abstract>
It is shown that the properties of the Gauss decomposition of quantum groups
and the known Jimbo homomorphism permit us to realize these groups as
subalgebras of well defined algebras constructed from generators of the
corresponding undeformed Lie algebras.

<id>
math/9801036v1
<category>
math.QA
<abstract>
A ``Wick rotation'' is applied to the noncommutative sphere to produce a
noncommutative version of the hyperboloids. A harmonic basis of the associated
algebra is given. It is noted that, for the one sheeted hyperboloid, the vector
space for the noncommutative algebra can be completed to a Hilbert space, where
multiplication is not continuous. A method of constructing noncommutative
analogues of surfaces of rotation, examples of which include the paraboloid and
the $q$-deformed sphere, is given. Also given are mappings between
noncommutative surfaces, stereographic projections to the complex plane and
unitary representations. A relationship with one dimensional crystals is
highlighted.

<id>
math/9801037v2
<category>
math.QA
<abstract>
This is a survey of our construction of current algebras, associated with
complex curves and rational differentials. We also study in detail two classes
of examples. The first is the case of a rational curve with differentials $z^n
dz$; these algebras are ``building blocks'' for the quantum current algebras
introduced in our earlier work. The second is the case of a genus $>1$ curve
$X$, endowed with a regular differential having only double zeroes.

<id>
math/9801043v2
<category>
math.QA
<abstract>
This paper is a continuation of "Quantization of Lie bialgebras, III"
(q-alg/9610030, revised version). In QLB-III, we introduced the Hopf algebra
F(R)_\z associated to a quantum R-matrix R(z) with a spectral parameter, and a
set of points \z=(z_1,...,z_n). This algebra is generated by entries of a
matrix power series T_i(u), i=1,...,n,subject to Faddeev-Reshetikhin-Takhtajan
type commutation relations, and is a quantization of the group GL_N[[t]].
  In this paper we consider the quotient F_0(R)_\z of F(R)_\z by the relations
\qdet_R(T_i)=1, where \qdet_R is the quantum determinant associated to R (for
rational, trigonometric, or elliptic R-matrices). This is also a Hopf algebra,
which is a quantization of the group SL_N[[t]].
  This paper was inspired by the pioneering paper of I.Frenkel and Reshetikhin.
The main goal of this paper is to study the representation theory of the
algebra F_0(R)_\z and of its quantum double, and show how the consideration of
coinvariants of this double (quantum conformal blocks) naturally leads to the
quantum Knizhnik-Zamolodchikov equations of Frenkel and Reshetikhin.
  Our construction for the rational R-matrix is a quantum analogue of the
standard derivation of the Knizhnik-Zamolodchikov equations in the
Wess-Zumino-Witten model of conformal field theory, and for the elliptic
R-matrix is a quantum analogue of the construction of Kuroki and Takebe.
  Our result is a generalization of the construction of Enriques and Felder,
which appeared while this paper was in preparation. Enriques and Felder gave a
derivation of the quantum KZ equations from coinvariants in the case of the
rational R-matrix and N=2.

<id>
math/9801047v2
<category>
math.QA
<abstract>
In 1992 V.Drinfeld formulated a number of problems in quantum group theory.
In particular, he suggested to consider ``set-theoretical'' solutions to the
quantum Yang-Baxter equation, i.e. solutions given by a permutation R of the
set $X\times X$, where X is a fixed set. In this paper we study such solutions,
which in addition satisfy the unitarity and nondegeneracy conditions. We
discuss the geometric and algebraic interpretations of such solutions,
introduce several constructions of them, and make first steps towards their
classification.

<id>
math/9801049v4
<category>
math.QA
<abstract>
We continue the work started in part I (q-alg/9706004) and prove the
invariance and universality in the class of finite type invariants of the
object defined and motivated there, namely the Aarhus integral of rational
homology 3-spheres. Our main tool in proving invariance is a translation scheme
that translates statements in multi-variable calculus (Gaussian integration,
integration by parts, etc.) to statements about diagrams. Using this scheme the
straight-forward "philosophical" calculus-level proofs of part I become
straight-forward honest diagram-level proofs here. The universality proof is
standard and utilizes a simple "locality" property of the Kontsevich integral.

<id>
math/9801062v4
<category>
math.QA
<abstract>
Elliptic current algebras E_{q,p}(\hat{g}) for arbitrary simply laced finite
dimensional Lie algebra g are defined and their co-algebraic structures are
studied. It is shown that under the Drinfeld like comultiplications, the
algebra E_{q,p}(\hat{g}) is not co-closed for any g. However putting the
algebras E_{q,p}(\hat{g}) with different deformation parameters together, we
can establish a structure of infinite Hopf family of algebras. The level 1
bosonic realization for the algebra E_{q,p}(\hat{g}) is also established.

<id>
math/9801065v1
<category>
math.QA
<abstract>
We use Kazhdan-Lusztig tensoring to, first, describe annihilating ideals of
highest weight modules over an affine Lie algebra in terms of the corresponding
VOA and, second, to classify tilting functors, an affine analogue of projective
functors known in the case of a simple Lie algebra. For the sake of
completeness, the classification of annihilating ideals is borrowed from our
previous work, q-alg/9711011; the part on tilting functors is new.

<id>
math/9801083v1
<category>
math.QA
<abstract>
It is well-known that the Macfarlane-Biedenharn $q$-oscillator and its
generalization has no Hopf structure, whereas the Hong Yan $q$-oscillator can
be endowed with a Hopf structure. In this letter, we demonstrate that it is
possible to construct a general $q$-oscillator algebra which includes the
Macfarlane-Biedenharn oscillator algebra and the Hong Yan oscillator algebra as
special cases.

<id>
math/9801084v1
<category>
math.QA
<abstract>
Starting from bosonization, we study the operator that commute or commute
up-to a total difference with of any quantized screen operator of a free field.
We show that if there exists a operator in the form of a sum of two vertex
operators which has the simplest correlation functions with the quantized
screen operator, namely a function with one pole and one zero, then, the screen
operator and this operator are uniquely determined, and this operator is the
quantized virasoro algebra. For the case when the screen is a fermion, there
are a family of this kind of operator, which give new algebraic structures.
Similarly we study the case of two quantized screen operator, which uniquely
gives us the quantized W-algebra corresponding to $sl(3)$ for the generic case,
and a new algebra, which is a quantized W-algebra corresponding to ${\frak
sl}(2,1)$, for the case that one of the two screening operators is or both are
fermions.

<id>
math/9801085v1
<category>
math.QA
<abstract>
We use the idea of partial Gauss decomposition to study structures related to
$U_q(\widehat{{{\frak{gl}}}(n-1)})$ inside $U_q(\widehat{{{\frak{gl}}}(n)}) $.
This gives a description of $U_q(\widehat{{{\frak{gl}}}(n)})$ as an extension
of $U_q(\widehat{{{\frak{gl}}}(n-1)})$ with Zamolodchikov algebras, We explain
the connection of this new realization with form factors.

<id>
math/9801087v1
<category>
math.QA
<abstract>
With the help of the multigraded Nijenhuis-- Richardson bracket and the
multigraded Gerstenhaber bracket from [7] for every $n\ge 2$ we define $n$-ary
associative algebras and their modules and also $n$-ary Lie algebras and their
modules, and we give the relevant formulas for Hochschild and Chevalley
cohomogy.

<id>
math/9801095v1
<category>
math.QA
<abstract>
We will consider P-graph complexes, where P is a cyclic operad. P-graph
complexes are natural generalizations of Kontsevich's graph complexes -- for P
= the operad for associative algebras it is the complex of ribbon graphs, for P
= the operad for commutative associative algebras, the complex of all graphs.
We construct a `universal class' in the cohomology of the graph complex with
coefficients in a theory. The Kontsevich-type invariant is then an evaluation,
on a concrete cyclic algebra, of this class. We also explain some results of M.
Penkava and A. Schwarz on the construction of an invariant from a cyclic
deformation of a cyclic algebra. Our constructions are illustrated by a `toy
model' of tree complexes.

<id>
math/9801108v1
<category>
math.QA
<abstract>
We construct a fully faithful functor from the category C_F of
finite-dimensional representations of Felder's (dynamical) elliptic quantum
group E_{tau,gamma}(gl(n)) to a cretain category D_B of (infinite-dimensional)
representations of Belavin's quantum elliptic algebra B by difference
operators, and a fully faithful functor from the category C_B of
finite-dimensional representations of B to D_B. As a corollary, we show that
the abelian subcategories of C_B and C_F generated by tensor products of vector
representations are equivalent.

<id>
math/9801116v1
<category>
math.QA
<abstract>
In the present paper we generalize the lifting formulas from [Sh] and obtain
the formula for the (2n+2l-1)-cocycle on the LIe algebra of differential
operators on a n-dimensional space for arbitrary n and l.

<id>
math/9801128v1
<category>
math.QA
<abstract>
In this paper we consider some properties of semisimple Hopf algebras of
dimension pq where p and q are distinct primes. These properties are useful for
classification of such Hopf algebras. In particular, we show that for such a
Hopf algebra H, if H and H^* are of Frobenius type then H or H^* is a group
algebra.

<id>
math/9801129v2
<category>
math.QA
<abstract>
Masuoka proved that for a prime p, semisimple Hopf algebras of dimension 2p
over an algebraically closed field k of characteristic 0, are trivial (i.e. are
either group algebras or the dual of group algebras). Westreich and the second
contributor obtained the same result for dimension 3p, and then pushed the analysis
further and among the rest obtained the same result for semisimple Hopf
algebras H of dimension pq so that H and H^* are of Frobenius type (i.e. the
dimensions of their irreducible representations divide the dimension of H).
They concluded with the conjecture that any semisimple Hopf algebra H of
dimension pq over k, is trivial. In this paper we use Theorem 1.4 in our
previous paper q-alg/9712033 to prove that both H and H^* are of Frobenius
type, and hence prove this conjecture.

<id>
math/9801130v2
<category>
math.QA
<abstract>
In this paper we construct and study two new families of finite dimensional
pointed Hopf algebras which generalize Radford's families. We show that over
any infinite field which contains a primitive nth root of unity, one of the
families contains infinitely many non-isomorphic Hopf algebras of any dimension
of the form Nn^2, where 2<n<N are integers so that n divides N. We thus answer
in the negative Kaplansky's 10th conjecture from 1975 on the finite number of
types of Hopf algebras of a given dimension.

<id>
math/9801135v2
<category>
math.QA
<abstract>
For any simple Lie algebra g and any complex number q which is not zero or a
nontrivial root of unity, we construct a dynamical quantum group (Hopf
algebroid), whose representation theory is essentially the same as the
representation theory of the quantum group U_q(g). This dynamical quantum group
is obtained from the fusion and exchange relations between intertwining
operators in representation theory of U_q(g), and is an algebraic structure
standing behind these relations.

<id>
math/9801141v1
<category>
math.QA
<abstract>
Using the natural irreducible 8-dimensional representation and the two spin
representations of the quantum group $U_q$(D$_4$) of D$_4$, we construct a
quantum analogue of the split octonions and study its properties. We prove that
the quantum octonion algebra satisfies the q-Principle of Local Triality and
has a nondegenerate bilinear form which satisfies a q-version of the
composition property. By its construction, the quantum octonion algebra is a
nonassociative algebra with a Yang-Baxter operator action coming from the
R-matrix of $U_q$(D$_4$). The product in the quantum octonions is a
$U_q$(D$_4$)-module homomorphism. Using that, we prove identities for the
quantum octonions, and as a consequence, obtain at $q = 1$ new ``representation
theory'' proofs for very well-known identities satisfied by the octonions. In
the process of constructing the quantum octonions we introduce an algebra which
is a q-analogue of the 8-dimensional para-Hurwitz algebra.

<id>
math/9802001v1
<category>
math.QA
<abstract>
We construct a certain solution to the Witten--Dijkgraf--Verlinde--Verlinde
equation related to the small quantum cohomology ring of flag variety, and
study the t-deformation of quantum Schubert polynomials corresponding to this
solution.

<id>
math/9802021v1
<category>
math.QA
<abstract>
The nth relative Kauffman bracket skein modules are defined and two theorems
are given relating them to the Kauffman bracket skein module of a 3-manifold.
The first theorem covers the case when the 3-manifold is split along a
separating closed orientable surface and the second theorem addresses the case
when the surface is nonseparating.

<id>
math/9802036v2
<category>
math.QA
<abstract>
We introduce an affinization of the quantum Kac-Moody algebra associated to a
symmetric generalized Cartan matrix. Based on the affinization, we construct a
representation of the quantum Kac-Moody algebra by vertex operators from
bosonic fields. We also obtain a combinatorial indentity about Hall-Littlewood
polynomials.

<id>
math/9802048v2
<category>
math.QA
<abstract>
An action of the Yangian of the general Lie algebra gl(N) is defined on every
irreducible integrable highest weight module of affine gl(N) with level greater
than 1. This action is derived, by means of the Drinfeld duality and a
subsequent semi-infinite limit, from a certain induced representation of the
degenerate double affine Hecke algebra H. Each vacuum module of affine gl(N) is
decomposed into irreducible Yangian subrepresentations by means of the
intertwiners of H. Components of this decomposition are parameterized by
semi-infinite skew Young diagrams.

<id>
math/9201256v1
<category>
math.RT
<abstract>
For any unitary representation of an arbitrary Lie group I construct a moment
mapping from the space of smooth vectors of the representation into the dual of
the Lie algebra. This moment mapping is equivariant and smooth. For the space
of analytic vectors the same construction is possible and leads to a real
analytic moment mapping.

<id>
math/9204222v1
<category>
math.RT
<abstract>
This is a review of [Michor, Peter W.: The moment mapping for a unitary
representation, Ann. Global Anal. Geometry, 8, No 3(1990), 299--313] including
a careful description of calculus in infinite dimensions. For any unitary
representation of an arbitrary Lie group I construct a moment mapping from the
space of smooth vectors of the representation into the dual of the Lie algebra.
This moment mapping is equivariant and smooth. For the space of analytic
vectors the same construction is possible and leads to a real analytic moment
mapping.

<id>
math/9204227v1
<category>
math.RT
<abstract>
Let $M$ be a $G$-covering of a nilpotent orbit in $\g$ where $G$ is a complex
semisimple Lie group and $\g=\text{Lie}(G)$. We prove that under Poisson
bracket the space $R[2]$ of homogeneous functions on $M$ of degree 2 is the
unique maximal semisimple Lie subalgebra of $R=R(M)$ containing $\g$. The
action of $\g'\simeq R[2]$ exponentiates to an action of the corresponding Lie
group $G'$ on a $G'$-cover $M'$ of a nilpotent orbit in $\g'$ such that $M$ is
open dense in $M'$. We determine all such pairs $(\g\subset\g')$.

<id>
math/9301214v1
<category>
math.RT
<abstract>
In this paper we study the reducibility, composition series and unitarity of
the components of some degenerate principal series representations of
$\RMO(p,q)$, $\RMU(p,q)$ and $\SP(p,q)$. This is done by realizing these
representations in paces of homogeneous functions on light cones and writing
down the explicit actions of the universal enveloping algebra of the group
concerned.

<id>
math/9304215v1
<category>
math.RT
<abstract>
The main aim of this paper is to present the ideas which lead first to the
solution of the unitarizability problem for $\GL(n)$ over nonarchimedean local
fields and to the recognition that the same result holds over archimedean local
fields, a result which was proved by Vogan using an internal approach. Let us
say that the approach that we are going to present may be characterized as
external. At no point do we go into the internal structure of representations.

<id>
math/9408212v1
<category>
math.RT
<abstract>
Let (G,V) be an irreducible prehomogeneous vector space defined over a number
field k, P in k[V] a relative invariant polynomial, and X a rational character
of G such that P(gx)=X(g)P(x). Let V_k^{ss}={x \in V_k such that P(x) is not
equal to 0}. For x in V_k^{ss}, let G_x be the stabilizer of x, and G_x^0 the
connected component of 1 of G_x. We define L_0 to be the set of x in V_k^{ss}
such that G_x^0 does not have a non-trivial rational character. We study the
zeta function for (G,V).

<id>
math/9410214v1
<category>
math.RT
<abstract>
Let $K$ be a compact connected Lie group acting unitarily on a
finite-dimensional complex vector space $V$. One calls this a {\em
multiplicity-free} action whenever the $K$-isotypic components of $\C[V]$ are
$K$-irreducible. We have shown that this is the case if and only if the moment
map $\tau:V\rightarrow\k^*$ for the action is finite-to-one on $K$-orbits. This
is equivalent to a result concerning \gp s associated with Heisenberg groups
that is motivated by the Orbit Method. Further details of this work will be
published elsewhere.

<id>
math/9505209v1
<category>
math.RT
<abstract>
Let $G$ be a split simply laced group defined over a $p$-adic field $F$. In
this paper we study the restriction of the minimal representation of $G$ to
various dual pairs in $G$. For example, the restriction of the minimal
representation of $E_7$ to the dual pair $G_2 \times{}$Sp(6) gives the
non-endoscopic Langlands lift of irreducible representations of $G_2$ to Sp(6).

<id>
math/9506217v1
<category>
math.RT
<abstract>
An integral transform for G=U(1,q) is studied. The transform maps the
positive spin ladder representations of G on a Bargmann-Segal-Fock space
F_n^1,q into a space of polynomial-valued functions on the bounded realization
B^q of G/K. An inversion is given for the transform and unitary structures are
given for the geometric realization of the positive spin ladder representations
over G/K.

<id>
math/9506218v1
<category>
math.RT
<abstract>
If G is a (connected) complex Lie Group and Z is a generalized flag manifold
for G, the the open orbits D of a (connected) real form G_0 of G form an
interesting class of complex homogeneous spaces, which play an important role
in the representation theory of G_0. We find that the group of automorphisms,
i.e., the holomorphic diffeomorphisms, is a finite-dimensional Lie group,
except for a small number of open orbits, where it is infinite dimensional. In
the finite-dimensional case, we determine its structure. Our results have some
consequences in representation theory.

<id>
math/9506219v1
<category>
math.RT
<abstract>
In this paper, we consider the most non-split parabolic D_4 type
prehomogeneous vector space. The vector space is an analogue of the space of
Hermitian forms. We determine the principal part of the zeta function.

<id>
math/9506220v1
<category>
math.RT
<abstract>
In this paper we consider an analogue of the zeta function for not
necessarily prehomogeneous representations of GL(2) and compute some of the
poles.

<id>
math/9510204v1
<category>
math.RT
<abstract>
We prove that the induced representation from a non trivial character of the
Coxeter torus of GL$(2,F)$, for a finite field $F$, is multiplicity-free; we
give an explicit description of the corresponding (twisted) spherical functions
and a version of the Heisenberg Uncertainty Principle.

<id>
math/9511215v1
<category>
math.RT
<abstract>
We classify extended Poincar\'e Lie super algebras and Lie algebras of any
signature (p,q), that is Lie super algebras and Z_2-graded Lie algebras g = g_0
+ g_1, where g_0 = so(V) + V is the (generalized) Poincar\'e Lie algebra of the
pseudo Euclidean vector space V = R^{p,q} of signature (p,q) and g_1 = S is the
spinor so(V)-module extended to a g_0-module with kernel V. The remaining super
commutators {g_1,g_1} (respectively, commutators [g_1, g_1]) are defined by an
so(V)-equivariant linear mapping vee^2 g_1 -> V (respectively, wedge^2 g_1 ->
V). Denote by P^+(n,s) (respectively, P^-(n,s)) the vector space of all such
Lie super algebras (respectively, Lie algebras), where n = p + q = dim V and s
= p - q is the signature. The description of P^+-(n,s) reduces to the
construction of all so(V)-invariant bilinear forms on S and to the calculation
of three Z_2-valued invariants for some of them.
  This calculation is based on a simple explicit model of an irreducible
Clifford module S for the Clifford algebra Cl_{p,q} of arbitrary signature
(p,q). As a result of the classification, we obtain the numbers L^+-(n,s) =
\dim P^+-(n,s) of independent Lie super algebras and algebras, which take
values 0,1,2,3,4 or 6. Due to Bott periodicity, L^+-(n,s) may be considered as
periodic functions with period 8 in each argument. They are invariant under the
group Gamma generated by the four reflections with respect to the axes n=-2,
n=2, s-1 = -2 and s-1 = 2. Moreover, the reflection (n,s) -> (-n,s) with
respect to the axis s=0 interchanges L^+ and L^- : L^+(-n,s) = L^-(n,s).

<id>
math/9511216v1
<category>
math.RT
<abstract>
We examine the theory of induced representations for non-connected reductive
$p$-adic groups for which $G/G^0$ is abelian. We first examine the structure of
those representations of the form $\Ind_{P^0}^G(\sigma),$ where $P^0$ is a
parabolic subgroup of $G^0$ and $\s$ is a discrete series representation of the
Levi component of $P^0.$ Here we develop a theory of $R$--groups, extending the
theory in the connected case. We then prove some general results in the theory
of representations of non-connected $p$-adic groups whose component group is
abelian. We define the notion of cuspidal parabolic for $G$ in order to give a
context for this discussion. Intertwining operators for the non-connected case
are examined and the notions of supercuspidal and discrete series are defined.
Finally, we examine parabolic induction from a cuspidal parabolic subgroup of
$G.$ Here we also develop a theory of $R$--groups, and show that these groups
parameterize the induced representations in a manner that is consistent with
the connected case and with the first set of results as well.

<id>
math/9511222v1
<category>
math.RT
<abstract>
Iwahori-Hecke algebras for the infinite series of complex reflection groups
$G(r,p,n)$ were constructed recently in the work of Ariki and Koike, Brou\'e
and Malle, and Ariki. In this paper we give Murnaghan-Nakayama type formulas
for computing the irreducible characters of these algebras. Our method is a
generalization of that in our earlier paper in which we derived
Murnaghan-Nakayama rules for the characters of the Iwahori-Hecke algebras of
the classical Weyl groups. In both papers we have been motivated by C. Greene,
who gave a new derivation of the Murnaghan-Nakayama formula for irreducible
symmetric group characters by summing diagonal matrix entries in Young's
seminormal representations. We use the analogous representations of the
Iwahori-Hecke algebra of $G(r,p,n)$ given by Ariki and Koike.

<id>
math/9511223v1
<category>
math.RT
<abstract>
The purpose of this paper is to describe a general procedure for computing
analogues of Young's seminormal representations of the symmetric groups. The
method is to generalize the Jucys-Murphy elements in the group algebras of the
symmetric groups to arbitrary Weyl groups and Iwahori-Hecke algebras. The
combinatorics of these elements allows one to compute irreducible
representations explicitly and often very easily. In this paper we do these
computations for Weyl groups and Iwahori-Hecke algebras of types $A_n$, $B_n$,
$D_n$, $G_2$. Although these computations are in reach for types $F_4$, $E_6$,
and $E_7$, we shall, in view of the length of the current paper, postpone this
to another work.

<id>
math/9602212v1
<category>
math.RT
<abstract>
This paper is concerned with representations of split orthogonal and
quasi-split unitary groups over a nonarchimedean local field which are not
generic, but which support a unique model of a different kind, the generalized
Bessel model. The properties of the Bessel models under induction are studied,
and an analogue of Rodier's theorem concerning the induction of Whittaker
models is proved for Bessel models which are minimal in a suitable sense. The
holomorphicity in the induction parameter of the Bessel functional is
established. Last, local coefficients are defined for each irreducible
supercuspidal representation which carries a Bessel functional and also for a
certain component of each representation parabolically induced from such a
supercuspidal.

<id>
math/9602220v1
<category>
math.RT
<abstract>
The aim of this paper is to explain, mostly through examples, what groupoids
are and how they describe symmetry. We will begin with elementary examples,
with discrete symmetry, and end with examples in the differentiable setting
which involve Lie groupoids and their corresponding infinitesimal objects, Lie
algebroids.

<id>
math/9605219v1
<category>
math.RT
<abstract>
This is part one of a series of papers. In this series of papers, we consider
problems analogous to the Oppenheim conjecture from the viewpoint of
prehomogeneous vector spaces.

<id>
math/9605231v1
<category>
math.RT
<abstract>
In this paper, we give an introduction to the rationality of the equivariant
Morse stratification, and state the contributor's results on zeta functions of
prehomogeneous vector spaces.

<id>
math/9607218v1
<category>
math.RT
<abstract>
We apply M. Ratner's theorem on closures of unipotent orbits to the study of
three families of prehomogeneous vector spaces. As a result, we prove analogues
of the Oppenheim Conjecture for simultaneous approximation by values of certain
alternating bilinear forms in an even number of variables and certain
alternating trilinear forms in six and seven variables.

<id>
math/9607220v1
<category>
math.RT
<abstract>
The statements of Main~Theorem~1.1 and Theorem~2.1 of the contributor's paper
[\emph{Trans.\ Amer.\ Math.\ Soc.}\ {\bf 345} (1994) 577--594] should assume
that $\Gamma $~is discrete and $G$~is connected. (Cors.~1.3, 5.6, and~5.8 are
affected similarly.) These restrictions can be removed if the conclusions of
the results are weakened to allow for the possible existence of transitive,
proper subgroups of~$G$. In this form, the results can be extended to the
setting where $G$ is a product of real and $p$-adic Lie groups.

<id>
math/9607221v1
<category>
math.RT
<abstract>
Let $\Gamma$ be a discrete subgroup of a simply connected, solvable Lie
group~$G$, such that $\Ad_G\Gamma$ has the same Zariski closure as $\Ad G$. If
$\alpha \colon \Gamma \to \GL_n(\real)$ is any finite-dimensional
representation of~$\Gamma $,we show that $\alpha$ virtually extends to a
continuous representation~$\sigma $ of~$G$. Furthermore, the image of~$\sigma$
is contained in the Zariski closure of the image of~$\alpha $.
  When $\Gamma$ is not discrete, the same conclusions are true if we make the
additional assumption that the closure of $[\Gamma, \Gamma]$ is a finite-index
subgroup of $[G,G] \cap \Gamma$ (and $\Gamma$ is closed and $\alpha$ is
continuous).

<id>
math/9607222v1
<category>
math.RT
<abstract>
Proposition~6.4 of the contributor's paper {\origpaper} is incorrect. This invalid
proposition was used in the proof of Corollary~6.5, so we provide a new proof
of the latter result.

<id>
math/9611220v1
<category>
math.RT
<abstract>
Let $\bold G$ be a reductive algebraic group defined over $\Q$, and let
$\Gamma$ be an arithmetic subgroup of $\bold G(\Q)$. Let $X$ be the symmetric
space for $\bold G(\R)$, and assume $X$ is contractible. Then the cohomology
(mod torsion) of the space $X/\Gamma$ is the same as the cohomology of
$\Gamma$. In turn, $X/\Gamma$ will have the same cohomology as $W/\Gamma$, if
$W$ is a ``spine'' in $X$. This means that $W$ (if it exists) is a deformation
retract of $X$ by a $\Gamma$-equivariant deformation retraction, that
$W/\Gamma$ is compact, and that $\dim W$ equals the virtual cohomological
dimension (vcd) of $\Gamma$. Then $W$ can be given the structure of a cell
complex on which $\Gamma$ acts cellularly, and the cohomology of $W/\Gamma$ can
be found combinatorially.

<id>
math/9701218v1
<category>
math.RT
<abstract>
In this paper, we determine the rational orbit decomposition for two
prehomogeneous vector spaces associated with the simple group of type G_2.

<id>
math/9702220v1
<category>
math.RT
<abstract>
Let H_1=SL(5), H_2=SL(3), H=H_1 \times H_2. It is known that (G,V) is a
prehomogeneous vector space (see [22], [26], [25], for the definition of
prehomogeneous vector spaces). A non-constant polynomial \delta(x) on V is
called a relative invariant polynomial if there exists a character \chi such
that \delta(gx)=\chi(g)\delta(x). Such \delta(x) exists for our case and is
essentially unique. So we define V^{ss}={x in V such that \delta(x) is not
equal to 0}. For x in V_R^{ss}, let H_{x R+}^0 be the connected component of 1
in classical topology of the stabilizer H_{x R}. We will prove that if x in
V_R^ss is "sufficiently irrational", H_{x R+}^0 H_Z is dense in H_R.

<id>
math/9708209v1
<category>
math.RT
<abstract>
In this note, we prove that if $(G,V)$ is a prehomogeneous vector space over
any field $k$ such that the stabilizer of a generic point is reductive, the set
of semi-stable points is a single orbit over the separable closure of $k$.

<id>
math/9710214v1
<category>
math.RT
<abstract>
In this note, we consider applications of Ratner's theorem to constructions
of families of polynomials with dense values on the set of primitive integer
points from the viewpoint of invariant theory.

<id>
math/9310230v1
<category>
math.RA
<abstract>
A new dimension function on countable-dimensional algebras (over a field) is
described. Its dimension values for finitely generated algebras exactly fill
the unit interval $[0,1]$. Since the free algebra on two generators turns out
to have dimension 0 (although conceivably some Noetherian algebras might have
positive dimension!), this dimension function promises to distinguish among
algebras of infinite GK-dimension.

<id>
math/9504224v1
<category>
math.RA
<abstract>
Maria Pia Sol\`er has recently proved that an orthomodular form that has an
infinite orthonormal sequence is real, complex, or quaternionic Hilbert space.
This paper provides an exposition of her result, and describes its consequences
for Baer $\ast$-rings, infinite-dimensional projective geometries, orthomodular
lattices, and Mackey's quantum logic.

<id>
math/9602217v1
<category>
math.RA
<abstract>
In this paper, we prove the number of countable models of a countable
supersimple theory is either 1 or infinite. This result is an extension of
Lachlan's theorem on a superstable theory.

<id>
math/9604246v1
<category>
math.RA
<abstract>
We clarify the relationship between the linear commutator and the ordinary
commutator by showing that in any variety satisfying a nontrivial idempotent
Mal'cev condition the linear commutator is definable in terms of the
centralizer relation. We derive from this that abelian algebras are
quasi-affine in such varieties. We refine this by showing that if A is an
abelian algebra and V(A) satifies an idempotent Mal'cev condition which fails
to hold in the variety of semilattices, then A is affine.

<id>
math/9605236v1
<category>
math.RA
<abstract>
We describe a sufficient condition for the localization functor to be a
categorical equivalence. Using this result we explain how to simplify the test
for projectivity. This leads to a description of the strictly simple algebras
which are projective in the variety they generate. A byproduct of our efforts
is the result that if A and B are strictly simple and generate the same
variety, then A=B or else both are strongly abelian.

<id>
math/9606213v1
<category>
math.RA
<abstract>
Let $A$ be an $n \times M$ matrix whose rows are orthonormal. Let $A_I$ be a
submatrix of $A$ whose column indexes belong to the set $I$. Given $\epsilon
>0$ we estimate the smallest cardinality of the set $I$, such that the operator
$A_I$ is an $\epsilon$-isometry.

<id>
math/9607229v1
<category>
math.RA
<abstract>
We show that a locally finite variety which omits abelian types is
self-regulating if and only if it has a compatible semilattice term operation.
Such varieties must have a type-set {5}. These varieties are residually small
and, when they are finitely generated, they have definable principal
congruences. We show that idempotent varieties with a compatible semilattice
term operation have the congruence extension property.

<id>
math/9607232v1
<category>
math.RA
<abstract>
We derive a general result about commuting actions on certain objects in
braided rigid monoidal categories. This enables us to define an action of the
Brauer algebra on the tensor space $V^{\otimes k}$ which commutes with the
action of the orthosymplectic Lie superalgebra $\spo(V)$ and the
orthosymplectic Lie color algebra $\spo(V,\beta)$. We use the Brauer algebra
action to compute maximal vectors in $V^{\otimes k}$ and to decompose
$V^{\otimes k}$ into a direct sum of submodules $T^\lambda$. We compute the
characters of the modules $T^\lambda$, give a combinatorial description of
these characters in terms of tableaux, and model the decomposition of
$V^{\otimes k}$ into the submodules $T^\lambda$ with a Robinson-Schensted-Knuth
type insertion scheme.

<id>
math/9608216v1
<category>
math.RA
<abstract>
Let T be a countable, small simple theory. In this paper, we prove for such
T, the notion of Lascar Strong type coincides with the notion of a strong
type,over an arbitrary set.

<id>
math/9609219v1
<category>
math.RA
<abstract>
We establish a direct correspondence between two congruence poroperties for
finite algebras. The first property is that minimal sets of type i omit tails.
The second property is that congruence lattices omit pentagons of type i.

<id>
math/9611224v1
<category>
math.RA
<abstract>
We extend a recent result of McKenzie, and show that it is an undecidable
problem to determine if 4 appears in the typeset of a finitely generated,
locally finite variety.

<id>
math/9701220v1
<category>
math.RA
<abstract>
Alternating bilinear maps with few relations allow to define a combinatorial
closure similarly as in [2]. For the $\aleph_0$-categorical case we show that
this closure is part of the algebraic closure.

<id>
math/9702230v1
<category>
math.RA
<abstract>
Let K be an algebraically closed field endowed with a complete
non-archimedean norm. Let f:Y -> X be a map of K-affinoid varieties. We prove
that for each point x in X, either f is flat at x, or there exists, at least
locally around x, a maximal locally closed analytic subvariety Z in X
containing x, such that the base change f^{-1}(Z) -> Z is flat at x, and,
moreover, g^{-1}(Z) has again this property in any point of the fibre of x
after base change over an arbitrary map g:X' -> X of affinoid varieties. If we
take the local blowing up \pi:X-tilde -> X with this centre Z, then the fibre
with respect to the strict transform f-tilde of f under \pi, of any point of
X-tilde lying above x, has grown strictly smaller. Among the corollaries to
these results we quote, that flatness in rigid analytic geometry is local in
the source; that flatness over a reduced quasi-compact rigid analytic variety
can be tested by surjective families; that an inclusion of affinoid domains is
flat in a point, if it is unramified in that point.

<id>
math/9707233v1
<category>
math.RA
<abstract>
We give a criterion for a group homomorphism on a valued abelian group to be
surjective and to preserve spherical completeness. We apply this to give a
criterion for the existence of integration on a valued differential field.
Further, we give a criterion for a sum of spherically complete subgroups of a
valued abelian group to be spherically complete. This in turn can be used to
determine elementary properties of power series fields in positive
characteristic.

<id>
math/9709232v1
<category>
math.RA
<abstract>
In this paper we show that Z_8 does not admit a natural duality. In fact, we
show that 2Z_8 = {2, 4, 6, 8 | +,.} is not dualizable, and this will imply that
the original ring is not dualizable, either. As a corollary we show that
Sindi's conjecture does not hold. Our technique will be similar to one due to
Quackenbush and Szab\'o, where non-dualizability is proved for the quaternion
group.

<id>
math/9710220v1
<category>
math.RA
<abstract>
Every o-minimal expansion R-tilde of the real field has an o-minimal
expansion P(R-tilde) in which the solutions to Pfaffian equations with
definable C^1 coefficients are definable.

<id>
math/9711224v1
<category>
math.RA
<abstract>
Complexity problems associated with finite rings and finite semigroups,
particularly semigroups of matrices over a field and the Rees matrix
semigroups, are examined. Let M_nF be the ring of n x n matrices over the
finite field F and let T_nF be the multiplicative semigroup of n x n matrices
over the finite field F. It is proved that for any finite field F and positive
integer n >= 2, the polynomial equivalence problem for the T_n F is
co-NPcomplete, thus POL-EQ_\Sigma(M_n F) (POL-EQ_\Sigma is a polynomial
equivalence problem for which polynomials are presented as sums of monomials)
is also co-NP-complete thereby resolving a problem of J. Lawrence and R.
Willard and completing the description of POL-EQ_\Sigma for the finite simple
rings. In connection with our results on rings, we exhibit a large class of
combinatorial Rees matrix semigroups whose polynomial equivalence problem is
co-NP-complete. On the other hand, if S is a combinatorial Rees matrix
semigroup with a totally balanced structure matrix M, then we prove that the
polynomial equivalence problem for S is in P. Fully determining the complexity
of the polynomial equivalence problem for combinatorial Rees matrix semigroups
may be a difficult problem. We describe a connection between the polynomial
equivalence problem for combinatorial Rees matrix semigroups and the retraction
problem RET for bipartite graphs, a problem which computer scientists suspect
may not admit a dichotomy into P and NP-complete problems (assuming P is not
equal to NP).

<id>
math/9711226v1
<category>
math.RA
<abstract>
Primitive representations of finite groups as well as primitive finite groups
were classified in the O'Nan-Scott Theorem. In this paper we classify faithful
finite primitive semigroup representations. To each finite primitive
representation, we associate an invariant, a finite dimensional matirix with
entries in a primitive finite groups representation. To a large extent, this
matrix determines the representation. In later papers in this series the
invariant is further explored. Our primitivity resoullts rely on two main
ideas, one of which is a small but important part of the theory of tame
algebras developed by R. McKenzie, while the other is our adaptation of Rees
matrix theory to the representation theory of finite semigroups. Using the
latter idea we are able to provide a description of all representations of a
regular T class of a finite semigroup.

<id>
math/9712291v1
<category>
math.RA
<abstract>
We derive a Mal'cev condition for congruence meet-semidistributivity and then
use it to prove two theorems. Theorem A: if a variety in a finite language is
congruence meet-semidistributive and residually less than some finite cardinal,
then it is finitely based. Theorem B: there is an algorithm which, given m<w
and a finite algebra in a finite language, determines whether the variety
generated by the algebra is congruence meet-semidistributive and residually
less then m.

<id>
math/9802039v1
<category>
math.RA
<abstract>
Denote by (R,.) the multiplicative semigroup of an associative algebra R over
an infinite field, and let (R,*) represent R when viewed as a semigroup via the
circle operation x*y=x+y+xy. In this paper we characterize the existence of an
identity in these semigroups in terms of the Lie structure of R. Namely, we
prove that the following conditions on R are equivalent: the semigroup (R,*)
satisfies an identity; the semigroup (R,.) satisfies a reduced identity; and,
the associated Lie algebra of R satisfies the Engel condition. When R is
finitely generated these conditions are each equivalent to R being upper Lie
nilpotent.

<id>
math/9802106v1
<category>
math.RA
<abstract>
An upper bound on operator norms of compound matrices is presented, and
special cases that involve the $\ell_1$, $\ell_2$ and $\ell_\infty$ norms are
investigated. The results are then used to obtain bounds on products of the
largest or smallest eigenvalues of a matrix.

<id>
math/9803060v1
<category>
math.RA
<abstract>
Let $\|A\|_{p,q}$ be the norm induced on the matrix $A$ with $n$ rows and $m$
columns by the H\"older $\ell_p$ and $\ell_q$ norms on $R^n$ and $R^m$ (or
$C^n$ and $C^m$), respectively. It is easy to find an upper bound for the ratio
$\|A\|_{r,s}/\|A\|_{p,q}$. In this paper we study the classes of matrices for
which the upper bound is attained. We shall show that for fixed $A$, attainment
of the bound depends only on the signs of $r-p$ and $s-q$. Various criteria
depending on these signs are obtained. For the special case $p=q=2$, the set of
all matrices for which the bound is attained is generated by means of singular
value decompositions.

<id>
math/9807019v1
<category>
math.RA
<abstract>
For any $n$-ary associative algebra we construct a $\Z_{n-1}$ graded algebra,
which is a universal object containing the $n$-ary algebra as a subspace of
elements of degree 1. Similar construction is carried out for semigroups.

<id>
math/9807112v1
<category>
math.RA
<abstract>
The main aim of this paper is to show that an AB5*-module whose small
submodules have Krull dimension has a radical having Krull dimension. The proof
uses the notion of dual Goldie dimension.

<id>
math/9807113v1
<category>
math.RA
<abstract>
It is well-known that a ring R is semiperfect if and only if R as a left (or
as a right) R-module is a supplemented module. Considering weak supplements
instead of supplements we show that weakly supplemented modules M are semilocal
(i.e., M/Rad(M) is semisimple) and that R is a semilocal ring if and only if R
as a left (or as a right) R-module is weakly supplemented. In this context the
notion of finite hollow dimension (or finite dual Goldie dimension) of modules
is of interest and yields a natural interpretation of the Camps-Dicks
characterization of semilocal rings. Finitely generated modules are weakly
supplemented if and only if they have finite hollow dimension (or are
semilocal).

<id>
math/9807121v1
<category>
math.RA
<abstract>
We show that a matrix is a Hermitian positive semidefinite matrix whose
nonzero entries have modulus 1 if and only if it similar to a direct sum of all
$1's$ matrices and a 0 matrix via a unitary monomial similarity. In particular,
the only such nonsingular matrix is the identity matrix and the only such
irreducible matrix is similar to an all 1's matrix by means of a unitary
diagonal similarity. Our results extend earlier results of Jain and Snyder for
the case in which the nonzero entries (actually) equal 1. Our methods of proof,
which rely on the so called principal submatrix rank property, differ from the
approach used by Jain and Snyder.

<id>
math/9807126v1
<category>
math.RA
<abstract>
We discuss the eigenvalue problem for 2x2 and 3x3 octonionic Hermitian
matrices. In both cases, we give the general solution for real eigenvalues, and
we show there are also solutions with non-real eigenvalues.

<id>
math/9807132v1
<category>
math.RA
<abstract>
The principal pivot transform (PPT) of a matrix A partitioned relative to an
invertible leading principal submatrix is a matrix B such that
  A [x_1^T x_2^T]^T = [y_1^T y_2^T]^T
  if and only if
  B [y_1^T x_2^T]^T = [x_1^T y_2^T]^T,
  where all vectors are partitioned conformally to A. The purpose of this paper
is to survey the properties and manifestations of PPTs relative to arbitrary
principal submatrices, make some new observations, present and possibly
motivate further applications of PPTs in matrix theory. We pay special
attention to PPTs of matrices whose principal minors are positive.

<id>
math/9807133v1
<category>
math.RA
<abstract>
The eigenvalue problem for 3x3 octonionic Hermitian matrices contains some
surprises, which we have reported elsewhere. In particular, the eigenvalues
need not be real, there are 6 rather than 3 real eigenvalues, and the
corresponding eigenvectors are not orthogonal in the usual sense. The
nonassociativity of the octonions makes computations tricky, and all of these
results were first obtained via brute force (but exact) Mathematica
computations. Some of them, such as the computation of real eigenvalues, have
subsequently been implemented more elegantly; others have not. We describe here
the use of Mathematica in analyzing this problem, and in particular its use in
proving a generalized orthogonality property for which no other proof is known.

<id>
math/9808123v1
<category>
math.RA
<abstract>
We show that group algebras kG of polycyclic-by-finite groups G, where k is a
field, are catenary: Given prime ideals P and P' of kG, with P contained in P',
all saturated chains of primes between P and P' have the same length.

<id>
math/9310229v1
<category>
math.SP
<abstract>
We extend the well-known trace formula for Hill's equation to general
one-dimensional Schr\"odinger operators. The new function $\xi$, which we
introduce, is used to study absolutely continuous spectrum and inverse
problems.

<id>
math/9410217v1
<category>
math.SP
<abstract>
In a variety of contexts, we prove that singular continuous spectrum is
generic in the sense that for certain natural complete metric spaces of
operators, those with singular spectrum are a dense $G_\delta$.

<id>
math/9606214v1
<category>
math.SP
<abstract>
We use Rokhlin's Theorem on the uniqueness of canonical systems to find a new
way to establish connections between Function Theory in the unit disk and rank
one perturbations of self-adjoint or unitary operators. In the n-dimensional
case, we prove that for any cyclic self-adjoint operator $A$, operator
$A_\lambda= A + \Sigma_{k=1}^n \lambda_k(\cdot,\phi_k)\phi_k$ is pure point for
a. e. $\lambda=(\lambda_1,\lambda_2,...,\lambda_n) \in\Bbb R^n$ iff operator
$A_\eta=A+\eta(\cdot,\phi_k)\phi_k$ is pure point for a.e.\ $\eta\in\Bbb R$ for
$k=1,2,...,n$. We also show that if $A_\lambda$ is pure point for a.e.\
$\lambda\in \Bbb R^n$ then $A_\lambda$ is pure point for a.e.\ $\lambda\in
\gamma$ for any analytic curve $\gamma\in\Bbb R^n$.

<id>
math/9801074v1
<category>
math.SP
<abstract>
The norm of an integral operator occurring in the partial wave decomposition
of an operator B introduced by Brown and Ravenhall in a model for relativistic
one-electron atoms is determined. The result implies that B is non-negative and
has no eigenvalue at 0 when the nuclear charge does not exceed a specified
critical value.

<id>
math/9802035v1
<category>
math.SP
<abstract>
A virial theorem is established for the operator proposed by Brown and
Ravenhall as a model for relativistic one-electron atoms. As a consequence, it
is proved that the operator has no eigenvalues greater than $\max(m c^2, 2
\alpha Z - \frac{1}{2})$, where $\alpha$ is the fine structure constant, for
all values of the nuclear charge $Z$ below the critical value $Z_c$: in
particular there are no eigenvalues embedded in the essential spectrum when $Z
\leq 3/4 \alpha$. Implications for the operators in the partial wave
decomposition are also described.

<id>
math/9804089v1
<category>
math.SP
<abstract>
Given a separable, locally compact Hausdorff space $X$ and a positive Radon
measure $m(dx)$ on it, we study the problem of finding the potential $V(x) \ge
0$ that maximizes the first eigenvalue of the Schr\"odinger-type operator
$L+V(x)$; $L$ is the generator of a local Dirichlet form $(a, D[a])$ on $L^2(X,
m(dx))$.

<id>
math/9809182v2
<category>
math.SP
<abstract>
We continue the study of the A-amplitude associated to a half-line
Schrodinger operator, -d^2/dx^2+ q in L^2 ((0,b)), b <= infinity. A is related
to the Weyl-Titchmarsh m-function via m(-\kappa^2) =-\kappa - \int_0^a
A(\alpha) e^{-2\alpha\kappa} d\alpha +O(e^{-(2a -\epsilon)\kappa}) for all
\epsilon > 0. We discuss five issues here. First, we extend the theory to
general q in L^1 ((0,a)) for all a, including q's which are limit circle at
infinity. Second, we prove the following relation between the A-amplitude and
the spectral measure \rho: A(\alpha) = -2\int_{-\infty}^\infty
\lambda^{-\frac12} \sin (2\alpha \sqrt{\lambda})\, d\rho(\lambda) (since the
integral is divergent, this formula has to be properly interpreted). Third, we
provide a Laplace transform representation for m without error term in the case
b<\infty. Fourth, we discuss m-functions associated to other boundary
conditions than the Dirichlet boundary conditions associated to the principal
Weyl-Titchmarsh m-function. Finally, we discuss some examples where one can
compute A exactly.

<id>
math/9810044v1
<category>
math.SP
<abstract>
The spectral problem (A + V(z))\psi=z\psi is considered with A, a
self-adjoint operator. The perturbation V(z) is assumed to depend on the
spectral parameter z as resolvent of another self-adjoint operator A':
V(z)=-B(A'-z)^{-1}B^{*}. It is supposed that the operator B has a finite
Hilbert-Schmidt norm and spectra of the operators A and A' are separated.
Conditions are formulated when the perturbation V(z) may be replaced with a
``potential'' W independent of z and such that the operator H=A+W has the same
spectrum and the same eigenfunctions (more precisely, a part of spectrum and a
respective part of eigenfunctions system) as the initial spectral problem. The
operator H is constructed as a solution of the non-linear operator equation
H=A+V(H) with a specially chosen operator-valued function V(H). In the case if
the initial spectral problem corresponds to a two-channel variant of the
Friedrichs model, a basis property of the eigenfunction system of the operator
H is proved. A scattering theory is developed for H in the case where the
operator A has continuous spectrum.

<id>
math/9811158v1
<category>
math.SP
<abstract>
Let H be the homogeneous space associated to the group PGL_3(R). Let
X=\Gamma/H where \Gamma=SL_3(Z) and consider the first non-trivial eigenvalue
\lambda_1 of the Laplacian on L^2(X). Using geometric considerations, we prove
the inequality \lambda_1<pi^2/10. Since the continuous spectrum is represented
by the band [1,\infty), our bound on \lambda_1 can be viewed as an analogue of
Selberg's eigenvalue conjecture for quotients of the hyperbolic half space.

<id>
math/9811176v1
<category>
math.SP
<abstract>
Consider the Schroeodinger equation: - Du(x) - l(x)u + s(x)u = 0, where D is
the Laplacian, l(x) > 0 and s(x) is dominated by l(x). We shall extend the
celebrated Kato's result on the asymptotic behavior of the solution to the case
where l(x) has unbounded discontinuity. The result will be used to establish
the limiting absorption principle for a class of reduced wave operators with
discontinuous coefficients.

<id>
math/9812090v1
<category>
math.SP
<abstract>
The note contains the proof of the uniqueness theorem for the inverse problem
in the case of $n$-th order differential equation.

<id>
math/9812091v1
<category>
math.SP
<abstract>
An uniqueness theorem for the inverse problem in the case of a second-order
equation defined on the interval [0,1] when the boundary forms contain
combinations of the values of functions at the points 0 and 1 is proved. The
auxiliary eigenvalue problems in our theorem are chose in the same manner as in
Borg's uniqueness theorem are not as in that of Sadovni\v ci$\check \imath $'s.
So number of conditions in our theorem is less than that in Sadovni\v
ci$\check\imath$'s.

<id>
math/9901112v1
<category>
math.SP
<abstract>
We introduce the concept of a spectral shift operator and use it to derive
Krein's spectral shift function for pairs of self-adjoint operators. Our
principal tools are operator-valued Herglotz functions and their logarithms.
Applications to Krein's trace formula and to the Birman-Solomyak spectral
averaging formula are discussed.

<id>
math/9902019v1
<category>
math.SP
<abstract>
The contributor tries to derive the asymptotic expression of the large eigevalues
of some vectorial Sturm-Liouville differential equations. A precise description
for the formula of the square root of the large eiegnvalues up to the
$O(1/n)$-term is obtained.

<id>
math/9902041v1
<category>
math.SP
<abstract>
The contributor extends the idea of Jodeit and Levitan for constructing
isospectral problems of the classical scalar Sturm-Liouville differential
equations to the vectorial Sturm-Liouville differential equations. Some
interesting relations are found.

<id>
math/9902084v1
<category>
math.SP
<abstract>
We shall investigate the asymptotic behavior of the extended resolvent R(s)
of the Dirac operator as |s| increases to infinity, where s is a real
parameter. It will be shown that the norm of R(s), as a bounded operator
between two weighted Hilbert spaces of square integrable functions on the
3-dimensional Euclidean space, stays bounded. Also we shall show that R(s)
converges 0 strongly as |s| increases to infinity. This result and a result of
Yamada [15] are combined to indicate that the extended resolvent of the Dirac
operator decays much more slowly than those of Schroedinger operators.

<id>
math/9902085v1
<category>
math.SP
<abstract>
Consider the differential operator H = -(1/m(x))L, where L is the
N-dimensional Laplacian, in the weighted Hilbert space of square integrable
functions on N-dimensional Euclidean space with weight m(x)dx. Here m(x) is a
positive step function with a surface S of discontinuity (the separation
surface). So far the stratified media in which the separating surface S
consists of paralell planes have been vigorously studied. Also the case where S
has a cone shape has been discussed. In this work we shall deal with a new type
of discontinuity which we call cylindrical discontinuity. Under this condition
we shall use the limiting absorption method to prove that H is absolute
continuous. Our method is based on a apriori estimates of radiation condition
term.

<id>
math/9902086v1
<category>
math.SP
<abstract>
Let H = -(1/m(x))L be the reduced wave operator defined on the N-dimensional
Euclidean space, where \f L is the Laplacian. Here m(x) is a positive step
function with possible countably infinte surfaces of discontinuity (separating
surfaces) under the compatibilty condition (1.12) on each separating surface.
These compatibily condition allows us to treat the cases, among others, the
separating surfaces are cylinders. The case where the separating surface has
only one connected component was discussed in [9]. Also the case where the
separating surface is cone-shaped was considered by Eidus [6] and others ([10],
[11]). We shall prove the limiting absorption principle for H. Also we shall
discuss the case where m(x) is perturbed by a short-range or long-range
function.

<id>
math/9903186v1
<category>
math.SP
<abstract>
The recently introduced concept of a spectral shift operator is applied in
several instances. Explicit applications include Krein's trace formula for
pairs of self-adjoint operators, the Birman-Solomyak spectral averaging formula
and its operator-valued extension, and an abstract approach to trace formulas
based on perturbation theory and the theory of self-adjoint extensions of
symmetric operators.

<id>
math/9904050v2
<category>
math.SP
<abstract>
We explore connections between Krein's spectral shift function
$\xi(\lambda,H_0,H)$ associated with the pair of self-adjoint operators
$(H_0,H)$, $H=H_0+V$ in a Hilbert space $\calH$ and the recently introduced
concept of a spectral shift operator $\Xi(J+K^*(H_0-\lambda-i0)^{-1}K)$
associated with the operator-valued Herglotz function $J+K^*(H_0-z)^{-1}K$,
$\Im(z)>0$ in $\calH$, where $V=KJK^*$ and $J=\sgn(V)$. Our principal results
include a new representation for $\xi(\lambda,H_0,H)$ in terms of an averaged
index for the Fredholm pair of self-adjoint spectral projections
$(E_{J+A(\lambda)+tB(\lambda)}((-\infty,0)),E_J((-\infty,0)))$, $t\in\bbR$,
where $A(\lambda)=\Re(K^*(H_0-\lambda-i0)^{-1}K)$,
$B(\lambda)=\Im(K^*(H_0-\lambda-i0)^{-1}K)$ a.e. Moreover, introducing the new
concept of a trindex for a pair of operators $(A,P)$ in $\calH$, where $A$ is
bounded and $P$ is an orthogonal projection, we prove that $\xi(\lambda,H_0,H)$
coincides with the trindex associated with the pair
$(\Xi(J+K^*(H_0-\lambda-i0)^{-1}K),\Xi(J))$. In addition, we discuss a variant
of the Birman-Krein formula relating the trindex of a pair of $\Xi$-operators
and the Fredholm determinant of the abstract scattering matrix.
  We also provide a generalization of the classical Birman-Schwinger principle,
replacing the traditional eigenvalue counting functions by appropriate spectral
shift functions.

<id>
math/9905070v2
<category>
math.SP
<abstract>
We explicitly determine the high-energy asymptotics for Weyl-Titchmarsh
matrices associated with general matrix-valued Schr\"odinger operators on a
half-line.

<id>
math/9905143v1
<category>
math.SP
<abstract>
A Borg-type uniqueness theorem for matrix-valued Schr\"odinger operators is
proved. More precisely, assuming a reflectionless potential matrix and spectrum
a half-line $[0,\infty)$, we derive triviality of the potential matrix. Our
approach is based on trace formulas and matrix-valued Herglotz representation
theorems. As a by-product of our techniques, we obtain an extension of Borg's
classical result from the class of periodic scalar potentials to the class of
reflectionless matrix-valued potentials.

<id>
math/9906118v2
<category>
math.SP
<abstract>
We present a new approach (distinct from Gel'fand-Levitan) to the theorem of
Borg-Marchenko that the m-function (equivalently, spectral measure) for a
finite interval or half-line Schr\"odinger operator determines the potential.
Our approach is an analog of the continued fraction approach for the moment
problem. We prove there is a representation for the m-function
  m(-\kappa^2) = -\kappa - \int_0^b A(\alpha) e^{-2\alpha\kappa}\, d\alpha +
O(e^{-(2b-\varepsilon)\kappa}).
  A on [0,a] is a function of q on [0,a] and vice-versa. A key role is played
by a differential equation that A obeys after allowing x-dependence:
  \frac{\partial A}{\partial x} = \frac{\partial A}{\partial \alpha} +
\int_0^\alpha A(\beta, x) A(\alpha -\beta, x)\, d\beta.
  Among our new results are necessary and sufficient conditions on the
m-functions for potentials q_1 and q_2 for q_1 to equal q_2 on [0,a].

<id>
math/9909076v1
<category>
math.SP
<abstract>
Let $H_0$ and $V(s)$ be self-adjoint, $V,V'$ continuously differentiable in
trace norm with $V''(s)\geq 0$ for $s\in (s_1,s_2)$, and denote by
$\{E_{H(s)}(\lambda)\}_{\lambda\in\bbR}$ the family of spectral projections of
$H(s)=H_0+V(s)$. Then we prove for given $\mu\in\bbR$, that $s\longmapsto
\tr\big (V'(s)E_{H(s)}((-\infty, \mu))\big) $ is a nonincreasing function with
respect to $s$, extending a result of Birman and Solomyak. Moreover, denoting
by $\zeta (\mu,s)=\int_{-\infty}^\mu d\lambda \xi(\lambda,H_0,H(s))$ the
integrated spectral shift function for the pair $(H_0,H(s))$, we prove
concavity of $\zeta (\mu,s)$ with respect to $s$, extending previous results by
Geisler, Kostrykin, and Schrader. Our proofs employ operator-valued Herglotz
functions and establish the latter as an effective tool in this context.

<id>
math/9909092v2
<category>
math.SP
<abstract>
S.G.Krein's conjecture concerning Birkhoff-regularity of dissipative
differential operators has been proved in the even order case. As a byproduct
an existence of the limit of characteristic matrix as in the lower half-plane
has been established. Up to multiplication by a nonvanishing matrix this limit
coincides with the ratio of the matrices of regularity determinants.

<id>
math/9910089v1
<category>
math.SP
<abstract>
We provide a new short proof of the following fact, first proved by one of us
in 1998: If two Weyl-Titchmarsh m-functions, $m_j(z)$, of two Schr\"odinger
operators $H_j = -\f{d^2}{dx^2} + q_j$, j=1,2 in $L^2 ((0,R))$, $0<R\leq
\infty$, are exponentially close, that is, $|m_1(z)- m_2(z)|
\underset{|z|\to\infty}{=} O(e^{-2\Ima (z^{1/2})a})$, 0<a<R, then $q_1 = q_2$
a.e.~on $[0,a]$. The result applies to any boundary conditions at x=0 and x=R
and should be considered a local version of the celebrated Borg-Marchenko
uniqueness result (which is quickly recovered as a corollary to our proof).
Moreover, we extend the local uniqueness result to matrix-valued Schr\"odinger
operators.

<id>
math/9911086v1
<category>
math.SP
<abstract>
We study quantum scattering theory off $n$ point inhomogeneities ($n\in\bbN$)
in three dimensions. The inhomogeneities (or generalized point interactions)
positioned at $\{\xi_1,...,\xi_n\}\subset\bbR^3$ are modeled in terms of the
$n^2$ (real) parameter family of self-adjoint extensions of
$-\Delta\big|_{C^\infty_0(\bbR^3\backslash\{\xi_1,...,\xi_n\})}$ in
$L^2(\bbR^3)$. The Green's function, the scattering solutions and the
scattering amplitude for this model are explicitly computed in terms of
elementary functions. Moreover, using the connection between fixed energy
quantum scattering and acoustical scattering, the following inverse spectral
result in acoustics is proved: The knowledge of the scattered field on a plane
outside these point-like inhomogeneities, with all inhomogeneities located on
one side of the plane, uniquely determines the positions and boundary
conditions associated with them.

<id>
math/9912244v1
<category>
math.SP
<abstract>
We introduce the notion of scattering space $S_b^r$ for $N$-body quantum
mechanical systems, where $b$ is a cluster decomposition with $2\le |b|\le N$
and $r$ is a real number $0\le r\le 1$. Utilizing these spaces, we give a
decomposition of continuous spectral subspace by $S_b^1$ for $N$-body quantum
systems with long-range pair potentials
$V_\alpha^L(x_\alpha)=O(|x_\al|^{-\ep})$. This is extended to a decomposition
by $S_b^r$ with $0\le r\le 1$ for some long-range case. We also prove a
characterization of ranges of wave operators by $S_b^0$.

<id>
math/0003044v1
<category>
math.SP
<abstract>
We give a spectral description of the semi-classical Schrodinger operator
with a piecewise linear, complex valued potential. Moreover, using these
results, we show how an arbitrarily small bounded perturbation of a
non-self-adjoint operator can completely change the spectrum of the operator.

<id>
math/0004120v2
<category>
math.SP
<abstract>
Let $g(z,x)$ denote the diagonal Green's matrix of a self-adjoint $m\times m$
matrix-valued Schr\"odinger operator $H= -\f{d^2}{dx^2}I_m +Q(x)$ in $L^2
(\bbR)^{m}$, $m\in\bbN$. One of the principal results proven in this paper
states that for a fixed $x_0\in\bbR$ and all $z\in\bbC_+$, $g(z,x_0)$ and
$g^\prime (z,x_0)$ uniquely determine the matrix-valued $m\times m$ potential
$Q(x)$ for a.e.~$x\in\bbR$. We also prove the following local version of this
result. Let $g_j(z,x)$, $j=1,2$ be the diagonal Green's matrices of the
self-adjoint Schr\"odinger operators $H_j=-\f{d^2}{dx^2}I_m +Q_j(x)$ in $L^2
(\bbR)^{m}$. Suppose that for fixed $a>0$ and $x_0\in\bbR$,
$\|g_1(z,x_0)-g_2(z,x_0)\|_{\bbC^{m\times m}}+ \|g_1^\prime (z,x_0)-g_2^\prime
(z,x_0)\|_{\bbC^{m\times m}}
\underset{|z|\to\infty}{=}O\big(e^{-2\Im(z^{1/2})a}\big)$ for $z$ inside a cone
along the imaginary axis with vertex zero and opening angle less than $\pi/2$,
excluding the real axis. Then $Q_1(x)=Q_2(x)$ for a.e.~$x\in [x_0-a,x_0+a]$.
Analogous results are proved for matrix-valued Jacobi and Dirac-type operators.

<id>
math/0107135v2
<category>
stat.TH
<abstract>
We consider two kinds of stochastic volatility models. Both kinds of models
contain a stationary volatility process, the density of which, at a fixed
instant in time, we aim to estimate.
  We discuss discrete time models where for instance a log price process is
modeled as the product of a volatility process and i.i.d. noise. We also
consider samples of certain continuous time diffusion processes. The sampled
time instants will be be equidistant with vanishing distance.
  A Fourier type deconvolution kernel density estimator based on the logarithm
of the squared processes is proposed to estimate the volatility density.
Expansions of the bias and bounds on the variances are derived.

<id>
math/0109002v1
<category>
stat.TH
<abstract>
We show that that the jackknife variance estimator $v_{jack}$ and the the
infinitesimal jackknife variance estimator are asymptotically equivalent if the
functional of interest is a smooth function of the mean or a smooth trimmed
L-statistic. We calculate the asymptotic variance of $v_{jack}$ for these
functionals.

<id>
math/0111152v1
<category>
stat.TH
<abstract>
In this paper an iterated function system on the space of distribution
functions is built. The inverse problem is introduced and studied by convex
optimization problems. Some applications of this method to approximation of
distribution functions and to estimation theory are given.

<id>
math/0111153v1
<category>
stat.TH
<abstract>
A subthreshold signal is transmitted through a channel and may be detected
when some noise -- with known structure and proportional to some level -- is
added to the data. There is an optimal noise level, called stochastic
resonance, that corresponds to the highest Fisher information in the problem of
estimation of the signal. As noise we consider an ergodic diffusion process and
the asymptotic is considered as time goes to infinity. We propose consistent
estimators of the subthreshold signal and we solve further a problem of
hypotheses testing. We also discuss evidence of stochastic resonance for both
estimation and hypotheses testing problems via examples.

<id>
math/0112032v1
<category>
stat.TH
<abstract>
We derive asymptotic normality of kernel type deconvolution estimators of the
density, the distribution function at a fixed point, and of the probability of
an interval. We consider the so called super smooth case where the
characteristic function of the known distribution decreases exponentially.
  It turns out that the limit behavior of the pointwise estimators of the
density and distribution function is relatively straightforward while the
asymptotics of the estimator of the probability of an interval depends in a
complicated way on the sequence of bandwidths.

<id>
math/0112298v1
<category>
stat.TH
<abstract>
In the article we consider accumulated values of annuities-certain with
yearly payments with independent random interest rates. We focus on annuities
with payments varying in arithmetic and geometric progression which are
important basic varying annuities (see Kellison, 1991). They appear to be a
generalization of the types studied recently by Zaks (2001). We derive, via
recursive relationships, mean and variance formulae of the final values of the
annuities. As a consequence, we obtain moments related to the already discussed
cases, which leads to a correction of main results from Zaks (2001).

<id>
math/0202274v1
<category>
stat.TH
<abstract>
This paper is speculated to propose a class of shrinkage estimators for shape
parameter beta in failure censored samples from two-parameter Weibull
distribution when some 'apriori' or guessed interval containing the parameter
beta is available in addition to sample information and analyses their
properties. Some estimators are generated from the proposed class and compared
with the minimum mean squared error (MMSE) estimator. Numerical computations in
terms of percent relative efficiency and absolute relative bias indicate that
certain of these estimators substantially improve the MMSE estimator in some
guessed interval of the parameter space of beta, especially for censored
samples with small sizes. Subsequently, a modified class of shrinkage
estimators is proposed with its properties.

<id>
math/0203080v2
<category>
stat.TH
<abstract>
By the method of Poissonization we confirm some existing results concerning
consistent estimation of the structural distribution function in the situation
of a large number of rare events. Inconsistency of the so called natural
estimator is proved. The method of grouping in cells of equal size is
investigated and its consistency derived. A bound on the mean squared error is
derived.

<id>
math/0206006v2
<category>
stat.TH
<abstract>
We give a visually appealing counterexample to the proposition that unbiased
estimators are better than biased estimators.

<id>
math/0206142v1
<category>
stat.TH
<abstract>
We consider discrete time models for asset prices with a stationary
volatility process. We aim at estimating the multivariate density of this
process at a set of consecutive time instants. A Fourier type deconvolution
kernel density estimator based on the logarithm of the squared process is
proposed to estimate the volatility density. Expansions of the bias and bounds
on the variance are derived.

<id>
math/0207044v1
<category>
stat.TH
<abstract>
We construct an on-line estimator with equidistant design for tracking a
smooth function from Stone-Ibragimov-Khasminskii class. This estimator has the
optimal convergence rate of risk to zero in sample size. The procedure for
setting coefficients of the estimator is controlled by a single parameter and
has a simple numerical solution. The off-line version of this estimator allows
to eliminate a boundary layer. Simulation results are given.

<id>
math/0210425v1
<category>
stat.TH
<abstract>
We consider estimation of the structural distribution function of the cell
probabilities of a multinomial sample in situations where the number of cells
is large. We review the performance of the natural estimator, an estimator
based on grouping the cells and a kernel type estimator. Inconsistency of the
natural estimator and weak consistency of the other two estimators is derived
by Poissonization and other, new, technical devices.

<id>
math/0211079v3
<category>
stat.TH
<abstract>
We construct a density estimator and an estimator of the distribution
function in the uniform deconvolution model. The estimators are based on
inversion formulas and kernel estimators of the density of the observations and
its derivative. Asymptotic normality and the asymptotic biases are derived.

<id>
math/0212007v2
<category>
stat.TH
<abstract>
We derive asymptotic normality of kernel type deconvolution density
estimators. In particular we consider deconvolution problems where the known
component of the convolution has a symmetric lambda-stable distribution,
0<lambda<= 2. It turns out that the limit behavior changes if the exponent
parameter lambda passes the value one, the case of Cauchy deconvolution.

<id>
math/0212350v1
<category>
stat.TH
<abstract>
In this paper we will discuss a procedure to improve the usual estimator of a
linear functional of the unknown regression function in inverse nonparametric
regression models. In Klaassen, Lee, and Ruymgaart (2001) it has been proved
that this traditional estimator is not asymptotically efficient (in the sense
of the H\'{a}jek - Le Cam convolution theorem) except, possibly, when the error
distribution is normal. Since this estimator, however, is still root-n
consistent a procedure in Bickel, Klaassen, Ritov, and Wellner (1993) applies
to construct a modification which is asymptotically efficient. A self-contained
proof of the asymptotic efficiency is included.

<id>
math/0212395v1
<category>
stat.TH
<abstract>
Classical multiscale analysis based on wavelets has a number of successful
applications, e.g. in data compression, fast algorithms, and noise removal.
Wavelets, however, are adapted to point singularities, and many phenomena in
several variables exhibit intermediate-dimensional singularities, such as
edges, filaments, and sheets. This suggests that in higher dimensions, wavelets
ought to be replaced in certain applications by multiscale analysis adapted to
intermediate-dimensional singularities.
  My lecture described various initial attempts in this direction. In
particular, I discussed two approaches to geometric multiscale analysis
originally arising in the work of Harmonic Analysts Hart Smith and Peter Jones
(and others): (a) a directional wavelet transform based on parabolic dilations;
and (b) analysis via anistropic strips. Perhaps surprisingly, these tools have
potential applications in data compression, inverse problems, noise removal,
and signal detection; applied mathematicians, statisticians, and engineers are
eagerly pursuing these leads.

<id>
math/0212410v1
<category>
stat.TH
<abstract>
State space models have long played an important role in signal processing.
The Gaussian case can be treated algorithmically using the famous Kalman
filter. Similarly since the 1970s there has been extensive application of
Hidden Markov models in speech recognition with prediction being the most
important goal. The basic theoretical work here, in the case $X$ and $Y$ finite
(small) providing both algorithms and asymptotic analysis for inference is that
of Baum and colleagues. During the last 30-40 years these general models have
proved of great value in applications ranging from genomics to finance.
  Unless the $X,Y$ are jointly Gaussian or $X$ is finite and small the problem
of calculating the distributions discussed and the likelihood exactly are
numerically intractable and if $Y$ is not finite asymptotic analysis becomes
much more difficult. Some new developments have been the construction of
so-called ``particle filters'' (Monte Carlo type) methods for approximate
calculation of these distributions (see Doucet et al. [4]) for instance and
general asymptotic methods for analysis of statistical methods in HMM [2] and
other contributors.
  We will discuss these methods and results in the light of exponential mixing
properties of the conditional (posterior) distribution of $(X_1,X_2,...)$ given
$(Y_1,Y_2,...)$ already noted by Baum and Petrie and recent work of the contributors
Bickel, Ritov and Ryden, Del Moral and Jacod, Douc and Matias.

<id>
math/0212411v1
<category>
stat.TH
<abstract>
A classical limit theorem of stochastic process theory concerns the sample
cumulative distribution function (CDF) from independent random variables. If
the variables are uniformly distributed then these centered CDFs converge in a
suitable sense to the sample paths of a Brownian Bridge. The so-called
Hungarian construction of Komlos, Major and Tusnady provides a strong form of
this result. In this construction the CDFs and the Brownian Bridge sample paths
are coupled through an appropriate representation of each on the same
measurable space, and the convergence is uniform at a suitable rate.
  Within the last decade several asymptotic statistical-equivalence theorems
for nonparametric problems have been proven, beginning with Brown and Low
(1996) and Nussbaum (1996). The approach here to statistical-equivalence is
firmly rooted within the asymptotic statistical theory created by L. Le Cam but
in some respects goes beyond earlier results.
  This talk demonstrates the analogy between these results and those from the
coupling method for proving stochastic process limit theorems. These two
classes of theorems possess a strong inter-relationship, and technical methods
from each domain can profitably be employed in the other. Results in a recent
paper by Carter, Low, Zhang and myself will be described from this perspective.

<id>
math/0301363v1
<category>
stat.TH
<abstract>
The jackknife variance estimator and the the infinitesimal jackknife variance
estimator are shown to be asymptotically equivalent if the functional of
interest is a smooth function of the mean or a trimmed L-statistic with Hoelder
continuous weight function.

<id>
math/0302079v1
<category>
stat.TH
<abstract>
Log-linear models are a well-established method for describing statistical
dependencies among a set of n random variables. The observed frequencies of the
n-tuples are explained by a joint probability such that its logarithm is a sum
of functions, where each function depends on as few variables as possible. We
obtain for this class a new model selection criterion using nonasymptotic
concepts of statistical learning theory. We calculate the VC dimension for the
class of k-factor log-linear models. In this way we are not only able to select
the model with the appropriate complexity, but obtain also statements on the
reliability of the estimated probability distribution. Furthermore we show that
the selection of the best model among a set of models with the same complexity
can be written as a convex optimization problem.

<id>
math/0305234v1
<category>
stat.TH
<abstract>
Consider estimation of the regression parameter in the accelerated failure
time model, when data are obtained by cross sectional sampling. It is shown
that it is possible under regularity of the model to construct an efficient
estimator of the unknown Euclidean regression parameter if the distribution of
the covariate vector is known and also if it is unknown with vanishing mean.

<id>
math/0305273v2
<category>
stat.TH
<abstract>
This paper introduces a family of recursively defined estimators of the
parameters of a diffusion process. We use ideas of stochastic algorithms for
the construction of the estimators. Asymptotic consistency of these estimators
and asymptotic normality of an appropriate normalization are proved. The
results are applied to two examples from the financial literature; viz.,
Cox-Ingersoll-Ross' model and the constant elasticity of variance (CEV) process
illustrate the use of the technique proposed herein.

<id>
math/0306237v1
<category>
stat.TH
<abstract>
Let $X$ and $Y$ be two independent identically distributed random variables
with density $p(x)$ and $Z=\alpha X+\beta Y$ for some constants $\alpha>0$ and
$\beta>0$. We consider the problem of estimating $p(x)$ by means of the samples
from the distribution of $Z$. Non-parametric estimator based on the sync kernel
is constructed and asymptotic behaviour of the corresponding mean integrated
square error is investigated.

<id>
math/0309355v1
<category>
stat.TH
<abstract>
Let X be a n*p matrix and l_1 the largest eigenvalue of the covariance matrix
X^{*}*X. The "null case" where X_{i,j} are independent Normal(0,1) is of
particular interest for principal component analysis. For this model, when n, p
tend to infinity and n/p tends to gamma in (0,\infty), it was shown in
Johnstone (2001) that l_1, properly centered and scaled, converges to the
Tracy-Widom law. We show that with the same centering and scaling, the result
is true even when p/n or n/p tends to infinity. The derivation uses ideas and
techniques quite similar to the ones presented in Johnstone (2001). Following
Soshnikov (2002), we also show that the same is true for the joint distribution
of the k largest eigenvalues, where k is a fixed integer. Numerical experiments
illustrate the fact that the Tracy-Widom approximation is reasonable even when
one of the dimension is "small".

<id>
math/0310006v3
<category>
stat.TH
<abstract>
The marginalization paradox involves a disagreement between two Bayesians who
use two different procedures for calculating a posterior in the presence of an
improper prior. We show that the argument used to justify the procedure of one
of the Bayesians is inapplicable. There is therefore no reason to expect
agreement, no paradox, and no evidence that improper priors are inherently
inconsistent. We show further that the procedure in question can be interpreted
as the cancellation of infinities in the formal posterior. We suggest that the
implicit use of this formal procedure is the source of the observed
disagreement.

<id>
math/0312056v1
<category>
stat.TH
<abstract>
The subject of robust estimation in time series is widely discussed in
literature. One of the approaches is to use GM-estimation. This method
incorporates a broad class of nonparametric estimators which under suitable
conditions includes estimators robust to outliers in data. For the linear
models the sensitivity of GM-estimators to outliers have been studied in the
work by Martin and Yohai [5], and influence functionals for this estimator were
derived. In this paper we follow this direction and examine the asymptotical
properties of the class of M-estimators, which is narrower than the class of
GM-estimators, but gives more insight into asymptotical properties of such
estimators. This paper gives an asymptotic expansion of the residual weighted
empirical process, which allows to prove asymptotic normality of these
estimators in case of non-smooth objective functions. For simplicity MA(1)
model is considered, but it will be shown that even in this case mathematical
techniques used to derive these asymptotic properties appear to be rather
complicated.However, the approach used in this paper could be applied to
GM-estimators and to more realistic models.

<id>
math/0403373v1
<category>
stat.TH
<abstract>
Grade of membership (GoM) analysis was introduced in 1974 as a means of
analyzing multivariate categorical data. Since then, it has been successfully
applied to many problems. The primary goal of GoM analysis is to derive
properties of individuals based on results of multivariate measurements; such
properties are given in the form of the expectations of a hidden random
variable (state of an individual) conditional on the result of observations.
  In this article, we present a new perspective for the GoM model, based on
considering distribution laws of observed random variables as realizations of
another random variable. It happens that some moments of this new random
variable are directly estimable from observations. Our approach allows us to
establish a number of important relations between estimable moments and values
of interest, which, in turn, provides a basis for a new numerical procedure.

<id>
math/0405511v1
<category>
stat.TH
<abstract>
Vertex direction algorithms have been around for a few decades in the
experimental design and mixture models literature. We briefly review this type
of algorithm and describe a new member of the family: the support reduction
algorithm. The support reduction algorithm is applied to the problem of
computing nonparametric estimates in two inverse problems: convex density
estimation and the Gaussian deconvolution problem. Usually, VD algorithms solve
a finite dimensional (version of the) optimization problem of interest. We
introduce a method to solve the true infinite dimensional optimization problem.

<id>
math/0406424v1
<category>
stat.TH
<abstract>
We describe here a framework for a certain class of multiscale likelihood
factorizations wherein, in analogy to a wavelet decomposition of an L^2
function, a given likelihood function has an alternative representation as a
product of conditional densities reflecting information in both the data and
the parameter vector localized in position and scale. The framework is
developed as a set of sufficient conditions for the existence of such
factorizations, formulated in analogy to those underlying a standard
multiresolution analysis for wavelets, and hence can be viewed as a
multiresolution analysis for likelihoods. We then consider the use of these
factorizations in the task of nonparametric, complexity penalized likelihood
estimation. We study the risk properties of certain thresholding and
partitioning estimators, and demonstrate their adaptivity and near-optimality,
in a minimax sense over a broad range of function spaces, based on squared
Hellinger distance as a loss function. In particular, our results provide an
illustration of how properties of classical wavelet-based estimators can be
obtained in a single, unified framework that includes models for continuous,
count and categorical data types.

<id>
math/0406425v1
<category>
stat.TH
<abstract>
Starting from the observation of an R^n-Gaussian vector of mean f and
covariance matrix \sigma^2 I_n (I_n is the identity matrix), we propose a
method for building a Euclidean confidence ball around f, with prescribed
probability of coverage. For each n, we describe its nonasymptotic property and
show its optimality with respect to some criteria.

<id>
math/9808008v1
<category>
math.SG
<abstract>
In this note we investigate the structure of the space $\Jj$ of smooth almost
complex structures on $S^2\times S^2$ that are compatible with some symplectic
form. This space has a natural stratification that changes as the cohomology
class of the form changes and whose properties are very closely connected to
the topology of the group of symplectomorphisms of $S^2\times S^2$.
  By globalizing standard gluing constructions in the theory of stable maps, we
show that the strata of $\Jj$ are Fr\'echet manifolds of finite codimension,
and that the normal link of each stratum is a finite dimensional stratified
space. The topology of these links turns out to be surprisingly intricate, and
we work out certain cases. Our arguments apply also to other ruled surfaces,
though they give complete information only for bundles over $S^2$ and $T^2$.

<id>
math/9809192v1
<category>
math.SG
<abstract>
Consider a holomorphic torus action on a possibly non-compact K\"ahler
manifold. We show that the higher cohomology groups appearing in the geometric
quantization of the symplectic quotient are isomorphic to the invariant parts
of the corresponding cohomology groups of the original manifold. For
non-Abelian group actions on compact K\"ahler manifolds, this result was proved
recently by Teleman and by Braverman. Our approach is applying the holomorphic
instanton complex to the prequantum line bundles over the symplectic cuts. We
also settle a conjecture of Zhang and the present contributor on the exact sequence
of higher cohomology groups in the context of symplectic cutting.

<id>
math/9810065v1
<category>
math.SG
<abstract>
Examples of nonformal simply connected symplectic manifolds are constructed.

<id>
math/9810122v1
<category>
math.SG
<abstract>
We calculate the Riemann-Roch number of some of the pentagon spaces defined
in [Klyachko,Kapovich-Millson,HK1]. Using this, we show that while the regular
pentagon space is diffeomorphic to a toric variety, even symplectomorphic to
one under arbitrarily small perturbations of its symplectic structure, it does
not admit a symplectic circle action. In particular, within the cohomology
classes of symplectic structures, the subset admitting a circle action is not
closed.

<id>
math/9811167v3
<category>
math.SG
<abstract>
For any $N \geq 5$ nonformal simply connected symplectic manifolds of
dimension $2N$ are constructed. This disproves the formality conjecture for
simply connected symplectic manifolds which was introduced by Lupton and Oprea.

<id>
math/9901117v1
<category>
math.SG
<abstract>
A class of n-ary Poisson structures of constant rank is indicated. Then, one
proves that the ternary Poisson brackets are exactly those which are defined by
a decomposable 3-vector field. The key point is the proof of a lemma which
tells that an n-vector $(n\geq3)$ is decomposable iff all its contractions with
up to n-2 covectors are decomposable.

<id>
math/9903049v2
<category>
math.SG
<abstract>
In the usual setup, the grading on Floer homology is relative: it is unique
only up to adding a constant. "Graded Lagrangian submanifolds" are Lagrangian
submanifolds with a bit of extra structure, which fixes the ambiguity in the
grading. The idea is originally due to Kontsevich. This paper contains an
exposition of the theory. Several applications are given, amongst them:
  (1) topological restrictions on Lagrangian submanifolds of projective space,
(2) the existence of "symplectically knotted" Lagrangian spheres on a K3
surface, (3) a result about the symplectic monodromy of weighted homogeneous
hypersurface singularities.
  Revised version: minor modifications, journal reference added.

<id>
math/9904185v1
<category>
math.SG
<abstract>
In this paper we study the question of when does a closed, simply connected,
integral symplectic manifold (W,omega) have the stability property for its
spaces of based holomorphic spheres? This property states that in a stable
limit under certain gluing operators, the space of based holomorphic maps from
a sphere to X, becomes homotopy equivalent to the space of all continuous maps,
  lim_{->} Hol_{x_0}(P^1,X) = Omega^2 X.
  This limit will be viewed as a kind of stabilization of Hol_{x_0}(P^1,X). We
conjecture that this stability holds if and only if an evaluation map E:
lim_{->} Hol_{x_0}(P^1,X) -> X is a quasifibration. In this paper we will prove
that in the presence of this quasifibration condition, then the stability
property holds if and only if the Morse theoretic flow category (defined in
[4]) of the symplectic action functional on the Z-cover of the loop space, L~X,
defined by the symplectic form, has a classifying space that realizes the
homotopy type of L~X. We conjecture that in the presence of this quasifibration
condition, this Morse theoretic condition always holds. We will prove this in
the case of X a homogeneous space, thereby giving an alternate proof of the
stability theorem for holomorphic spheres for a projective homogeneous variety
originally due to Gravesen [7].

<id>
math/9905052v1
<category>
math.SG
<abstract>
An afinne-invariant view of generating functions of symplectic
transformations of an affine symplectic space is discussed. More generally, it
works for symmetric symplectic spaces. The note is completely elementary, but
it yields some nice pictures.

<id>
math/9906202v2
<category>
math.SG
<abstract>
We present a slight generalization of the notion of completely integrable
systems to get them being integrable by quadratures. We use this generalization
to integrate dynamical systems on double Lie groups.

<id>
math/9907200v1
<category>
math.SG
<abstract>
Integral symplectic 4-manifolds may be described in terms of Lefschetz
fibrations. In this note we give a formula for the signature of any Lefschetz
fibration in terms of the second cohomology of the moduli space of stable
curves. As a consequence we see that the sphere in moduli space defined by any
(not necessarily holomorphic) Lefschetz fibration has positive "symplectic
volume"; it evaluates positively with the Kahler class. Some other applications
of the signature formula and some more general results for genus two fibrations
are discussed.

<id>
math/9909004v1
<category>
math.SG
<abstract>
Let G be a finite dimensional simple complex group equipped with the standard
Poisson Lie group structure. We show that all G-homogeneous (holomorphic)
Poisson structures on $G/H$, where $H \subset G$ is a Cartan subgroup, come
from solutions to the Classical Dynamical Yang-Baxter equations which are
classified by Etingof and Varchenko. A similar result holds for the maximal
compact subgroup K, and we get a family of K-homogeneous Poisson structures on
$K/T$, where $T = K \cap H$ is a maximal torus of K. This family exhausts all
K-homogeneous Poisson structures on $K/T$ up to isomorphisms. We study some
Poisson geometrical properties of members of this family such as their
symplectic leaves, their modular classes, and the moment maps for the T-action.

<id>
math/9909064v1
<category>
math.SG
<abstract>
Pulling back sets of functions in involution by Poisson mappings and adding
Casimir functions during the process allows to construct completely integrable
systems. Some examples are investigated in detail.

<id>
math/9909195v1
<category>
math.SG
<abstract>
The contributions of Sophya Kowalewski to the integrability theory of the
equations for the heavy top extend to a larger class of Hamiltonian systems on
Lie groups; this paper explains these extensions, and along the way reveals
further geometric significance of her work in the theory of elliptic curves.
Specifically, in this paper we shall be concerned with the solutions of the
following differential system in six variables h_1,h_2,h_3,H_1,H_2,H_3
  dH_1/dt = H_2 H_3 (1/c_3 - 1/c_2) + h_2 a_3 - h_3 a_2,
  dH_2/dt = H_1 H_3 (1/c_1 - 1/c_3) + h_3 a_1 - h_1 a_3,
  dH_3/dt = H_1 H_2 (1/c_2 - 1/c_1) + h_1 a_2 - h_2 a_1,
  dh_1/dt = h_2 H_3/c_3 - h_3 H_2/c_2 + k (H_2 a_3 - H_3 a_2),
  dh_2/dt = h_3 H_1/c_1 - h_1 H_3/c_3 + k (H_3 a_1 - H_1 a_3),
  dh_3/dt = h_1 H_2/c_2 - h_2 H_1/c_1 + k (H_1 a_2 - H_2 a_1),
  in which a_1,a_2,a_3,c_1,c_2,c_3 and k are constants.

<id>
math/9911100v1
<category>
math.SG
<abstract>
We define a symplectic structure on the space of non parametrized loops in
$G_2$ manifold. We also develop some basics of intersection theory of
Lagrangian submanifolds.

<id>
math/9911107v2
<category>
math.SG
<abstract>
I study flux groups of compact symplectic manifolds. Under some topological
assumptions, I give a new estimate of the rank of flux groups and give a method
of construcion of compact symplectic aspherical manifolds.

<id>
math/0001001v1
<category>
math.SG
<abstract>
This paper gives methods for understanding invariants of symplectic
quotients. The symplectic quotients considered here are compact symplectic
manifolds (or more generally orbifolds), which arise as the symplectic
quotients of a symplectic manifold by a compact torus. (A companion paper
examines symplectic quotients by a nonabelian group, showing how to reduce to
the maximal torus.)
  Let X be a symplectic manifold, with a Hamiltonian action of a compact torus
T. The main topological result of this paper describes an explicit cobordism
that exists between a symplectic quotient of X by T, and a collection of
iterated projective bundles over components of the set of T-fixed-points.
  The characteristic classes of these bundles can be determined explicitly, and
another result uses this to give formulae for integrals of cohomology classes
over the symplectic quotient, in terms of data localized at the T-fixed points
of X.

<id>
math/0001002v1
<category>
math.SG
<abstract>
This paper examines the relationship between the symplectic quotient X//G of
a Hamiltonian G-manifold X, and the associated symplectic quotient X//T, where
T is a maximal torus, in the case in which X//G is a compact manifold or
orbifold.
  The three main results are: a formula expressing the rational cohomology ring
of X//G in terms of the rational cohomology ring of X//T; an `integration'
formula, which expresses cohomology pairings on X//G in terms of cohomology
pairings on X//T; and an index formula, which expresses the indices of elliptic
operators on X//G in terms of indices on X//T.
  (The results of this paper are complemented by the results in a companion
paper, in which different techniques are used to derive formulae for cohomology
pairings on symplectic quotients X//T, where T is a torus, in terms of the
T-fixed points of X. That paper also gives some applications of the formulae
proved here.)

<id>
math/0002053v1
<category>
math.SG
<abstract>
In the present paper we study the variation of the dimensions $h_k$ of spaces
of symplectically harmonic cohomology classes (in the sense of Brylinski) on
closed symplectic manifolds. We give a description of such variation for all
6-dimensional nilmanifolds equipped with symplectic forms. In particular, it
turns out that certain 6-dimensional nilmanifolds possess families of
homogeneous symplectic forms $\omega_t$ for which numbers $h_k(M,\omega_t)$
vary with respect to t. This gives an affirmative answer to a question raised
by Boris Khesin and Dusa McDuff. Our result is in contrast with the case of
4-dimensional nilmanifolds which do not admit such variations by a remark of
Dong Yan.

<id>
math/0002071v1
<category>
math.SG
<abstract>
The paper deals with relations between the Hard Lefschetz property,
(non)vanishing of Massey products and the evenness of odd-degree Betti numbers
of closed symplectic manifolds. It is known that closed symplectic manifolds
can violate all these properties (in contrast with the case of Kaehler
manifolds). However, the relations between such homotopy properties seem to be
not analyzed. This analysis may shed a new light on topology of symplectic
manifolds. In the paper, we summarize our knowledge in tables (different in the
simply-connected and in symplectically aspherical cases). Also, we discuss the
variation of symplectically harmonic Betti numbers on some 6-dimensional
manifolds.

<id>
math/0002222v1
<category>
math.SG
<abstract>
We quantize the bending deformations of n-gon linkages by linearizing the
bending fields at a degenerate n-gon to get a representation of the Malcev Lie
algebra of the pure braid group. This linearization yields a flat connection on
the space of n distinct points on the complex line. We show that the monodromy
is (essentially) the Gassner representation.

<id>
math/0003079v2
<category>
math.SG
<abstract>
The main theorem of this paper asserts that the inclusion of the space of
projective Lagrangian planes into the space of Lagrangian submanifolds of
complex projective space induces an injective homomorphism of fundamental
groups. We introduce three invariants of exact loops of Lagrangian submanifolds
that are modelled on invariants introduced by Polterovich for loops of
Hamiltonian symplectomorphisms. One of these is the minimal Hofer length in a
given Hamiltonian isotopy class. We determine the exact values of these
invariants for loops of projective Lagrangian planes. The proof uses the Gromov
invariants of an associated symplectic fibration over the 2-disc with a
Lagrangian subbundle over the boundary.

<id>
math/0004061v1
<category>
math.SG
<abstract>
We extend the famous convexity theorem of Atiyah, Guillemin and Sternberg to
the case of non-Hamiltonian actions. We show that the image of a generalized
momentum map is a bounded polytope times a vector space. We prove that this
picture is stable for small perturbations of the symplectic form. We also
observe that an n-dimensional torus symplectic action on a 2n-dimensional
symplectic manifold, with fixed point, is Hamiltonian. We finally prove that an
extension of Kirwan's convexity theorem (for compact group actions) is also
true.

<id>
math/0008014v1
<category>
math.SG
<abstract>
We prove (a weak version of) Arnold's Chord Conjecture using Gromov's
``classical'' idea in to produce holomorphic disks with boundary on a
Lagrangian submanifold.

<id>
math/0008097v1
<category>
math.SG
<abstract>
We discuss symplectic manifolds where, locally, the structure is that
encountered in Lagrangian dynamics. Exemples and characteristic properties are
given. Then, we refer to the computation of the Maslov classes of a Lagrangian
submanifold. Finally, we indicate the generalization of this type of structures
to Poisson manifolds.

<id>
math/0008162v3
<category>
math.SG
<abstract>
In the framework of the connection theory, a contravariant analog of the
Sternberg coupling procedure is developed for studying a natural class of
Poisson structures on fiber bundles, called coupling tensors. We show that
every Poisson structure near a closed symplectic leaf can be realized as a
coupling tensor. Our main result is a geometric criterion for the neighborhood
equivalence between Poisson structures over the same leaf. This criterion gives
a Poisson analog of the relative Darboux theorem due to Weinstein. Within the
category of the algebroids, coupling tensors are introduced on the dual of the
isotropy of a transitive Lie algebroid over a symplectic base. As a basic
application of these results, we show that there is a well defined notion of a
``linearized'' Poisson structure over a symplectic leaf which gives rise to a
natural model for the linearization problem.

<id>
math/0008178v1
<category>
math.SG
<abstract>
We show that if a Lie group acts properly on a co-oriented contact manifold
preserving the contact structure, then the contact quotient is topologically a
stratified space (in the sense that a neighborhood of a point in the quotient
is a product of a disk with a cone on a compact stratified space). As a
corollary, we obtain that symplectic quotients for proper Hamiltonian actions
are topologically stratified spaces in this strong sense thereby extending and
simplifying previous work.

<id>
math/0009206v3
<category>
math.SG
<abstract>
On the space ${\cal L}$, of loops in the group of Hamiltonian
symplectomorphisms of a symplectic quantizable manifold, we define a closed
${\bf Z}$-valued 1-form $\Omega$. If $\Omega$ vanishes, the prequantization map
can be extended to a group representation. On ${\cal L}$ one can define an
action integral as an ${\bf R}/{\bf Z}$-valued function, and the cohomology
class $[\Omega]$ is the obstruction to the lifting of that action integral to
an ${\bf R}$-valued function. The form $\Omega$ also defines a natural grading
on $\pi_1({\cal L})$.

<id>
math/0009220v2
<category>
math.SG
<abstract>
In this paper we discuss the topology of the symplectomorphism group of a
product of two 2-dimensional spheres when the ratio of their areas lies in the
interval (1,2]. More precisely we compute the homotopy type of this
symplectomorphism group and we also show that the group contains two finite
dimensional Lie groups generating the homotopy. A key step in this work is to
calculate the mod 2 homology of the group of symplectomorphisms. Although this
homology has a finite number of generators with respect to the Pontryagin
product, it is unexpected large containing in particular a free noncommutative
ring with 3 generators.

<id>
math/0010274v2
<category>
math.SG
<abstract>
This paper studies groups of symplectomorphisms of ruled surfaces for
symplectic forms with varying cohomology class. This class is characterized by
the ratio R of the size of the base to that of the fiber. By considering
appropriate spaces of almost complex structures, we investigate how the
topological type of these groups changes as R increases. If the base is a
sphere, this changes precisely when R passes an integer, and for general bases
it stabilizes as R goes to infinity. Our results extend and make more precise
some of the conclusions of Abreu--McDuff concerning the rational homotopy type
of these groups for rational ruled surfaces.

<id>
astro-ph/9204001v1
<category>
astro-ph
<abstract>
It is proposed that gamma-ray bursts are created in the mergers of double
neutron star binaries and black hole neutron star binaries at cosmological
distances. Bursts with complex profiles and relatively long durations are the
result of magnetic flares generated by the Parker instability in a post-merger
differentially-rotating disk. Some bursts may also be produced through
neutrino-antineutrino annihilation into electrons and positrons. In both cases,
an optically thick fireball of size $\sles\ 100$ km is initially created, which
expands ultrarelativistically to large radii before radiating. Several previous
objections to the cosmological merger model are eliminated. It is predicted
that $\gamma$-ray bursts will be accompanied by a burst of gravitational
radiation from the spiraling-in binary which could be detected by LIGO.

<id>
astro-ph/9204002v1
<category>
astro-ph
<abstract>
The four observables associated with gravitational lensing of distant quasars
by intervening galaxies: image splittings, relative amplifications, time
delays, and optical depths, provide separate measures of the strength of the
gravitational constant $G$ at cosmological distances. These allow one, in
principle, to factor out unknown lensing parameters to directly to probe the
variation of $G$ over cosmological time. We estimate constraints on $\dot{G}$
which may be derivable by this method both now and in the future. The limits
one may obtain can compete or exceed other direct limits on $\dot{G}$ today,
but unfortunately extracting this information, is not independent of the effort
to fix other cosmological parameters such as $H_0$ and $\Omega_0$ from lensing
observations.

<id>
astro-ph/9204003v2
<category>
astro-ph
<abstract>
The BATSE experiment on GRO has demonstrated the isotropic arrival directions
and flat $\log N$ {\it vs.} $\log S$ of cosmic gamma-ray bursts. These data are
best explained if the burst sources are distributed throughout an extended
spherical Galactic halo, as previously suggested by Jennings. The halo's radius
is at least 40 Kpc, and probably is more than 100 Kpc. I consider possible
origins of this halo, including primordial formation and neutron stars
recoiling from their birthplaces in the Galactic disc. A simple geometrical
model leads to a predicted relation between the dipole and quadrupole
anisotropy. I suggest that neutron stars born with low recoil become
millisecond pulsars, while those born with high recoil become the sources of
gamma-ray bursts; these populations are nearly disjoint. Quiescent counterparts
of gamma-ray bursts are predicted to be undetectably faint.

<id>
astro-ph/9204004v1
<category>
astro-ph
<abstract>
We use the Expanding Photosphere Method to determine distances to 10 type II
supernovae. The effects of asymmetries, extinction, and flux dilution are
explored. Using empirical evidence and time-independent, spherical models which
treat H and He in non-LTE, we show that blackbody corrections caused by flux
dilution are small for type II supernovae in the infrared, and in the optical
when their color temperatures are less than 6000~K. The extinction to a type
II-P supernova can be estimated from its light curve: the uncertainty
introduced into a distance measurement due to extinction is usually less than
10\%. Correcting for extinction and flux dilution we derive distances to 10
supernovae: SN 1968L, SN 1969L, SN 1970G, SN 1973R, SN 1979C, SN 1980K, SN
1987A, SN 1988A, SN 1990E, and SN 1990ae. The distance measurements span a wide
range, 50 kpc to 120 Mpc, which is unique among the methods for establishing
the extragalactic distance scale. The distances measured to SN 1970G in M101
and SN 1987A in the LMC are in good agreement with distances determined from
Cepheid variable stars. Our distance to the Virgo Cluster, 22 +- 3 Mpc, is
larger than recent distances estimates made using surface brightness
fluctuations, planetary nebula luminosity functions, and the Tully-Fisher
method. Using the distances determined from these type II supernovae we derive
a value of $H_0 = 60 \pm 10$ km sec$^{-1}$Mpc$^{-1}$. This value is subject to
errors caused by local deviations in the Hubble flow, but will soon be improved
by applying the Expanding Photosphere Method to several distant type II
supernovae.

<id>
astro-ph/9204005v1
<category>
astro-ph
<abstract>
We have calculated gamma-ray radiative transport in regions of high energy
density, such as gamma-ray burst source regions, using a discrete ordinate,
discrete energy group method. The calculations include two-photon pair
production and annihilation, as well as three-photon pair annihilation. The
radiation field itself acts as an absorbing medium, and the optical depth
depends on its intensity, so the problem is intrinsically nonlinear. Spherical
divergence produces effective collimation of the flux. At high optical depth
the high energy ($E > 1$ MeV) portion of the emergent spectrum assumes a nearly
universal form. An approximate limit is derived for the high energy flux from a
gamma-ray burst source region of given size, and the implications of this limit
for the distance to the March 5, 1979 event are briefly discussed. We discuss
more generally the problem of very luminous bursts, and implications of
Galactic halo distances for flare models.

<id>
astro-ph/9204006v1
<category>
astro-ph
<abstract>
Dust is observed to form in nova ejecta. The grain temperature is determined
by the diluted nova radiation field rather than the gas kinetic temperature,
making classical nucleation theory inapplicable. We used kinetic equations to
calculate the growth of carbon nuclei in these ejecta. For expected values of
the parameters too many clusters grew, despite the small sticking probability
of atoms to small clusters, and the clusters only reached radii of about
100\AA\ when the carbon vapor was depleted. We then included the effects of
cluster photodissociation by ultraviolet radiation from the nova. This
suppresses nucleation, but too well, and no grains form at all. Finally we
suggest that a few growing carbon nuclei may be protected from
photodissociation by a sacrificial surface layer of hydrogen.

<id>
astro-ph/9205001v1
<category>
astro-ph
<abstract>
We apply Perlick's (1990a) rigorous formulation of the Fermat principle in
arbitrary spacetimes to prove the correctness of the description of
gravitational lensing by gravitational waves, given in the literature using the
scalar and vector formalisms. We obtain an expression for the time delay due to
such nonstationary lenses; the advantage over previous papers is that Perlick's
formulation of the Fermat principle is very rigorous and more suitable for
practical calculations in some cases. It is also shown that ordinary moving
gravitational lenses must be considered as a stationary case.

<id>
astro-ph/9205002v1
<category>
astro-ph
<abstract>
The effect of the magnetic skew on the Parker instability is investigated by
means of the linear stability analysis for a gravitationally stratified gas
layer permeated by a horizontal magnetic field. When the magnetic field is
skewed (i.e., the field line direction is a function of the height), the
wavelength of the most unstable mode is $ \lambda \; \sim \; 10 H $ where $ H $
is the pressure scale height. The growth rate of the short wavelength modes is
greatly reduced when the gradient in the magnetic field direction exceeds 0.5
radian per scale height. Our results indicate that the Parker instability in a
skewed magnetic field preferentially forms large scale structures like giant
molecular clouds.

<id>
astro-ph/9205003v1
<category>
astro-ph
<abstract>
I present a model for acceleration of protons by the second-order Fermi
process acting on randomly scrambled magnetic flux arches above an accretion
disc. The accelerated protons collide with thermal protons in the disc,
producing degraded energetic protons, charged and neutral pions, and neutrons.
The pions produce gamma-rays by spontaneous decay of $\pi^0$ and by
bremsstrahlung and Compton processes following the decay of $\pi^\pm$ to
$e^\pm$.

<id>
astro-ph/9205004v1
<category>
astro-ph
<abstract>
We compare the spatial distributions of galaxy clusters in the northern and
southern galactic hemispheres, and the Abell and ACO clusters distributions. We
perform a statistical (correlation and cluster) analysis of a sample of Abell
and ACO galaxy clusters in the southern galactic hemisphere. We compare these
results with a symmetric sample at northern galactic latitude taken from
Postman et al. (1992). For the northern sample, we substantially confirm the
results of Postman et al. We find that the two-point spatial correlation
function of northern and southern clusters is comparable, with mean correlation
length 19.6 Mpc and slope -1.8 positive up to about 45 Mpc. Percolation
properties are remarkably similar in the northern and southern cluster samples.
We give also a catalog of superclusters. In the south galactic hemisphere the
main feature is a very rich, extended supercluster in the Horologium region at
a redshift 0.06, near to a large void.

<id>
astro-ph/9205005v1
<category>
astro-ph
<abstract>
A very weakly coupled scalar field with mass $m$ and initial vacuum
expectation value $V$ will provide enough mass to close the universe provided
$V\simeq (3\times 10^8\gev)(100\gev/m)^{1/4}$. We discuss possible models in
which such a field could arise.

<id>
astro-ph/9206002v1
<category>
astro-ph
<abstract>
The ability to now make measurements of Be and B as well as put constraints
on \lisix\ abundances in metal-poor stars has led to a detailed reexamination
of Big Bang Nucleosynthesis in the $A\groughly6$ regime. The nuclear reaction
network has been significantly expanded with many new rates added. It is
demonstrated that although a number of $A>7$ reaction rates are poorly
determined, even with extreme values chosen, the standard homogeneous model is
unable to produce significant yields (Be/H and B/H $<10^{-17}$ when $A\le7$
abundances fit) above $A=7$ and the \liseven/\lisix\ ratio always exceeds 500.
We also preliminarily explore inhomogeneous models, such as those inspired by a
first order quark-hadron phase transition, where regions with high
neutron/proton ratios can allow some leakage up to $A>7$. However models that
fit the $A\le7$ abundances still seem to have difficulty in obtaining
significant $A>7$ yields.

<id>
astro-ph/9206004v1
<category>
astro-ph
<abstract>
In this paper we investigate the internal dynamics of the LMC cluster NGC
1978 through the use of Photometric (CCD images) and kinematic (stellar radial
velocities) data. We apply a variety of dynamical models to this data,
including multi-mass King-Michie models and rotating and non-rotating oblate
spheroid models. We discuss the cluster mass-to-light ratio and place
constraints on the cluster mass function.

<id>
astro-ph/9208002v1
<category>
astro-ph
<abstract>
In the standard inflationary scenario with inflaton potential
$V(\Phi)=M^4-{1\over4}\lambda\Phi^4$, the resulting density perturbations
$\delta\rho/\rho$ are proportional to $\lambda^{1/2}$. Upper bounds on
$\delta\rho/\rho$ require $\lambda < 10^{-13}$. Ratra has shown that an
alternative treatment of reheating results in $\delta\rho/\rho \propto
\lambda^{-1}$, so that an upper bound on $\delta\rho/\rho$ does not put an
obvious upper bound on $\lambda$. We verify that $\delta\rho/\rho \propto
\lambda^{-1}$ is indeed a possibility, but show that $\lambda < 10^{-13}$ is
still required.

<id>
astro-ph/9209001v2
<category>
astro-ph
<abstract>
Three colour photometry on CCD frames in the Special Area SpA23 provides a
deep probe of the galactic disc in a low absorption window towards the
anticenter. Magnitudes to better than 10% at V = 25 and B-V colour down to V =
23 have been obtained. These new data, used in combination with lower magnitude
photographic data in a wider field, give a strong evidence that the galactic
density scale length is rather short (2.5 kpc) and drops abruptly beyond 6 kpc.

<id>
astro-ph/9209002v1
<category>
astro-ph
<abstract>
Reanalysis of Einstein IPC data and new observations from the GINGA
  LAC indicate the presence of extended X-ray emission (10-50 kpc) around the
starburst galaxy M82. Here we model this emission by calculating numerical
hydrodynamic simulations of the starburst event to much later times and larger
scales than previously considered. For our models, we adopt a supernova rate of
0.1 ${\rm yr}^{-1}$, and an extended low-density static halo that is bound to
the galaxy. There are three stages to the evolution of the wind-blown bubble
and the propagation of the shock front: the bubble expands in an almost uniform
density disk gas, with a deceleration of the shock front ($t \alt $ 3.6 Myr);
breakout from the disk and the upward acceleration of the shock front (3.6
  Myr $\alt t \alt$ 18 Myr); propagation into the halo, leading to a more
spherical system and shock deceleration (18 Myr $\alt t$). For a halo density
of $10^{-3} {\rm cm}^{-3}$, the outflow reaches a distance of 40-50 kpc from
the center of the starburst galaxy in 50 Myr. We calculate the time evolution
of the X-ray luminosity and find that the extended starburst emits $3\times
10^{39}\lcgs$ to $10^{40}\lcgs$ in the
  GINGA LAC band and $\sim 10^{41}\lcgs$ in the Einstein or ROSAT
  HRI band. The degree of the ionization equilibrium in the outflow and its
effect on the iron K$\alpha$ line emission are discussed.

<id>
astro-ph/9209004v1
<category>
astro-ph
<abstract>
The tail problem for the propagation of a scalar field is considered in a
cosmological background, taking a Robertson-Walker spacetime as a specific
example. The explicit radial dependence of the general solution of the
Klein-Gordon equation with nonminimal coupling is derived, and the
inapplicability of the standard calculation of the reflection and transmission
coefficients to the study of scattering of waves by the cosmological curvature
is discussed.

<id>
astro-ph/9210001v1
<category>
astro-ph
<abstract>
As part of a stellar population sampling program, a series of photometric
probes at various field sizes and depths have been obtained in a low extinction
window in the galactic anticentre direction. Such data set strong constraints
on the radial structure of the disc. At the forefront of this "drilling"
program, very deep CCD frames probe the most external parts of the disc. Over
the whole effective magnitude range (18 to 25), all contributions in the
statistics which should be expected from old disc stars beyond 6 kpc vanish,
although such stars dominate by far at distances less than 5 kpc. This is the
signature of a sharp cut-off in the star density: the edge of the galactic disc
between 5.5 and 6 kpc. As a consequence, the galactic radius does not exceed 14
kpc (assuming $R/-/(//sun/)$=8.5). Colours of elliptical galaxies measured in
the field rule out the risk of being misled by undetected extinction.

<id>
astro-ph/9210002v1
<category>
astro-ph
<abstract>
The average flux decrement shortward the Ly$_{\alpha}$ emission, due to the
well-known ``forest'' of absorptions, has been measured in the spectra of 8
quasars. Quasi-simultaneous optical and IUE observations of the two low
redshift quasars PKS 0637--75 (z=0.654) and MC 1104+16 (z=0.632) have been
carried out, obtaining relatively high S/N, spectrophotometrically calibrated
data on their energy distribution from the rest frame H$_{\beta}$ to the Lyman
continuum. Six more quasars in the redshift range 2.5-3.4 have been observed in
the optical domain. For all the quasars the ``intrinsic'' continuum slope and
normalization have been estimated longward the Ly$_{\alpha}$ emission and
extrapolated towards the Lyman continuum to measure the average depressions,
which have been compared with the model statistics of the Ly$_{\alpha}$ clouds.
When all the known classes of absorbers are taken into account with plausible
values for their equivalent width distribution and evolution, a good agreement
is obtained with the observations. The results for the observed continuum
decrement at $z \sim 0.65$ are identical to those predicted by the evolution
with redshift of the number of Ly$_{\alpha}$ forest systems including the HST
data and within $2\sigma$ of the predicted value using the ``standard''
Ly$_{\alpha}$ evolution (as determined only at high z).

<id>
astro-ph/9210003v1
<category>
astro-ph
<abstract>
The suggestion has been made that the energy spectrum from point sources such
as AGN (Active Galactic Nuclei) and GBHC (Galactic Black Hole Candidates) is
universal, irrespective of the nature of the emitted particles. A comparison of
the energy spectrum for cosmic rays at the source and $\gamma$-rays from
quasars obtained recently by CGRO (Compton Gamma Ray Observatory) indicates
that the prediction is in agreement with the data in the average sense. This
suggests that neutrinos from point sources should have a spectral index
identical to that of $\gamma$-rays for an individual point source. This
prediction is also consistent with the recent observation of neutrinos by
Kamiokande and IMB in which the ratio of $\nu_\mu/\nu_e$ is close to 1, instead
of 2 as expected from atmospheric neutrinos. For a further test of the model,
analysis of the time variation of $\gamma$-ray spectra from quasars is
suggested.

<id>
astro-ph/9210004v1
<category>
astro-ph
<abstract>
In this paper we have examined the age and internal dynamics of the young
binary LMC cluster NGC 1850 using BV CCD images and echelle spectra of 52
supergiants. Isochrone fits to a BV color-magnitude diagram revealed that the
primary cluster has an age of $\tau = 90 \pm 30$ Myr while the secondary member
has $\tau = 6 \pm 5$ Myr. BV surface brightness profiles were constructed out
to R $>$ 40 pc, and single-component King-Michie (KM) models were applied. The
total cluster luminosity varied from L$_B$ = 2.60 - 2.65 $\times 10^6$
L$_B$\sol\ and L$_V$ = 1.25 - 1.35 $\times 10^6$ as the anisotropy radius
varied from infinity to three times the scale radius with the isotropic models
providing the best agreement with the data. Of the 52 stars with echelle
spectra, a subset of 36 were used to study the cluster dynamics. The KM radial
velocity distributions were fitted to these velocities yielding total cluster
masses of 5.4 - 5.9 $\pm 2.4 \times 10^4$ M\sol\ corresponding to M/L$_B$ =
0.02 $\pm 0.01$ M\sol/L$_B$\sol\ or M/L$_V$ = 0.05 $\pm 0.02$ M\sol/L$_V$\sol.
A rotational signal in the radial velocities has been detected at the 93\%
confidence level implying a rotation axis at a position angle of 100\deg. A
variety of rotating models were fit to the velocity data assuming cluster
ellipticities of $\epsilon = 0.1 - 0.3$. These models provided slightly better
agreement with the radial velocity data than the KM models and had masses that
were systematically lower by a few percent. The preferred value for the slope
of a power-law IMF is a relatively shallow, $x = 0.29 \pmm{+0.3}{-0.8}$
assuming the B-band M/L or $x = 0.71 \pmm{+0.2}{-0.4}$ for the V-band.

<id>
astro-ph/9210005v1
<category>
astro-ph
<abstract>
We present a high S/N spectrum for the quasar PG 1138+222. We detect a very
broad HeII$\lambda$4686 emission line component with twice the FWHM of the
conventional broad line region (BLR) as evidenced by the Balmer lines. The
profile shape and centroid redshift also distinguish this HeII component from
the BLR features. The large ratio of HeII\l 4686 to any analogous H$\beta$
emission component is an indicator that it arises in a very high density region
($n_e \simgt 10^{11}-10^{13}$ \cm3). This Helium component is probably emitted
in a Very Broad Line Region (VBLR), where the radiation field is so strong that
the Str\"omgren depth becomes similar to the geometrical thickness of the
emitting clouds. The gas could therefore be optically thin to the Lyman
continuum.

<id>
astro-ph/9210006v1
<category>
astro-ph
<abstract>
The galactic magnetic field is commonly supposed to be due to a dynamo acting
on some large scale seed field. A major difficulty with this idea is that
estimates of reasonable seed field strengths tend to be quite low, on the order
of $\sim10^{-20}$ gauss. Here we examine the contribution due to the flux
entrained in winds from protostars formed in the first dynamo e-folding time of
a galaxy's existence. Using a minimal estimate of a protostellar magnetic field
we find that if each protostar ejects a single current ring, sufficient to
maintain flux freezing in the wind, than the large scale average dipole field
from all such current rings will be at least 5 orders of magnitude larger than
previous seed field estimates. Allowing for a reasonable amount of magnetic
activity in protostars during an extended period of mass loss increases this to
a dipole seed field of $\sim10^{-12}$ gauss. For the purposes of producing a
seed field it is irrelevant whether or not this initial injection of flux takes
place in a newly formed galactic disk, or in star forming proto-galactic
clouds. The compression of this dipole field into a thin disk will lead to a
large scale $B_r\sim 10^{-10.5}$ gauss. Initially, field strengths on smaller
scales will be larger, but nowhere near current levels.

<id>
astro-ph/9211001v1
<category>
astro-ph
<abstract>
Self-consistent models of gamma-ray burst source regions at 100 Kpc distance
are possible if the radiating plasma is confined to very thin sheets, and I
estimate parameters. Energy sources might be elastic (by starquakes) or
magnetic (by reconnection), but mechanisms remain obscure. I discuss a very
speculative model involving collisions between comets in a hypothetical inner
Oort cloud.

<id>
astro-ph/9211002v1
<category>
astro-ph
<abstract>
The presence of brown dwarfs in the dark galactic halo could be detected
through their gravitational lensing effect and experiments under way monitor
about one million stars to observe a few lensing events per year. We show that
if the photon flux from a galaxy is measured with a good precision, it is not
necessary to resolve the stars and besides more events could be observed.

<id>
astro-ph/9211003v1
<category>
astro-ph
<abstract>
We present a multiphase model for the evolution of elliptical galaxies.
Diffuse gas, molecular clouds, stars and remnants are taken into account.
Cloud--cloud collisions and stimulated processes are the main causes of star
formation. The occurrence of winds driven by Supernovae is considered, and the
evolution of the system is computed also after the first wind, allowing for
further star formation from the restored gas. The evolution of the abundances
of 15 elements or isotopes is followed with detailed stellar nucleosynthesis.
Stellar lifetimes are taken into account and a new IMF has been adopted. The
gas removal due to the Supernovae explosions depends on the galactic mass and
the presence of dark matter; the subsequent wind episodes are crucial to the
intergalactic gas enrichment. Good agreement is obtained for current SNs rates,
Star Formation Rate and gas masses when compared to the available data.

<id>
astro-ph/9211005v2
<category>
astro-ph
<abstract>
Besides having some very interesting perturbatively unstable orbits, it seems
that for a Schwarzschild black hole, below $r=3M$, the force always increases
inward with increasing angular momentum. Here this previously known result is
derived with greater simplicity, and a similar analysis is performed for black
holes with angular momentum and charge.

<id>
astro-ph/9211006v1
<category>
astro-ph
<abstract>
Two families of hot stellar systems, named {\it `ordinary'\/} and {\it
'bright'\/}, are identified in the ($\log R_e, \mu_e$) plane built with a
luminosity--limited sample of ellipticals and bulges of S0s and spirals of the
Virgo and Fornax clusters. This finding, based on {\it ad hoc\/} new
observations, is confirmed by a much larger set of literature data for
$\sim1500$ galaxies. The {\it `ordinary'\/} family is biparametric: $L_T\propto
I_e\,R_e^2$; its members are fainter that $M_B\simeq-19.3$ and smaller than
$\re\simeq3$ kpc (whatever $M_B$ is). The {\it 'bright'\/} family is
uniparametric ($\mu_e$ depends on $R_e$ alone) and hosts brightest cluster
members and QSO parent galaxies. We show that the segregation in the ($\log
R_e, \mu_e$) plane has an important counterpart in the behavior of various
physical parameters, which is markedly different for galaxies smaller ({\it
`ordinary'\/} family) and larger ({\it 'bright'\/} family) than $R_e=3$ kpc.

<id>
astro-ph/9211009v1
<category>
astro-ph
<abstract>
If gamma ray burst sources are in the galactic halo they inevitably involve a
creation of an opaque pair plasma fireball, just like in cosmological sources.
We find that the typical physical conditions in a galactic halo fireball are:
optical depth $\ap 10^8$, thermal energy $\ap 100 KeV$, maximal relativstic
expansion $\G \ap 300 $ and a maximal baryonic load of $\ap 10^{-15} M_\odot$.
This does not rule out galctic halo models but it poses an addtional severe
constraint on all such sources. A comparison of these conditions with the
physical conditions at cosmological fireballs reveal that galactic halo
fireballs are less favorable than cosmological ones as sources of gamma ray
bursts.

<id>
astro-ph/9211010v1
<category>
astro-ph
<abstract>
$\gamma$-ray bursts have baffled theorists ever since their accidental
discovery at the sixties. We suggest that these bursts originate in merger of
neutron star binaries, taking place at cosmological distances. These mergers
release $\approx 10^{54}ergs$, in what are possibly the strongest explosions in
the Universe. If even a small fraction of this energy is channeled to an
electromagnetic signal it will be detected as a grbs. We examine the virtues
and limitations of this model and compare it with the recent Compton \g-ray
observatory results.

<id>
cond-mat/9602070v2
<category>
cond-mat.dis-nn
<abstract>
We present a new field theoretic approach for finite dimensional site
disordered spin systems by introducing the notion of grand canonical disorder,
where the number of spins in the system is random but quenched. We analyze this
field theory using the variational replica formalism. For a variety of
interactions we find a spin glass phase (with continuous replica symmetry
breaking) and explicitly discuss a three dimensional system where this occurs.
This approximation also suggests that any ferromagnetic transition occur within
the spin glass phase.

<id>
cond-mat/9604033v3
<category>
cond-mat.dis-nn
<abstract>
We study toy aging processes in hierarchically decomposed phase spaces where
the equilibrium probability distributions are multifractal. We found that the
an auto-correlation function, survival-return probability, shows crossover
behavior from a power law $t^{-x}$ in the quasi-equilibrium regime ($t\ll\tw$)
to another power law $t^{-\lambda}$ ($\lambda \geq x$) in the off-equilibrium
regime ($t\gg\tw$) obeying a simple $t/\tw$ scaling law. The exponents $x$ and
$\lambda$ are related with the so called mass exponents which characterize the
multifractality.

<id>
cond-mat/9604065v2
<category>
cond-mat.dis-nn
<abstract>
We investigate the retrieval phase diagrams of an asynchronous
fully-connected attractor network with non-monotonic transfer function by means
of a mean-field approximation. We find for the noiseless zero-temperature case
that this non-monotonic Hopfield network can store more patterns than a network
with monotonic transfer function investigated by Amit et al. Properties of
retrieval phase diagrams of non-monotonic networks agree with the results
obtained by Nishimori and Opris who treated synchronous networks. We also
investigate the optimal storage capacity of the non-monotonic Hopfield model
with state-dependent synaptic couplings introduced by Zertuche et el. We show
that the non-monotonic Hopfield model with state-dependent synapses stores more
patterns than the conventional Hopfield model. Our formulation can be easily
extended to a general transfer function.

<id>
cond-mat/9605140v2
<category>
cond-mat.dis-nn
<abstract>
We introduce a matrix-operator formulation of the Anderson model in d=2. In a
single slice, we can then derive an analogy between our model and a standard
random matrices problem. This enables us to construct and control the Green
function in one slice, which is an important prerequisite to a full multi-scale
study of the problem using the Renormalisation Group approach.

<id>
cond-mat/9606033v2
<category>
cond-mat.dis-nn
<abstract>
We define a replica field theory describing finite dimensional site
disordered spin systems by introducing the notion of grand canonical disorder,
where the number of spins in the system is random but quenched. A general
analysis of this field theory is made using the Gaussian variational or Hartree
Fock method, and illustrated with several specific examples. Irrespective of
the form of interaction between the spins this approximation predicts a spin
glass phase. We discuss the replica symmetric phase at length, explicitly
identifying the correlator that diverges at the spin glass transition. We also
discuss the form of continuous replica symmetry breaking found just below the
transition. Finally we show how an analysis of ferromagnetic ordering indicates
a breakdown of the approximation.

<id>
cond-mat/9606215v2
<category>
cond-mat.dis-nn
<abstract>
The Random K-Satisfiability Problem, consisting in verifying the existence of
an assignment of N Boolean variables that satisfy a set of M=alpha N random
logical clauses containing K variables each, is studied using the replica
symmetric framework of diluted disordered systems. We present an exact
iterative scheme for the replica symmetric functional order parameter together
for the different cases of interest K=2, K>= 3 and K>>1. The calculation of the
number of solutions, which allowed us [Phys. Rev. Lett. 76, 3881 (1996)] to
predict a first order jump at the threshold where the Boolean expressions
become unsatisfiable with probability one, is thoroughly displayed. In the case
K=2, the (rigorously known) critical value (alpha=1) of the number of clauses
per Boolean variable is recovered while for K>=3 we show that the system
exhibits a replica symmetry breaking transition. The annealed approximation is
proven to be exact for large K.

<id>
cond-mat/9607103v2
<category>
cond-mat.dis-nn
<abstract>
We give a comprehensive self-contained review on the rigorous analysis of the
thermodynamics of a class of random spin systems of mean field type whose most
prominent example is the Hopfield model. We focus on the low temperature phase
and the analysis of the Gibbs measures with large deviation techniques. There
is a very detailed and complete picture in the regime of ``small $\a$''; a
particularly satisfactory result concerns a non-trivial regime of parameters in
which we prove 1) the convergence of the local ``mean fields'' to gaussian
random variables with constant variance and random mean; the random means are
from site to site independent gaussians themselves; 2) ``propagation of
chaos'', i.e. factorization of the extremal infinite volume Gibbs measures, and
3) the correctness of the ``replica symmetric solution'' of Amit, Gutfreund and
Sompolinsky [AGS]. This last result was first proven by M. Talagrand [T4],
using different techniques.

<id>
cond-mat/9608092v2
<category>
cond-mat.dis-nn
<abstract>
The coupling space of perceptrons with continuous as well as with binary
weights gets partitioned into a disordered multifractal by a set of $p=\gamma
N$ random input patterns. The multifractal spectrum $f(\alpha)$ can be
calculated analytically using the replica formalism. The storage capacity and
the generalization behaviour of the perceptron are shown to be related to
properties of $f(\alpha)$ which are correctly described within the replica
symmetric ansatz. Replica symmetry breaking is interpreted geometrically as a
transition from percolating to non-percolating cells. The existence of empty
cells gives rise to singularities in the multifractal spectrum. The analytical
results for binary couplings are corroborated by numerical studies.

<id>
cond-mat/9608136v3
<category>
cond-mat.dis-nn
<abstract>
We study the glassy super-rough phase of a class of solid-on-solid models
with a disordered substrate in the limit of vanishing temperature by means of
exact ground states, which we determine with a newly developed minimum cost
flow algorithm. Results for the height-height correlation function are compared
with analytical and numerical predictions. The domain wall energy of a boundary
induced step grows logarithmically with system size, indicating the marginal
stability of the ground state, and the fractal dimension of the step is
estimated. The sensibility of the ground state with respect to infinitesimal
variations of the quenched disorder is analyzed.

<id>
cond-mat/9610071v1
<category>
cond-mat.dis-nn
<abstract>
We summarize recent work on a frustrated periodic long-range Josephson array
in a parameter regime where its dynamical behavior is identical to that of the
$p=4$ disordered spherical model. We also discuss the physical requirements
imposed by the theory on the experimental realization of this superconducting
network.

<id>
cond-mat/9610114v1
<category>
cond-mat.dis-nn
<abstract>
The interaction between three localized vibrational modes is shown to be as
relevant for the lifetimes of localized modes as the interaction involving two
localized and one extended, and one localized and two extended modes. This
contrasts with previous views. I support my arguments by a numerical study of a
strongly disordered linear atomic chain.

<id>
cond-mat/9610150v1
<category>
cond-mat.dis-nn
<abstract>
The formation of Stationary Localized states due to a nonlinear dimeric
impurity embedded in a perfect 1-d chain is studied here using the appropriate
Discrete Nonlinear Schr$\ddot{o}$dinger Equation. Furthermore, the nonlinearity
has the form, $\chi |C|^\sigma$ where $C$ is the complex amplitude. A proper
ansatz for the Localized state is introduced in the appropriate Hamiltonian of
the system to obtain the reduced effective Hamiltonian. The Hamiltonian
contains a parameter, $\beta = \phi_1/\phi_0$ which is the ratio of stationary
amplitudes at impurity sites. Relevant equations for Localized states are
obtained from the fixed point of the reduced dynamical system. $|\beta|$ = 1 is
always a permissible solution. We also find solutions for which $|\beta| \ne
1$. Complete phase diagram in the $(\chi, \sigma)$ plane comprising of both
cases is discussed. Several critical lines separating various regions are
found. Maximum number of Localized states is found to be six. Furthermore, the
phase diagram continuously extrapolates from one region to the other. The
importance of our results in relation to solitonic solutions in a fully
nonlinear system is discussed.

<id>
cond-mat/9610160v1
<category>
cond-mat.dis-nn
<abstract>
We consider an ensemble of $K$ single-layer perceptrons exposed to random
inputs and investigate the conditions under which the couplings of these
perceptrons can be chosen such that prescribed correlations between the outputs
occur. A general formalism is introduced using a multi-perceptron costfunction
that allows to determine the maximal number of random inputs as a function of
the desired values of the correlations. Replica-symmetric results for $K=2$ and
$K=3$ are compared with properties of two-layer networks of tree-structure and
fixed Boolean function between hidden units and output. The results show which
correlations in the hidden layer of multi-layer neural networks are crucial for
the value of the storage capacity.

<id>
cond-mat/9610165v1
<category>
cond-mat.dis-nn
<abstract>
Within a Kuhn-Tucker cavity method introduced in a former paper, we study
optimal stability learning for situations, where in the replica formalism the
replica symmetry may be broken, namely
  (i) the case of a simple perceptron above the critical loading, and
  (ii) the case of two-layer AND-perceptrons, if one learns with maximal
stability.
  We find that the deviation of our cavity solution from the replica symmetric
one in these cases is a clear indication of the necessity of replica symmetry
breaking. In any case the cavity solution tends to underestimate the storage
capabilities of the networks.

<id>
cond-mat/9610192v1
<category>
cond-mat.dis-nn
<abstract>
We expose a functional integration method for the averaging of continuous
products $\hat{P}_t$ of $N\times N$ random matrices. As an application, we
compute exactly the statistics of the Lyapunov spectrum of $\hat{P}_t$. This
problem is relevant to the study of the statistical properties of various
disordered physical systems, and specifically to the computation of the
multipoint correlators of a passive scalar advected by a random velocity field.
Apart from these applications, our method provides a general setting for
computing statistical properties of linear evolutionary systems subjected to a
white noise force field.

<id>
cond-mat/9611003v1
<category>
cond-mat.dis-nn
<abstract>
Transport properties of one-dimensional Kronig-Penney models with binary
correlated disorder are analyzed using an approach based on classical
Hamiltonian maps. In this method, extended states correspond to bound
trajectories in the phase space of a parametrically excited linear oscillator,
while the on site-potential of the original model is transformed to an external
force. We show that in this representation the two probe conductance takes a
simple geometrical form in terms of evolution areas in phase-space. We also
analyze the case of a general N-mer model.

<id>
cond-mat/9611017v1
<category>
cond-mat.dis-nn
<abstract>
We present a new method to study disordered systems in the low temperature
limit. The method uses the replicated Hamiltonian. It studies the saddle points
of this Hamiltonian and shows how the various saddle point contributions can be
resummed in order to obtain the scaling behaviour at low temperatures. In a
large class of strongly disordered systems, it is necessary to include saddle
points of the Hamiltonian which break the replica symmetry in a vector sector,
as opposed to the usual matrix sector breaking of spin glass mean field theory.

<id>
cond-mat/9611022v1
<category>
cond-mat.dis-nn
<abstract>
By numerically solving the Schr\"oedinger equation for small sizes we
investigate the quantum critical point of the infinite-range Ising spin glass
in a transverse field at zero temperature. Despite its simplicity the method
yields accurate information on the value of the critical field and critical
exponents. We obtain $\Gamma_c=1.47\pm 0.01$ and check that exponents are in
agreement with analytical approaches.

<id>
cond-mat/9611034v1
<category>
cond-mat.dis-nn
<abstract>
In this talk I review some recent developments which shed light on the main
connections between structural glasses and mean-field spin glass models with a
discontinuous transition. I also discuss the role of quantum fluctuations on
the dynamical instability found in mean-field spin glasses with a discontinuous
transition. In mean-field models with pairwise interactions in a transverse
field it is shown, in the framework of the static approximation, that such
instability is suppressed at zero temperature.

<id>
cond-mat/9611051v1
<category>
cond-mat.dis-nn
<abstract>
The low-temperature generalization of the mode-coupling equations corresponds
to the dynamics of mean-field disordered models in the glassy phase. The system
never achieves equilibrium, preserving the memory of the time elapsed after the
quench throughout its evolution. A concept of effective temperature can be made
quite rigorous in this context by considering readings of thermometers in
different time-scales and the thermalization of weakly coupled subsystems.

<id>
cond-mat/9611060v2
<category>
cond-mat.dis-nn
<abstract>
The critical behaviour of three-dimensional disordered systems is
investigated by analysing the spectral fluctuations of the energy spectrum. Our
results suggest that the initial symmetries (orthogonal, unitary and
symplectic) are broken by the disorder at the critical point. The critical
behaviour, determinedby the symmetry at the critical point, should therefore be
independent of the previous invariances and be described by a ``super''
universality class. This result is strongly supported by the fact that we
obtain the same critical exponent $\nu \simeq 1.35$ in the three cases:
orthogonal, unitary and symplectic.

<id>
cond-mat/9611068v1
<category>
cond-mat.dis-nn
<abstract>
We introduce a three replica potential useful to examine the structure of
metastables states above the static transition temperature, in the spherical
p-spin model. Studying the minima of the potential we are able to find which is
the distance between the nearest equilibrium and local equilibrium states,
obtaining in this way information on the dynamics of the system. Furthermore,
the analysis of the potential at the dynamical transition temperature suggests
that equilibrium states are not randomly distributed in the phase space.

<id>
cond-mat/9611106v1
<category>
cond-mat.dis-nn
<abstract>
We study the dynamics of the SK model modified by a small non-hamiltonian
perturbation. We study aging, and we find that on the time scales investigated
by our numerical simulations it survives a small perturbation (and is destroyed
by a large one). If we assume we are observing a transient behavior the scaling
of correlation times versus the asymmetry strength is not compatible with the
one expected for the spherical model. We discuss the slow power law decay of
observable quantities to equilibrium, and we show that for small perturbations
power like decay is preserved. We also discuss the asymptotically large time
region on small lattices.

<id>
cond-mat/9611121v1
<category>
cond-mat.dis-nn
<abstract>
Some frustrated pyrochlore antiferromagnets, such as Y2Mo2O7, show a
spin-freezing transition and magnetic irreversibilities below a temperature Tf
similar to what is observed nonlinear magnetization measurements on Y2Mo2O7
that provide strong evidence that there is an underlying thermodynamic phase
transition at Tf, which is characterized by critical exponents \gamma \approx
2.8 and \beta \approx 0.8. These values are typical of those found in random
spin glasses, despite the fact that the level of random disorder in Y2Mo2O7 is
immeasurably small.

<id>
cond-mat/9611130v1
<category>
cond-mat.dis-nn
<abstract>
A perceptron with N random weights can store of the order of N patterns by
removing a fraction of the weights without changing their strengths. The
critical storage capacity as a function of the concentration of the remaining
bonds for random outputs and for outputs given by a teacher perceptron is
calculated. A simple Hebb-like dilution algorithm is presented which in the
teacher case reaches the optimal generalization ability.

<id>
cond-mat/9611230v3
<category>
cond-mat.dis-nn
<abstract>
The relaxational modes and aging of the Glauber dynamics of the mean-field
spin-glass model, SK model, are studied by a numerical diagonalization
technique and Monte Carlo simulations. We found that the aging process of the
model is understood as hierarchical growth of quasi-equilibrium domain in the
phase-space.

<id>
cond-mat/9611233v1
<category>
cond-mat.dis-nn
<abstract>
On the basis of a recent field theory for site-disordered spin glasses a
Ginzburg-Landau free energy is proposed to describe the low temperatures glassy
phase(s) of site-disordered magnets. The prefactors of the cubic and dominant
quartic terms change gradually along the transition line in the
concentration-temperature phase diagram. Either of them may vanish at certain
points $(c_*, T_*)$, where new transition lines originate. The new phases are
classified

<id>
cond-mat/9612015v1
<category>
cond-mat.dis-nn
<abstract>
We study the kinetic roughening of a driven domain wall between spin-up and
spin-down domains for a model with non-conserved order parameter and quenched
disorder. To understand the scaling behavior of this interface we construct an
equation of motion and study it theoretically.

<id>
cond-mat/9612034v1
<category>
cond-mat.dis-nn
<abstract>
We study two interacting particles in a random potential chain by means of
the transfer matrix method. The dependence of the two-particle localization
length $\lambda_2$ on disorder and interaction strength is investigated. Our
results demonstrate that the recently proposed enhancement of $\lambda_2$ as
compared to the results for single particles is entirely due to the finite size
of the systems considered. This is shown for a Hubbard-like onsite interaction
and also a long-range interaction.

<id>
cond-mat/9612062v1
<category>
cond-mat.dis-nn
<abstract>
Random walks are studied on disordered cellular networks in 2-and
3-dimensional spaces with arbitrary curvature. The coefficients of the
evolution equation are calculated in term of the structural properties of the
cellular system. The effects of disorder and space-curvature on the diffusion
phenomena are investigated. In disordered systems the mean square displacement
displays an enhancement at short time and a lowering at long ones, with respect
to the ordered case. The asymptotic expression for the diffusion equation on
hyperbolic cellular systems relates random walk on curved lattices to
hyperbolic Brownian motion.

<id>
cond-mat/9406083v2
<category>
cond-mat.mes-hall
<abstract>
Recent experiments on conduction between a semiconductor and a superconductor
have revealed a variety of new mesoscopic phenomena. Here is a review of the
present status of this rapidly developing field. A scattering theory is
described which leads to a conductance formula analogous to Landauer's formula
in normal-state conduction. The theory is used to identify features in the
conductance which can serve as "signatures" of phase-coherent Andreev
reflection, i.e. for which the phase coherence of the electrons and the
Andreev-reflected holes is essential. The applications of the theory include a
quantum point contact, quantum dot, weak localization, universal conductance
fluctuations, shot noise, and reflectionless tunneling. This review is based on
lectures at the Les Houches summer school, Session LXI, 1994.

<id>
cond-mat/9510156v2
<category>
cond-mat.mes-hall
<abstract>
We study the effect of an ac drive on the current-voltage (I-V)
characteristics of a tunnel junction between two fractional Quantum Hall fluids
at filling $\nu ^{-1}$ an odd integer. Within the chiral Luttinger liquid model
of edge states, the point contact dynamics is described by a driven damped
quantum mechanical pendulum. In a semi-classical limit which ignores electron
tunnelling, this model exhibits mode-locking, which corresponds to current
plateaus in the I-V curve at integer multiples of $I= e\omega /2\pi$, with
$\omega$ the ac drive angular frequency. By analyzing the full quantum model at
non-zero $\nu$ using perturbative and exact methods, we study the effect of
quantum fluctuation on the mode-locked plateaus. For $\nu=1$ quantum
fluctuations smear completely the plateaus, leaving no trace of the ac drive.
For $\nu \ge 1/2$ smeared plateaus remain in the I-V curve, but are not
centered at the currents $I=n e \omega /2\pi$. For $\nu < 1/2$ rounded plateaus
centered around the quantized current values are found. The possibility of
using mode locking in FQHE point contacts as a current-to-frequency standard is
discussed.

<id>
cond-mat/9602071v2
<category>
cond-mat.mes-hall
<abstract>
This is a review of the properties of spectral fluctations in disordered
metals, their relation with Random Matrix Theory and semiclassical picture. We
also review the physics of persistent currents in mesoscopic isolated rings,
the parametric correlations and curvature distributions.

<id>
cond-mat/9603098v3
<category>
cond-mat.mes-hall
<abstract>
The ``orthodox theory'' of a single electron double junction is dealt with.
It is shown that the stationary solution of the underlying master equation
allows the construction of any time-dependent solution in terms of orthogonal
polynomials. The approach pays off if the stationary solution becomes simple.
Two special cases are considered. We use the time-dependent solution to
calculate the current noise in these cases.

<id>
cond-mat/9604101v2
<category>
cond-mat.mes-hall
<abstract>
The inelastic quasiparticle lifetime due to the electron-electron interaction
(out-scattering time in the kinetic equation formalism) is calculated for
finite metallic diffusive systems (quantum dots) in the whole range of
parameters. Both cases of ``continuous'' (the inelastic level broadening much
exceeds the mean level spacing) and ``discrete'' spectrum are analyzed. In
particular, crossover between one- and zero-dimensional regimes is studied in
detail. In the case of continuous spectrum the out-scattering time is shown to
be the same as the inelastic time entering expressions for universal
conductance fluctuations and persistent currents. It is also found to be
shorter than the phase-breaking time in two- and one-dimensional systems, while
in zero-dimensional systems these two times coincide. In the case of discrete
spectrum for small enough systems a universal behavior of the scattering time
is obtained. For temperatures below the mean level spacing the out-scattering
rate is shown to be vanishingly small.

<id>
cond-mat/9604137v2
<category>
cond-mat.mes-hall
<abstract>
The propagation of a surface acoustic wave (SAW) on GaAs/AlGaAs
heterostructures is studied in the case where the two-dimensional electron gas
(2DEG) is subject to a strong magnetic field and a smooth random potential with
correlation length Lambda and amplitude Delta. The electron wave functions are
described in a quasiclassical picture using results of percolation theory for
two-dimensional systems. In accordance with the experimental situation, Lambda
is assumed to be much smaller than the sound wavelength 2*pi/q. This restricts
the absorption of surface phonons at a filling factor \bar{\nu} approx 1/2 to
electrons occupying extended trajectories of fractal structure. Both
piezoelectric and deformation potential interactions of surface acoustic
phonons with electrons are considered and the corresponding interaction
vertices are derived. These vertices are found to differ from those valid for
three-dimensional bulk phonon systems with respect to the phonon wave vector
dependence. We derive the appropriate dielectric function varepsilon(omega,q)
to describe the effect of screening on the electron-phonon coupling. In the low
temperature, high frequency regime T << Delta (omega_q*Lambda
/v_D)^{alpha/2/nu}, where omega_q is the SAW frequency and v_D is the electron
drift velocity, both the attenuation coefficient Gamma and varepsilon(omega,q)
are independent of temperature. The classical percolation indices give
alpha/2/nu=3/7. The width of the region where a strong absorption of the SAW
occurs is found to be given by the scaling law |Delta \bar{\nu}| approx
(omega_q*Lambda/v_D)^{alpha/2/nu}. The dependence of the electron-phonon
coupling and the screening due to the 2DEG on the filling factor leads to a
double-peak structure for Gamma(\bar{\nu}).

<id>
cond-mat/9604138v2
<category>
cond-mat.mes-hall
<abstract>
We propose a self-consistent scheme for the determination of the ground-state
(GS) properties of interacting electrons in a magnetic field, and of systems
whose GS's time-reversal-symmetry (TRS) is spontaneously broken. It is based on
a newly-developed many-body perturbation theory that is valid, irrespective of
the strength of correlation, provided the GS number densities
$n_{\uparrow}({\bf r})$, $n_{\downarrow}({\bf r})$, and the {\sl total}
paramagnetic particle flux density are pure-state non-interacting
$v$-representable. Our approach can in particular be applied to (modulated)
two-dimensional electron systems in the fractional quantum-Hall regime.

<id>
cond-mat/9605003v3
<category>
cond-mat.mes-hall
<abstract>
1) For the hard core interaction there is some freedom left in the choice of
the exact multiskyrmionic wave function's topology. The statistics of textured
quasiholes, analyzed by calculation of the Berry phase, depends on this choice
of topology.
  2) We find a class of textured two-hole eigenstates of the Coulomb
interaction. There is no definite quantum statistics but there is a definite
rule of how to construct Coulomb eigenstates out of the hard core wave
functions.
  3) A wave function for the 5/2 state is constructed according to this rule.

<id>
cond-mat/9605014v2
<category>
cond-mat.mes-hall
<abstract>
Transport through a one-dimensional wire of interacting electrons connected
to semi infinite leads is investigated using a bosonization approach. The
dynamic nonlocal conductivity is rigorously expressed in terms of the
transmission. For abrupt variations of the interaction parameters at the
junctions, an incident electron is transmitted as a sequence of partial
charges: the central wire acts as a Fabry-P\'erot resonator. The dc conductance
is shown to be given by the total transmission which turns out to be perfect.
When the wire has a tendency towards superconducting order, partial Andreev
reflection of an incident electron occurs. Finally, we study the role of a weak
barrier at one contact or inside the wire by a renormalization group method at
finite temperature. We compute the conductance in the presence of localized or
extended disorder, and compare our results to recent experiments on quantum
wires.

<id>
cond-mat/9605124v2
<category>
cond-mat.mes-hall
<abstract>
We study intersubband spin density collective modes in double-layer quantum
Hall systems at $\nu=2$ within the time-dependent Hartree-Fock approximation.
We find that these intersubband spin density excitations may soften under
experimentally accessible conditions, signaling a phase transition to a new
quantum Hall state with interlayer inplane antiferromagnetic spin correlations.
We show that this novel canted antiferromagnetic phase is energetically stable
and that the phase transition is continuous.

<id>
cond-mat/9606015v2
<category>
cond-mat.mes-hall
<abstract>
Quantum dots in the fractional quantum Hall regime are studied using a
Hartree formulation of composite fermion theory. Under appropriate conditions
the chemical potential of the dots will oscillate periodically with B due to
the transfer of composite fermions between quasi-Landau bands. This effect is
analogous to the addition spectrum oscillations which occur in quantum dots in
the integer quantum Hall regime. Period phi_0 oscillations are found in sharply
confined dots with filling factors nu=2/5 and nu=2/3. Period 3*phi_0
oscillations are found in a parabolically confined nu=2/5 dot. More generally,
we argue that the oscillation period of dots with band pinning should vary
continuously with B whereas the period of dots without band pinning is phi_0.
Finally, we discuss the possibility of detecting fractionally charged
excitations using the observed period of addition spectrum oscillations.

<id>
cond-mat/9606114v3
<category>
cond-mat.mes-hall
<abstract>
We study experimentally the reproducible conductance fluctuations between the
quantum Hall plateaus in the conductance of two-terminal submicron silicon
MOSFETs. For the dramatic fluctuations at the insulator-to-first-plateau
transition we find a conductance distribution that is approximately uniform
between zero and $e^2/h$. We point out that this is consistent with the
prediction of random $S$-matrix theory for a conductor with single-channel
leads in a magnetic field.

<id>
cond-mat/9607002v2
<category>
cond-mat.mes-hall
<abstract>
We study the theory of intrinsic photoluminescence of two-dimensional
electron systems in the vicinity of the $\nu=1$ quantum Hall state. We focus
predominantly on the recombination of a band of initial ``excitonic states''
that are the low-lying energy states of our model at $\nu=1$. It is shown that
the recombination of excitonic states can account for recent observations of
the polarization-resolved spectra of a high-mobility GaAs quantum well. The
asymmetric broadening of the spectral line in the $\sigma_-$ polarization is
explained to be the result of the ``shake-up'' of spin-waves upon radiative
recombination of excitonic states. We derive line shapes for the recombination
of excitonic states in the presence of long-range disorder that compare
favourably with the experimental observations. We also discuss the stabilities
and recombination spectra of other (``charged'') initial states of our model.
An additional high-energy line observed in experiment is shown to be consistent
with the recombination of a positively-charged state. The recombination
spectrum of a negatively-charged initial state, predicted by our model but not
observed in the present experiments, is shown to provide a direct measure of
the formation energy of the smallest ``charged spin-texture'' of the $\nu=1$
state.

<id>
cond-mat/9607139v3
<category>
cond-mat.mes-hall
<abstract>
We study cotunneling in a double junction Coulomb blockade device under the
influence of time dependent potentials. It is shown that the ac-bias leads to
photon assisted cotunneling which in some cases may dominate the transport. We
derive a general non-perturbative expression for the tunneling current in the
presence of oscillating potentials and give a perturbative expression for the
photon assisted cotunneling current.

<id>
cond-mat/9608027v2
<category>
cond-mat.mes-hall
<abstract>
We use the Lanczos method to calculate the variance of the number of energy
levels in an energy window of width E below the Fermi energy for
non-interacting disordered electrons on a thin three-dimensional ring threaded
by an Aharonov-Bohm flux . We find that for small E the flux-dependent part of
the variance is well described by a well-known Feynman diagram involving two
Cooperons. However, this result cannot be extrapolated to energies E where the
energy-dependence of the average density of states becomes significant. We
discuss consequences for persistent currents.

<id>
cond-mat/9608064v2
<category>
cond-mat.mes-hall
<abstract>
We calculate in an analyical fashion the energies and net spins of skyrmions
in fractional quantum Hall systems, based on the suggestion that skyrmion
states are spontaneously $L_Z$ and $S_Z$ symmetry-breaking states. The
quasihole-skyrmion state with a charge $-e/3$ around $\nu$ = 1/3, where the
ground state is known as a spin-polarized ferromagnetic state, is found to
exist even in high magnetic fields up to about 7 T for GaAs samples.

<id>
cond-mat/9608073v2
<category>
cond-mat.mes-hall
<abstract>
A recent theory has provided a possible explanation for the ``non-universal
scaling'' of the low-temperature conductance (and conductivity) peak-heights of
two-dimensional electron systems in the integer and fractional quantum Hall
regimes. This explanation is based on the hypothesis that samples which show
this behavior contain density inhomogeneities. Theory then relates the
non-universal conductance peak-heights to the ``number of alternating
percolation clusters'' of a continuum percolation model defined on the
spatially-varying local carrier density. We discuss the statistical properties
of the number of alternating percolation clusters for Corbino disc samples
characterized by random density fluctuations which have a correlation length
small compared to the sample size. This allows a determination of the
statistical properties of the low-temperature conductance peak-heights of such
samples. We focus on a range of filling fraction at the center of the plateau
transition for which the percolation model may be considered to be critical. We
appeal to conformal invariance of critical percolation and argue that the
properties of interest are directly related to the corresponding quantities
calculated numerically for bond-percolation on a cylinder. Our results allow a
lower bound to be placed on the non-universal conductance peak-heights, and we
compare these results with recent experimental measurements.

<id>
cond-mat/9608111v2
<category>
cond-mat.mes-hall
<abstract>
Under periodic boundary condition in the transverse direction, we calculate
the averaged zero-temperature two-terminal conductance ($<G>$) and its
statistical fluctuations ($<(\dg)^{2n}>$ for $n\le 4$) at the critical point of
integer quantum Hall plateau transitions. We find {\it universal} values for
$<G>=(0.58\pm0.03){e^2\over h}$, and $<(\dg)^{2n}>=({e^2\over h})^{2n}A_{2n}$,
where $A_{2,4,6,8}=0.081\pm0.005$; $0.013\pm0.003$; $0.0026\pm0.005$; and
$(8\pm2)\times10^{-4}$ respectively. We also determine the leading finite size
scaling corrections to these observables. Comparisons with experiments will be
made.

<id>
cond-mat/9608125v2
<category>
cond-mat.mes-hall
<abstract>
Elementary excitations of incompressible quantum liquids (IQL's) are anyons,
i.e., quasiparticles carrying fractional charges and obeying fractional
statistics. To find out how the properties of these quasiparticles manifest
themselves in the optical spectra, we have developed the anyon exciton model
(AEM) and compared the results with the finite-size data for excitons of nu=1/3
and nu=2/3 IQL's. The model considers an exciton as a neutral composite
consisting of three quasielectrons and a single hole. The AEM works well when
the separation between electron and hole confinement planes, h, is larger than
the magnetic length l. In the framework of the AEM an exciton possesses
momentum k and two internal quantum numbers, one of which can be chosen as the
angular momentum, L, of the k=0 state. Existence of the internal degrees of
freedom results in the multiple branch energy spectrum, crater-like electron
density shape and 120 degrees density correlations for k=0 excitons, and the
splitting of the electron shell into bunches for non-zero k excitons. For h
larger than 2l the bottom states obey the superselection rule L=3m (m are
integers starting from 2), all of them are hard core states. For h nearly 2l
there is one-to-one correspondence between the low-energy spectra found for the
AEM and the many- electron exciton spectra of the nu=2/3 IQL, whereas some
states are absent from the many-electron spectra of the nu=1/3 IQL. We argue
that this striking difference in the spectra originates from the different
populational statistics of the quasielectrons of charge conjugate IQL's and
show that the proper account of the statistical requirements eliminates
excessive states from the spectrum. Apparently, this phenomenon is the first
manifestation of the exclusion statistics in the anyon bound states.

<id>
cond-mat/9609048v2
<category>
cond-mat.mes-hall
<abstract>
We study numerically conductance fluctuations near the integer quantum Hall
effect plateau transition. The system is presumed to be in a mesoscopic regime,
with phase coherence length comparable to the system size. We focus on a
two-terminal conductance G for square samples, considering both periodic and
open boundary conditions transverse to the current. At the plateau transition,
G is broadly distributed, with a distribution function close to uniform on the
interval between zero and one in units of e^2/h. Our results are consistent
with a recent experiment by Cobden and Kogan on a mesoscopic quantum Hall
effect sample.

<id>
cond-mat/9609224v2
<category>
cond-mat.mes-hall
<abstract>
We develop a Luttinger liquid theory of the Coulomb drag of persistent
currents flowing in concentric mesoscopic rings, by incorporating non-linear
corrections to the electron dispersion relation. We demonstrate that at low
temperatures, interactions between electrons in different rings generate an
additional phase and thus alter the period of Aharonov-Bohm oscillations. The
resulting nondissipative drag depends strongly on the relative parity of the
electron numbers. We also show that interactions set a new temperature scale
below which the linear response theory does not apply at certain values of
external flux.

<id>
cond-mat/9609288v2
<category>
cond-mat.mes-hall
<abstract>
We study influence of electron-electron interaction on statistics of Coulomb
blockade peak spacings in disordered quantum dots. It is shown that the
interaction combined with fluctuations of eigenfunctions of the Fermi sea,
enhances the peak spacing fluctuations, in accordance with recent experiments.
In addition, account of the spin degrees of freedom leads to a pronounced
odd-even structure for weak interaction ($e^2/\epsilon \ll v_F$); in the
opposite case ($e^2/\epsilon \gtrsim v_F$) this structure is washed out.

<id>
cond-mat/9610012v2
<category>
cond-mat.mes-hall
<abstract>
We consider the influence of an external periodic potential on the fractional
quantum Hall effect of two-dimensional interacting electron systems. For many
electrons on a torus, we find that the splitting of incompressible ground state
degeneracies by a weak external potential diminishes as $\exp ( - L/ \xi)$ at
large system size $L$. We present numerical results consistent with a scenario
in which $\xi$ diverges at continuous phase transitions from fractional to
integer quantum Hall states which occur with increasing external potential
strength.

<id>
cond-mat/9610025v1
<category>
cond-mat.mes-hall
<abstract>
We present a discussion of the admittance (ac-conductance) and nonlinear
I-V-characteristic for a number of mesoscopic conductors. Our approach is based
on a generalization of the scattering approach which now includes the effects
of the (long-range) Coulomb interaction. We discuss the admittance of a wire
with an impurity and with a nearby gate. We extend a discussion of the
low-frequency admittance of a quantum point contact to investigate the effects
of the gates used to form the contact. We discuss the nonlinear I-V
characteristic of a resonant double barrier structure and discuss the
admittance for the double barrier for a large range of frequencies. Our
approach emphasizes the overall conservation of charge (gauge invariance) and
current conservation and the resulting sum rules for the admittance matrix and
nonlinear transport coefficients.

<id>
cond-mat/9610037v1
<category>
cond-mat.mes-hall
<abstract>
In this paper we review recent theoretical results for transport in a
one-dimensional (1d) Luttinger liquid. For simplicity, we ignore electron spin,
and focus exclusively on the case of a single-mode. Moreover, we consider only
the effects of a single (or perhaps several) spatially localized impurities.
Even with these restrictions, the predicted behavior is very rich, and
strikingly different than for a 1d non-interacting electron gas. The method of
bosonization is reviewed, with an emphasis on physical motivation, rather than
mathematical rigor. Transport through a single impurity is reviewed from
several different perspectives, as a pinned strongly interacting ``Wigner"
crystal and in the limit of weak interactions. The existence of fractionally
charged quasiparticles is also revealed. Inter-edge tunnelling in the quantum
Hall effect, and charge fluctuations in a quantum dot under the conditions of
Coulomb blockade are considered as examples of the developed techniques.

<id>
cond-mat/9610040v1
<category>
cond-mat.mes-hall
<abstract>
We study interacting electrons in two dimensions moving in the lowest Landau
level under the condition that the Zeeman energy is much smaller than the
Coulomb energy and the filling factor is one. In this case, Skyrmion
quasiparticles play an important role. Here, we present a simple and
transparent derivation of the corresponding effective Lagrangian. In its
kinetic part, we find a non-zero Hopf term the prefactor of which we determine
rigorously. In the Hamiltonian part, we calculate, by means of a gradient
expansion, the Skyrmion-Skyrmion interaction completely up to fourth order in
spatial derivatives.

<id>
cond-mat/9610043v1
<category>
cond-mat.mes-hall
<abstract>
We derive an exact result for the averaged Feynman propagator and the
corresponding density of states of an electron in two dimensions in a
perpendicular homogeneous magnetic field and a Gaussian random potential with
long-range spatial correlations described by a quadratic correlation function.

<id>
cond-mat/9610068v1
<category>
cond-mat.mes-hall
<abstract>
We study the effect of an electric charge in the middle of a ring of
electrons in a magnetic field such as $\nu = 1/2$. In the absence of the
central charge, a residual current should appear due to an Aharanov-Bohm
effect. As the charge varies, periodic currents should appear in the ring. We
evaluate the amplitude of these currents, as well as their period as the
central charge varies. The presence of these currents should be a direct
signature of the existence of a statistical gauge field in the $\nu=1/2$
quantum Hall effect. Numerical diagonalizations for a small number of electrons
on the sphere are also carried out. The numerical results up to 9 electrons are
qualitatively consistent with the mean field picture.

<id>
cond-mat/9610074v1
<category>
cond-mat.mes-hall
<abstract>
We investigate the classical and quantum dynamics of an electron confined to
a circular quantum dot in the presence of homogeneous $B_{dc}+B_{ac}$ magnetic
fields. The classical motion shows a transition to chaotic behavior depending
on the ratio $\epsilon=B_{ac}/B_{dc}$ of field magnitudes and the cyclotron
frequency ${\tilde\omega_c}$ in units of the drive frequency. We determine a
phase boundary between regular and chaotic classical behavior in the $\epsilon$
vs ${\tilde\omega_c}$ plane. In the quantum regime we evaluate the quasi-energy
spectrum of the time-evolution operator. We show that the nearest neighbor
quasi-energy eigenvalues show a transition from level clustering to level
repulsion as one moves from the regular to chaotic regime in the
$(\epsilon,{\tilde\omega_c})$ plane. The $\Delta_3$ statistic confirms this
transition. In the chaotic regime, the eigenfunction statistics coincides with
the Porter-Thomas prediction. Finally, we explicitly establish the phase space
correspondence between the classical and quantum solutions via the Husimi phase
space distributions of the model. Possible experimentally feasible conditions
to see these effects are discussed.

<id>
cond-mat/9610078v1
<category>
cond-mat.mes-hall
<abstract>
We study the dynamical behavior of disordered quantum-well-based
semiconductor superlattices where the disorder is intentional and short-range
correlated. We show that, whereas the transmission time of a particle grows
exponentially with the number of wells in an usual disordered superlattice for
any value of the incident particle energy, for specific values of the incident
energy this time increases linearly when correlated disorder is included. As
expected, those values of the energy coincide with a narrow subband of extended
states predicted by the static calculations of Dom\'{\i}nguez-Adame {\em et
al.} [Phys. Rev. B {\bf 51}, 14 ,359 (1994)]; such states are seen in our
dynamical results to exhibit a ballistic regime, very close to the WKB
approximation of a perfect superlattice. Fourier transform of the output signal
for an incident Gaussian wave packet reveals a dramatic filtering of the
original signal, which makes us confident that devices based on this property
may be designed and used for nanotechnological applications. This is more so in
view of the possibility of controllingthe outp ut band using a dc electric
field, which we also discuss. In the conclusion we summarize our results and
present an outlook for future developments arising from this work.

<id>
cond-mat/9312040v1
<category>
cond-mat.mtrl-sci
<abstract>
We study the electronic states of giant single-shell and the recently
discovered nested multi-shell carbon fullerenes within the tight-binding
approximation. We use two different approaches, one based on iterations and the
other on symmetry, to obtain the $\pi$-state energy spectra of large fullerene
cages: $C_{240}$, $C_{540}$, $C_{960}$, $C_{1500}$, $C_{2160}$ and $C_{2940}$.
Our iteration technique reduces the dimensionality of the problem by more than
one order of magnitude (factors of $\sim 12$ and $20$), while the
symmetry-based approach reduces it by a factor of $10$. We also find formulae
for the highest occupied and lowest unoccupied molecular orbital (HOMO and
LUMO) energies of $C_{60{\cdot}n^{2}}$ fullerenes as a function of $n$,
demonstrating a tendency towards metallic regime for increasing $n$. For
multi-shell fullerenes, we analytically obtain the eigenvalues of the
intershell interaction.

<id>
cond-mat/9312041v1
<category>
cond-mat.mtrl-sci
<abstract>
The recursion and path-integral methods are applied to analytically study the
electronic structure of a neutral $C_{60}$ molecule. We employ a tight-binding
Hamiltonian which considers both the $s$ and $p$ valence electrons of carbon.
From the recursion method, we obtain closed-form {\it analytic} expressions for
the $\pi$ and $\sigma$ eigenvalues and eigenfunctions, including the highest
occupied molecular orbital (HOMO) and the lowest unoccupied molecular orbital
(LUMO) states, and the Green's functions. We also present the local densities
of states around several ring clusters, which can be probed experimentally by
using, for instance, a scanning tunneling microscope. {}From a path-integral
method, identical results for the energy spectrum are also derived. In
addition, the local density of states on one carbon atom is obtained; from this
we can derive the degree of degeneracy of the energy levels.

<id>
cond-mat/9402078v1
<category>
cond-mat.mtrl-sci
<abstract>
We analytically study phonon transmission and localization in random
superlattices by using a Green's function approach. We derive expressions for
the average transmission rate and localization length, or Lyapunov exponent, in
terms of the superlattice structure factor. This is done by considering the
backscattering of phonons, due to the complex mass density fluctuations, which
incorporates all of the forward scattering processes. These analytical results
are applied to two types of random superlattices and compared with numerical
simulations based on the transfer matrix method. Our analytical results show
excellent agreement with the numerical data. A universal relation for the
transmission fluctuations versus the average transmission is derived
explicitly, and independently confirmed by numerical simulations. The transient
of the distribution of transmission to the log-normal distribution for the
localized phonons is also studied.

<id>
cond-mat/9402079v1
<category>
cond-mat.mtrl-sci
<abstract>
We study both analytically and numerically phonon transmission fluctuations
and localization in partially ordered superlattices with correlations among
neighboring layers. In order to generate a sequence of layers with a varying
degree of order we employ a model proposed by Hendricks and Teller as well as
partially ordered versions of deterministic aperiodic superlattices. By
changing a parameter measuring the correlation among adjacent layers, the
Hendricks- Teller superlattice exhibits a transition from periodic ordering,
with alterna- ting layers, to the phase separated opposite limit; including
many intermediate arrangements and the completely random case. In the partially
ordered versions of deterministic superlattices, there is short-range order
(among any $N$ conse- cutive layers) and long range disorder, as in the N-state
Markov chains. The average and fluctuations in the transmission, the
backscattering rate, and the localization length in these multilayered systems
are calculated based on the superlattice structure factors we derive
analytically. The standard deviation of the transmission versus the average
transmission lies on a {\it universal\/} curve irrespective of the specific
type of disorder of the SL. We illustrate these general results by applying
them to several GaAs-AlAs superlattices for the proposed experimental
observation of phonon universal transmission fluctuations.

<id>
cond-mat/9610001v1
<category>
cond-mat.mtrl-sci
<abstract>
We present a model for thin film growth by particle deposition that takes
into account the possible evaporation of the particles deposited on the
surface. Our model focuses on the formation of two-dimensional structures. We
find that the presence of evaporation can dramatically affect the growth
kinetics of the film, and can give rise to regimes characterized by different
``growth'' exponents and island size distributions. Our results are obtained by
extensive computer simulations as well as through a simple scaling approach and
the analysis of rate equations describing the system. We carefully discuss the
relationship of our model with previous studies by Venables and Stoyanov of the
same physical situation, and we show that our analysis is more general.

<id>
cond-mat/9610008v1
<category>
cond-mat.mtrl-sci
<abstract>
We study a class of models for brittle fracture: elastic theory models which
allow for cracks but not for plastic flow. We show that these models exhibit,
at all finite temperatures, a transition to fracture under applied load similar
to that at a first order liquid-gas transition. We study this transition at low
temperature for small tension. We discuss the appropriate thermodynamic limit
of these theories: a large class of boundary conditions is identified for which
the energy release for a crack becomes independent of the macroscopic shape of
the material. Using the complex variable method in a two-dimensional elastic
theory, we prove that the energy release in an isotropically stretched material
due to the creation of an arbitrary curvy cut is the same to cubic order as the
energy release for the straight cut with the same end points. We find the
normal modes and the energy spectrum for crack shape fluctuations and for crack
surface phonons, under a uniform isotropic tension. For small uniform isotropic
tension in two dimensions we calculate the essential singularity associated
with fracturing the material in a saddle point approximation including
quadratic fluctuations. This singularity determines the lifetime of the
material (half-life for fracture), and also determines the asymptotic
divergence of the high-order corrections to the zero temperature elastic
coefficients. We calculate the asymptotic ratio of the high-order elastic
coefficients of the inverse bulk modulus and argue that the result is unchanged
by nonlinearities --- the ratio of the high-order nonlinear terms are
determined solely by the linear theory.

<id>
cond-mat/9610021v1
<category>
cond-mat.mtrl-sci
<abstract>
The structure of CO adsorbates on the Rh(110) surface is studied at full
coverage using first-principles techniques. The relative energies of different
adsorbate geometries are determined by means of accurate structure
optimizations. In agreement with experiments, we find that a p2mg(2x1) 2CO
structure is the most stable. The CO molecules sit on the short-bridge site
(carbon below) with the molecular axis slightly tilted off the surface normal,
along the (001) direction. Configurations corresponding to different
distributions of tilt angles are mapped onto an anisotropic 2D Ising model
whose parameters are extracted from our ab-initio calculations. We find that an
order-disorder phase-transition occurs at a temperature T_c=350 K.

<id>
cond-mat/9610023v1
<category>
cond-mat.mtrl-sci
<abstract>
A first-principles atomic orbital-based electronic structure method is used
to investigate the low index surfaces of rutile Titanium Dioxide. The method is
relatively cheap in computational terms, making it attractive for the study of
oxide surfaces, many of which undergo large reconstructions, and may be
governed by the presence of Oxygen vacancy defects. Calculated surface charge
densities are presented for low-index surfaces of TiO$_2$, and the relation of
these results to experimental STM images is discussed. Atomic resolution images
at these surfaces tend to be produced at positive bias, probing states which
largely consist of unoccupied Ti 3$d$ bands, with a small contribution from O
2$p$. These experiments are particularly interesting since the O atoms tend to
sit up to 1 angstrom above the Ti atoms, so providing a play-off between
electronic and geometric structure in image formation.

<id>
cond-mat/9610036v1
<category>
cond-mat.mtrl-sci
<abstract>
The traditional magnetic storage mechanisms (both analog and digital) apply
an external field signal H(t) to a hysteretic magnetic material, and read the
remanent magnetization M(t), which is (roughly) proportional to H(t). We
propose a new analog method of recovering the signal from the magnetic
material, making use of the shape of the hysteresis loop M(H). The field H,
``stored'' in a region with N domains or particles, can be recovered with
fluctuations of order 1/N using the new method - much superior to the 1/sqrt{N}
fluctuations in traditional analog storage.

<id>
cond-mat/9610044v1
<category>
cond-mat.mtrl-sci
<abstract>
Zn in GaN forms an efficient radiative center and acts as a deep acceptor
which can make the crystal insulating. Four different Zn-related centers have
been by now identified, leading to light emission in the range between 1.8 eV
and 2.9 eV. We present a first-principles investigation total energy and
electronic structure calculations for Ga-substitutional and hetero-antisite
N-substitutional Zn in wurtzite GaN, using ultrasoft pseudopotentials and a
conjugate-gradient total energy minimization method. Our results permit the
identification of the blue-light emission center as the substitutional
acceptor, while contrary to a common belief the Zn_N heteroantisite has a very
high formation energy and donor behavior, which seems to exclude it as the
origin of the other centers.

<id>
cond-mat/9610045v1
<category>
cond-mat.mtrl-sci
<abstract>
Impurity levels and formation energies of acceptors in wurtzite GaN are
predicted ab initio. Be_Ga is found to be the shallow (thermal ionization
energy $\sim$ 0.06 eV); $Mg_{Ga}$ and $Zn_{Ga}$ are mid-deep acceptors (0.23 eV
and 0.33 eV respectively); $Ca_{Ga}$ and $Cd_{Ga}$ are deep acceptors
($\sim$0.65 eV); $Si_N$ is a midgap trap with high formation energy; finally,
contrary to recent claims, $C_N$ is a deep acceptor (0.65 eV). Interstitials
and heteroantisites are energetically not competitive with substitutional
incorporation.

<id>
cond-mat/9610046v1
<category>
cond-mat.mtrl-sci
<abstract>
The formation energy of a solid surface can be extracted from slab
calculations if the bulk energy per atom is known. It has been pointed out
previously that the resulting surface energy will diverge with slab thickness
if the bulk energy is in error, in the context of calculations which used
different methods to study the bulk and slab systems. We show here that this
result is equally relevant for state-of-the-art computational methods which
carefully treat bulk and slab systems in the same way. Here we compare
different approaches, and present a solution to the problem that eliminates the
divergence and leads to rapidly convergent and accurate surface energies.

<id>
cond-mat/9610061v1
<category>
cond-mat.mtrl-sci
<abstract>
The noise in the depth profiles of secondary ion mass spectrometry (SIMS) is
studied using different samples under various experimental conditions. Despite
the noise contributions from various parts of the dynamic SIMS process, its
overall character agrees very well with the Poissonian rather than the Gaussian
distribution in all circumstances. The Poissonian relation between the measured
mean-square error (MSE) and mean can be used to describe our data in the range
of four orders. The departure from this relation at high counts is analyzed and
found to be due to the saturation of the channeltron used. Once saturated, the
detector was found to exhibit hysteresis between rising and falling input flux
and output counts.

<id>
cond-mat/9610108v2
<category>
cond-mat.mtrl-sci
<abstract>
Ab initio calculations predict that Be is a shallow acceptor in GaN. Its
thermal ionization energy is 0.06 eV in wurtzite GaN; the level is valence
resonant in the zincblende phase. Be incorporation is severely limited by the
formation of Be_3N_2. We show however that co-incorporation with reactive
species can enhance the solubility. H-assisted incorporation should lead to
high doping levels in MOCVD growth after post-growth annealing at about 850 K.
Be-O co-incorporation produces high Be and O concentrations at MBE growth
temperatures.

<id>
cond-mat/9610111v2
<category>
cond-mat.mtrl-sci
<abstract>
Extrinsic levels, formation energies, and relaxation geometries are
calculated ab initio for oxygen vacancies in alpha-quartz SiO2. The vacancy is
found to be thermodynamically stable in the charge states Q=+3, Q=0, Q=--2, and
Q=-3. The charged states are stabilized by large and asymmetric distortions
near the vacancy site. Concurrently, Franck-Condon shifts for absorption and
recombination related to these states are found to be strongly asymmetric. In
undoped quartz, the ground state of the vacancy is the neutral charge state,
while for moderate p-type and n-type doping, the +3 and -3 states are favored,
respectively, over a wide Fermi level window. Optical transitions related to
the vacancy are predicted at around 3 eV and 6.5 eV (absorption) and 2.5 to 3.0
eV (emission), depending on the charge state of the ground state.

<id>
cond-mat/9610118v2
<category>
cond-mat.mtrl-sci
<abstract>
We present a statistical fragmentation study of doubly charged alkali (Li,
Na, K) and antimony clusters. The evaporation of one charged trimer is the most
dominant decay channel (asymmetric fission) at low excitation energies. For
small sodium clusters this was quite early found in molecular dynamical
calculations by Landman et al. For doubly charged lithium clusters, we predict
Li$_{9}^{+}$ to be the preferential dissociation channel. As already seen
experimentally a more symmetric fission is found for doubly charged antimony
clusters. This different behavior compared to the alkali metal clusters is in
our model essentially due to a larger fissility of antimony. This is checked by
repeating the calculations for Na$_{52}^{++}$ with a bulk fissility parameter
set artificially equal to the value of Sb.

<id>
cond-mat/9610130v1
<category>
cond-mat.mtrl-sci
<abstract>
Core-level shifts and core-hole screening effects in alloy formation are
studied ``ab initio'' by constrained-density-functional total-energy
calculations. For our case study, the ordered intermetallic alloy MgAu,
final-state effects are essential to account for the experimental Mg 1s shift,
while they are negligible for Au 4f. We explain the differences in the
screening by analyzing the calculated charge density response to the core hole
perturbation.

<id>
cond-mat/9610145v1
<category>
cond-mat.mtrl-sci
<abstract>
The fatigue fracture surfaces of a metallic alloy, and the stress corrosion
fracture surfaces of glass are investigated as a function of crack velocity. It
is shown that in both cases, there are two fracture regimes, which have a well
defined self-affine signature. At high enough length scales, the universal
roughness index 0.78 is recovered. At smaller length scales, the roughness
exponent is close to 0.50. The crossover length $\xi_c$ separating these two
regimes strongly depends on the material, and exhibits a power-law decrease
with the measured crack velocity $\xi_c \propto v^{-\phi}$, with $\phi \simeq
1$. The exponents $\nu$ and $\beta$ characterising the dependence of $\xi_c$
and $v$ upon the pulling force are shown to be close to $\nu \simeq 2$ and
$\beta \simeq 2$.

<id>
cond-mat/9610149v2
<category>
cond-mat.mtrl-sci
<abstract>
The cubic-to-orthorhombic structural transition occurring in CsH at a
pressure of about 17 GPa is studied by ab initio calculations. The relative
stability of the competing structures and the transition pressure are correctly
predicted. We show that this pressure-induced first-order transition is
intimately related to a displacive second-order transition which would occur
upon application of a shear strain to the (110) planes. The resulting
instability is rationalized in terms of the pressure-induced modifications of
the electronic structure.

<id>
cond-mat/9610154v1
<category>
cond-mat.mtrl-sci
<abstract>
Physical properties of alloys are compared as computed from ``direct'' and
``inverse'' procedures. The direct procedure involves Monte Carlo simulations
of a set of local density approximation (LDA)-derived pair and multibody
interactions {\nu_f}, generating short-range order (SRO), ground states, order-
disorder transition temperatures, and structural energy differences. The
inverse procedure involves ``inverting'' the SRO generated from {\nu_f} via
inverse-Monte-Carlo to obtain a set of pair only interactions {\tilde{\nu}_f}.
The physical properties generated from {\tilde{\nu}_f} are then compared with
those from {\nu_f}. We find that (i) inversion of the SRO is possible (even
when {\nu_f} contains multibody interactions but {\tilde{\nu}_f} does not) but,
(ii) the resulting interactions {\tilde{\nu}_f} agree with the input
interactions {\nu_f} only when the problem is dominated by pair interactions.
Otherwise, {\tilde{\nu}_f} are very different from {\nu_f}. (iii) The same SRO
pattern can be produced by drastically different sets {\nu_f}. Thus, the
effective interactions deduced from inverting SRO are not unique. (iv)
Inverting SRO always misses configuration-independent (but composition-
dependent) energies such as the volume deformation energy G(x); consequently,
the ensuing {\tilde{\nu}_f} cannot be used to describe formation enthalpies or
two-phase regions of the phase diagram, which depend on G(x).

<id>
cond-mat/9610183v2
<category>
cond-mat.mtrl-sci
<abstract>
We present a systematical method for obtaining analytical long-range
embedded-atom potentials based on the lattice-inversion method. The potentials
converge faster (exponentially) than Sutton and Chen's power-law potentials
(Philos. Mag. Lett. 61, 2480(1990)). An interesting relationship between the
embedded-atom method and the universal binding energy equation of Rose et al.
(Phys. Rev. B 29, 2963 (1984)) is also pointed out. The potentials are tested
by calculating the elastic constants, phonon dispersions, phase stabilities,
surface properties and melting temperatures of the fcc transition metals.

<id>
cond-mat/9610197v1
<category>
cond-mat.mtrl-sci
<abstract>
Using the recently-proposed ``activation-relaxation technique'' for
optimizing complex structures, we develop a structural model appropriate to
a-GaAs which is almost free of odd-membered rings, i.e., wrong bonds, and
possesses an almost perfect coordination of four. The model is found to be
superior to structures obtained from much more computer-intensive tight-binding
or quantum molecular-dynamics simulations. For the elemental system a-Si, where
wrong bonds do not exist, the cost in elastic energy for removing odd-membered
rings is such that the traditional continuous-random network is appropriate.
Our study thus provides, for the first time, direct information on the nature
of intermediate-range topology in amorphous tetrahedral semiconductors.

<id>
cond-mat/9611015v1
<category>
cond-mat.mtrl-sci
<abstract>
A rigorous connection is established between the local porosity entropy
introduced by Boger et al. (Physica A 187, 55 (1992)) and the configurational
entropy of Andraud et al. (Physica A 207, 208 (1994)). These entropies were
introduced as morphological descriptors derived from local volume fluctuations
in arbitrary correlated microstructures occuring in porous media, composites or
other heterogeneous systems. It is found that the entropy lengths at which the
entropies assume an extremum become identical for high enough resolution of the
underlying configurations. Several examples of porous and heterogeneous media
are given which demonstrate the usefulness and importance of this morphological
local entropy concept.

<id>
cond-mat/9611029v1
<category>
cond-mat.mtrl-sci
<abstract>
Lattice constants and bulk moduli of eleven cubic III-V semiconductors are
calculated using an ab initio scheme. Correlation contributions of the valence
electrons, in particular, are determined using increments for localized bonds
and for pairs and triples of such bonds; individual increments, in turn, are
evaluated using the coupled cluster approach with single and double
excitations. Core-valence correlation is taken into account by means of a core
polarization potential. Combining the results at the correlated level with
corresponding Hartree-Fock data, we obtain lattice constants which agree with
experiment within an average error of -0.2%; bulk moduli are accurate to +4%.
We discuss in detail the influence of the various correlation contributions on
lattice constants and bulk moduli.

<id>
cond-mat/9611030v1
<category>
cond-mat.mtrl-sci
<abstract>
Assembling clusters on surfaces has emerged as a novel way to grow thin films
with targeted properties. In particular, it has been proposed from experimental
findings that fullerenes deposited on surfaces could give rise to thin films
retaining the bonding properties of the incident clusters. However the
microscopic structure of such films is still unclear. By performing quantum
molecular dynamics simulations, we show that C_28 fullerenes can be deposited
on a surface to form a thin film of nearly defect free molecules, which act as
carbon superatoms. Our findings help clarify the structure of disordered small
fullerene films and also support the recently proposed hyperdiamond model for
solid C_28.

<id>
cond-mat/9611055v1
<category>
cond-mat.mtrl-sci
<abstract>
We present a theory for the ab-initio computation of NMR chemical shifts
(sigma) in condensed matter systems, using periodic boundary conditions. Our
approach can be applied to periodic systems such as crystals, surfaces, or
polymers and, with a super-cell technique, to non-periodic systems such as
amorphous materials, liquids, or solids with defects. We have computed the
hydrogen sigma for a set of free molecules, for an ionic crystal, LiH, and for
a H-bonded crystal, HF, using density functional theory in the local density
approximation. The results are in excellent agreement with experimental data.

<id>
cond-mat/9611093v2
<category>
cond-mat.mtrl-sci
<abstract>
The effects of mobility of small islands on island growth in molecular beam
epitaxy are studied. It is shown that small island mobility affects both the
scaling and morphology of islands during growth. Three microscopic models are
considered, in which the critical island sizes are $i^*=1,2$ and 3 (such that
islands of size $s \le i^*$ are mobile while islands of size $s \ge i^{\ast}+1$
are immobile). As i^* increases, islands become more compact, while the
exponent $\gamma$ which relates the island density to deposition rate
increases. The morphological changes are quantified by using fractal analysis.
It is shown that the fractal dimensions are rather insensitive to changes in
i^*. However, the prefactors provide a quantitative measure of the changing
morphologies.

<id>
cond-mat/9611095v2
<category>
cond-mat.mtrl-sci
<abstract>
A model that describes self diffusion, island nucleation and film growth on
FCC(001) metal substrates is presented. The parameters of the model are
optimized to describe Cu diffusion on Cu(001), by comparing activation energy
barriers to a full set of barriers obtained from semi-empirical potentials via
the embedded atom method. It is found that this model (model I), with only
three parameters, provides a very good description of the full landscape of
hopping energy barriers. These energy barriers are grouped in four main peaks.
A reduced model (model II) with only two parameters, is also presented, in
which each peak is collapsed into a single energy value. From the results of
our simulations, we find that this model still maintains the essential features
of diffusion and growth on this model surface. We find that hopping rates along
island edges are much higher than for isolated atoms (giving rise to compact
island shapes) and that vacancy mobility is higher than adatom mobility. We
observe substantial dimer mobility (comparable to the single atom mobility) as
well as some mobility of trimers. Mobility of small islands affects the scaling
of island density $N$ vs. deposition rate $F$, $N ~ F^\gamma$, as well as the
island size distribution. In the asymptotic limit of slow deposition, scaling
arguments and rate equations show that $\gamma = i*/(2 i* + 1)$ where $i*$ is
the size of the largest mobile island. Our Monte Carlo results, obtained for a
range of experimentally relevant conditions, show $\gamma = 0.32$ for the EAM,
0.33 for model I and 0.31 for model II barriers. These results are lower than
the anticipated $\gamma >= 0.4$ due to dimer (and trimer) mobility.

<id>
cond-mat/9611107v1
<category>
cond-mat.mtrl-sci
<abstract>
We propose a minimal nonlinear model of brittle crack propagation by
considering only the motion of the crack-tip atom. The model captures many
essential features of steady-state crack velocity and is in excellent
quantitative agreement with many-body dynamical simulations. The model exhibits
lattice-trapping. For loads just above this, the crack velocity rises sharply,
reaching a limiting value well below that predicted by elastic continuum
theory. We trace the origin of the low limiting velocity to the anharmonicity
of the potential well experienced by the crack-tip atom.

<id>
cond-mat/9611119v1
<category>
cond-mat.mtrl-sci
<abstract>
In the present paper, the dielectric behaviour of the
PbLixNb3xZr0.51Ti0.49-4xO3 ( PLNZT) system is analyzed. It is shown that the
decrease in the dielectric constant with respect to an increase of the impurity
content may be due to an aggregation trend of the interacting off-centre
dipoles rather than due to cooperative phenomena which renormalizes the dipole
moment of the off-centre impurity.

<id>
cond-mat/9906222v3
<category>
cond-mat.other
<abstract>
The high-precision data for the specific heat C_{V}(T,V) of normal-liquid
Helium-3 obtained by Greywall, taken together with the molar volume V(T_0,P) at
one temperature T_0, are shown to contain the complete thermodynamic
information about this phase in zero magnetic field. This enables us to
calculate the T and P dependence of all equilibrium properties of normal-liquid
Helium-3 in a thermodynamically consistent way for a wide range of parameters.
The results for the entropy S(T,P), specific heat at constant pressure
C_P(T,P), molar volume V(T,P), compressibility kappa(T,P), and thermal
expansion coefficient alpha(T,P) are collected in the form of figures and
tables. This provides the first complete set of thermodynamically consistent
values of the equilibrium quantities of normal-liquid Helium-3. We find, for
example, that alpha(T,P) has a surprisingly intricate pressure dependence at
low temperatures, and that the curves alpha(T,P) vs T do not cross at one
single temperature for all pressures, in contrast to the curves presented in
the comprehensive survey of helium by Wilks.
  Corrected in cond-mat/9906222v3: The sign of the coefficient d_0 was
misprinted in Table I of cond-mat/9906222v1 and v2. It now correctly reads
d_0=-7.1613436. All results in the paper were obtained with the correct value
of d_0. (We would like to thank for E. Collin, H. Godfrin, and Y. Bunkov for
finding this misprint.)

<id>
cond-mat/0203049v1
<category>
cond-mat.other
<abstract>
We show that the conclusions of a recent PRL by Pu et al is incorrect.

<id>
cond-mat/0303055v2
<category>
cond-mat.other
<abstract>
We revisit the mean-field treatment of photoassociation and Feshbach
resonances in a Bose-Einstein condensate previously used by various contributors.
Generalizing the Cherny and Shanenko approach (Phys. Rev. E 62, 1646-59 (2000)
) where the finite size of the potentials is explicitly introduced, we develop
a two-channel model for a mixed atomic-molecular condensate. Besides the
individual dynamics of the condensed and non-condensed atoms, the model also
takes into account their pair dynamics by means of pair wave functions. We show
that the resulting set of coupled equations can be reduced to the usual coupled
Gross-Pitaevskii equations when the time scale of the pair dynamics is short
compared to that of the individual dynamics. Such time scales are discussed in
the case of typical photoassociation experiments with cw lasers. We show that
the individual dynamics plays a minor role, demonstrating the validity of the
rates predicted by the usual models describing photoassociation in a
nondegenerate gas.

<id>
cond-mat/0304248v2
<category>
cond-mat.other
<abstract>
We investigate the phonon thermal conductivity $\kappa_{\mathrm{ph}}$ of
doped $\rm La_2CuO_4$ based on out-of-plane thermal conductivity measurements.
When room temperature is approached the temperature dependence of
$\kappa_{\mathrm{ph}}$ strongly deviates from the $T^{-1}$-decrease which is
usually expected for heat transport by acoustic phonons. Instead,
$\kappa_{\mathrm{ph}}$ decreases much weaker or even increases with rising
temperature. Simple arguments suggest that such unusual temperature
dependencies of $\kappa_{\mathrm{ph}}$ are caused by heat transport via
dispersive optical phonons.

<id>
cond-mat/0304358v2
<category>
cond-mat.other
<abstract>
We present a detailed experimental study of the velocity distribution of
atoms cooled in an optical lattice. Our results are supported by full-quantum
numerical simulations. Even though the Sisyphus effect, the responsible cooling
mechanism, has been used extensively in many cold atom experiments, no detailed
study of the velocity distribution has been reported previously. For the
experimental as well as for the numerical investigation, it turns out that a
Gaussian function is not the one that best reproduce the data for all
parameters. We also fit the data to alternative functions, such as Lorentzians,
Tsallis functions and double Gaussians. In particular, a double Gaussian
provides a more precise fitting to our results.

<id>
cond-mat/0305060v4
<category>
cond-mat.other
<abstract>
The production of pairs of fast atoms leads to a pronounced loss of atoms
during upward ramps of Feshbach resonance levels in dilute Bose-Einstein
condensates. We provide comparative studies on the formation of these bursts of
atoms containing the physical predictions of several theoretical approaches at
different levels of approximation. We show that despite their very different
description of the microscopic binary physics during the passage of a Feshbach
resonance, all approaches lead to virtually the same prediction on the total
loss of condensate atoms, provided that the ramp of the magnetic field strength
is purely linear. We give the reasons for this remarkable insensitivity of the
remnant condensate fraction to the microscopic physical processes and compare
the theoretical predictions with recent Feshbach resonance crossing experiments
on 23Na and 85Rb.

<id>
cond-mat/0305363v3
<category>
cond-mat.other
<abstract>
We study the evolution of diffuse elastodynamic spectral energy density under
the influence of weak nonlinearity. It is shown that the rate of change of this
quantity is given by a convolution of the linear energy at two frequencies.
Quantitative estimates are given for sample aluminum and fused silica blocks of
experimental interest.

<id>
cond-mat/0306180v3
<category>
cond-mat.other
<abstract>
A mixture of ultracold bosons and fermions placed in an optical lattice
constitutes a novel kind of quantum gas, and leads to phenomena, which so far
have been discussed neither in atomic physics, nor in condensed matter physics.
We discuss the phase diagram at low temperatures, and in the limit of strong
atom-atom interactions, and predict the existence of quantum phases that
involve pairing of fermions with one or more bosons, or, respectively, bosonic
holes. The resulting composite fermions may form, depending on the system
parameters, a normal Fermi liquid, a density wave, a superfluid liquid, or an
insulator with fermionic domains. We discuss the feasibility for observing such
phases in current experiments.

<id>
cond-mat/0306718v2
<category>
cond-mat.other
<abstract>
The characteristics of an imaging system formed by a left-handed material
(LHM) slab of finite length are studied, and the influence of the finite length
of the slab on the image quality is analyzed. Unusual phenomena such as surface
bright spots and negative energy stream at the image side are observed and
explained as the cavity effects of surface plasmons excited by the evanescent
components of the incident field. For a thin LHM slab, the cavity effects are
found rather sensitive to the length of the slab, and the bright spots on the
bottom surface of the slab may stretch to the image plane and degrade the image
quality.

<id>
cond-mat/0307014v2
<category>
cond-mat.other
<abstract>
A magnetic bipolar transistor is a bipolar junction transistor with one or
more magnetic regions, and/or with an externally injected nonequilibrium
(source) spin. It is shown that electrical spin injection through the
transistor is possible in the forward active regime. It is predicted that the
current amplification of the transistor can be tuned by spin.

<id>
cond-mat/0307635v2
<category>
cond-mat.other
<abstract>
We determine the phase diagram of a mixture of ultracold bosons and polarized
fermions placed in an optical lattice using mean field theory. In the limit of
strong atom-atom interactions, there exist quantum phases that involve pairing
of fermions with one or more bosons, or bosonic holes, respectively. We obtain
the analytic form of the phase boundaries separating these composite fermion
phases from the bosonic superfluid coexisting with Fermi liquid. We compare the
results with numerical simulations and discuss their validity and relevance for
current experiments.

<id>
cond-mat/0308341v3
<category>
cond-mat.other
<abstract>
We consider a single quantum particle in a spherical box interacting with a
fixed scatterer at the center, to construct a model of a degenerate atomic
Fermi gas close to a Feshbach resonance. One of the key predictions of the
model is the existence of two branches for the macroscopic state of the gas, as
a function of the magnetic field controlling the value of the scattering
length.This model is able to draw a qualitative picture of all the different
features recently observed in a degenerate atomic Fermi gas close to the
resonance, even in the unitary limit.

<id>
cond-mat/0308475v2
<category>
cond-mat.other
<abstract>
We analyze a quantum computer (QC) design based on nuclear spin qubits in a
quasi-one-dimensional (1D) chain of non-Kramers doublet atoms. We explore the
use of spatial symmetry breaking to obtain control over the local dynamics of a
qubit. We also study the decoherence mechanisms at the single qubit level and
the interactions mediated by the magnetic media. The design can be realized in
$\textrm{PrBr}_{\textrm{3}-x}\textrm{F}_{x}$ with nuclear magnetic resonance
(NMR) techniques.

<id>
cond-mat/0308624v2
<category>
cond-mat.other
<abstract>
The properties of scale-free random trees are investigated using both
preconditioning on non-extinction and fixed size averages, in order to study
the thermodynamic limit. The scaling form of volume probability is found, the
connectivity dimensions are determined and compared with other exponents which
describe the growth. The (local) spectral dimension is also determined, through
the study of the massless limit of the Gaussian model on such trees.

<id>
cond-mat/0309141v2
<category>
cond-mat.other
<abstract>
Large but rare cascades triggered by small initial shocks are present in most
of the infrastructure networks. Here we present a simple model for cascading
failures based on the dynamical redistribution of the flow on the network. We
show that the breakdown of a single node is sufficient to collapse the
efficiency of the entire system if the node is among the ones with largest
load. This is particularly important for real-world networks with an highly
hetereogeneous distribution of loads as the Internet and electrical power
grids.

<id>
cond-mat/0309212v2
<category>
cond-mat.other
<abstract>
We construct a many-body Gaussian variational approach for the
two-dimensional trapped Bose gas in the condensate phase. Interaction between
particles is modelized by a generalized pseudo-potential of zero range that
allows recovering perturbative results in the ultra-dilute limit, while back
action of non-condensate particles on the condensate part is taken into account
for higher density. As an application, we derive the equation of state and
solve stability problems encountered in similar mean-field formalisms for a
single vortex configuration.

<id>
cond-mat/0309318v3
<category>
cond-mat.other
<abstract>
We report on the observation of a mixed spin channel Feshbach resonance at
the low magnetic field value of (9.09 +/- 0.01) G for a mixture of |2,-1> and
|1,+1> states in 87Rb. This mixture is important for applications of
multi-component BECs of 87Rb, e.g. in spin mixture physics and for quantum
entanglement. Values for position, height and width of the resonance are
reported and compared to a recent theoretical calculation of this resonance.

<id>
cond-mat/0311141v2
<category>
cond-mat.other
<abstract>
Dark solitons have been observed in optical systems (optical fibres,
dielectric guides and bulk media), and, more recently, in harmonically confined
atomic Bose-Einstein condensates. This paper presents an overview of some of
the common features and analogies experienced by these two intrinsically
nonlinear systems, with emphasis on the stability of dark solitons in such
systems and their decay via emission of radiation. The closely related issue of
vortex dynamics in such systems is also briefly discussed.

<id>
cond-mat/0312214v2
<category>
cond-mat.other
<abstract>
We demonstrate experimentally an unusual behavior of the parametric polariton
scattering in semiconductor microcavity under a strong cw resonant excitation.
The maximum of the scattered signal above the threshold of stimulated
parametric scattering does not shift along the microcavity lower polariton
branch with the change of pump detuning or angle of incidence but is stuck
around the normal direction. We show theoretically that such a behavior can be
modelled numerically by a system of Maxwell and nonlinear Schroedinger
equations for cavity polaritons and explained via the competition between the
bistability of a driven nonlinear MC polariton and the instabilities of
parametric polariton-polariton scattering.

<id>
cond-mat/0312396v2
<category>
cond-mat.other
<abstract>
This paper discusses the feasibility of experimental control of the flow
direction of atomic Bose-Einstein condensates in a double-well potential using
phase-imprinting. The flow is induced by the application of a time-dependent
potential gradient, providing a clear signature of macroscopic quantum
tunneling in atomic condensates. By studying both initial state preparation and
subsequent tunneling dynamics we find the parameters to optimise the phase
induced Josephson current. We find that the effect is largest for condensates
up to a few thousand atoms, and is only weakly-dependent on trap geometry.

<id>
cond-mat/0312398v2
<category>
cond-mat.other
<abstract>
We present an approximate analytical solution of the Cahn-Hilliard equation
describing the coalescence during a first order phase transition. We have
identified all the intermediate profiles, stationary solutions of the noiseless
Cahn-Hilliard equation. Using properties of the soliton lattices, periodic
solutions of the Ginzburg-Landau equation, we have construct a family of ansatz
describing continuously the processus of destabilization and period doubling
predicted in Langer's self similar scenario.

<id>
cond-mat/0312466v2
<category>
cond-mat.other
<abstract>
Ferromagnetic electrodes of a lateral semiconductor-based spin-valve
structure are designed to provide a maximum of spin-polarized injection
current. A single-domain state in remanence is a prerequisite obtained by
nanostructuring Permalloy thin film electrodes. Three regimes of aspect ratios
$m$ are identified by room temperature magnetic force microscopy: (i)
high-aspect ratios of $m \ge 20$ provide the favored remanent single-domain
magnetization states, (ii) medium-aspect ratios $m \sim 3$ to $m \sim 20$ yield
highly remanent states with closure domains and (iii) low-aspect ratios of $m
\le 3$ lead to multi-domain structures. Lateral kinks, introduced to bridge the
gap between micro- and macroscale, disturb the uniform magnetization of
electrodes with high- and medium-aspect ratios. However, vertical flanks help
to maintain a uniformly magnetized state at the ferromagnet-semiconcuctor
contact by domain wall pinning.

<id>
cond-mat/0312492v3
<category>
cond-mat.other
<abstract>
Pairs of trapped atoms can be associated to make a diatomic molecule using a
time dependent magnetic field to ramp the energy of a scattering resonance
state from above to below the scattering threshold. A relatively simple model,
parameterized in terms of the background scattering length and resonance width
and magnetic moment, can be used to predict conversion probabilities from atoms
to molecules. The model and its Landau-Zener interpretation are described and
illustrated by specific calculations for $^{23}$Na, $^{87}$Rb, and $^{133}$Cs
resonances. The model can be readily adapted to Bose-Einstein condensates.
Comparison with full many-body calculations for the condensate case show that
the model is very useful for making simple estimates of molecule conversion
efficiencies.

<id>
cond-mat/0312656v4
<category>
cond-mat.other
<abstract>
We study the height one, two, three, and four variables in the Abelian
sandpile model. We argue that correlation functions along closed boundaries, as
well as general conformal field theory principles, show that the four variables
are not represented by the same operator along closed boundaries, or in the
bulk. Along open boundaries, we calculate all n-point correlations, and find
that there, all height variables are represented by the same operator. We
introduce dissipative defect points, and show that along open boundaries they
are represented by the same operator as the height variables.

<id>
cond-mat/0401036v3
<category>
cond-mat.other
<abstract>
We report an insulator-metal-insulator transition in films of five metal
phthalocyanines (MPc) doped with alkali atoms. Electrical conduction
measurements demonstrate that increasing the alkali concentration results in
the formation of a metallic state for all systems. Upon further doping, the
films reenter the insulating state. Structural and Raman spectroscopy studies
reveal the formation of new crystalline phases upon doping and are consistent
with the phenomena originating from charge transfer between the intercalated
alkali atoms and MPc, in a similar fashion to what has been so far observed
only in C60. Due to the presence of a molecular spin, large exchange energy,
and a two-fold orbital degeneracy in MPc, our findings are of interest in the
study of controllable magnetism in molecular materials and in the investigation
of new, recently predicted electronic phases.

<id>
cond-mat/0401037v2
<category>
cond-mat.other
<abstract>
We study the dependence of the electronic density of states (DOS) on the
distance from the boundary for a ferromagnet/superconductor bilayer. We
calculate the electron density of states in such structure taking into account
the two-band model of the ferromagnet (FM) with conducting s and localized d
electrons and a simple s-wave superconductor (SC). It is demonstrated that due
to the electron s-d scattering in the ferromagnetic layer in the third order of
s-d scattering parameter the oscillation of the density of states has larger
period and more drastic decrease in comparison with the oscillation period for
the electron density of states in the zero order.

<id>
cond-mat/0401109v2
<category>
cond-mat.other
<abstract>
We demonstrate a reversible conversion of a Li2 molecular Bose-Einstein
condensate to a degenerate Fermi gas of atoms by adiabatically crossing a
Feshbach resonance. By optical in situ imaging, we observe a smooth change of
the cloud size in the crossover regime. On the Feshbach resonance, the ensemble
is strongly interacting and the measured cloud size is 75(7)% of the one of a
non-interacting zero-temperature Fermi gas. The high condensate fraction of
more than 90% and the adiabatic crossover suggest our Fermi gas to be cold
enough to form a superfluid.

<id>
cond-mat/0401113v2
<category>
cond-mat.other
<abstract>
We perform a study of the effect of the high in-plane electric field on the
spin precession and spin dephasing due to the D'yakonov-Perel' mechanism in
$n$-type GaAs (100) quantum wells by constructing and numerically solving the
kinetic Bloch equations. We self-consistently include all of the scattering
such as electron-phonon, electron-non-magnetic impurity as well as the
electron-electron Coulomb scattering in our theory and systematically
investigate how the spin precession and spin dephasing are affected by the high
electric field under various conditions. The hot-electron distribution
functions and the spin correlations are calculated rigorously in our theory. It
is found that the D'yakonov-Perel' term in the electric field provides a
non-vanishing effective magnetic field that alters the spin precession period.
Moreover, spin dephasing is markedly affected by the electric field. The
important contribution of the electron-electron scattering to the spin
dephasing is also discussed.

<id>
cond-mat/0401156v2
<category>
cond-mat.other
<abstract>
The ability to localize, identify and measure the electronic environment of
individual atoms will provide fundamental insights into many issues in
materials science, physics and nanotechnology. We demonstrate, using an
aberration-corrected scanning transmission microscope, the spectroscopic
imaging of single La atoms inside CaTiO3. Dynamical simulations confirm that
the spectroscopic information is spatially confined around the scattering atom.
Furthermore we show how the depth of the atom within the crystal may be
estimated.

<id>
cond-mat/0401217v2
<category>
cond-mat.other
<abstract>
We report an experiment measuring simultaneously the temperatureand the flux
of ions produced by a cloud of triplet metastablehelium atoms at the
Bose-Einstein critical temperature. The onsetof condensation is revealed by a
sharp increase of the ion fluxduring evaporative cooling. Combining our
measurements withprevious measurements of ionization in a pure BEC,we extract
an improved value of the scattering length$a=11.3^{+2}_{-1}$ nm. The analysis
includes corrections takinginto accountthe effect of atomic interactions on the
criticaltemperature, and thus an independent measurement of the
scatteringlength would allow a new test of these calculations.

<id>
cond-mat/9512157v4
<category>
cond-mat.soft
<abstract>
We present a Monte Carlo simulation of a polymer nematic for varying volume
fractions, concentrating on the structure function of the sample. We achieve
nematic ordering with stiff polymers made of spherical monomers that would
otherwise not form a nematic state. Our results are in good qualitative
agreement with theoretical and experimental predictions, most notably the
bowtie pattern in the static structure function.

<id>
cond-mat/9606111v3
<category>
cond-mat.soft
<abstract>
We study the power spectrum of a class of noise effects generated by means of
a digital-like disorder in the traveling variable of the conjectured
Ginzburg-Landau-Montroll kink excitations moving along the walls of the
microtubules. We have found a 1/f^{\alpha} noise with \alpha \in (1.82-2.04) on
all the time scales we have considered

<id>
cond-mat/9609125v2
<category>
cond-mat.soft
<abstract>
We consider the tilted-hexatic Hamiltonian on the fluctuating membranes. A
renormalization-group analysis leads us to find two critical regions; one
corresponds to the strong coupling regime of the gradient cross coupling, the
other to the weak coupling regime. In the strong coupling regime, we find the
locked tilted-hexatic to liquid phase transition when disclinations and
vortices unbind simultaneously. On the other hand, in the weak coupling regime
we find four phases; the unlocked tilted-hexatic phase, the hexatic phase, the
tilted phase, and liquid phase. Disclinations and vortices unbind
independently. The crinkled-to-crumpled transition of the fluctuating
tilted-hexatic membranes is also described. The crinkled phase in the strong
coupling regime is stiffer than that in the weak coupling regime.

<id>
cond-mat/9610042v1
<category>
cond-mat.soft
<abstract>
A nonlinear two-fluid stochastic hydrodynamical description of velocity and
concentration fluctuations in sedimenting suspensions is constructed, and
analyzed using self-consistent (SC) and renormalization group (RG) methods. The
advection of particles by velocity fluctuations is shown to be ``relevant'' in
all dimensions $d < 6$ . Both RG and SC analyses predict a strong reduction in
the dependence of velocity fluctuations on system-size $L$ relative to the
$L^{1/2}$ obtained in the linearized theory of Caflisch and Luke [Phys. Fluids
{\bf 28}, 785 (1985)]. This is an important step towards resolving a ten-year
old puzzle in the field.

<id>
cond-mat/9610048v1
<category>
cond-mat.soft
<abstract>
We study the mechanism of the `pearling' instability seen recently in
experiments on lipid tubules under a local applied laser intensity. We argue
that the correct boundary conditions are fixed chemical potentials, or surface
tensions \Sigma, at the laser spot and the reservoir in contact with the
tubule. We support this with a microscopic picture which includes the intensity
profile of the laser beam, and show how this leads to a steady-state flow of
lipid along the surface and gradients in the local lipid concentration and
surface tension (or chemical potential). This leads to a natural explanation
for front propagation and makes several predictions based on the tubule length.
While most of the qualitative conclusions of previous studies remain the same,
the `ramped' control parameter (surface tension) implies several new
qualitative results. We also explore some of the consequences of front
propagation into a noisy (due to pre-existing thermal fluctuations) unstable
medium.

<id>
cond-mat/9610069v1
<category>
cond-mat.soft
<abstract>
Lipid bilayer membranes below their main transition have two tilt order
parameters, corresponding to the two monolayers. These two tilts may be
strongly coupled to membrane shape but only weakly coupled to each other. We
discuss some implications of this observation for rippled and saddle phases,
bilayer tubules, and bicontinuous phases. Tilt difference introduces a length
scale into the elastic theory of tilted fluid membranes. It can drive an
instability of the flat phase; it also provides a simple mechanism for the
spontaneous breaking of inversion symmetry seen in some recent experiments.

<id>
cond-mat/9610082v1
<category>
cond-mat.soft
<abstract>
Recent work on the complete wetting transition has emphasized the role played
by the coupling of fluctuations of the order parameter at the wall and at the
depinning fluid interface. Extending this approach to the wetting transition
itself we predict a novel crossover effect associated with the decoupling of
fluctuations as the temperature is lowered towards the transition temperature
T_W. Using this we are able to reanalyse recent Monte-Carlo simulation studies
and extract a value \omega(T_W)=0.8 at T_W=0.9T_C in very good agreement with
long standing theoretical predictions.

<id>
cond-mat/9610090v1
<category>
cond-mat.soft
<abstract>
We study the problem of fluid adsorption at a non-planar wall with a view to
understanding the influence of surface roughness on the wetting transition.
Starting from an appropriate Landau-type free energy functional we develop a
linear response theory relating the free energy of the non-planar system to the
correlation functions in its planar counterpart. We generalize the well known
graphical construction method used to study the planar surface phase diagram
and derive analytical expressions for the shift in the phase boundary for first
and second-order wetting transitions. Of particular interest is the influence
of surface roughness on a second order wetting transition which is driven first
order, even for small deviations from the plane.

<id>
cond-mat/9610098v1
<category>
cond-mat.soft
<abstract>
We investigate the propagation of a suddenly applied tension along a
thermally excited semi-flexible polymer using analytical approximations,
scaling arguments and numerical simulation. This problem is inherently
non-linear. We find sub-diffusive propagation with a dynamical exponent of 1/4.
By generalizing the internal elasticity, we show that tense strings exhibit
qualitatively different tension profiles and propagation with an exponent of
1/2.

<id>
cond-mat/9610113v1
<category>
cond-mat.soft
<abstract>
We study the dynamics of a polymer when it is quenched from a $\theta$
solvent into a good or bad solvent by means of a Langevin equation. The
variation of the radius of gyration is studied as a function of time. For the
first stage of collapse or swelling, the characteristic time-scale is found to
be independent of the number of monomers. Other scaling laws are derived for
the diffusion regime at larger times. Although the present model is solved only
for homopolymers and doesn't include hydrodynamic interactions, these results
may be a first step towards the understanding of the early stages of protein
folding.

<id>
cond-mat/9610116v1
<category>
cond-mat.soft
<abstract>
Monte Carlo simulations of proteins are hindered by the system's high density
which often makes local moves ineffective. Here we devise and test a set of
long range moves that work well even when all sites of a lattice simulation are
filled. We demonstrate that for a 27-mer cube, the ground state of random
heteropolymers can quickly be reached. We discuss results for 48-mer systems
where the ground state is known exactly. For ten sequences that were examined,
the introduction of long range moves speeds up the search for the ground state
by about one order of magnitude. The method is compared to a fast folding chain
growth algorithm that had previously been used with much success. The new
algorithm here appears to be more efficient. The point is illustrated by the
folding of an 80-mer four-helix bundle considered previously.

<id>
cond-mat/9610124v1
<category>
cond-mat.soft
<abstract>
The Born effective charge Z^{*} and dielectric tensor \epsilon_{\infty} of
KNbO_3 are found to be very sensitive to the atomic geometry, changing by as
much as 27% between the paraelectric cubic and ferroelectric tetragonal and
rhombohedral phases. Subtracting the bare ionic contribution reveals changes of
the dynamic component of Z^{*} as large as 50%, for atomic displacements that
are typically only a few percent of the lattice constant. Z^{*},
\epsilon_{\infty} and all phonon frequencies at the Brillouin zone center were
calculated using the {\it ab initio} linearized augmented plane-wave linear
response method with respect to the reference cubic, experimental tetragonal,
and theoretically determined rhombohedral ground state structures. The ground
state rhombohedral structure of KNbO_3 was determined by minimizing the forces
on the relaxed atoms. By contrast with the cubic structure, all zone center
phonon modes of the rhombohedral structure are stable and their frequencies are
in good agreement with experiment. In the tetragonal phase, one of the soft
zone center modes in the cubic phase is stablized. In view of the small atomic
displacements involved in the ferroelectric transitions, it is evident that not
only the soft mode frequencies but also the Born effective charge and
dielectric constants are very sensitive to the atomic geometry.

<id>
cond-mat/9611021v2
<category>
cond-mat.soft
<abstract>
I propose a double-twist texture with local smectic order, which may have
been seen in recent experiments. As in the Renn-Lubensky TGB phase, the smectic
order is broken only through a lattice of screw dislocations. A melted lattice
of screw dislocations can produce a double-twist texture as can an unmelted
lattice. In the latter case I show that geometry only allows for certain angles
between smectic regions. I discuss the possibility of connecting these
double-twist tubes together to form a smectic blue phase.

<id>
cond-mat/9611039v1
<category>
cond-mat.soft
<abstract>
A new technique for the separation of macromolecules is proposed and
investigated. A thin mesh with pores comparable to the radius of gyration of a
free chain is used to filter chains according to their length. Without a field
it has previously been shown that the permeability decays as a power law with
chain length. However by applying particular configurations of pulsed fields,
it is possible to have a permeability that decays as an exponential. This
faster decay gives much higher resolution of separation. We also propose a
modified screen containing an array of holes with barb-like protrusions running
parallel to the surface. When static friction is present between the
macromolecule and the protrusion, some of the chains get trapped for long
durations of time. By using this and a periodic modulation of an applied
electric field, high resolution can be attained.

<id>
cond-mat/9611041v1
<category>
cond-mat.soft
<abstract>
We present a novel and rigorous approach to the Langevin dynamics of ideal
polymer chains subject to internal distance constraints. The permanent
constraints are modelled by harmonic potentials in the limit when the strength
of the potential approaches infinity (hard crosslinks). The crosslinks are
assumed to exist between arbitrary pairs of monomers. Formally exact
expressions for the resolvent and spectral density matrix of the system are
derived. To illustrate the method we study the diffusional behavior of monomers
in the vicinity of a single crosslink within the framework of the Rouse model.
The same problem has been studied previously by Warner (J. Phys. C: Solid State
Phys. {\bf 14}, 4985, (1981)) on the basis of Lagrangian multipliers. Here we
derive the full, hence exact, solution to the problem.

<id>
cond-mat/9611066v1
<category>
cond-mat.soft
<abstract>
We introduce a model to describe columnar phases of chiral discotic liquid
phases in which the normals to disc-like molecules are constrained to lie
parallel to columnar axes. The model includes separate chiral interactions
favoring, respectively, relative twist of chiral molecules along the axes of
the columns and twist of the two-dimensional columnar lattice. It also includes
a coupling between the lattice and the orientation of the discotic molecules.
We discuss the instability of the aligned hexagonal lattice phase to the
formation of a soliton lattice in which molecules twist within their columns
without affecting the lattice and to the formation of a moir\'{e} phase
consisting of a periodic array of twist grain boundaries perpendicular to the
columns.

<id>
cond-mat/9611075v1
<category>
cond-mat.soft
<abstract>
I consider the effects of polymers on the smectic phase of a host liquid
crystal matrix. Focusing on the regime in which the polymers are predominately
confined between the smectic layers, I find that the presence of the polymers
can lead to a reentrant phase diagram with the smectic-C sandwiching the
smectic-A phase from both the high and low temperature sides. Simple
entropy-energy arguments predict the shape of the reentrant phase boundary.

<id>
cond-mat/9611113v2
<category>
cond-mat.soft
<abstract>
Using a twisted nematic liquid crystal system exhibiting planar Ising model
dynamics, we have measured the scaling exponent $\theta$ which characterizes
the time evolution, $p(t) \sim t^{-\theta}$, of the probability p(t) that the
local order parameter has not switched its state by the time t. For 0.4 seconds
to 200 seconds following the phase quench, the system exhibits scaling behavior
and, measured over this interval, $\theta = 0.19 \pm 0.031$, in good agreement
with theoretical analysis and numerical simulations.

<id>
cond-mat/9611228v1
<category>
cond-mat.soft
<abstract>
We attribute similarities in the rheology of many soft materials (foams,
emulsions, slurries, etc.) to the shared features of structural disorder and
metastability. A generic model for the mesoscopic dynamics of ``soft glassy
matter'' is introduced, with interactions represented by a mean-field noise
temperature x. We find power law fluid behavior either with (x<1) or without
(1<x<2) a yield stress. For 1<x<2, both storage and loss modulus vary with
frequency as $\omega^{x-1}$, becoming flat near a glass transition (x=1).
Values of $x\approx 1$ may result from marginal dynamics as seen in some spin
glass models.

<id>
cond-mat/9612048v1
<category>
cond-mat.soft
<abstract>
We discuss the repton model of agarose gel electrophoresis of DNA. We review
previous results, both analytic and numerical, as well as presenting a new
numerical algorithm for the efficient simulation of the model, and suggesting a
new approach to the model's analytic solution.

<id>
cond-mat/9612102v1
<category>
cond-mat.soft
<abstract>
Critical effects at complete and critical wetting in three dimensions are
studied using a coupled effective Hamiltonian H[s(y),\ell]. The model is
constructed via a novel variational principle which ensures that the choice of
collective coordinate s(y) near the wall is optimal. We highlight the
importance of a new wetting parameter \Omega(T) which has a strong influence on
critical properties and allows the status of long-standing Monte-Carlo
simulation controversies to be re-examined.

<id>
cond-mat/9612152v4
<category>
cond-mat.soft
<abstract>
Two new general representations (the series and the integral) for the mass
current $\vj$ in weakly inhomogeneous superfluid A-phase of Helium--3 are
obtained near zero of temperature by solving the Dyson-Gorkov equation. These
representations result in additional correcting contribution to the standard
leading expression for $\vj$ which is of first order in gradients of the
orbital angular momentum vector $\hl$. The total supplementary term is found as
integral, and, provided the London limit holds, the procedure is advanced to
expand it at T=0 asymptotically by the Laplace method in powers of gradients of
$\hl$. Three special static orientations of $\hl$ with respect to its curl are
considered to calculate the higher correcting terms up to third order.
Coefficients at the quadratic terms are estimated numerically, new cubic
contributions are found which contain the logarithm of the London parameter.

<id>
cond-mat/9612169v2
<category>
cond-mat.soft
<abstract>
We consider a smectic-A* in a capillary with surface anchoring that favors
parallel alignment. If the bulk phase of the smectic is the standard
twist-grain-boundary phase of chiral smectics, then there will be a critical
radius below which the smectic will not have any topological defects. Above
this radius a single screw dislocation in the center of the capillary will be
favored. Along with surface anchoring, a magnetic field will also suppress the
formation of a screw dislocation. In this note, we calculate the critical field
at which a defect is energetically preferred as a function of the surface
anchoring strength and the capillary radius. Experiments at a few different
radii could thus determine the anchoring strength.

<id>
cond-mat/9612182v1
<category>
cond-mat.soft
<abstract>
In the last decade Diffusing Wave Spectroscopy (DWS) has emerged as a
powerful tool to study turbid media. In this article we develop the formalism
to describe light diffusion in general anisotropic turbid media. We give
explicit formulas to calculate the diffusion tensor and the dynamic absorption
coefficient, measured in DWS experiments. We apply our theory to uniaxial
systems, namely nematic liquid crystals, where light is scattered from thermal
fluctuations of the local optical axis, called director. We perform a detailed
analysis of the two essential diffusion constants, parallel and perpendicular
to the director, in terms of Frank elastic constants, dielectric anisotropy,
and applied magnetic field. We also point out the relevance of our results to
different liquid crystalline systems, such as discotic nematics, smectic-A
phases, and polymer liquid crystals. Finally, we show that the dynamic
absorption coefficient is the angular average over the inverse viscosity, which
governs the dynamics of director fluctuations.

<id>
cond-mat/9612222v1
<category>
cond-mat.soft
<abstract>
We present results of molecular dynamics simulations of strong, flexible
polyelectrolyte chains in solution with added salt. The effect of added salt on
the polyelectrolyte chain structure is fully treated for the first time as a
function of polymer density. Systems above and below the Manning condensation
limit are studied. The chain contraction due to added salt is weaker than
expected from simple screening arguments. The chain structure is intimately
tied to the ion density near the chain even for chains in the counterion
condensation regime. In contradiction to Manning counterion condensation
theory, the ion density near the polymer chain depends on the amount of added
salt, and above the condensation limit the chains significantly contract due to
added salt.

<id>
cond-mat/9701030v1
<category>
cond-mat.soft
<abstract>
We calculate the quantum tunneling rate of a ferromagnetic particle of $\sim
100 \AA$ diameter in a magnetic field of arbitrary angle. We consider the
magnetocrystalline anisotropy with the biaxial symmetry and that with the
tetragonal symmetry. Using the spin-coherent-state path integral, we obtain
approximate analytic formulas of the tunneling rates in the small $\epsilon
(=1- H/H_c)$-limit for the magnetic field normal to the easy axis ($\theta_H =
\pi/2$), for the field opposite to the initial easy axis ($\theta_H = \pi$),
and for the field at an angle between these two orientations ($\pi/2 <<
\theta_H << \pi$). In addition, we obtain numerically the tunneling rates for
the biaxial symmetry in the full range of the angle $\theta_H$ of the magnetic
field ($\pi/2 < \theta_H \leq \pi$), for the values of \epsilon =0.01 and
0.001.

<id>
cond-mat/9701054v2
<category>
cond-mat.soft
<abstract>
Domains in Langmuir monolayers support a texture that is the two-dimensional
version of the feature known as a boojum. Such a texture has a quantifiable
effect on the shape of the domain with which it is associated. The most
noticeable consequence is a cusp-like feature on the domain boundary. We report
the results of an experimental and theoretical investigation of the shape of a
domain in a Langmuir monolayer. A further aspect of the investigation is the
study of the shape of a ``bubble'' of gas-like phase in such a monolayer. This
structure supports a texture having the form of an inverse boojum. The
distortion of a bubble resulting from this texture is also studied. The
correspondence between theory and experiment, while not perfect, indicates that
a qualitative understanding of the relationship between textures and domain
shapes has been achieved.

<id>
cond-mat/9701110v2
<category>
cond-mat.soft
<abstract>
We show that the rate of separation of two phases of different densities
(e.g. gas and solid) can be radically altered by the presence of a metastable
intermediate phase (e.g. liquid). Within a Cahn-Hilliard theory we study the
growth in one dimension of a solid droplet from a supersaturated gas. A moving
interface between solid and gas phases (say) can, for sufficient (transient)
supersaturation, unbind into two interfaces separated by a slab of metastable
liquid phase. We investigate the criteria for unbinding, and show that it may
strongly impede the growth of the solid phase.

<id>
cond-mat/9701125v1
<category>
cond-mat.soft
<abstract>
The asymptotic frequency $\omega$, dependence of the dynamic viscosity of
neutral hard sphere colloidal suspensions is shown to be of the form $\eta_0
A(\phi) (\omega \tau_P)^{-1/2}$, where $A(\phi)$ has been determined as a
function of the volume fraction $\phi$, for all concentrations in the fluid
range, $\eta_0$ is the solvent viscosity and $\tau_P$ the P\'{e}clet time. For
a soft potential it is shown that, to leading order steepness, the asymptotic
behavior is the same as that for the hard sphere potential and a condition for
the cross-over behavior to $1/\omega \tau_P$ is given. Our result for the hard
sphere potential generalizes a result of Cichocki and Felderhof obtained at low
concentrations and agrees well with the experiments of van der Werff et al, if
the usual Stokes-Einstein diffusion coefficient $D_0$ in the Smoluchowski
operator is consistently replaced by the short-time self diffusion coefficient
$D_s(\phi)$ for non-dilute colloidal suspensions.

<id>
cond-mat/9701138v1
<category>
cond-mat.soft
<abstract>
We find analytical solutions to the Cahn-Hilliard equation for the dynamics
of an interface in a system with a conserved order parameter (Model B). We show
that, although steady-state solutions of Model B are unphysical in the
far-field, they shed light on the local dynamics of an interface. Exact
solutions are given for a particular class of order-parameter potentials, and
an expandable integral equation is derived for the general case. As well as
revealing some generic properties of interfaces moving under condensation or
evaporation, the formalism is used to investigate two distinct modes of
interface propagation in systems with a metastable potential well. Given a
sufficient transient increase in the flux of material onto a condensation
nucleus, the normal motion of the interface can be disrupted by interfacial
unbinding, leading to growth of a macroscopic amount of a metastable phase.

<id>
cond-mat/9301017v2
<category>
cond-mat.stat-mech
<abstract>
We give a strong evidence that noncrystalline materials such as quasicrystals
or incommensurate solids are not exceptions but rather are generic in some
regions of a phase space. We show this by constructing classical lattice-gas
models with translation-invariant, finite-range interactions and with a unique
quasiperiodic ground state which is stable against small perturbations of
two-body potentials. More generally, we provide a criterion for stability of
nonperiodic ground states.

<id>
cond-mat/9408076v4
<category>
cond-mat.stat-mech
<abstract>
The fractal structure of directed percolation clusters, grown at the
percolation threshold inside parabolic-like systems, is studied in two
dimensions via Monte Carlo simulations. With a free surface at y=\pm Cx^k and a
dynamical exponent z, the surface shape is a relevant perturbation when k<1/z
and the fractal dimensions of the anisotropic clusters vary continuously with
k. Analytic expressions for these variations are obtained using a blob picture
approach.

<id>
cond-mat/9408077v2
<category>
cond-mat.stat-mech
<abstract>
We investigate the influence of aperiodic perturbations on the critical
behaviour at a second order phase transition. The bond and site problems are
compared for layered systems and aperiodic sequences generated through
substitution. In the bond problem, the interactions between the layers are
distributed according to an aperiodic sequence whereas in the site problem, the
layers themselves follow the sequence. A relevance-irrelevance criterion
introduced by Luck for the bond problem is extended to discuss the site
problem. It involves a wandering exponent for pairs, which can be larger than
the one considered before in the bond problem. The surface magnetization of the
layered two-dimensional Ising model is obtained, in the extreme anisotropic
limit, for the period-doubling and Thue-Morse sequences.

<id>
cond-mat/9411077v2
<category>
cond-mat.stat-mech
<abstract>
Two-dimensional directed site percolation is studied in systems directed
along the x-axis and limited by a free surface at y=\pm Cx^k. Scaling
considerations show that the surface is a relevant perturbation to the local
critical behaviour when k<1/z where z=\nu_\parallel/\nu is the dynamical
exponent. The tip-to-bulk order parameter correlation function is calculated in
the mean-field approximation. The tip percolation probability and the fractal
dimensions of critical clusters are obtained through Monte-Carlo simulations.
The tip order parameter has a nonuniversal, C-dependent, scaling dimension in
the marginal case, k=1/z, and displays a stretched exponential behaviour when
the perturbation is relevant. The k-dependence of the fractal dimensions in the
relevant case is in agreement with the results of a blob picture approach.

<id>
cond-mat/9501059v2
<category>
cond-mat.stat-mech
<abstract>
The influence of a layered aperiodic modulation of the couplings on the
critical behaviour of the two-dimensional Ising model is studied in the case of
marginal perturbations. The aperiodicity is found to induce anisotropic
scaling. The anisotropy exponent z, given by the sum of the surface
magnetization scaling dimensions, depends continuously on the modulation
amplitude. Thus these systems are scale invariant but not conformally invariant
at the critical point.

<id>
cond-mat/9509063v2
<category>
cond-mat.stat-mech
<abstract>
We present the first rigorous examples of non-singular Hubbard models which
exhibit ferromagnetism at zero temperature. The models are defined in arbitrary
dimensions, and are characterized by finite-ranged hoppings, dispersive bands,
and finite on-site Coulomb interaction U. The picture, which goes back to
Heisenberg, that sufficiently large Coulomb interaction can revert Pauli
paramagnetism into ferromagnetism has finally been confirmed in concrete
examples.

<id>
cond-mat/9509069v2
<category>
cond-mat.stat-mech
<abstract>
The stability of antiferromagnetic long-range order against quenched disorder
is considered. A simple model of an antiferromagnet with a spatially varying
Neel temperature is shown to possess a nontrivial fixed point corresponding to
long-range order that is stable unless either the order parameter or the
spatial dimensionality exceeds a critical value. The instability of this fixed
point corresponds to the system entering a random-singlet phase. The
stabilization of long-range order is due to quantum fluctuations, whose role in
determining the phase diagram is discussed.

<id>
cond-mat/9510146v4
<category>
cond-mat.stat-mech
<abstract>
The quantum ferromagnetic transition of itinerant electrons is considered. It
is shown that the Landau-Ginzburg-Wilson theory described by Hertz and others
breaks down due to a singular coupling between fluctuations of the conserved
order parameter. This coupling induces an effective long-range interaction
between the spins of the form 1/r^{2d-1}. It leads to unusual scaling behavior
at the quantum critical point in $1<d\leq 3$ dimensions, which is determined
exactly.

<id>
cond-mat/9601008v2
<category>
cond-mat.stat-mech
<abstract>
The quantum ferromagnetic transition at zero temperature in disordered
itinerant electron systems is considered. Nonmagnetic quenched disorder leads
to diffusive electron dynamics that induces an effective long-range interaction
between the spin or order parameter fluctuations of the form r^{2-2d}, with d
the spatial dimension. This leads to unusual scaling behavior at the quantum
critical point, which is determined exactly. In three-dimensional systems the
quantum critical exponents are substantially different from their finite
temperature counterparts, a difference that should be easily observable.
Experiments to check these predictions are proposed.

<id>
cond-mat/9601036v2
<category>
cond-mat.stat-mech
<abstract>
Certain systems with slow driving and avalanche-like dissipation events are
naturally close to a critical point when the ratio of two energy scales is
large. The first energy scale is the threshold above which an avalanche is
triggered, the second scale is the threshold above which a site is affected by
an avalanche. I present results of computer simulations, and a mean-field
theory.

<id>
cond-mat/9602037v2
<category>
cond-mat.stat-mech
<abstract>
Two-dimensional layered aperiodic Ising systems are studied in the extreme
anisotropic limit where they correspond to quantum Ising chains in a transverse
field. The modulation of the couplings follows an aperiodic sequence generated
through substitution. According to Luck's criterion, such a perturbation
becomes marginal when the wandering exponent of the sequence vanishes. Three
marginal sequences are considered: the period-doubling, paper-folding and
three-folding sequences. They correspond to bulk perturbations for which the
critical temperature is shifted. The surface magnetization is obtained exactly
for the three sequences. The scaling dimensions of the local magnetization on
both surfaces vary continuously with the modulation factor. The low-energy
excitations of the quantum chains are found to scale as L^z with the size L of
the system. This is the behaviour expected for a strongly anisotropic system,
where z is the ratio of the exponents of the correlation lengths in the two
directions. The anisotropy exponent z is equal to the sum of the scaling
dimensions of the local magnetization on the two surfaces. The anisotropic
scaling behaviour is verified numerically for other surface and bulk critical
properties as well.

<id>
cond-mat/9604030v4
<category>
cond-mat.stat-mech
<abstract>
We use the negative replica method, which was originally developed for the
study of overfrustation in disordered system, to investigate the statistical
behaviour of the cost function of minimax games. These games are treated as
hierarchical statistical mechanical systems, in which one of the components is
at negative temperature.

<id>
cond-mat/9604174v3
<category>
cond-mat.stat-mech
<abstract>
We consider a two-letter self-avoiding (square) lattice heteropolymer model
of N_H (out ofN) attracting sites. At zero temperature, permanent links are
formed leading to collapse structures for any fraction rho_H=N_H/N. The average
chain size scales as R = N^{1/d}F(rho_H) (d is space dimension). As rho_H -->
0, F(rho_H) ~ rho_H^z with z={1/d-nu}=-1/4 for d=2. Moreover, for 0 < rho_H <
1, entropy approaches zero as N --> infty (being finite for a homopolymer). An
abrupt decrease in entropy occurs at the phase boundary between the swollen (R
~ N^nu) and collapsed region. Scaling arguments predict different regimes
depending on the ensemble of crosslinks. Some implications to the protein
folding problem are discussed.

<id>
cond-mat/9605105v2
<category>
cond-mat.stat-mech
<abstract>
We study the phase transition of isotropic spin-1 models in the vicinity of
the Uimin-Lai-Sutherland model by using the SU(3)_1 WZW model with certain
marginal perturbations. The unstable RG trajectory by a marginally relevant
perturbation generates a mass gap for the Haldane phase, and thus the
universality class of the transition from the massless phase to the Haldane
phase at ULS point becomes the BKT type. Our results support recent numerical
studies by F\'ath and S\'olyom. In the massless phase, we calculate logarithmic
finite-size corrections of the energy for the SU(\nu)-symmetric and asymmetric
models.

<id>
cond-mat/9606088v2
<category>
cond-mat.stat-mech
<abstract>
We investigate with the help of analytical and numerical methods the reaction
A+A->A on a one-dimensional lattice opened at one end and with an input of
particles at the other end. We show that if the diffusion rates to the left and
to the right are equal, for large x, the particle concentration c(x) behaves
like As/x (x measures the distance to the input end). If the diffusion rate in
the direction pointing away from the source is larger than the one
corresponding to the opposite direction the particle concentration behaves like
Aa/sqrt(x). The constants As and Aa are independent of the input and the two
coagulation rates. The universality of Aa comes as a surprise since in the
asymmetric case the system has a massive spectrum.

<id>
cond-mat/9606205v2
<category>
cond-mat.stat-mech
<abstract>
We introduce a self-organized surface growth model in 2+1 dimensions with
anisotropic avalanche process, which is expected to be in the universality
class of the anisotropic quenched Kardar-Parisi-Zhang equation with alternative
signs of the nonlinear KPZ terms. It turns out that the surface height
correlation functions in each direction scales distinctively. The anisotropic
behavior is attributed to the asymmetric behavior of the quenched KPZ equation
in 1+1 dimensions with respect to the sign of the nonlinear KPZ term.

<id>
cond-mat/9607175v2
<category>
cond-mat.stat-mech
<abstract>
The two-dimensional (2D) random-bond Ising model has a novel multicritical
point on the ferromagnetic to paramagnetic phase boundary. This random phase
transition is one of the simplest examples of a 2D critical point occurring at
both finite temperatures and disorder strength. We study the associated
critical properties, by mapping the random 2D Ising model onto a network model.
The model closely resembles network models of quantum Hall plateau transitions,
but has different symmetries. Numerical transfer matrix calculations enable us
to obtain estimates for the critical exponents at the random Ising phase
transition. The values are consistent with recent estimates obtained from
high-temperature series.

<id>
cond-mat/9608065v2
<category>
cond-mat.stat-mech
<abstract>
We study two models with n equivalent absorbing states that generalize the
Domany-Kinzel cellular automaton and the contact process. Numerical
investigations show that for n=2 both models belong to the same universality
class as branching annihilating walks with an even number of offspring. Unlike
previously known models, these models have no explicit parity conservation law.

<id>
cond-mat/9609070v2
<category>
cond-mat.stat-mech
<abstract>
The quantum ferromagnetic transition of itinerant electrons is considered. We
give a pedagogical review of recent results which show that zero-temperature
soft modes that are commonly neglected, invalidate the standard
Landau-Ginzburg-Wilson description of this transition. If these modes are taken
into account, then the resulting order parameter field theory is nonlocal in
space and time. Nevertheless, for both disordered and clean systems the
critical behavior has been exactly determined for spatial dimensions d>2 and
d>1, respectively. The critical exponents characterizing the
paramagnetic-to-ferromagnetic transition are dimensionality dependent, and
substantially different from both mean-field critical exponents, and from the
classical Heisenberg exponents that characterize the transition at finite
temperatures. Our results should be easily observable, particularly those for
the disordered case, and experiments to check our predictions are proposed.

<id>
cond-mat/9609151v2
<category>
cond-mat.stat-mech
<abstract>
A systematic theory for the diffusion--limited reaction processes $A + A \to
0$ and $A \to (m+1) A$ is developed. Fluctuations are taken into account via
the field--theoretic dynamical renormalization group. For $m$ even the mean
field rate equation, which predicts only an active phase, remains qualitatively
correct near $d_c = 2$ dimensions; but below $d_c' \approx 4/3$ a nontrivial
transition to an inactive phase governed by power law behavior appears. For $m$
odd there is a dynamic phase transition for any $d \leq 2$ which is described
by the directed percolation universality class.

<id>
cond-mat/9609245v2
<category>
cond-mat.stat-mech
<abstract>
The surface critical behavior of semi-infinite
  (a) binary alloys with a continuous order-disorder transition and
  (b) Ising antiferromagnets in the presence of a magnetic field is considered.
In contrast to ferromagnets, the surface universality class of these systems
depends on the orientation of the surface with respect to the crystal axes.
There is ordinary and extraordinary surface critical behavior for orientations
that preserve and break the two-sublattice symmetry, respectively. This is
confirmed by transfer-matrix calculations for the two-dimensional
antiferromagnet and other evidence.

<id>
cond-mat/9609292v2
<category>
cond-mat.stat-mech
<abstract>
In this paper we address the relationship between zero temperature Glauber
dynamics and the diffusion-annihilation problem in the free fermion case. We
show that the well-known duality transformation between the two problems can be
formulated as a similarity transformation if one uses appropriate (toroidal)
boundary conditions. This allow us to establish and clarify the precise nature
of the relationship between the two models. In this way we obtain a one-to-one
correspondence between observables and initial states in the two problems. A
random initial state in Glauber dynamics is related to a short range correlated
state in the annihilation problem. In particular the long-time behaviour of the
density in this state is seen to depend on the initial conditions. Hence, we
show that the presence of correlations in the initial state determine the
dependence of the long time behaviour of the density on the initial conditions,
even if such correlations are short-ranged. We also apply a field-theoretical
method to the calculation of multi-time correlation functions in this initial
state.

<id>
cond-mat/9610004v2
<category>
cond-mat.stat-mech
<abstract>
We analyze the Optimal Channel Network model for river networks using both
analytical and numerical approaches. This is a lattice model in which a
functional describing the dissipated energy is introduced and minimized in
order to find the optimal configurations. The fractal character of river
networks is reflected in the power law behaviour of various quantities
characterising the morphology of the basin. In the context of a finite size
scaling Ansatz, the exponents describing the power law behaviour are calculated
exactly and show mean field behaviour, except for two limiting values of a
parameter characterizing the dissipated energy, for which the system belongs to
different universality classes. Two modified versions of the model,
incorporating quenched disorder are considered: the first simulates
heterogeneities in the local properties of the soil, the second considers the
effects of a non-uniform rainfall. In the region of mean field behaviour, the
model is shown to be robust to both kinds of perturbations. In the two limiting
cases the random rainfall is still irrelevant, whereas the heterogeneity in the
soil properties leads to new universality classes. Results of a numerical
analysis of the model are reported that confirm and complement the theoretical
analysis of the global minimum. The statistics of the local minima are found to
more strongly resemble observational data on real rivers.

<id>
cond-mat/9610007v2
<category>
cond-mat.stat-mech
<abstract>
A remarkable theoretical prediction for a crystalline (polymerized) surface
is that its Poisson ratio (\sigma) is negative. Using a large scale Monte Carlo
simulation of a simple model of such surfaces we show that this is indeed true.
The precise numerical value we find is (\sigma \simeq -0.32) on a (128^2)
lattice at bending rigidity (kappa = 1.1). This is in excellent agreement with
the prediction (\sigma = -1/3) following from the self-consistent screening
approximation of Le Doussal and Radzihovsky.

<id>
cond-mat/9610010v1
<category>
cond-mat.stat-mech
<abstract>
We present a new model for relaxations in piles of granular material. The
relaxations are determined by a stochastic rule which models the effect of
friction between the grains. We find power-law distributions for avalanche
sizes and lifetimes characterized by the exponents $\tau = 1.53 \pm 0.05$ and
$y = 1.84 \pm 0.05$, respectively. For the discharge events, we find a
characteristic size that scales with the system size as $L^\mu$, with $\mu =
1.20 \pm 0.05$. We also find that the frequency of the discharge events
decrease with the system size as $L^{-\mu'}$ with $\mu' = 1.20 \pm 0.05$.

<id>
cond-mat/9610011v1
<category>
cond-mat.stat-mech
<abstract>
We investigate a one-dimensional rice-pile model. We show that the
distribution of dissipated potential energy decays as a power law with an
exponent $\alpha=1.53$. The system thus provides a one-dimensional example of
self-organized criticality. Different driving conditions are examined in order
to allow for comparison with experiments.

<id>
cond-mat/9610013v1
<category>
cond-mat.stat-mech
<abstract>
Using stability arguments, this Brief Report suggests that a term that
enhances the surface tension in the presence of large height fluctuations
should be included in the Kardar-Parisi-Zhang equation. A one-loop
renormalization group analysis then shows for interface dimensions larger than
$\simeq 3.3$ an unstable strong-coupling fixed point that enters the system
from infinity. The relevance of these results to the roughening transition is
discussed.

<id>
cond-mat/9610014v1
<category>
cond-mat.stat-mech
<abstract>
We study a simple growth model for (d+1)-dimensional films of binary alloys
in which atoms are allowed to interact and equilibrate at the surface, but are
frozen in the bulk. The resulting crystal is highly anisotropic: Correlations
perpendicular to the growth direction are identical to a d-dimensional
two-layer system in equilibrium, while parallel correlations generally reflect
the (Glauber) dynamics of such a system. For stronger in-plane interactions,
the correlation volumes change from oblate to highly prolate shapes near a
critical demixing or ordering transition. In d=1, the critical exponent z
relating the scaling of the two correlation lengths varies continuously with
the chemical interactions.

<id>
cond-mat/9610019v1
<category>
cond-mat.stat-mech
<abstract>
We study analytically and numerically the winding of directed polymers of
length $t$ around each other or around a rod. Unconfined polymers in pure media
have exponentially decaying winding angle distributions, the decay constant
depending on whether the interaction is repulsive or neutral, but not on
microscopic details. In the presence of a chiral asymmetry, the exponential
tails become non universal. In all these cases the mean winding angle is
proportional to $\ln t$. When the polymer is confined to a finite region around
the winding center, e.g. due to an attractive interaction, the winding angle
distribution is Gaussian, with a variance proportional to $t$. We also examine
the windings of polymers in random systems. Our results suggest that randomness
reduces entanglements, leading to a narrow (Gaussian) distribution with a mean
winding angle of the order of $\sqrt{\ln t}$.

<id>
cond-mat/9610029v2
<category>
cond-mat.stat-mech
<abstract>
We show that all zero energy eigenstates of an arbitrary $m$--state quantum
spin chain Hamiltonian with nearest neighbor interaction in the bulk and single
site boundary terms, which can also describe the dynamics of stochastic models,
can be written as matrix product states. This means that the weights in these
states can be expressed as expectation values in a Fock representation of an
algebra generated by $2m$ operators fulfilling $m^2$ quadratic relations which
are defined by the Hamiltonian.

<id>
cond-mat/9312031v2
<category>
cond-mat.str-el
<abstract>
In view of a recent controversy we investigated the Mott-Hubbard transition
in D=infinity with a novel cluster approach. i) We show that any truncated
Bethe lattice of order n can be mapped exactly to a finite Hubbard-like
cluster. ii) We evaluate the self-energy numerically for n=0,1,2 and compare
with a series of self-consistent equation-of-motion solutions. iii) We find the
gap to open continously at the critical U_c~2.5t* (t = t* / sqrt{4d}). iv) A
low-energy theory for the Mott-Hubbard transition is developed and relations
between critical exponents are presented.

<id>
cond-mat/9410024v1
<category>
cond-mat.str-el
<abstract>
I summarize some of the key questions to have emerged during the 1994
conference on ``Strongly Correlated Electron Systems'', held in Amsterdam,
August 1994. Issues addressed include: Hunds rule interactions and how they
renormalize; the Luttinger sum rule and metamagnetism; heavy fermion
insulators, the nature of the charge gap, spectral weight transfer in the
optical conductivity; non-Fermi liquid behavior in transition and heavy fermion
metals; order parameter symmetry and the unusual nature of quasiparticle
excitations in heavy fermion superconductors.

<id>
cond-mat/9512169v4
<category>
cond-mat.str-el
<abstract>
The Hubbard model is a "highly oversimplified model" for electrons in a solid
which interact with each other through extremely short ranged repulsive
(Coulomb) interaction. The Hamiltonian of the Hubbard model consists of two
pieces; H_hop which describes quantum mechanical hopping of electrons, and Hint
which describes nonlinear repulsive interaction. Either H_hop or H_int alone is
easy to analyze, and does not favor any specific order. But their sum
H=H_hop+H_int is believed to exhibit various nontrivial phenomena including
metal-insulator transition, antiferromagnetism, ferrimagnetism, ferromagnetism,
Tomonaga-Luttinger liquid, and superconductivity. It is believed that we can
find various interesting "universality classes" of strongly interacting
electron systems by studying the idealized Hubbard model.
  In the present article we review some mathematically rigorous results on the
Hubbard model which shed light on "physics" of this fascinating model. We
mainly concentrate on magnetic properties of the model at its ground states. We
discuss Lieb-Mattis theorem on the absence of ferromagnetism in one dimension,
Koma-Tasaki bounds on decay of correlations at finite temperatures in
two-dimensions, Yamanaka-Oshikawa-Affleck theorem on low-lying excitations in
one-dimension, Lieb's important theorem for half-filled model on a bipartite
lattice, Kubo-Kishi bounds on the charge and superconducting susceptibilities
of half-filled models at finite temperatures, and three rigorous examples of
saturated ferromagnetism due to Nagaoka, Mielke, and Tasaki. We have tried to
make the article accessible to nonexperts by describing basic definitions and
elementary materials in detail.

<id>
cond-mat/9604096v2
<category>
cond-mat.str-el
<abstract>
Asymptotic properties of nearly-half-filled one-dimensional conductors
coupled with phonons are studied through a renormalization group method. Due to
spin-charge coupling via electron-phonon interaction, the spin correlation
varies with filling as well as the charge correlation. Depending on the
relation between cut-off energy scales of the Umklapp process and of the
electron-phonon interaction, various phases appear. We found a metallic phase
with a spin gap and a dominant charge- density-wave correlation near half
filling between a gapless density-wave phase (like in the doped repulsive
Hubbard model) and a superconductor phase with a spin gap. The spin gap is
produced by phonon-assisted backward scatterings which are interfered with the
Umklapp process constructively or destructively depending on the character of
electron-phonon coupling.

<id>
cond-mat/9607105v2
<category>
cond-mat.str-el
<abstract>
A model of copper-oxygen bonding and anti-bonding bands with the most general
two-body interactions allowable by symmetry is considered. The model has a
continuous transition as a function of hole-density x and temperature T to a
phase in which a current circulates in each unit cell. This phase preserves the
translational symmetry of the lattice while breaking time-reversal invariance
and the four-fold rotational symmetry. The product of time-reversal and
four-fold rotation is preserved. The circulating current phase terminates at a
critical point at $x=x_c, T=0$. In the quantum-critical region about this point
the logarithm of the frequency of the current fluctuations scales with their
momentum. The microscopic basis for the marginal Fermi-liquid phenemenology and
the observed long wavelength transport anomalies near $x=x_c$ are derived from
such fluctuations. The symmetry of the current fluctuations is such that the
associated magnetic field fluctuations are absent at oxygen sites and have the
correct form to explain the anomalous copper nuclear relaxation rate.
Cross-overs to the Fermi-liquid phase on either side of $x_c$ and the role of
disorder are briefly considered. The current fluctuations promote
superconductive instability with a propensity towards ``D-wave" symmetry or
``extended S-wave"symmetry depending on details of the band-structure.

<id>
cond-mat/9609057v5
<category>
cond-mat.str-el
<abstract>
We reinvestigate the two channel flavor anisotropic model (2CFAK) and one
channel compacitified Kondo model (1CCK). For these two models, all the
possible fixed points and their symmetries are identified; the finite size
spectra, the electron conductivity and pairing susceptibility are calculated.
It is shown that the only non-fermi liquid (NFL) fixed point of the 2CFAK is
the NFL of the two channel Kondo model (2CK) with the symmetry $O(3) \times
O(5)$. Any flavor anisotropies between the two channels drive the system to the
fermi-liquid (FL) fixed point with the symmetry $O(4) \times O(4)$ where one of
the two channels suffers the phase shift \pi/2 and the other remains free. The
NFL fixed point of the 1CCK has the symmetry $O(3) \times O(1)$ and has the
same thermodynamics as the NFL fixed point of the 2CK. However, in contrast to
the 2CK, its conductivity shows $T^{2}$ behavior and there is no pairing
susceptibility divergence. Any anisotropies between the spin and isospin
sectors drive the system to the FL fixed point with the symmetry O(4) where the
electrons suffer the phase shift \pi/2. The connection and differences between
the two models are explicitly demonstrated. The recent conjectures and claims
on the NFL behaviors of the two models are commented.

<id>
cond-mat/9609058v4
<category>
cond-mat.str-el
<abstract>
We investigate a model where an impurity couples to both the spin and the
flavor currents of the two channel conduction electrons. This model can be used
as a prototype model of a magnetic impurity tunneling between two sites in a
metal and of some heavy fermion systems where the ground state of the impurity
has a fourfold degeneracy. The system is shown to flow to a doubly degenerate
non fermi-liquid(NFL) fixed point; the thermodynamic quantities show NFL
behaviors, but the transport quantities show fermi liquid (FL) behaviors . A
spin-flavor coupling double tensor term is shown to drive the system to one of
the two singlet FL fixed points. The relation with SU(4) Coqblin-Schrieffer
model is studied. The implications on the possible experiments are given.

<id>
cond-mat/9609076v6
<category>
cond-mat.str-el
<abstract>
We analyze in detail all the possible fixed points of the effective
Hamiltonian of a non-magnetic impurity hopping between two sites in a metal
obtained by Moustakas and Fisher(MF). We find a line of non-fermi liquid fixed
points which continuously interpolates between the 2-channel Kondo fixed
point(2CK) and the one channel, two impurity Kondo (2IK) fixed point. The
additional non-fermi liquid fixed point found by MF has the same symmetry as
the 2IK, The system is shown to flow to a line of fermi-liquid fixed points
which continuously interpolates between the non-interacting fixed point and the
2 channel spin-flavor Kondo fixed point (2CSFK) discussed by the contributor
previously. The effect of particle-hole symmetry breaking is discussed. The
effective Hamiltonian in the external magnetic field is analysed. The scaling
functions for the physical measurable quantities are derived in the different
regimes; their predictions for the experiments are given. Finally the
implications are given for a non-magnetic impurity hopping around three sites
with triangular symmetry discussed by MF.

<id>
cond-mat/9610009v1
<category>
cond-mat.str-el
<abstract>
FeGe_2, and lightly doped compounds based on it, have a Fermi surface driven
instability which drive them into an incommensurate spin density wave state.
Studies of the temperature and magnetic field dependence of the resistivity
have been used to determine the magnetic phase diagram of the pure material
which displays an incommensurate phase at high temperatures and a commensurate
structure below 263 K in zero field. Application of a magnetic field in the
tetragonal basal plane decreases the range of temperatures over which the
incommensurate phase is stable. We have used inelastic neutron scattering to
measure the spin dynamics of FeGe_2. Despite the relatively isotropic transport
the magnetic dynamics is quasi-one dimensional in nature. Measurements carried
out on HET at ISIS have been used to map out the spin wave dispersion along the
c-axis up the 400 meV, more than an order of magnitude higher than the zone
boundary magnon for wavevectors in the basal plane.

<id>
cond-mat/9610026v1
<category>
cond-mat.str-el
<abstract>
We study effect of the adiabatic electron renormalization on the parameters
of the dynamical defects in the ballistic metallic point contact. The upper
energy states of the ``dressed'' defect are shown to give a smaller
contribution to a resistance of the contact than the lower energy ones. This
holds both for the "classical" renormalization related to defect coupling with
average local electron density and for the "mesoscopic" renormalization caused
by the mesoscopic fluctuations of electronic density the dynamical defects are
coupled with. In the case of mesoscopic renormalization one may treat the
dynamical defect as coupled with Friedel oscillations originated by the other
defects, both static and mobile. Such coupling lifts the energy degeneracy of
the states of the dynamical defects giving different mesoscopic contribution to
resistance, and provides a new model for the fluctuator as for the object
originated by the electronic mesoscopic disorder rather than by the structural
one. The correlation between the defect energy and the defect contribution to
the resistance leads to zero-temperature and zero-bias anomalies of the point
contact resistance.
  A comparison of these anomalies with those predicted by the Two Channel Kondo
Model (TCKM) is made. It is shown, that although the proposed model is based on
a completely different from TCKM physical background, it leads to a zero-bias
anomalies of the point contact resistance, which are qualitatively similar to
TCKM predictions.

<id>
cond-mat/9610027v1
<category>
cond-mat.str-el
<abstract>
We construct a generalized multiplicative renormalization group
transformation to study the low energy dynamics of a heavy particle tunneling
among $M$ different positions and interacting with $N_f$ independent conduction
electron channels. Using a $1/N_f$-expansion we show that this M-level scales
towards a fixed point equivalent to the $N_f$ channel $SU(M) \times SU(N_f)$
Coqblin-Schrieffer model. Solving numerically the scaling equations we find
that a realistic M-level system scales close to this fixed point (FP) and its
Kondo temperature is in the experimentally observable range $1-10 K$.

<id>
cond-mat/9610030v1
<category>
cond-mat.str-el
<abstract>
We present a restricted path integral approach to the 2D and 3D repulsive
Hubbard model. In this approach the partition function is approximated by
restricting the summation over all states to a (small) subclass which is chosen
such as to well represent the important states. This procedure generalizes mean
field theory and can be systematically improved by including more states or
fluctuations. We analyze in detail the simplest of these approximations which
corresponds to summing over states with local antiferromagnetic (AF) order. If
in the states considered the AF order changes sufficiently little in space and
time, the path integral becomes a finite dimensional integral for which the
saddle point evaluation is exact. This leads to generalized mean field
equations allowing for the possibility of more than one relevant saddle points.
In a big parameter regime (both in temperature and filling), we find that this
integral has {\em two} relevant saddle points, one corresponding to finite AF
order and the other without. These degenerate saddle points describe a phase of
AF ordered fermions coexisting with free, metallic fermions. We argue that this
mixed phase is a simple mean field description of a variety of possible
inhomogeneous states, appropriate on length scales where these states appear
homogeneous. We sketch systematic refinements of this approximation which can
give more detailed descriptions of the system.

<id>
cond-mat/9610031v1
<category>
cond-mat.str-el
<abstract>
We report the temperature dependence of transport and magnetic properties of
La$_{1-x}$Ca$_x$VO$_3$ for x = 0.0, 0.1, 0.2, 0.3, 0.4 and 0.5. The system
exhibits an insulator-to-metal transition concomitant with an
antiferromagnetic-to-paramagnetic transition near x = 0.2 with increasing
substitution. Disorder effects are found to influence the low temperature
transport properties of both insulating and metallic compositions near the
critical concentration. At higher temperatures, the resistivity of the metallic
samples is found to exhibit either a $T^{1.5}$ or a $T^2$ dependence depending
on the composition. The molar susceptibility for the metallic samples indicate
substantial enhancements due to electron correlation.

<id>
cond-mat/9610038v1
<category>
cond-mat.str-el
<abstract>
An extensive Quantum Monte Carlo calculation is performed for the two-leg
Hubbard ladder model to clarify whether the singlet pairing correlation decays
slowly, which is predicted from the weak-coupling theory but controversial from
numerical studies. Our result suggests that the discreteness of energy levels
in finite systems affects the correlation enormously, where the enhanced
pairing correlation is indeed detected if we make the energy levels of the
bonding and anti-bonding bands lie close to each other at the Fermi level to
mimic the thermodynamic limit.

<id>
cond-mat/9610056v1
<category>
cond-mat.str-el
<abstract>
We derive a general set of Poor Man's scaling equations and analyze the
stability of the Luttinger state in a system composed of a finite number N of
one dimensional spinless fermionic chains, coupled through a general two body
interaction. The effect of processes with momentum transfer parallel to the
Fermi surface in destroying massless states is investigated. It will be shown
that there are two processes competing: one in which two electrons exchange
chains and the other in which they jump into a same chain. When periodic
boundary conditions in the transverse direction are taken into account this
competition leads always to massive states (except in hyperplanes of the phase
diagram), a well known example being the generalized sine-Gordon model. If
instead open boundary conditions are taken, massless states are possible but
due to this competition the system is placed near instabilities. We argue that
this kind of analysis has relevance for understanding the instabilities of 2D
fermionic systems.

<id>
cond-mat/9610058v1
<category>
cond-mat.str-el
<abstract>
Buckminsterfullerene compounds exibit remarkable physics at low temperatures,
e.g. high temperature superconductivity in alkali-fullerenes, and
ferromagnetism in TDAE -C60. Here we review recent theoretical studies of
electron correlations in these compounds. In particular, we discuss models of
electron-vibron interactions, electron-electron interactions, and
intermolecular hopping. We show that the origin of novel electronic phases lies
in {\em local} degeneracies of C60; a direct consequence of the high molecular
symmetry.

<id>
cond-mat/9610075v1
<category>
cond-mat.str-el
<abstract>
We propose that the normal-state transport in the cuprate superconductors can
be understood in terms of a two-fluid model of spinons and holons. In our
scenario, the resistivity is determined by the properties of the holons while
magnetotransport involves the recombination of holons and spinons to form
physical electrons. Our model implies that the Hall transport time is a measure
of the electron lifetime, which is shorter than the longitudinal transport
time. This agrees with our analysis of the normal-state data. We predict a
strong increase in linewidth with increasing temperature in photoemission. Our
model also suggests that the AC Hall effect is controlled by the transport
time.

<id>
cond-mat/9610076v1
<category>
cond-mat.str-el
<abstract>
A physical picture of normal liquid $^3$He, which accounts for both ``almost
localized'' and ``almost ferromagnetic'' aspects, is proposed and confronted to
experiments.

<id>
cond-mat/9610081v1
<category>
cond-mat.str-el
<abstract>
The two-dimensional $t$-$J$ model in the ground state is investigated by the
power Lanczos method. The pairing-pairing correlation function for
$d_{x^2-y^2}$-wave symmetry is enhanced in the realistic parameter regime for
high-$T_c$ superconductors. The charge susceptibility $\chi_c$ shows divergent
behavior as $\chi_c\propto\delta^{-1}$ near half-filling for the doping
concentration $\delta$, indicating that the value of the dynamical exponent $z$
is four under the assumption of hyperscaling. The peak height of the spin
structure factor $S_{max}(Q)$ also behaves as $S_{max}(Q) \propto\delta^{-1}$
near half-filling, which leads to the divergence of the antiferromagnetic
correlation length $\xi_m$ as $\xi_m \propto\delta^{-1/2}$. The boundary of
phase separation is estimated on the basis of the Maxwell construction.
Numerical results are compared with experimental features observed in
high-$T_c$ cuprates.

<id>
cond-mat/9610087v1
<category>
cond-mat.str-el
<abstract>
A general method for obtaining the oscillation periods of the interlayer
exchange coupling is presented. It is shown that it is possible for the
coupling to oscillate with additional periods beyond the ones predicted by the
RKKY theory. The relation between the oscillation periods and the spacer Fermi
surface is clarified, showing that non-RKKY periods do not bear a direct
correspondence with the Fermi surface. The interesting case of a FCC(110)
structure is investigated, unmistakably proving the existence and relevance of
non-RKKY oscillations. The general conditions for the occurrence of non-RKKY
oscillations are also presented.

<id>
cond-mat/9610094v1
<category>
cond-mat.str-el
<abstract>
When holes are doped into an antiferromagnetic insulator they form a slowly
fluctuating array of ``topological defects'' (metallic stripes) in which the
motion of the holes exhibits a self-organized quasi one-dimensional electronic
character. The accompanying lateral confinement of the intervening
Mott-insulating regions induces a spin gap or pseudogap in the environment of
the stripes. We present a theory of underdoped high temperature superconductors
and show that there is a {\it local} separation of spin and charge, and that
the mobile holes on an individual stripe acquire a spin gap via pair hopping
between the stripe and its environment; i.e. via a magnetic analog of the usual
superconducting proximity effect. In this way a high pairing scale without a
large mass renormalization is established despite the strong Coulomb repulsion
between the holes. Thus the {\it mechanism} of pairing is the generation of a
spin gap in spatially-confined {\it Mott-insulating} regions of the material in
the proximity of the metallic stripes. At non-vanishing stripe densities,
Josephson coupling between stripes produces a dimensional crossover to a state
with long-range superconducting phase coherence. This picture is established by
obtaining exact and well-controlled approximate solutions of a model of a
one-dimensional electron gas in an active environment. An extended discussion
of the experimental evidence supporting the relevance of these results to the
cuprate superconductors is given.

<id>
cond-mat/9610115v1
<category>
cond-mat.str-el
<abstract>
An effective, low temperature, classical model for spin transport in the
one-dimensional, gapped, quantum $O(3)$ non-linear $\sigma$-model is developed.
Its correlators are obtained by a mapping to a model solved earlier by Jepsen.
We obtain universal functions for the ballistic-to-diffusive crossover and the
value of the spin diffusion constant, and these are claimed to be exact at low
temperatures. Implications for experiments on one-dimensional insulators with a
spin gap are noted.

<id>
cond-mat/9610121v1
<category>
cond-mat.str-el
<abstract>
We investigate the origin of the long period oscillation of the interlayer
exchange coupling in Fe/Cr trilayer systems. Within the stationary phase
approximation the periods of the oscillations are associated with extremal
vectors of the Fermi sphere of Cr. Using a realistic tight-binding model with
spin-orbit interaction we calculate the coupling strength for each extremal
vector based on the spin-asymmetry of the reflection amplitude for a
propagating state impinging from the Cr to Fe layer. We find that for (001) and
(110) growth direction the biggest coupling strength comes from the extremal
vector centered at the ellipsoid N of the Fermi surface of Cr.

<id>
cond-mat/9610142v1
<category>
cond-mat.str-el
<abstract>
The energy level statistics of the Hubbard model for $L \times L$ square
lattices (L=3,4,5,6) at low filling (four electrons) is studied numerically for
a wide range of the coupling strength. All known symmetries of the model
(space, spin and pseudospin symmetry) have been taken into account explicitly
from the beginning of the calculation by projecting into symmetry invariant
subspaces. The details of this group theoretical treatment are presented with
special attention to the nongeneric case of L=4, where a particular complicated
space group appears. For all the lattices studied, a significant amount of
levels within each symmetry invariant subspaces remains degenerated, but except
for L=4 the ground state is nondegenerate. We explain the remaining
degeneracies, which occur only for very specific interaction independent
states, and we disregard these states in the statistical spectral analysis. The
intricate structure of the Hubbard spectra necessitates a careful unfolding
procedure, which is thoroughly discussed. Finally, we present our results for
the level spacing distribution, the number variance $\Sigma^2$, and the
spectral rigidity $\Delta_3$, which essentially all are close to the
corresponding statistics for random matrices of the Gaussian ensemble
independent of the lattice size and the coupling strength. Even very small
coupling strengths approaching the integrable zero coupling limit lead to the
Gaussian ensemble statistics stressing the nonperturbative nature of the
Hubbard model.

<id>
cond-mat/9610147v1
<category>
cond-mat.str-el
<abstract>
We report calculations of the electronic structure of FeO in the LDA and
LDA+U approximation with and without rhombohedral distortion. In both cases LDA
renders an antiferromagnetic metal, and LDA+U opens a Hubbard gap. However, the
character of the gap is qualitatively different in the two structure, and the
difference can be traced down to underlying LDA band structure. An analysis of
the calculations gives a new insight on the origin of the insulating gap in 3d
monoxides and on the role of the k-dependency of U, missing in the contemporary
LDA+U method.

<id>
cond-mat/9610148v3
<category>
cond-mat.str-el
<abstract>
We derive a path-integral expression for the effective action in the
continuum limit of an AFM Heisenberg spin ladder with an arbitrary number of
legs. The map is onto an $O(3)$ nonlinear $\sigma$-model (NL$\sigma$M) with the
addition of a topological term that is effective only for odd-legged ladders
and half-odd integer spins. We derive the parameters of the effective
NL$\sigma$M and the behaviour of the spin gap for the case of even-legged
ladders.

<id>
cond-mat/9610164v1
<category>
cond-mat.str-el
<abstract>
It is shown that White's density matrix renormalization group technique can
be adapted to obtain thermodynamic quantities. As an illustration, the magnetic
susceptibility of Heisenberg S=1/2 and S=3/2 spin chains are computed. A
careful finite size analysis is made to determine the range of temperatures
where the results are reliable. For the S=1/2 chain, the comparison with the
exact Bethe ansatz curve shows an agreement within 1% down to T=0.05J.

<id>
cond-mat/9610168v3
<category>
cond-mat.str-el
<abstract>
We discuss zero-temperature quantum spin chains in a uniform magnetic field,
with axial symmetry. For integer or half-integer spin, $S$, the magnetization
curve can have plateaus and we argue that the magnetization per site $m$ is
topologically quantized as $q (S - m)= integer$ at the plateaus, where $q$ is
the period of the groundstate. We also discuss conditions for the presence of
the plateau at those quantized values. For $S=3/2$ and $m=1/2$, we study
several models and find two distinct types of massive phases at the plateau.
One of them is argued to be a ``Haldane gap phase'' for half-integer $S$.

<id>
cond-mat/9610172v1
<category>
cond-mat.str-el
<abstract>
We study the so-called nonmagnetic phases (dimer and flux states) in the t-J
model below half filling. We present a new phase diagram, at zero and finite
temperature, that includes broad areas of phase coexistence (dimer-flux or
flux-uniform), in accordance with experimental and numerical data on the
possibility of separation into hole-rich and hole-poor regions. We also briefly
comment on some techniques used in the literature to discuss phase separation
in the t-J model.

<id>
cond-mat/9610188v1
<category>
cond-mat.str-el
<abstract>
The asymmetric infinite-dimensional periodic Anderson model is examined with
a quantum Monte Carlo simulation. For small conduction band filling, we find a
severe reduction in the Kondo scale, compared to the impurity value, as well as
protracted spin screening consistent with some recent controversial
photoemission experiments. The Kondo screening drives a ferromagnetic
transition when the conduction band is quarter-filled and both the RKKY and
superexchange favor antiferromagnetism. We also find RKKY-driven ferromagnetic
and antiferromagnetic transitions.

<id>
cond-mat/9402081v1
<category>
cond-mat.supr-con
<abstract>
We utilize a geometric argument to determine the effects of boundary
scattering on the carrier mean-free path in samples of various cross sections.
Analytic expressions for samples with rectangular and circular cross sections
are obtained. We also outline a method for incorporating these results into
calculations of the thermal conductivity.

<id>
cond-mat/9410020v1
<category>
cond-mat.supr-con
<abstract>
We study quantum interference effects due to electron motion on the Kagom\'e
lattice in a perpendicular magnetic field. These effects arise from the
interference between phase factors associated with different electron
closed-paths. From these we compute, analytically and numerically, the
superconducting-normal phase boundary for Kagom\'e superconducting wire
networks and Josephson junction arrays. We use an analytical approach to
analyze the relationship between the interference and the complex structure
present in the phase boundary, including the origin of the overall and fine
structure. Our results are obtained by exactly summing over one thousand
billion billions ($\sim 10^{21}$) closed paths, each one weighted by its
corresponding phase factor representing the net flux enclosed by each path. We
expect our computed mean-field phase diagrams to compare well with several
proposed experiments.

<id>
cond-mat/9602132v2
<category>
cond-mat.supr-con
<abstract>
Scanning tunneling microscopy can provide a probe for the detailed study of
quasiparticle states in high-Tc superconductors. We propose that it can also be
used to acquire specific information about impurity-induced quasiparticle
states and the superconducting order-parameter structure. In particular, the
local density of states is found to be sensitive to impurity-induced resonances
and to the symmetry of the order parameter.

<id>
cond-mat/9604049v2
<category>
cond-mat.supr-con
<abstract>
For a two layer system in a weak coupling BCS formalism any interlayer
interaction, regardless of its sign, enhances the critical temperature. The
sign has an effect upon the relative phase of the order parameter in each of
the two planes but not upon its magnitude. When one of the planes has a
dispersion consistent with CuO chains and no intrinsic pairing interaction
there is both an enhancement of the critical temperature and an s+d mixing in
both layers as the interlayer interaction is increased. The magnetic
penetration depth, c-axis Josephson tunneling, density of states and Knight
shift are calculated for several sets of model parameters.

<id>
cond-mat/9608057v3
<category>
cond-mat.supr-con
<abstract>
The influence of different types of disorder (both uncorrelated and
correlated) on the superfluid properties of a weakly interacting or dilute Bose
gas, as well as on the corresponding quantities for flux line liquids in
high-temperature superconductors at low magnetic fields are reviewed,
investigated and compared. We exploit the formal analogy between superfluid
bosons and the statistical mechanics of directed lines, and explore the
influence of the different "imaginary time" boundary conditions appropriate for
a flux line liquid. For superfluids, we discuss the density and momentum
correlations, the condensate fraction, and the normal-fluid density as function
of temperature for two- and three-dimensional systems subject to a space- and
time-dependent random potential as well as conventional point-, line-, and
plane-like defects. In the case of vortex liquids subject to point disorder,
twin boundaries, screw dislocations, and various configurations of columnar
damage tracks, we calculate the corresponding quantities, namely density and
tilt correlations, the ``boson'' order parameter, and the tilt modulus. The
finite-size corrections due to periodic vs. open "imaginary time" boundary
conditions differ in interesting and important ways. Experimental implications
for vortex lines are described briefly.

<id>
cond-mat/9609065v2
<category>
cond-mat.supr-con
<abstract>
We study the problem of the phonon-induced electron-electron interaction in a
solid. Starting with a Hamiltonian that contains an electron-phonon
interaction, we perform a similarity renormalization transformation to
calculate an effective Hamiltonian. Using this transformation singularities due
to degeneracies are avoided explicitely. The effective interactions are
calculated to second order in the electron-phonon coupling. It is shown that
the effective interaction between two electrons forming a Cooper pair is
attractive in the whole parameter space. For a simple Einstein model we
calculate the renormalization of the electronic energies and the critical
temperature of superconductivity.

<id>
cond-mat/9609099v1
<category>
cond-mat.supr-con
<abstract>
The normal and superconducting state of YBa_{2}Cu_{3}O_{6+\delta} and
Bi_{2}Sr_{2}CaCu_{2}O_{8+\delta} are investigated by using the mono- and
bilayer Hubbard model within the fluctuation exchange approximation and a
proper description of the Fermi surface topology. The inter- and intra-layer
interactions, the renormalization of the bilayer splitting and the formation of
shadow bands are investigated in detail. Although the shadow states are not
visible in the monolayer, we find that the additional correlations in bilayers
boost the shadow state intensity and will lead to their observability. In the
superconducting state we find a $d_{x^2-y^2}$ symmetry of the order parameter
and demonstrate the importance of inter-plane Copper pairing.

<id>
cond-mat/9609142v2
<category>
cond-mat.supr-con
<abstract>
We study the Ginzburg-Landau model with a nonlocal quartic term as a simple
phenomenological model for superconductors in the presence of coupling between
the vortex lattice and the underlying crystal lattice. In mean-field theory,
our model is consistent with a general oblique vortex lattice ranging from a
triangular lattice to a square lattice. This simple formulation enables us to
study the effect of thermal fluctuations in the vortex liquid regime. We
calculate the structure factor of the vortex liquid nonperturbatively and find
Bragg-like peaks with four-fold symmetry appearing in the structure factor even
though there is only a short-range crystalline order.

<id>
cond-mat/9610003v2
<category>
cond-mat.supr-con
<abstract>
We study the melting of a moving vortex lattice through numerical simulations
with the current driven 3D XY model with disorder. We find that there is a
first-order phase transition even for large disorder when the corresponding
equilibrium transition is continuous. The low temperature phase is an
anisotropic moving glass.

<id>
cond-mat/9610006v1
<category>
cond-mat.supr-con
<abstract>
We generalize the London free energy to include four-fold anisotropies which
could arise from d-wave pairing or other sources in a tetragonal material. We
use this simple model to study vortex lattice structure and discuss neutron
scattering, STM, Bitter decoration and $\mu$SR experiments.

<id>
cond-mat/9610018v2
<category>
cond-mat.supr-con
<abstract>
We study a lattice fermion model for superconductivity in the presence of an
antiferromagnetic background, described as a fixed external staggered magnetic
field. We discuss the possibility that our model provides an effective
description of coexistence of antiferromagnetic correlations and
superconductivity, and its possible application to high temperature
superconductivity. We also argue that, under certain conditions, this model
describes a variant of the periodic Anderson model for heavy fermions. Using a
path integral formulation we construct mean field equations, which we study in
some detail. We evaluate the superconducting critical temperature and show that
it is strongly enhanced by antiferromagnetic order. We also evaluate the
superconducting gap, the superconducting density of states, and the tunneling
conductivity, and show that the most stable channel usually has a
$d_{x^2-y^2}$-wave gap.

<id>
cond-mat/9610032v3
<category>
cond-mat.supr-con
<abstract>
New features are reported in precision measurements of the complex microwave
conductivity of high quality $YBa_{2}Cu_{3}O_{6.95}$ crystals grown in
$BaZrO_{3}$ crucibles. A third peak in the normal conductivity,
$\sigma_{1}(T)$, at around $80K$, and enhanced pair conductivity
$\sigma_{2}(T)$ below $\sim 65K$ are observed. The data are inconsistent with a
single order parameter, and instead are indicative of multi-component
superconductivity. Overall, these results point to the presence of multiple
pairing interactions in $YBa_{2}Cu_{3}O_{6.95}$ and also provide a natural
explanation to account for the low temperature $35K$ conductivity peak observed
in all $YBa_{2}Cu_{3}O_{6.95}$ crystals.

<id>
cond-mat/9610033v1
<category>
cond-mat.supr-con
<abstract>
The Fermi-liquid theory of superconductivity is applicable to a broad range
of systems that are candidates for unconventional pairing. Fundamental
differences between unconventional and conventional anisotropic superconductors
are illustrated by the unique effects that impurities have on the
low-temperature transport properties of unconventional superconductors. For
special classes of unconventional superconductors the low-temperature transport
coefficients are {\it universal}, i.e. independent of the impurity
concentration and scattering phase shift. The existence of a universal limit
depends on the symmetry of the order parameter and is achieved at low
temperatures $k_B T \ll \gamma \ll \Delta_0$, where $\gamma$ is the bandwidth
of the impurity induced Andreev bound states. In the case of UPt$_3$ thermal
conductivity measurements favor an $E_{1g}$ or $E_{2u}$ ground state.
Measurements at ultra-low temperatures should distinguish different pairing
states.

<id>
cond-mat/9610039v1
<category>
cond-mat.supr-con
<abstract>
We study the problem of flux penetration into type--I superconductors with
high demagnetization factor (slab geometry).Assuming that the interface between
the normal and superconducting regions is sharp, that flux diffuses rapidly in
the normal regions, and that thermal effects are negligible, we analyze the
process by which flux invades the sample as the applied field is increased
slowly from zero.We find that flux does not penetrate gradually.Rather there is
an instability in the process and the flux penetrates from the boundary in a
series of bursts, accompanied by the formation of isolated droplets of the
normal phase, leading to a multiply connected flux domain structure similar to
that seen in experiments.

<id>
cond-mat/9610050v1
<category>
cond-mat.supr-con
<abstract>
The interplay between the band Jahn-Teller distortion and the
superconductivity is studied for the system whose Fermi level lies in two-fold
degenerate band. Assuming that the lattice distortion is coupled to the orbital
electron density and the superconductivity arises due to BCS pairing mechanism
between the electrons, the phase diagram is obtained for different doping with
respect to half-filled band situation. The coexistence phase of
superconductivity and distortion occurs within limited range of doping and the
distortion lowers the superconducting transition temperature $T_c$. In presence
of strong electron-lattice interaction the lattice strain is found to be
maximum at half-filling and superconductivity does not appear for low doping.
The maximum value of $T_c$ obtainable for an optimum doping is limited by the
structural transition temperature $T_s$. The growth of distortion is arrested
with the onset of superconductivity and the distortion is found to disappear at
lower temperature for some hole density. Such arresting of the growth of
distortion at $T_c$ produces discontinuous jump in thermal expansion
coefficient. The variation of strain with temperature as well as with doping,
thermal expansion coefficient, the $T_c$ vs $\delta$ behaviour are in
qualitative agreement with recent experimental observations on interplay of
distortion and superconductivity in cuprates.

<id>
cond-mat/9610051v1
<category>
cond-mat.supr-con
<abstract>
We present results from an extensive analytic and numerical study of a
two-dimensional model of a square array of ultrasmall Josephson junctions. We
include the ultrasmall self and mutual capacitances of the junctions, for the
same parameter ranges as those produced in the experiments. The model
Hamiltonian studied includes the Josephson, $E_J$, as well as the charging,
$E_C$, energies between superconducting islands. The corresponding quantum
partition function is expressed in different calculationally convenient ways
within its path-integral representation. The phase diagram is analytically
studied using a WKB renormalization group (WKB-RG) plus a self-consistent
harmonic approximation (SCHA) analysis, together with non-perturbative quantum
Monte Carlo simulations. Most of the results presented here pertain to the
superconductor to normal (S-N) region, although some results for the insulating
to normal (I-N) region are also included. We find very good agreement between
the WKB-RG and QMC results when compared to the experimental data. To fit the
data, we only used the experimentally determined capacitances as fitting
parameters. The WKB-RG analysis in the S-N region predicts a low temperature
instability i.e. a Quantum Induced Transition (QUIT). We carefully simulations
and carry out a finite size analysis of $T_{QUIT}$ as a function of the
magnitude of imaginary time axis $L_\tau$. We find that for some relatively
large values of $\alpha=E_C/E_J$ ($1\leq \alpha \leq 2.25)$, the
$L_\tau\to\infty$ limit does appear to give a {\it non-zero} $T_{QUIT}$, while
for $\alpha \ge 2.5$, $T_{QUIT}=0$. We use the SCHA to analytically understand
the $L_\tau$ dependence of the QMC results with good agreement between them.
Finally, we also carried out a WKB-RG analysis in the I-N region and found no
evidence of a low temperature QUIT, up to lowest order in ${\alpha}^{-1}$

<id>
cond-mat/9610063v1
<category>
cond-mat.supr-con
<abstract>
It is shown, within the framework of the Ginzburg-Landau theory for a
superconductor with d_{x^2-y^2} symmetry, that the passing of a supercurrent
through the sample results, in general, in the induction of order-parameter
components of distinct symmetry. The induction of s-wave and
d_{xy(x^2-y^2)-wave components are considered in detail. It is shown that in
both cases the order parameter remains gapless; however, the structure of the
lines of nodes and the lobes of the order parameter are modified in distinct
ways, and the magnitudes of these modifications differ in their dependence on
the (a-b plane) current direction. The magnitude of the induced s-wave
component is estimated using the results of the calculations of Ren et al.
[Phys. Rev. Lett. 74, 3680 (1995)], which are based on a microscopic approach.

<id>
cond-mat/9610065v1
<category>
cond-mat.supr-con
<abstract>
The resonance Wigner scattering of charged bosons (small bipolarons) in a
random potential leads to logarithmically divergent low-temperature resistivity
as observed in several cuprates. Unusual temperature dependence of resistivity
of $La_{2-x}Sr_{x}CuO_{4}$ as well as of the Hall effect is quantitatively
described in a wide temperature range providing an evidence for $2e$ charged
Bose-liquid in high-$T_{c}$ cuprates.

<id>
cond-mat/9610077v1
<category>
cond-mat.supr-con
<abstract>
In d-wave superconductors, the electronic density of states (DOS) induced by
a vortex exhibits 1/|E| divergency at low energies. It is the result of gap
nodes in the excitations spectrum outside the vortex core. The heat capacity in
two regimes, (T/T_c)^2 >> B/B_{c2} and (T/T_c)^2 << B/B_{c2}, is discussed.

<id>
cond-mat/9610083v1
<category>
cond-mat.supr-con
<abstract>
The superconducting transition in presence of strong columnar disorder
parallel to the magnetic field is considered. A solvable model appropriate for
description of the broad crossover regime towards the true "glassy" critical
behavior is constructed, and the behavior of the thermodynamic quantities and
of the Edwards-Anderson order-parameter is obtained. The critical exponents for
the correlation lengths orthogonal and parallel to the magnetic field are equal
and in agreement with the experimental values. The dynamical critical exponent
is $z=2$, also in agreement with the measured value. Several perturbations to
the solvable model are considered and shown to be irrelevant for the critical
behavior. It is argued that there exists an optimal density of defects at which
the transition temperature at given magnetic field reaches its maximum.

<id>
cond-mat/9610091v1
<category>
cond-mat.supr-con
<abstract>
The structure of an isolated single vortex and the vortex lattice, and the
magnetization in a $d$-wave superconductor are investigated within a
phenomenological Ginzburg-Landau (GL) model including the mixture of the
$d_{x^2-y^2}$-wave and $d_{xy}$-wave symmetry. The isolated single vortex
structure in a week magnetic field is studied both numerically and
asymptotically. Near the upper critical field $H_{c2}$, the vortex lattice
structure and the magnetization are calculated analytically.

<id>
cond-mat/9610101v1
<category>
cond-mat.supr-con
<abstract>
We present a consistent theory of superconductive tunneling in single-mode
junctions within a scattering formulation of Bogoliubov-de Gennes quantum
mechanics. Both dc Josephson effect and dc quasiparticle transport in voltage
biased junctions are considered. Elastic quasiparticle scattering by the
junction determines equilibrium Josephson current. We discuss the origin of
Andreev bound states in tunnel junctions and their role in equilibrium
Josephson transport. In contrast, quasiparticle tunneling in voltage biased
junctions is determined by inelastic scattering. We derive a general expression
for inelastic scattering amplitudes and calculate the quasiparticle current at
all voltages with emphasis on a discussion of the properties of subgap tunnel
current and the nature of subharmonic gap structure.

<id>
cond-mat/9610104v1
<category>
cond-mat.supr-con
<abstract>
Using the density matrix renormalization group, we study domain wall
structures in the t-J model at a hole doping of x=1/8. We find that the domain
walls are composed of d_{x^2-y^2} pairs and that the regions between the domain
walls have antiferromagnetic correlations that are pi phase shifted across a
domain wall. At x=1/8, the hole filling corresponds to one hole per two domain
wall unit cells. When the pairs in a domain wall are pinned by an external
field, the d_{x^2-y^2} pairing response is suppressed, but when the pinning is
weakened, d_{x^2-y^2} pair-field correlations can develop.

<id>
cond-mat/9610122v3
<category>
cond-mat.supr-con
<abstract>
The Hall conductivity in the mixed state of a clean ($l \gg \xi_0$) type-II
s-wave superconductor is determined from a microscopic calculation within a
quasiclassical approximation. We find that below the superconducting transition
the contribution to the transverse conductivity due to dynamical fluctuations
of the order parameter is compensated by the modification of the quasiparticle
contribution. In this regime the nonlinear behaviour of the Hall angle is
governed by the change in the effective quasiparticle scattering rate due to
the reduction in the density of states at the Fermi level. The connection with
experimental results is discussed.

<id>
cond-mat/9610141v3
<category>
cond-mat.supr-con
<abstract>
The effects of finite size and of finite current on the current-voltage
characteristics of Josephson junction arrays is studied both theoretically and
by numerical simulations. The cross-over from non-linear to linear behavior at
low temperature is shown to be a finite size effect and the non-linear behavior
at higher temperature, $T>T_{KT}$, is shown to be a finite current effect.
These are argued to result from competition between the three length scales
characterizing the system. The importance of boundary effects is discussed and
it is shown that these may dominate the behavior in small arrays.

<id>
cond-mat/9610146v1
<category>
cond-mat.supr-con
<abstract>
A description of a dislocation-free elastic glass phase in terms of domain
walls is developed and used as the basis of a renormalization group analysis of
the energetics of dislocation loops added to the system. It is found that even
after optimizing over possible paths of large dislocation loops, their energy
is still very likely to be positive when the dislocation core energy is large.
This implies the existence of an equilibrium elastic glass phase in three
dimensional random field X-Y magnets, and a dislocation free,
bond-orientationally ordered ``Bragg glass'' phase of vortices in dirty Type II
superconductors.

<id>
cond-mat/9610151v1
<category>
cond-mat.supr-con
<abstract>
We carry out simulations of the anisotropic uniformly frustrated 3D XY model,
as a model for vortex line fluctuations in high Tc superconductors. We compute
the phase diagram as a function of temperature and anisotropy, for a fixed
applied magnetic field. We find that superconducting coherence parallel to the
field persists into the vortex line liquid state, and that this transition lies
well below the "mean-field" cross-over from the vortex line liquid to the
normal state.

<id>
cond-mat/9610152v1
<category>
cond-mat.supr-con
<abstract>
We calculate the electronic thermal conductivity in a d-wave superconductor,
including both the effect of impurity scattering and inelastic scattering by
antiferromagnetic spin fluctuations. We analyze existing experiments,
particularly with regard to the question of the relative importance of
electronic and phononic contributions to the heat current, and to the influence
of disorder on low-temperature properties. We find that phonons dominate heat
transport near T_c, but that electrons are responsible for most of the peak
observed in clean samples, in agreement with a recent analysis of Krishana et
al. In agreement with recent data on YBa_2(Cu_1-xZn_x)_3O_7-\delta the peak
position is found to vary nonmonotonically with disorder.

<id>
cond-mat/9610157v3
<category>
cond-mat.supr-con
<abstract>
Poisson brackets for the Hamiltonian dynamics of vortices are discussed for 3
regimes, in which the dissipation can be neglected and the vortex dynamics is
reversible: (i) The superclean regime when the spectral flow is suppressed.
(ii) The regime when the fermions are pinned by crystal lattice. This includes
also the regime of the extreme spectral flow of fermions in the vortex core:
these fermions are effectively pinned by the normal component. (iii) The case
when the vortices are strongly pinned by the normal component. All these limits
are described by the single parameter $C_0$, which physical meaning is
discussed for superconductors containing several bands of electrons and holes.
The effect of the Fermi-surface topology on the vortex dynamics is also
discussed.

<id>
cond-mat/9610167v2
<category>
cond-mat.supr-con
<abstract>
We argue that the standard Abrikosov-Gorkov (AG) type theory of $T_c$ in
disordered $d$-wave superconductors breaks down in short coherence length
high-$T_c$ cuprates. Numerical calculations within the Bogoliubov-de Gennes
formalism demonstrate that the correct description of such systems must allow
for the spatial variation of the order parameter, which is strongly suppressed
in the vicinity of impurities but mostly unaffected elsewhere. Suppression of
$T_c$ as measured with respect to the attendant decrease in the superfluid
density is found to be significantly weaker than that predicted by the AG
theory, in good agreement with experiment.

<id>
gr-qc/9207001v1
<category>
gr-qc
<abstract>
The non-minimal coupling of a scalar field is considered in the framework of
Ashtekar's new variables formulation of gravity. A first order action
functional for this system is derived in which the field variables are a tetrad
field, and an SL(2,C) connection, together with the scalar field. The tetrad
field and the SL(2,C) connection are related to the Ashtekar variables for the
vacuum case by a conformal transformation. A canonical analysis shows that for
this coupling the equations of Ashtekar's formulation of canonical gravity are
non-polynomial in the scalar field. (to be published in Phys. Rev. D)

<id>
gr-qc/9207002v1
<category>
gr-qc
<abstract>
In the derivation of a pure spin connection action functional for gravity two
methods have been proposed. The first starts from a first order lagrangian
formulation, the second from a hamiltonian formulation. In this note we show
that they lead to identical results for the specific cases of pure gravity with
or without a cosmological constant.

<id>
gr-qc/9207003v1
<category>
gr-qc
<abstract>
Motivated by the apparent dependence of string $\sigma$--models on the sum of
spacetime metric and antisymmetric tensor fields, we reconsider gravity
theories constructed from a nonsymmetric metric. We first show that all such
"geometrical" theories homogeneous in second derivatives violate standard
physical requirements: ghost-freedom, absence of algebraic inconsistencies or
continuity of degree-of-freedom content. This no-go result applies in
particular to the old unified theory of Einstein and its recent avatars.
However, we find that the addition of nonderivative, ``cosmological'' terms
formally restores consistency by giving a mass to the antisymmetric tensor
field, thereby transmuting it into a fifth-force-like massive vector but with
novel possible matter couplings. The resulting macroscopic models also exhibit
``van der Waals''-type gravitational effects, and may provide useful
phenomenological foils to general relativity.

<id>
gr-qc/9207005v1
<category>
gr-qc
<abstract>
General relativity has previously been extended to incorporate degenerate
metrics using Ashtekar's hamiltonian formulation of the theory. In this letter,
we show that a natural alternative choice for the form of the hamiltonian
constraints leads to a theory which agrees with GR for non-degenerate metrics,
but differs in the degenerate sector from Ashtekar's original degenerate
extension. The Poisson bracket algebra of the alternative constraints closes in
the non-degenerate sector, with structure functions that involve the {\it
inverse} of the spatial triad. Thus, the algebra does {\it not} close in the
degenerate sector. We find that it must be supplemented by an infinite number
ofsecondary constraints, which are shown to be first class (although their
explicit form is not worked out in detail). All of the constraints taken
together are implied by, but do not imply, Ashtekar's original form of
constraints. Thus, the alternative constraints give rise to a different
degenerate extension of GR. In the corresponding quantum theory, the single
loop and intersecting loop holonomy states found in the connection
representation satisfy {\it all} of the constraints. These states are therefore
exact (formal) solutions to this alternative degenerate extension of quantum
gravity, even though they are {\it not} solutions to the usual vector
constraint.

<id>
gr-qc/9207006v1
<category>
gr-qc
<abstract>
It has recently been shown by Goldberg et al that the holonomy group of the
chiral spin-connection is preserved under time evolution in vacuum general
relativity. Here, the underlying reason for the time-independence of the
holonomy group is traced to the self-duality of the curvature 2-form for an
Einstein space. This observation reveals that the holonomy group is
time-independent not only in vacuum, but also in the presence of a cosmological
constant. It also shows that once matter is coupled to gravity, the
"conservation of holonomy" is lost. When the fundamental group of space is
non-trivial, the holonomy group need not be connected. For each homotopy class
of loops, the holonomies comprise a coset of the full holonomy group modulo its
connected component. These cosets are also time-independent. All possible
holonomy groups that can arise are classified, and examples are given of
connections with these holonomy groups. The classification of local and global
solutions with given holonomy groups is discussed.

<id>
gr-qc/9207008v1
<category>
gr-qc
<abstract>
We consider the general procedure for proving no-hair theorems for static,
spherically symmetric black holes. We apply this method to the abelian Higgs
model and find a proof of the no-hair conjecture that circumvents the
objections raised against the original proof due to Adler and Pearson.

<id>
gr-qc/9207009v1
<category>
gr-qc
<abstract>
We examine gravitational waves in an isolated axi--symmetric reflexion
symmetric NGT system. The structure of the vacuum field equations is analyzed
and the exact solutions for the field variables in the metric tensor are found
in the form of expansions in powers of a radial coordinate. We find that in the
NGT axially symmetric case the mass of the system remains constant only if the
system is static (as it necessarily is in the case of spherical symmetry). If
the system radiates, then the mass decreases monotonically and the energy flux
associated with waves is positive.

<id>
gr-qc/9207010v1
<category>
gr-qc
<abstract>
Using the tetrad formalism, we carry out the separation of variables for the
massive complex Dirac equation in the gravitational and electromagnetic field
of a four-parameter (mass, angular momentum, electric and magnetic charges)
black hole.

<id>
gr-qc/9207011v1
<category>
gr-qc
<abstract>
We have obtained the correct expression for the centrifugal force acting on a
particle at the equatorial circumference of a rotating body in the locally
non-rotating frame of the Kerr geometry. Using this expression for the
equilibrium of an element on the surface of a slowly rotating Maclaurin
spheroid, we obtain the expression for the ellipticity (as discussed earlier by
Abramowicz and Miller) and determine the radius at which the ellipticity is
maximum.

<id>
gr-qc/9208005v1
<category>
gr-qc
<abstract>
This paper derives and analyzes exact, nonlocal Langevin equations
appropriate in a cosmological setting to describe the interaction of some
collective degree of freedom with a surrounding ``environment.'' Formally,
these equations are much more general, involving as they do a more or less
arbitrary ``system,'' characterized by some time-dependent potential, which is
coupled via a nonlinear, time-dependent interaction to a ``bath'' of
oscillators with time-dependent frequencies. The analysis reveals that, even in
a Markov limit, which can often be justified, the time dependences and
nonlinearities can induce new and potentially significant effects, such as
systematic and stochastic mass renormalizations and state-dependent ``memory''
functions, aside from the standard ``friction'' of a heuristic Langevin
description. One specific example is discussed in detail, namely the case of an
inflaton field, characterized by a Landau-Ginsburg potential, that is coupled
quadratically to a bath of scalar ``radiation.'' The principal conclusion
derived from this example is that nonlinearities and time-dependent couplings
do {\em not} preclude the possibility of deriving a fluctuation-dissipation
theorem, and do {\em not} change the form of the late-time steady state
solution for the system, but {\em can} significantly shorten the time scale for
the approach towards the steady state.

<id>
gr-qc/9208006v1
<category>
gr-qc
<abstract>
In this paper a quantum mechanical phase space picture is constructed for
coarse-grained free quantum fields in an inflationary Universe. The appropriate
stochastic quantum Liouville equation is derived. Explicit solutions for the
phase space quantum distribution function are found for the cases of power law
and exponential expansions. The expectation values of dynamical variables with
respect to these solutions are compared to the corresponding cutoff regularized
field theoretic results (we do not restrict ourselves only to $\VEV{\F^2}$).
Fair agreement is found provided the coarse-graining scale is kept within
certain limits. By focusing on the full phase space distribution function
rather than a reduced distribution it is shown that the thermodynamic
interpretation of the stochastic formalism faces several difficulties (e.g.,
there is no fluctuation-dissipation theorem). The coarse-graining does not
guarantee an automatic classical limit as quantum correlations turn out to be
crucial in order to get results consistent with standard quantum field theory.
Therefore, the method does {\em not} by itself constitute an explanation of the
quantum to classical transition in the early Universe. In particular, we argue
that the stochastic equations do not lead to decoherence.

<id>
gr-qc/9209001v1
<category>
gr-qc
<abstract>
The local Lorentz and diffeomorphism symmetries of Einstein's gravitational
theory are spontaneously broken by a Higgs mechanism by invoking a phase
transition in the early Universe, at a critical temperature $T_c$ below which
the symmetry is restored. The spontaneous breakdown of the vacuum state
generates an external time and the wave function of the Universe satisfies a
time dependent Schrodinger equation, which reduces to the Wheeler-deWitt
equation in the classical regime for $T < T_c$, allowing a semi-classical WKB
approximation to the wave function. The conservation of energy is spontaneously
violated for $T > T_c$ and matter is created fractions of seconds after the big
bang, generating the matter in the Universe. The time direction of the vacuum
expectation value of the scalar Higgs field generates a time asymmetry, which
defines the cosmological arrow of time and the direction of increasing entropy
as the Lorentz symmetry is restored at low temperatures.

<id>
gr-qc/9209002v2
<category>
gr-qc
<abstract>
A family of solutions to low energy string theory is found. These solutions
represent waves traveling along "extremal black strings"

<id>
gr-qc/9209006v1
<category>
gr-qc
<abstract>
We present new evidence in support of the Penrose's strong cosmic censorship
conjecture in the class of Gowdy spacetimes with $T^3$ spatial topology.
Solving Einstein's equations perturbatively to all orders we show that
asymptotically close to the boundary of the maximal Cauchy development the
dominant term in the expansion gives rise to curvature singularity for almost
all initial data. The dominant term, which we call the ``geodesic loop
solution'', is a solution of the Einstein's equations with all space
derivatives dropped. We also describe the extent to which our perturbative
results can be rigorously justified.

<id>
gr-qc/9209007v1
<category>
gr-qc
<abstract>
We present a class of exact solutions to the constraint equations of General
Relativity coupled to a Klein - Gordon field, these solutions being isotropic
but not homogeneous. We analyze the subsequent evolution of the consistent
Cauchy data represented by those solutions, showing that only certain special
initial conditions eventually lead to successfull Inflationary cosmologies. We
argue, however, that these initial conditions are precisely the likely outcomes
of quantum events occurred before the inflationary era.

<id>
gr-qc/9209009v1
<category>
gr-qc
<abstract>
After reviewing the context in which Euclidean propagation is useful we
compare and contrast Euclidean and Lorentzian Maxwell-Einstein theory and give
some examples of Euclidean solutions.

<id>
gr-qc/9209013v2
<category>
gr-qc
<abstract>
We have shown that two of the most studied models of lineal gravities -
Liouville gravity and a ``string-inspired'' model exhibiting the main
characteristic features of a black-hole solution - can be formulated as gauge
invariant theories of the Poincar\'e group. The gauge invariant couplings to
matter (particles, scalar and spinor fields) and explicit solutions for some
matter field configurations, are provided. It is shown that both the models, as
well as the couplings to matter, can be obtained as suitable dimensional
reductions of a 2+1-dimensional ISO(2,1) gauge invariant theory.

<id>
gr-qc/9210001v1
<category>
gr-qc
<abstract>
I review the equivalence between duality operators on two-forms and conformal
structures in four dimensions, from a Clifford algebra point of view (due to
Urbantke and Harnett). I also review an application, which leads to a set of
"neighbours" of Einstein's equations. An attempt to formulate reality
conditions for the "neighbours" is discussed.

<id>
gr-qc/9210002v1
<category>
gr-qc
<abstract>
Bounds to the Nordtvedt parameter are obtained from the motion of the first
twelve Trojan asteroids in the period 1906-1990. From the analysis performed,
we derive a value for the inverse of the Saturn mass 3497.80 \pm 0.81 and the
Nordtvedt parameter -0.56 \pm 0.48, from a simultaneous solution for all
asteroids.

<id>
gr-qc/9210011v1
<category>
gr-qc
<abstract>
This is the write-up of my lectures at the NATO Summer School held in
Salamanca in June 1992. The paper deals with the problem of time in quantum
gravity. All the major schemes are reviewed. Please note that the paper is in
two parts for ease of email transmission; this is part 1. The mailer from gr-qc
may further subdivide these two sections.

<id>
gr-qc/9210013v1
<category>
gr-qc
<abstract>
The Weyl curvature inside a black hole formed in a generic collapse grows,
classically without bound, near to the inner horizon, due to partial absorption
and blueshifting of the radiative tail of the collapse. Using a spherical
model, we examine how this growth is modified by quantum effects of conformally
coupled massless fields.

<id>
gr-qc/9210015v2
<category>
gr-qc
<abstract>
Various physical properties of cosmological models in (1+1) dimensions are
investigated. We demonstrate how a hot big bang and a hot big crunch can arise
in some models. In particular, we examine why particle horizons do not occur in
matter and radiation models. We also discuss under what circumstances
exponential inflation and matter/radiation decoupling can happen. Finally,
without assuming any particular equation of state, we show that physical
singularities can occur in both untilted and tilted universe models if certain
assumptions are satisfied, similar to the (3+1)-dimensional cases.

<id>
gr-qc/9210016v1
<category>
gr-qc
<abstract>
Progress in numerical relativity has been hindered for 30 years because of
the difficulties of avoiding spacetime singularities in numerical evolution. We
propose a scheme which excises a region inside an apparent horizon containing
the singularity. Two major ingredients of the scheme are the use of a
horizon-locking coordinate and a finite differencing which respects the causal
structure of the spacetime. Encouraging results of the scheme in the spherical
collapse case are given.

<id>
gr-qc/9210017v1
<category>
gr-qc
<abstract>
A very simple wormhole geometry is considered as a model of a mode of
topological fluctutation in Planck-scale spacetime foam. Quantum dynamics of
the hole reduces to quantum mechanics of one variable, throat radius, and
admits a WKB analysis. The hole is quantum-mechanically unstable: It has no
bound states. Wormhole wave functions must eventually leak to large radii. This
suggests that stability considerations along these lines may place strong
constraints on the nature and even the existence of spacetime foam.

<id>
gr-qc/9210018v1
<category>
gr-qc
<abstract>
Simon argued that the semi-classical theory of gravity, unless with some of
its solutions excluded, is unacceptable for reasons of both self-consistency
and experiment, and that it has to be replaced by a constrained semi-classical
theory. We examined whether the evidence is conclusive.

<id>
gr-qc/9210019v1
<category>
gr-qc
<abstract>
The simple physics of a free particle reveals important features of the
path-integral formulation of relativistic quantum theories. The exact
quantum-mechanical propagator is calculated here for a particle described by
the simple relativistic action proportional to its proper time. This propagator
is nonvanishing outside the light cone, implying that spacelike trajectories
must be included in the path integral. The propagator matches the WKB
approximation to the corresponding configuration-space path integral far from
the light cone; outside the light cone that approximation consists of the
contribution from a single spacelike geodesic. This propagator also has the
unusual property that its short-time limit does not coincide with the WKB
approximation, making the construction of a concrete skeletonized version of
the path integral more complicated than in nonrelativistic theory.

<id>
gr-qc/9210020v3
<category>
gr-qc
<abstract>
The phenomenon of linearisation instability is identified in models of
quantum cosmology that are perturbations of mini-superspace models. In
particular, constraints that are second order in the perturbations must be
imposed on wave functions calculated in such models. It is shown explicitly
that in the case of a model which is a perturbation of the mini-superspace
which has $S^3$ spatial sections these constraints imply that any wave
functions calculated in this model must be SO(4) invariant. (This replaces the
previous corrupted version.)

<id>
gr-qc/9211001v2
<category>
gr-qc
<abstract>
In the context of a Poincar\'e gauge theoretical formulation, pure gravity in
3+1-dimensions is dimensionally reduced to gravity in 2+1-dimensions with or
without cosmological constant $\Lambda$. The dimensional reductions are
consistent with the gauge symmetries, mapping ISO(3, 1) gauge transformations
into ISO(2,1) ones. One of the reductions leads to Chern-Simons-Witten gravity.
The solutions of 2+1-gravity with $\Lambda\le 0$ (in particular the black-hole
solution recently found by Banados, Teitelboim and Zanelli) and those of
1+1-dimensional Liouville gravity, are thus mapped into 3+1-dimensional vacuum
solutions.

<id>
gr-qc/9211002v1
<category>
gr-qc
<abstract>
We consider the Einstein equation with first order (semiclassical) quantum
corrections. Although the quantum corrections contain up to fourth order
derivatives of the metric, the solutions which are physically relevant satisfy
a reduced equations which contain derivatives no higher than second order. We
obtain the reduced equations for a range of stress-energy tensors. These
reduced equations are suitable for numerical solution, are expected to contain
fewer numerical instabilities than the original fourth order equations, and
yield only physically relevant solutions. We give analytic and numerical
solutions or reduced equations for particular examples, including
Friedmann-Lema\^\i tre universes with cosmological constant, a spherical body
of constant density, and more general conformally flat metrics.

<id>
gr-qc/9211005v1
<category>
gr-qc
<abstract>
Accurate limits for the violation of the Principle of Equivalence have been
found from the comparison of the redshifts of two identical nuclear species in
different chemical environments.

<id>
hep-ex/9405002v1
<category>
hep-ex
<abstract>
Parity violation at LEP or SLC can be measured through the charge asymmetry.
An optimal method of moments is developed here to measure this asymmetry, as
well as similar asymmetries. This method is equivalent to the likelihood fit.
It is simpler in use, as it gives analytical formulas for both the asymmetry
and its statistical error. These formulas give the dependence of the accuracy
on the experimental angular acceptance explicitly.

<id>
hep-ex/9405003v1
<category>
hep-ex
<abstract>
Event shapes for Au + Au collisions at 11.4 GeV/c per nucleon were studied
over nearly the full solid angle with the E877 apparatus. The analysis was
performed by Fourier expansion of azimuthal distributions of the transverse
energy (E_T) measured in different pseudorapidity intervals. For semicentral
collisions a pronounced event anisotropy is identified beyond that expected due
to fluctuations in particle multiplicity. The signal decreases for peripheral
and very central collisions. The amplitude of the flow signal reaches up to 7%
of the mean E_T.

<id>
hep-ex/9405006v2
<category>
hep-ex
<abstract>
We have determined the strong coupling alpha-s from a comprehensive study of
energy-energy correlations (EEC) and their asymmetry (AEEC) in hadronic decays
of Z0 bosons collected by the SLD experiment at SLAC. The data were compared
with all four available predictions of QCD calculated up to O(alpha-s**2) in
perturbation theory, and also with a resummed calculation matched to all four
of these calculations. We find large discrepancies between alpha-s values
extracted from the different O(alpha-s**2) calculations. We also find a large
renormalization scale ambiguity in alpha-s determined from the EEC using the
O(alpha-s**2) calculations; this ambiguity is reduced in the case of the AEEC,
and is very small when the matched calculations are used. Averaging over all
calculations, and over the EEC and AEEC results, we obtain
alpha-s(MZ)=0.124+0.003-0.004(exp.) +-0.009(theory).

<id>
hep-ex/9405008v1
<category>
hep-ex
<abstract>
We report a precise measurement of the weak mixing angle from the ratio of
neutral current to charged current inclusive cross-sections in deep-inelastic
neutrino-nucleon scattering. The data were gathered at the CCFR neutrino
detector in the Fermilab quadrupole-triplet neutrino beam, with neutrino
energies up to 600 GeV. Using the on-shell definition, ${\rm sin ^2\theta_W}
\equiv 1 - \frac{{\rm M_W} ^2}{{\rm M_Z} ^2}$, we obtain ${\rm sin ^2\theta_W}
= 0.2218 \pm 0.0025 ({\rm stat.}) \pm 0.0036 ({\rm exp.\: syst.}) \pm 0.0040
({\rm model})$.

<id>
hep-ex/9405011v1
<category>
hep-ex
<abstract>
In the 1993 SLC/SLD run, the SLD recorded 50,000 $\z0$ events produced by the
collision of longitudinally polarized electrons on unpolarized positrons at a
center-of-mass energy of 91.26 GeV. The luminosity-weighted average
polarization of the SLC electron beam was (63.0$\pm$1.1)\%. We measure the
left-right cross-section asymmetry in $\z0$ boson production, $\alr$, to be
0.1628$\pm$0.0071(stat.)$\pm$0.0028(syst.) which determines the effective weak
mixing angle to be $\swein=0.2292\pm0.0009({\rm stat.})\pm0.0004({\rm syst.}).$

<id>
hep-ex/9406001v1
<category>
hep-ex
<abstract>
The lepton and quark asymmetries measured at LEP are presented. The results
of the Standard Model fits to the electoweak data presented at this conference
are given. The top mass obtained from the fit to the LEP data is
$172^{+13+18}_{-14-20}$ GeV; it is $177^{+11+18}_{-11-19}$ when also the
collider, $\nu$ and $A_{LR}$ data are included.

<id>
hep-ex/9406002v1
<category>
hep-ex
<abstract>
The ratio of average multiplicities in gluon and quark jets is shown to
become noticeably smaller in higher-order QCD compared to its lowest order
value what improves agreement with experiment.
  QCD anomalous dimension has been calculated. It has been used to get energy
dependence of mean multiplicities.

<id>
hep-ex/9406003v2
<category>
hep-ex
<abstract>
Using simulated collider data for $p+p\rightarrow 2{\rm Jets}\ $ interactions
in a 2-barrel pixel detector, a neural network is trained to construct the
coordinate of the primary vertex to a high degree of accuracy. Three other
estimates of this coordinate are also considered and compared to that of the
neural network. It is shown that the network can match the best of the
traditional estimates.

<id>
hep-ex/9406004v1
<category>
hep-ex
<abstract>
An examination of leptons in ${\Upsilon (4S)}$ events tagged by reconstructed
$B$ decays yields semileptonic branching fractions of $b_-=(10.1 \pm 1.8\pm
1.4)\%$ for charged and $b_0=(10.9 \pm 0.7\pm 1.1)\%$ for neutral $B$ mesons.
This is the first measurement for charged $B$. Assuming equality of the charged
and neutral semileptonic widths, the ratio $b_-/b_0=0.93 \pm 0.18 \pm 0.12$ is
equivalent to the ratio of lifetimes. A postscript version is available through
World-Wide-Web in http://w4.lns.cornell.edu/public/CLNS/1994

<id>
hep-ex/9406005v1
<category>
hep-ex
<abstract>
We study the exclusive semileptonic B meson decays B- -> D*0 l- nu and B0 ->
D*+ l- nu using data collected with the CLEO II detector at CESR. We present
measurements of the branching fractions B(B0 -> D*+ l-nu) = 0.5/f00*
[4.49+/-0.32+/-0.39]% and B(B- -> D*0 l-nu) = 0.5/f+-*[5.13+/-0.54+/-0.64]%,
where f00 and f+- are the neutral and charged B meson production fractions at
the Upsilon(4s) resonance. Assuming isospion invariance and taking the charged
to neutral B meson lifetimes measured at higher energy machines, we determine
the ratio f+-/f00=1.04+/-0.14+/-0.13+-/-0.10; further assuming f+- + f00 = 1 we
also determine the partial width G(B->D* l nu) = 29.9+/-1.9+/-2.7+/-2.0 ns-1
(independent of f+-/f00). From this partial width we calculate B -> D* l nu
branching fractions that do not depend on f+-/f00, nor the individual B
lifetimes, but only on the charged to neutral lifetime ratio. The product of
the CKM matrix element |Vcb| times the normalization of the decay form factor
at the point of zero recoil of the D* meson, F(y=1), is determined from a
linear fit to the combined differential decay rate of the exclusive B->D* l nu
decays: |Vcb|F(y) = 0.0351 +/- 0.0019 +/- 0.0018 +/- 0.0008. Using theoretical
calculations of the form factor normalization we extract a value for |Vcb|.
LATEX (REVTEX style) file with uuencoded figures attached (uses PSBOX).
Available on WWW http://w4.lns.cornell.edu/public/CLNS/

<id>
hep-ex/9406006v1
<category>
hep-ex
<abstract>
Particle track reconstruction capabilities of the silicon tracking detector
system have been studied. As the multiple Coulomb scattering (MCS) induces
unavoidable uncertainties on the coordinate measurement, the corresponding
error estimates and the associated correlations have been used to find the best
track fit parameters and their errors. Finally it permits to find the proper
particle characteristics, as vertex position and resolution, flight direction
and the error.

<id>
hep-ex/9406007v1
<category>
hep-ex
<abstract>
We present the first next-to-leading-order QCD analysis of neutrino charm
production, using a sample of 6090 $\nu_\mu$- and $\bar\nu_\mu$-induced
opposite-sign dimuon events observed in the CCFR detector at the Fermilab
Tevatron. We find that the nucleon strange quark content is suppressed with
respect to the non-strange sea quarks by a factor $\kappa = 0.477 \:
^{+\:0.063}_{-\:0.053}$, where the error includes statistical, systematic and
QCD scale uncertainties. In contrast to previous leading order analyses, we
find that the strange sea $x$-dependence is similar to that of the non-strange
sea, and that the measured charm quark mass, $m_c = 1.70 \pm 0.19 \:{\rm
GeV/c}^2$, is larger and consistent with that determined in other processes.
Further analysis finds that the difference in $x$-distributions between $xs(x)$
and $x\bar s(x)$ is small. A measurement of the Cabibbo-Kobayashi-Maskawa
matrix element $|V_{cd}|=0.232 ^{+\:0.018}_{-\:0.020}$ is also presented.
uufile containing compressed postscript files of five Figures is appended at
the end of the LaTeX source.

<id>
hep-ex/9407001v1
<category>
hep-ex
<abstract>
The FNAL experiment E773 has measured the phases Phi_{+-} = (43.35 +- 0.70 +-
0.79) degree and Phi_{00}-Phi_{+-} = (0.67 +- 0.85 +- 1.1) degree of the CP
violating parameters eta_{+-} and eta_{00} in the decay of neutral kaons into
two charged or neutral pions. These preliminary results test CPT symmetry and
show no evidence for a violation. In addition we present a preliminary
measurement of Delta m = m_L - m_S = (0.5286 +- 0.0029 +- 0.0022)*E+10 hbar/sec
and tau_S = (0.8929 +- 0.0014 +- 0.0014)*E-10 sec. The first errors are
statistical and the second errors are systematic for above results.

<id>
hep-ex/9407002v1
<category>
hep-ex
<abstract>
The electroweak measurements made at LEP using 1989-1993 data are presented
in preliminary form. The agreement with the Standard Model is satisfactory, and
allows a combined fit to all available data for the masses of the top quark and
standard Higgs boson. The fit yields M_t = 177 +11 -11 +18 -19 GeV/c2, where
the second error reflects the uncertainty in the Higgs mass.
  Talk given at the XXIXth Rencontre de Moriond, `QCD and High Energy Hadronic
Interactions', March 1994, Meribel France

<id>
hep-ex/9408001v2
<category>
hep-ex
<abstract>
We measured the spin asymmetry in the scattering of 100 GeV
longitudinally-polarized muons on transversely polarized protons. The asymmetry
was found to be compatible with zero in the kinematic range $0.006<x<0.6$,
$1<Q^2<30\,~\mbox{GeV}^2$. {}From this result we derive the upper limits for
the virtual photon--proton asymmetry $A_2$, and for the spin structure function
$g_2$. For $x<0.15$, $A_2$ is significantly smaller than its positivity limit
$\sqrt{R}$.

<id>
hep-ex/9408002v1
<category>
hep-ex
<abstract>
Using a technique which employs a pair of solid scintillator regenerators,
the E773 collaboration has measured several CP violation parameters in K meson
decay at Fermilab. We report new results for the phase of eta_+-, the K_L-K_S
mass difference, the K_S lifetime, and the phase difference
Arg(eta_00)-Arg(eta_+-) in K ->pi pi decay. In addition, we report a
measurement of the magnitude and phase of eta_+-gamma in K ->pi+ pi- gamma
decay. Our preliminary results are compared with theoretical expectations based
on CPT symmetry. (Glasgow ICHEP94 paper Ref. gls0167, set with REVTeX)

<id>
hep-ex/9408003v2
<category>
hep-ex
<abstract>
Results on semi-hadronic decays of the $\tau$ lepton are presented, from
studies of $e^+e^-$ annihilation data obtained at the Cornell Electron Storage
Ring with the CLEO-II detector. Branching fractions have been measured for
decays to two, three and four hadrons, namely $\tau^-\!\!\rightarrow\! \nu_\tau
h^-\pi^0$, $\tau^-\!\!\rightarrow\! \nu_\tau h^-h^+h^-$, and
$\tau^-\!\!\rightarrow\! \nu_\tau h^-h^+h^-\pi^0$, where $h^\pm$ represents a
charged pion or kaon. CLEO-II has also observed decays with charged and/or
neutral kaons; preliminary results for branching ratios and structure arising
from the decay dynamics are given. Connections are made with predictions
derived from theoretical models, the Conserved Vector Current theorem, isospin
constraints and sum rules.

<id>
hep-ex/9409001v1
<category>
hep-ex
<abstract>
The polarization of Lambda0, AntiLambda0, Sigma+ and Xi- inclusively produced
in Sigma- induced interactions at 330 GeV has been measured in the experiment
WA89 at CERN. This is the first measurement of polarization of baryons produced
by a hyperon beam. No polarization of AntiLambda is observed, as was also the
case in proton beam data. At transverse momenta of about 1 GeV/c Lambda0 and
Sigma+ show little polarization, significantly lower than in the proton beam
data, while Xi- have a polarization comparable to the polarization of Lambda0
produced in proton beams.

<id>
hep-ex/9409002v1
<category>
hep-ex
<abstract>
We review $B^+$ and $B^0$ mean lifetime measurements, including direct
measurements and determination of the lifetime ratio via measurements of the
ratio of branching ratios. We present world averages.

<id>
hep-ex/9409003v1
<category>
hep-ex
<abstract>
Improved branching ratios were measured for the $K_L \to 3 \pi^0 $ decay in a
neutral beam at the CERN SPS with the NA31 detector: $\Gamma (K_L \to 3 \pi^0)
/ \Gamma (K_L \to \pi^+ \pi^- \pi^0) = 1.611 \pm 0.037$ and $\Gamma (K_L \to 3
\pi^0) / \Gamma (K_L \to \pi e \nu ) = 0.545 \pm 0.010$.
  From the first number an upper limit for $\Delta I =5/2$ and $\Delta I = 7/2
$ transitions in neutral kaon decay is derived. Using older results for the
Ke3/K$\mu $3 fraction, the 3$\pi^0$ branching ratio is found to be $\Gamma (K_L
\to 3 \pi^0 )/ \Gamma_{tot} = (0.211 \pm 0.003)$, about a factor three more
precise than from previous experiments.

<id>
hep-ex/9409004v1
<category>
hep-ex
<abstract>
We have studied the leptonic decay of the $\Upsilon (1S)$ resonance into tau
pairs using the CLEO II detector. A clean sample of tau pair events is
identified via events containing two charged particles where exactly one of the
particles is an identified electron. We find $B(\Upsilon(1S) \to \tau^+ \tau^-)
= (2.61~\pm~0.12~{+0.09\atop{-0.13}})%$. The result is consistent with
expectations from lepton universality.

<id>
hep-ex/9409005v1
<category>
hep-ex
<abstract>
We present preliminary results for the search for the top quark in D-Zero in
the electron + jets channel where one of the b quark jets is tagged by means of
a soft muon, using 13.5 pb-1 of data. Standard model decay modes for the top
quark are assumed. We present the resulting top cross section and error as a
function of top mass using this channel combined with the dilepton channel and
the untagged lepton + jets channel . At present, no significant signal for top
quark production can be established.

<id>
hep-ex/9409006v1
<category>
hep-ex
<abstract>
We review the search for the top quark conducted by the D-Zero collaboration
using data from the Fermilab pbar-p collider. Based upon a preliminary analysis
of an integrated luminosity of about 13.5 pb-1, we have searched for t-tbar
production and decay in the experimental channels involving a pair of dileptons
(electron or muon) plus jets, or single leptons plus jets. Summed over all
channels, we observe 7 events in our data, to be compared with an expectation
from background processes of 3.2 +/- 1.1 events. The t-tbar ~cross-section
deduced from the small excess of events is presented as a function of the top
quark mass. The statistics are sufficiently limited that no clear evidence for
the existence of the top quark can be obtained. We also comment upon
contributions to the Parallel session devoted to the top quark at this
conference.

<id>
hep-ex/9409007v1
<category>
hep-ex
<abstract>
We have searched for production of t-tbar pairs in p-pbar interactions at 1.8
TeV center-of-mass energy at the FNAL Tevatron collider. The search assumes
standard model decay for top quark into W + b quark. We observe in e+jet and
mu+jet final states a small, not statistically significant, excess above the
background estimated by two different methods. The results presented are
preliminary.

<id>
hep-ex/9409008v2
<category>
hep-ex
<abstract>
A search for first and second generation leptoquarks has been done with the
D\O \ detector at Fermilab's p\={p} collider with $\sqrt{s}=1.8$ TeV. 95\% C.L.
mass limits for first generation scalar leptoquarks have been recently
published. The number for the total integrated luminosity used in the first
generation leptoquark analysis has changed (via a change in the total inelastic
cross section) since the publication. The new limits are 130 GeV/c$^{2}$ and
116 GeV/c$^{2}$ for a respective 100\% and 50\% decay branching ratio of the
leptoquark to electron. The preliminary upper limit on the cross section from
the search for second generation scalar leptoquarks has set limits on the mass
of the second generation leptoquark of 97 GeV/c$^{2}$ for 100\% branching to
muons and 80 GeV/c$^{2}$ for 50\% branching. In contrast with leptoquark
detection thresholds at e$^{+}$e$^{-}$ and e-p machines, these limits are
independent of the unknown coupling of the leptoquark to leptons and quarks.

<id>
hep-ex/9410002v1
<category>
hep-ex
<abstract>
The status of a search for the pair production of the lightest chargino and
second lightest neutralino states of the minimal supersymmetric model is
presented. We have searched for four tri-lepton final states: eee, eemu, emumu,
and mumumu, all with missing transverse energy.

<id>
hep-ex/9410003v3
<category>
hep-ex
<abstract>
Searches have been made for first generation scalar and vector leptoquarks by
the D0 collaboration and for second generation scalar leptoquarks by the CDF
collaboration. Lower leptoquark mass limits were set. A search for squarks and
gluinos, predicted by Supersymmetric models, was made by D0 in the three or
more jets plus missing Et channel. The number of events observed was consistent
with background. Squark and gluino mass limits were set.

<id>
hep-ex/9410004v1
<category>
hep-ex
<abstract>
For the 1994 Tevatron collider run, CDF has upgraded the electron and photon
trigger hardware to make use of shower position and size information from the
central shower maximum detector. For electrons, the upgrade has resulted in a
50\% reduction in backgrounds while retaining approximately 90\% of the signal.
The new trigger also eliminates the background to photon triggers from
single-phototube discharge.

<id>
hep-ex/9410005v1
<category>
hep-ex
<abstract>
Using data collected during the 1992-93 collider run at Fermilab, CDF has
made measurements of correlated $b$ quark cross section where one $b$ is
detected via a muon from semileptonic decay and the second $b$ is detected with
secondary vertex techniques. We report on measurements of the cross section as
a function of the momentum of the second $b$ and as a function of the azimuthal
separation of the two $b$ quarks, for transverse momentum of the initial $b$
quark greater than 15 GeV. Results are compared to QCD predictions.

<id>
hep-ex/9410006v1
<category>
hep-ex
<abstract>
We report a search for slowly moving magnetic monopoles in the cosmic
radiation with the first supermodule of the MACRO detector at Gran Sasso. The
absence of candidates established an upper limit on the monopole flux of
$5.6\times10^{-15}\mbox{cm}^{-2}\mbox{sr}^{-1}\mbox{s}^{-1}$ at 90\% confidence
level for the velocity range of $1.8\times10^{-4}<\beta<3\times10^{-3}$.

<id>
hep-lat/9107001v1
<category>
hep-lat
<abstract>
Lattice work, exploring the Higgs mass triviality bound, seems to indicate
that a strongly interacting scalar sector in the minimal standard model cannot
exist while low energy QCD phenomenology seems to indicate that it could. We
attack this puzzle using the 1/N expansion and discover a simple criterion for
selecting a lattice action that is more likely to produce a heavy Higgs
particle. Our large $N$ calculation suggests that the Higgs mass bound might be
around $850 GeV$, which is about 30% higher than previously obtained.

<id>
hep-lat/9107002v1
<category>
hep-lat
<abstract>
We present spectral density reweighting techniques adapted to the analysis of
a time series of data with a continuous range of allowed values. In a first
application we analyze action and Polyakov line data from a Monte Carlo
simulation on $L_t L^3 (L_t=2,4)$ lattices for the SU(3) deconfining phase
transition. We calculate partition function zeros, as well as maxima of the
specific heat and of the order parameter susceptibility. Details and warnings
are given concerning i) autocorrelations in computer time and ii) a reliable
extraction of partition function zeros. The finite size scaling analysis of
these data leads to precise results for the critical couplings $\beta_c$, for
the critical exponent $\nu$ and for the latent heat $\triangle s$. In both
cases ($L_t=2$ and 4), the first order nature of the transition is
substantiated.

<id>
hep-lat/9112001v1
<category>
hep-lat
<abstract>
Contrary to conventional wisdom, the construction of clusters on a lattice
can easily be vectorized, namely over each ``generation'' in a breadth first
search. This applies directly to, e.g., the {\it single cluster} variant of the
Swendsen-Wang algorithm. On a Cray Y-MP, total CPU time was reduced by a factor
3.5 -- 7 in actual applications.

<id>
hep-lat/9112002v1
<category>
hep-lat
<abstract>
We study the dynamic critical behavior of the multi-grid Monte Carlo (MGMC)
algorithm with piecewise-constant interpolation applied to the two-dimensional
O(4)-symmetric nonlinear $\sigma$-model [= SU(2) principal chiral model], on
lattices up to $256 \times 256$. We find a dynamic critical exponent
$z_{int,{\cal M}^2} = 0.60 \pm 0.07$ for the W-cycle and $z_{int,{\cal M}^2} =
1.13 \pm 0.11$ for the V-cycle, compared to $z_{int,{\cal M}^2} = 2.0 \pm 0.15$
for the single-site heat-bath algorithm (subjective 68% confidence intervals).
Thus, for this asymptotically free model, critical slowing-down is greatly
reduced compared to local algorithms, but not completely eliminated. For a $256
\times 256$ lattice, W-cycle MGMC is about 35 times as efficient as a
single-site heat-bath algorithm.

<id>
hep-lat/9201001v1
<category>
hep-lat
<abstract>
To gain understanding of the Higgs-fermion sector of the standard model, we
study the one-component $Z_2$ symmetric and the four-component O(4) symmetric
scalar models coupled to staggered fermions using the hybrid Monte Carlo
algorithm. We map out the phase diagrams, and show that the $Z_2$ model has a
tree level perturbative behaviour at all points in the broken phase. The O(4)
model on the other hand is shown to have two characteristically different
behaviours; one for large Yukawa couplings where the fermions get infinitely
heavy and decouple in the continuum limit, and one for small Yukawa couplings
where the fermions remain light. For very small Yukawa couplings the fermions
show the expected tree level perturbative behaviour and for larger values the
influence of the fermions becomes substantial. After estimating the finite size
effects at small Yukawa couplings we make relatively accurate measurements of
the scalar mass and wave function renormalization constants at the point
$\kappa=0.0$ and $y=0.85-0.95$. Even though this is not the largest value
possible for the Yukawa coupling we are able to show that the bound of the
Higgs mass will move up significantly, from around $600 GeV$ to around $900
GeV$, by including fermions in the model. Likewise we show that a bound can be
put on the fermion mass, around $200 GeV$. The largest value of the bare Yukawa
coupling is obtained at rather large negative $\kappa$. Due to bad convergence
rates in the inversion of the fermion matrix, which is needed in the updating
procedure, this region has not been possible to investigate.

<id>
hep-lat/9201002v1
<category>
hep-lat
<abstract>
Results of investigations of the O(4) spin model at finite temperature using
anisotropic lattices are presented. In both the large $N$ approximation and the
numerical simulations using the Wolff cluster algorithm we find that the ratio
of the symmetry restoration temperature $T_{\rm SR}$ to the Higgs mass $m_{\rm
H}$ is independent of the anisotropy. We obtain a lower bound of $0.59 \pm
0.04$ for the ratio, $T_{\rm SR}/m_{\rm H}$, at $m_{\rm H}a \simeq 0.5$, which
is lowered further by about 10% at $m_{\rm H}a \simeq 1.$

<id>
hep-lat/9201003v1
<category>
hep-lat
<abstract>
We study the performance of a Wolff-type embedding algorithm for $RP^N$
$\sigma$-models. We find that the algorithm in which we update the embedded
Ising model \`a la Swendsen-Wang has critical slowing-down as $z_\chi \approx
1$. If instead we update the Ising spins with a perfect algorithm which at
every iteration produces a new independent configuration, we obtain $z_\chi
\approx 0$. This shows that the Ising embedding encodes well the collective
modes of the system, and that the behaviour of the first algorithm is connected
to the poor performance of the Swendsen-Wang algorithm in dealing with a
frustrated Ising model.

<id>
hep-lat/9201004v1
<category>
hep-lat
<abstract>
The cutoff dependence of the Scalar Sector of the Minimal Standard Model can
result in an increase of the existing triviality bound estimates of the Higgs
mass. We present a large $N$ calculation and some preliminary N=4 results that
suggest that the increase can be as large as 30%, resulting to a bound of about
850 G eV.

<id>
hep-lat/9201005v1
<category>
hep-lat
<abstract>
We present preliminary results from the 1991 HEMCGC simulations with
staggered dynamical fermions on a $16^3 \times 32$ lattice at $\beta = 5.6$
with sea quark masses $am_q = 0.025$ and 0.01. The spectroscopy was done both
for staggered valence quarks with mass equal to the sea quark masses and for
Wilson valence quarks at six different values for $\kappa$, 0.1320, 0.1410,
0.1525, 0.1565, 0.1585, and 0.1600. In addition to the measurements performed
in our earlier work, we also measured the $\Delta$ and other `extended' hadrons
for staggered valence quarks and pseudo-scalar decay constants and vector meson
matrix elements, the wave function at the origin, for Wilson valence quarks.

<id>
hep-lat/9201006v1
<category>
hep-lat
<abstract>
We explore a new approach to the path integral for a latticized quantum
theory. This talk is based on work with N. Khuri and H. Ren.

<id>
hep-lat/9201007v1
<category>
hep-lat
<abstract>
We describe recent results obtained as part of the High Energy Monte Carlo
Grand Challenge (HEMCGC) project concerning the behaviour of lattice QCD with
light dynamical Wilson quarks. We show that it is possible to reach regions of
parameter space with light pions $m_\pi\ll0.2/a$, but that the equilibration
time for such a system is at least of the order of 1,000 unit-length Hybrid
Monte Carlo (HMC) trajectories (about a Gigaflop/sec-year). If the Hybrid
Molecular Dynamics (HMD) algorithm is used with the same parameters it gives
incorrect results.

<id>
hep-lat/9202001v1
<category>
hep-lat
<abstract>
Using the recently proposed multicanonical ensemble, we perform Monte Carlo
simulation for the 2d 7-state Potts model and calculate its surface free energy
density (surface tension) to be $2 f^s = 0.0241 \pm 0.0010$. This is an order
of magnitude smaller than other estimates in the recent literature. Relying on
existing Monte Carlo data, we also give a preliminary estimate for the surface
tension of 4d SU(3) lattice gauge theory with $L_t=2$.

<id>
hep-lat/9202002v1
<category>
hep-lat
<abstract>
The distance scale for a quantum field theory is the correlation length
$\xi$, which diverges with exponent $\nu$ as the bare mass approaches a
critical value. If $t=m^{2}-m_{c}^{2}$, then $\xi=m_{P}^{-1} \sim t^{-\nu}$ as
$t \to 0$. The two-point function of a scalar field has a random walk
representation. The walk takes place in a background of fluctuations (closed
walks) of the field itself. We describe the connection between properties of
the walk and of the two-point function. Using the known behavior of the two
point function, we deduce that the dimension of the walk is $d_{w}=\phi / \nu$
and that there is a singular relation between $t$ and the energy per unit
length of the walk $\theta \sim t^{\phi}$ that is due to the singular behavior
of the background at $t=0$. ($\phi$ is a computable crossover exponent.)

<id>
hep-lat/9202003v1
<category>
hep-lat
<abstract>
Lattice QCD with 2 light staggered quark flavours is being simulated on a
$16^3\times8$ lattice to study the transition from hadronic matter to a quark
gluon plasma. We have completed runs at $m_q=0.0125$ and are extending this to
$m_q=0.00625$. We also examine the addition of a non-dynamical "strange" quark.
Thermodynamic order parameters are being measured across the transition and
further into the plasma phase, as are various screening lengths. No evidence
for a first order transition is seen, and we estimate the transition
temperature to be $TY_c=143(7) MeV$.

<id>
hep-lat/9202004v1
<category>
hep-lat
<abstract>
Relying on the recently proposed multicanonical algorithm, we present a
numerical simulation of the first order phase transition in the 2d 10-state
Potts model on lattices up to sizes $100\times100$. It is demonstrated that the
new algorithm $lacks$ an exponentially fast increase of the tunneling time
between metastable states as a function of the linear size $L$ of the system.
Instead, the tunneling time diverges approximately proportional to $L^{2.65}$.
Thus the computational effort as counted per degree of freedom for generating
an independent configuration in the unstable region of the model rises
proportional to $V^{2.3}$, where $V$ is the volume of the system. On our
largest lattice we gain more than two orders of magnitude as compared to a
standard heat bath algorithm. As a first physical application we report a high
precision computation of the interfacial tension.

<id>
hep-lat/9203002v1
<category>
hep-lat
<abstract>
We modified the recently proposed multicanonical MC algorithm for the case of
a magnetic field driven order--order phase transition. We test this {\it
multimagnetic} Monte Carlo algorithm for the D=2 Ising model at $\beta=0.5$ and
simulate square lattices up to size $100 \times 100$. On these lattices with
periodic boundary conditions it is possible to enhance the appearance of
order-order interfaces during the simulation by many orders of magnitude as
compared to the standard Monte Carlo simulation.

<id>
hep-lat/9204001v1
<category>
hep-lat
<abstract>
We present a recursive procedure to calculate the parameters of the recently
introduced multicanonical ensemble and explore the approach for spin glasses.
Temperature dependence of the energy, the entropy and other physical quantities
are easily calculable and we report results for the zero temperature limit. Our
data provide evidence that the large $L$ increase of the ergodicity time is
greatly improved. The multicanonical ensemble seems to open new horizons for
simulations of spin glasses and other systems which have to cope with
conflicting constraints.

<id>
hep-lat/9204002v1
<category>
hep-lat
<abstract>
We perform Monte Carlo simulations using the Wolff cluster algorithm of the
q=2 (Ising), 3, 4 and q=10 Potts models on dynamical phi-cubed graphs of
spherical topology with up to 5000 nodes. We find that the measured critical
exponents are in reasonable agreement with those from the exact solution of the
Ising model and with those calculated from KPZ scaling for q=3,4 where no exact
solution is available. Using Binder's cumulant we find that the q=10 Potts
model displays a first order phase transition on a dynamical graph, as it does
on a fixed lattice. We also examine the internal geometry of the graphs
generated in the simulation, finding a linear relationship between ring length
probabilities and the central charge of the Potts model

<id>
hep-lat/9204003v1
<category>
hep-lat
<abstract>
We perform Monte Carlo simulations using the Wolff cluster algorithm of {\it
multiple} $q=2,3,4$ state Potts models on dynamical phi-cubed graphs of
spherical topology in order to investigate the $c>1$ region of two-dimensional
quantum gravity. Contrary to naive expectation we find no obvious signs of
pathological behaviour for $c>1$. We discuss the results in the light of
suggestions that have been made for a modified DDK ansatz for $c>1$.

<id>
hep-lat/9204004v1
<category>
hep-lat
<abstract>
We performed detailed study of the phase transition region in Four
  Dimensional Simplicial Quantum Gravity, using the dynamical triangulation
approach. The phase transition between the Gravity and
  Antigravity phases turned out to be asymmetrical, so that we observed the
scaling laws only when the Newton constant approached the critical value from
perturbative side. The curvature susceptibility diverges with the scaling index
$-.6$. The physical (i.e. measured with heavy particle propagation) Hausdorff
dimension of the manifolds, which is
  2.3 in the Gravity phase and 4.6 in the Antigravity phase, turned out to be 4
at the critical point, within the measurement accuracy. These facts indicate
the existence of the continuum limit in Four
  Dimensional Euclidean Quantum Gravity.

<id>
hep-lat/9204005v1
<category>
hep-lat
<abstract>
I propose a numerical simulation algorithm for statistical systems which
combines a microcanonical transfer of energy with global changes in clusters of
spins. The advantages of the cluster approach near a critical point augment the
speed increases associated with multi-spin coding in the microcanonical
approach. The method also provides a limited ability to tune the average
cluster size.

<id>
hep-lat/9204006v1
<category>
hep-lat
<abstract>
The matrix element which determines the B meson decay constant can be
measured on the lattice using an effective field theory for heavy quarks.
Various discretizations of the heavy-light bilinears which appear in this and
other B decay matrix elements are possible. The heavy-light bilinear currently
used for the determination of the B meson decay constant on the lattice suffers
a substantial one-loop renormalization. In this paper, we compute the one-loop
renormalizations of the discretizations in which the heavy and light fields in
the bilinear are separated by one lattice spacing, and discuss their
application. Readers of this paper may also be interested in our paper on the
application of Symanzik's improvement program to heavy-light currents (paper
number 9203221 on hep-ph).

<id>
hep-lat/9204007v2
<category>
hep-lat
<abstract>
[This version is a minor revision of a previously submitted preprint. Only
references have been changed.] We describe a technique for constructing the
effective chiral theory for quenched QCD. The effective theory which results is
a lagrangian one, with a graded symmetry group which mixes Goldstone bosons and
fermions, and with a definite (though slightly peculiar) set of Feynman rules.
The straightforward application of these rules gives automatic cancellation of
diagrams which would arise from virtual quark loops. The techniques are used to
calculate chiral logarithms in $f_K/f_\pi$, $m_\pi$, $m_K$, and the ratio of
$\langle{\bar s}s\rangle$ to $\langle{\bar u}u\rangle$. The leading
finite-volume corrections to these quantities are also computed. Problems for
future study are described.

<id>
hep-lat/9204008v1
<category>
hep-lat
<abstract>
We present an analysis of hadronic spectroscopy for Wilson valence quarks
with dynamical staggered fermions at lattice coupling $6/g^2 = \beta=5.6$ at
sea quark mass $am_q=0.01$ and 0.025, and of Wilson valence quarks in quenched
approximation at $\beta=5.85$ and 5.95, both on $16^3 \times 32$ lattices. We
make comparisons with our previous results with dynamical staggered fermions at
the same parameter values but on $16^4$ lattices doubled in the temporal
direction.

<id>
hep-lat/9204009v1
<category>
hep-lat
<abstract>
Motivated by some previous work on fermions on random lattices and by
suggestions that impurities could trigger parity breaking in 2d crystals, we
have analyzed the spectrum of the Dirac equation on a two dimensional square
lattice where sites have been removed randomly --- a doped lattice. We have
found that the system is well described by a sine-Gordon action. The solitons
of this model are the lattice fermions, which pick a quartic interaction due to
the doping and become Thirring fermions. They also get an effective mass
different from the lagrangian mass. The system seems to exhibit spontaneous
symmetry breaking, exactly as it happens for a randomly triangulated lattice.
The associated ``Goldstone boson" is the sine-Gordon scalar. We argue, however,
that the peculiar behaviour of the chiral condensate is due to finite size
effects.

<id>
hep-lat/9204010v1
<category>
hep-lat
<abstract>
We investigate the influence of the measure in the path integral for
Euclidean quantum gravity in four dimensions within the Regge calculus. The
action is bounded without additional terms by fixing the average lattice
spacing. We set the length scale by a parameter $\beta$ and consider a scale
invariant and a uniform measure. In the low $\beta$ region we observe a phase
with negative curvature and a homogeneous distribution of the link lengths
independent of the measure. The large $\beta$ region is characterized by
inhomogeneous link lengths distributions with spikes and positive curvature
depending on the measure.

<id>
hep-lat/9204011v1
<category>
hep-lat
<abstract>
We investigate the use of two types of non-local (``smeared'') sources for
quark propagators in quenched lattice QCD at $\beta=6.0$ using Wilson fermions
at $\kappa=0.154$ and $0.155$. We present results for the hadron mass spectrum,
meson decay constants, quark masses, the chiral condensate and the quark
distribution amplitude of the pion. The use of smeared sources leads to a
considerable improvement over previous results. We find a disturbing
discrepancy between the baryon spectra obtained using Wuppertal and wall
sources. We find good signals in the ratio of correlators used to calculate the
quark mass and the chiral condensate and show that the extrapolation to the
chiral limit is smooth.

<id>
hep-lat/9204012v1
<category>
hep-lat
<abstract>
Sea quark contributions to the scalar density and the axial current matrix
elements of the nucleon are studied in lattice qcd with two flavours of
dynamical wilson fermions. the results are compared to trends in heavy quark
mass expansions, and contrasted with the numbers obtained using dynamical
staggered fermions.

<id>
hep-lat/9204013v1
<category>
hep-lat
<abstract>
We study the coefficients of the expansion $F(R) = 1/3 c_3 R^3 + 1/2 c_2 R^2
+ c_1 R$ of the free energy of spherical bubbles at $T=T_c$ in pure glue QCD
using lattice Monte Carlo techniques. The coefficient $c_3$ vanishes at $T=T_c$
and our results suggest that the sign and the order of magnitude of $c_1$ is in
agreement with the value $c_1=\pm 32\pi T_c^2/9$ (- for hadronic bubbles in
quark phase, + for quark bubbles in hadronic phase) computed by Mardor and
Svetitsky from the MIT bag model. The parameter $c_2$ is small in agreement
with earlier determinations.

<id>
hep-lat/9204014v1
<category>
hep-lat
<abstract>
The multigrid methodology is reviewed. By integrating numerical processes at
all scales of a problem, it seeks to perform various computational tasks at a
cost that rises as slowly as possible as a function of $n$, the number of
degrees of freedom in the problem. Current and potential benefits for lattice
field computations are outlined. They include: $O(n)$ solution of Dirac
equations; just $O(1)$ operations in updating the solution (upon any local
change of data, including the gauge field); similar efficiency in gauge fixing
and updating; $O(1)$ operations in updating the inverse matrix and in
calculating the change in the logarithm of its determinant; $O(n)$ operations
per producing each independent configuration in statistical simulations
(eliminating CSD), and, more important, effectively just $O(1)$ operations per
each independent measurement (eliminating the volume factor as well). These
potential capabilities have been demonstrated on simple model problems.
Extensions to real life are explored.

<id>
hep-ph/9203201v1
<category>
hep-ph
<abstract>
We report on an investigation of various problems related to the theory of
the electroweak phase transition. This includes a determination of the nature
of the phase transition, a discussion of the possible role of higher order
radiative corrections and the theory of the formation and evolution of the
bubbles of the new phase. We find in particular that no dangerous linear terms
appear in the effective potential. However, the strength of the first order
phase transition is 2/3 times less than what follows from the one-loop
approximation. This rules out baryogenesis in the minimal version of the
electroweak theory.

<id>
hep-ph/9203202v1
<category>
hep-ph
<abstract>
Motivated by recent experimental claims for the existence of a 17 keV
neutrino and by the solar neutrino problem, we construct a class of models
which contain in their low-energy spectrum a single light sterile neutrino and
one or more Nambu-Goldstone bosons. In these models the required pattern of
breaking of lepton-number symmetry takes place near the electroweak scale and
all mass heirarchies are technically natural. The models are compatible with
all cosmological and astrophysical constraints, and can solve the solar
neutrino problem via either the MSW effect or vacuum oscillations. The deficit
in atmospheric muon neutrinos seen in the Kamiokande and IMB detectors can also
be explained in these models.

<id>
hep-ph/9203203v1
<category>
hep-ph
<abstract>
We discuss recent progress (and controversies) in the theory of finite
temperature phase transitions. This includes the structure of the effective
potential at a finite temperature, the infrared problem in quantum statistics
of gauge fields, the theory of formation of critical and subcritical bubbles
and the theory of bubble wall propagation.

<id>
hep-ph/9203204v1
<category>
hep-ph
<abstract>
The motivations for low-energy supersymmetry and the main features of the
minimal supersymmetric extension of the Standard Model are reviewed. Possible
non-minimal models and the issue of gauge coupling unification are also
discussed. Theoretical results relevant for supersymmetric particle searches at
present and future accelerators are presented, with emphasis on the role of a
proposed $\epem$ collider with $\sqrt{s}
  = 500 \gev$. In particular, recent results on radiative corrections to
supersymmetric Higgs boson masses and couplings are summarized, and their
implications for experimental searches are discussed in some detail. (Plenary
talk at the Workshop on Physics and Experiments with Linear Colliders,
Saariselk\"a, Lapland, Finland, 9--14 September 1991)

<id>
hep-ph/9203205v1
<category>
hep-ph
<abstract>
We present a class of models in which the top quark, by mixing with new
physics at a higher energy scale, is naturally heavier than the other standard
model particles. We take this new physics to be extended color. Our models
contain new particles with masses between 100 GeV and 1 TeV, some of which may
be just within the reach of the next generation of experiments. In particular
one of our models implies the existence of two right handed top quarks. These
models demonstrate the existence of a standard model-like theory consistent
with experiment, and leading to new physics below the TeV scale, in which the
third generation is treated differently than the first two.

<id>
hep-ph/9203206v1
<category>
hep-ph
<abstract>
We examine the sensitivity of several solutions of the strong-CP problem to
violations of global symmetries by Planck scale physics. We find that the
Peccei-Quinn solution is extremely sensitive to U(1)_PQ violating operators of
dimension less than 10. We construct models in which the PQ symmetry is
protected by gauge symmetries to the requisite level.

<id>
hep-ph/9203207v1
<category>
hep-ph
<abstract>
We extend the method of differential renormalization to massive quantum field
theories treating in particular $\ph4$-theory and QED. As in the massless case,
the method proves to be simple and powerful, and we are able to find, in
particular, compact explicit coordinate space expressions for the finite parts
of two notably complicated diagrams, namely, the 2-loop 2-point function in
$\ph4$ and the 1-loop vertex in QED.

<id>
hep-ph/9203208v1
<category>
hep-ph
<abstract>
We examine the issue of monopole annihilation at the electroweak scale
induced by flux tube confinement, concentrating first on the simplest
possibility---one which requires no new physics beyond the standard model.
Monopoles existing at the time of the electroweak phase transition may trigger
$W$ condensation which can confine magnetic flux into flux tubes. However we
show on very general grounds, using several independent estimates, that such a
mechanism is impotent. We then present several general dynamical arguments
constraining the possibility of monopole annihilation through any confining
phase near the electroweak scale.

<id>
hep-ph/9203209v1
<category>
hep-ph
<abstract>
We investigate the dynamics of monopole annihilation by the Langacker-Pi
mechanism. We find taht considerations of causality, flux-tube energetics and
the friction from Aharonov-Bohm scatteering suggest that the monopole
annihilation is most efficient if electromagnetism is spontaneously broken at
the lowest temperature ($T_{em} \approx 10^6 GeV$) consistent with not having
the monopoles dominate the energy density of the universe.

<id>
hep-ph/9203210v1
<category>
hep-ph
<abstract>
We perform a fit to precise electroweak data to determine the Higgs and top
masses. Penalty functions taking into account their production limits are
included. We find ${\displaystyle m_H=65^{+245}_{-4}\ GeV}$ and ${\displaystyle
m_t=122^{+25}_{-20}\ GeV}$. However whereas the top $\chi^2$ distribution
behaves properly near the minimum, the Higgs $\chi^2$ distribution does not,
indicating a statistical fluctuation or new physics. In fact no significative
bound on the Higgs mass can be given at present. However, if the LEP accuracy
is improved and the top is discovered in the preferred range of top masses, a
meaningful bound on the Higgs mass could be obtained within the standard model
framework.

<id>
hep-ph/9203211v1
<category>
hep-ph
<abstract>
Burgess and Marini have recently pointed out that the leading contribution to
the damping rate of energetic gluons and quarks in the QCD plasma, given by
$\gamma=c g^2\ln(1/g)T$, can be obtained by simple arguments obviating the need
of a fully resummed perturbation theory as developed by Braaten and Pisarski.
Their calculation confirmed previous results of Braaten and Pisarski, but
contradicted those proposed by Lebedev and Smilga. While agreeing with the
general considerations made by Burgess and Marini, I correct their actual
calculation of the damping rates, which is based on a wrong expression for the
static limit of the resummed gluon propagator. The effect of this, however,
turns out to be cancelled fortuitously by another mistake, so as to leave all
of their conclusions unchanged. I also verify the gauge independence of the
results, which in the corrected calculation arises in a less obvious manner.

<id>
hep-ph/9203212v2
<category>
hep-ph
<abstract>
Utilizing results on the cosmology of anomalous discrete symmetries we show
that models of spontaneous CP violation can in principle avoid the domain wall
problem first pointed out by Zel'dovich, Kobzarev and Okun. A small but nonzero
$\theta_{QCD}$ explicitly breaks CP and can lift the degeneracy of the two CP
conjugate vacua through nonperturbative effects so that the domain walls become
unstable, but survive to cosmologically interesting epochs. We explore the
viability of spontaneous CP violation in the context of two Higgs models, and
find that the invisible axion solution of the strong CP problem cannot be
implemented without further extensions of the Higgs sector.

<id>
hep-ph/9203213v1
<category>
hep-ph
<abstract>
The complete and concurrent Homestake and Kamiokande solar neutrino data sets
(including backgrounds), when compared to detailed model predictions, provide
no unambiguous indication of the solution to the solar neutrino problem. All
neutrino-based solutions, including time-varying models, provide reasonable
fits to both the 3 year concurrent data and the full 20 year data set. A simple
constant B neutrino flux reduction is ruled out at greater than the 4$\sigma$
level for both data sets. While such a flux reduction provides a marginal fit
to the unweighted averages of the concurrent data, it does not provide a good
fit to the average of the full 20 year sample. Gallium experiments may not be
able to distinguish between the currently allowed neutrino-based possibilities.

<id>
hep-ph/9203214v1
<category>
hep-ph
<abstract>
We discuss relationship between inflation and various models of production of
density inhomogeneities due to strings, global monopoles, textures and other
topological and non-topological defects. Neither of these models leads to a
consistent cosmological theory without the help of inflation. However, each of
these models can be incorporated into inflationary cosmology. We propose a
model of inflationary phase transitions, which, in addition to topological and
non-topological defects, may provide adiabatic density perturbations with a
sharp maximum between the galaxy scale $l_g$ and the horizon scale $l_H$.

<id>
hep-ph/9203215v2
<category>
hep-ph
<abstract>
Recently, calculations which consider the implications of anomalous trilinear
gauge-boson couplings, both at tree-level and in loop-induced processes, have
been criticized on the grounds that the lagrangians employed are not \gwk gauge
invariant. We prove that, in fact, the general Lorentz-invariant and $U(1)_\em$
invariant but {\it not} $SU_L(2)\times U_Y(1)$ invariant action is equivalent
to the general lagrangian in which $SU_L(2)\times U_Y(1)$ appears but is
nonlinearly realized. We demonstrate this equivalence in an explicit
calculation, and show how it is reconciled with loop calculations in which the
different formulations can (superficially) appear to give different answers. In
this sense any effective theory containing light spin-one particles is seen to
be automatically gauge invariant.

<id>
hep-ph/9203216v3
<category>
hep-ph
<abstract>
Motivated by past and recent analyses we critically re-examine the use of
effective lagrangians in the literature to constrain new physics and to
determine the `physics reach' of future experiments. We demonstrate that many
calculations, such as those involving anomalous trilinear gauge-boson
couplings, either considerably overestimate loop-induced effects, or give
ambiguous answers. The source of these problems is the use of cutoffs to
evaluate the size of such operators in loop diagrams. In contrast to other
critics of these loop estimates, we prove that the inclusion of
nonlinearly-realized gauge invariance into the low-energy lagrangian is
irrelevant to this conclusion. We use an explicit example using known
multi-Higgs physics above the weak scale to underline these points. We show how
to draw conclusions regarding the nature of the unknown high-energy physics
without making reference to low-energy cutoffs.

<id>
hep-ph/9203217v1
<category>
hep-ph
<abstract>
We use the theory of Yennie, Frautschi and Suura to realize, via Monte Carlo
methods, the process $f\,\bbbarf\to f'\,\bbbarf'+n\gamma$ at SSC and LHC
energies, where $f$ and $f'$ are quarks or leptons. QED infrared divergences
are canceled to all orders in perturbation theory. The resulting Monte Carlo
event generator, SSC-YFS2, is used to study the effects of initial-state photon
radiation on these processes in the SSC environment. Sample Monte Carlo data
are presented and discussed. We find that the respective multiple-photon
effects must be taken into account in discussing precise predictions for SSC
physics processes.

<id>
hep-ph/9203218v2
<category>
hep-ph
<abstract>
We consider a theory of gauge fields and fermions which we argue gives rise
to dynamics similar to that of the Nambu--Jona-Lasinio (NJL) model when a gauge
coupling constant is appropriately fine-tuned. We discuss the application of
this model to dynamical electroweak symmetry breaking by a top-quark
condensate. In this model, custodial symmetry is violated solely by
perturbatively weak interactions, and the top--bottom mass splitting is due to
the enhanced sensitivity to custodial symmetry violation near the critical
point. We also consider models in which electroweak symmetry is broken by new
strongly- interacting fermions with NJL-like dynamics. We argue that these
models require additional fine-tuning in order to keep corrections to the
electroweak $\rho$-parameter acceptably small.

<id>
hep-ph/9203219v1
<category>
hep-ph
<abstract>
We discuss various reactions at future e+e- and gamma-gamma colliders
involving real (beamstrahlung or backscattered laser) or quasi--real
(bremsstrahlung) photons in the initial state and hadrons in the final state.
The production of two central jets with large pT is described in some detail;
we give distributions for the rapidity and pT of the jets as well as the
di--jet invariant mass, and discuss the relative importance of various initial
state configurations and the uncertainties in our predictions. We also present
results for `mono--jet' production where one jet goes down a beam pipe, for the
production of charm, bottom and top quarks, and for single production of W and
Z bosons. Where appropriate, the two--photon processes are compared with
annihilation reactions leading to similar final states. We also argue that the
behaviour of the total inelastic gamma-gamma cross section at high energies
will probably have little impact on the severity of background problems caused
by soft and semi--hard (`minijet') two--photon reactions. We find very large
differences in cross sections for all two--photon processes between existing
designs for future e+e- colliders, due to the different beamstrahlung spectra;
in particular, both designs with <<1 and >>1 events per bunch crossing exist.

<id>
hep-ph/9203220v1
<category>
hep-ph
<abstract>
By evolution of fermion mass matrices of the Fritzsch and the Georgi-Jarlskog
forms from the supersymmetric grand unified scale, DHR obtained predictions for
the quark masses and mixings. Using Monte Carlo methods we test these
predictions against the latest determinations of the mixings, the CP-violating
parameter epsilon_K and the B_d^0--Bbar_d^0 mixing parameter r_d. The
acceptable solutions closely specify the quark masses and mixings, but lie at
the edges of allowed regions at 90% confidence level.

<id>
hep-ph/9203222v1
<category>
hep-ph
<abstract>
It is shown that, in QCD, the same universal function
$\Gamma_{cusp}(\vartheta, \alpha_\s)$ determines the infrared behaviour of the
on-shell quark form factor, the velocity-dependent anomalous dimension in the
heavy quark effective field theory (HQET) and the renormalization properties of
the vacuum averaged Wilson lines with a cusp. It is demonstrated that a
combined use of the methods developed in the relevant different branches of
quantum field theory essentially facilitates the all-order study of the
asymptotic and analytic properties of this function.

<id>
hep-ph/9203223v1
<category>
hep-ph
<abstract>
We study the Higgs sector of the Minimal Supersymmetric Standard Model, in
the context of proton-proton collisions at LHC and SSC energies. We assume a
relatively heavy supersymmetric particle spectrum, and include recent results
on one-loop radiative corrections to Higgs-boson masses and couplings. We begin
by discussing present and future constraints from the LEP experiments. We then
compute branching ratios and total widths for the neutral ($h,H,A$) and charged
($H^\pm$) Higgs particles. We present total cross-sections and event rates for
the important discovery channels at the LHC and SSC. Promising physics
signatures are given by $h \to \gamma \gamma$, $H \to \gamma \gamma$ or $Z^*
Z^*$ or $\tau^+ \tau^-$, $A \to \tau^+ \tau^-$, and $t \to b H^+$ followed by
$H^+ \to \tau^+ \nu_{\tau}$, which should allow for an almost complete coverage
of the parameter space of the model.

<id>
hep-ph/9203224v1
<category>
hep-ph
<abstract>
We explore the phenomenology of new R-parity violating operators that can
occur in E6 models. The set of allowed operators is found to depend sensitively
on the nature of the extension of the standard model gauge group. These new
interactions lead to additional production processes for the exotic particles
in such models and allow the LSP to decay but with a highly suppressed rate.
The implications of these new interations are examined for the Tevatron, SSC,
LHC, HERA, and sqrt{s} = 0.5 and 1 TeV e+e- colliders.

<id>
hep-ph/9203225v1
<category>
hep-ph
<abstract>
Charm and bottom mesons and baryons are incorporated into a low energy chiral
Lagrangian. Interactions of the heavy hadrons with light octet Goldstone bosons
are studied in a framework which represents a synthesis of chiral perturbation
theory and the heavy quark effective theory. The differential decay rate for
the semileptonic process $\LBzero \to \Sigma_c^{++} + e^- + \bar{\nu}_e +
\pi^-$ is calculated at the zero recoil point using this hybrid formalism.

<id>
hep-ph/9203226v2
<category>
hep-ph
<abstract>
We consider an addition of the term which is a square of the scalar curvature
to the Einstein-Hilbert action. Under this generalized action, we attempt to
explain i) the flat rotation curves observed in spiral galaxies, which is
usually attributed to the existence of dark matter, and ii) the contradicting
observations of uniform cosmic microwave background and non-uniform galaxy
distributions against redshift. For the former, we attain the flatness of
velocities, although the magnitudes remain about half of the observations. For
the latter, we obtain a solution with oscillating Hubble parameter under
uniform mass distributions. This solution leads to several peaks of galaxy
number counts as a function of redshift with the first peak corresponding to
the Great Wall.

<id>
hep-ph/9204201v1
<category>
hep-ph
<abstract>
We discuss the mechanism for electroweak symmetry breaking in supersymmetric
versions of the standard model. After briefly reviewing the possible sources of
supersymmetry breaking, we show how the required pattern of symmetry breaking
can automatically result from the structure of quantum corrections in the
theory. We demonstrate that this radiative breaking mechanism works well for a
heavy top quark and can be combined in unified versions of the theory with
excellent predictions for the running couplings of the model. (To be published
in ``Perspectives in Higgs Physics'', G. Kane editor.)

<id>
hep-ph/9204202v1
<category>
hep-ph
<abstract>
We explore the potential of a future e^+ e^- collider in the 0.5 TeV
center-of-mass energy range to detect intermediate or heavy Higgs bosons in the
Standard Model. We first briefly assess the production cross sections and
update the decay branching fractions for a Higgs boson of intermediate mass,
with M_Z < m_H < 2M_W. We then study in detail the possibility of detecting a
heavy Higgs boson, with m_H > 2M_W, through the production of pairs of weak
bosons. We quantitatively analyze the sensitivity of the process e^+ e^- --> nu
nubar W^+ W^- (ZZ) to the presence of a heavy Higgs-boson resonance in the
Standard Model. We compare this signal to various backgrounds and to the
smaller signal from e^+ e^- --> ZH --> mu^+ mu^- W^+ W^- (ZZ), assuming the
weak-boson pairs to be detected and measured in their dominant hadronic decay
modes W^+ W^- (ZZ) --> 4jets. A related Higgs-boson signal in 6-jet final
states is also estimated. We show how the main backgrounds from e^+ e^- W^+ W^-
(ZZ), e nu WZ, and t tbar production can be reduced by suitable acceptance
cuts. Bremsstrahlung and typical beamstrahlung corrections are calculated.
These corrections reduce Higgs-boson production by scattering mechanisms but
increase production by annihilation mechanisms; they also smear out some
dynamical features such as Jacobian peaks in p_T(H). With all these corrections
included, we conclude that it should be possible to detect a heavy Higgs-boson
signal in the nu nubar W^+ W^-(ZZ) channels up to mass m_H=350 GeV.

<id>
hep-ph/9204203v1
<category>
hep-ph
<abstract>
We study the prospects of testing the $WW\gamma$ vertex in $e^- p\to\nu\gamma
X$ and $e^+ p\to\nu\gamma X$ at HERA and LEP/LHC. Destructive interference
effects between the Standard Model and the anomalous contributions to the
amplitude severely limit the sensitivity of both processes to non-standard
$WW\gamma$ couplings. Sensitivity limits for the anomalous $WW\gamma$ couplings
$\kappa$ and $\lambda$ at HERA and LEP/LHC are derived, taking into account
experimental cuts and uncertainties, and the form factor behaviour of
nonstandard couplings. These limits are found to be significantly weaker than
those which can be expected from other collider processes within the next few
years. At HERA, they are comparable to bounds obtained from $S$-matrix
unitarity.

<id>
hep-ph/9204204v2
<category>
hep-ph
<abstract>
We propose a renormalizable model with no fundamental scalars which breaks
itself in the manner of a "tumbling" gauge theory down to the standard model
with a top-quark condensate. Because of anomaly cancellation requirements, this
model contains two color sextet fermions (quixes), which are vector-like with
respect to the standard model gauge group. The model also has a large number of
pseudo-Nambu-Goldstone bosons, some of which can be light. The top-quark
condensate is responsible for breaking the electroweak gauge symmetry and gives
the top quark a large mass. We discuss the qualitative features and instructive
shortcomings of the model in its present form. We also show that this model can
be naturally embedded into an aesthetically pleasing model in which the
standard model fermion families appear symmetrically.

<id>
hep-ph/9204205v1
<category>
hep-ph
<abstract>
It has been argued that any primordial B+L asymmetry existing at very high
temperatures can be subsequently erased by anomalous electroweak effects. We
argue that this is not necessarily the case in the supersymmetric standard
model because, apart from B and/or L, there are, above a certain temperature
$T_{SS}$, two other anomalous U(1) currents. As a consequence, anomalous
electroweak effects are only able to partially transform a B+L excess into a
generation of primordial sparticle (e.g. gaugino) density. This relaxes recent
bounds on B,L-violating non-renormalizable couplings by several orders of
magnitude. In particular, dimension-5 couplings inducing neutrino masses may be
4 orders of magnitude larger than in the non-supersymmetric case, allowing for
neutrino masses of the order of 10 eV. These values are consistent with a
MSW+see-saw explanation of the solar-neutrino data and also with possible
neutrino oscillations measurable at accelerators. Cosmological bounds on other
rare processes, such as neutron-antineutron oscillations get also relaxed by
several orders of magnitude compared with previous estimates.

<id>
hep-th/9108001v1
<category>
hep-th
<abstract>
A family of exact conformal field theories is constructed which describe
charged black strings in three dimensions. Unlike previous charged black hole
or extended black hole solutions in string theory, the low energy spacetime
metric has a regular inner horizon (in addition to the event horizon) and a
timelike singularity. As the charge to mass ratio approaches unity, the event
horizon remains but the singularity disappears.

<id>
hep-th/9108002v1
<category>
hep-th
<abstract>
We show that all W-gravity actions can be easilly constructed and understood
from the point of view of the Hamiltonian formalism for the constrained
systems. This formalism also gives a method of constructing gauge invariant
actions for arbitrary conformally extended algebras.

<id>
hep-th/9108003v1
<category>
hep-th
<abstract>
We study the classical version of supersymmetric $W$-algebras. Using the
second Gelfand-Dickey Hamiltonian structure we work out in detail $W_2$ and
$W_3$-algebras.

<id>
hep-th/9108004v1
<category>
hep-th
<abstract>
String theories with two dimensional space-time target spaces are
characterized by the existence of a ``ground ring'' of operators of spin
$(0,0)$. By understanding this ring, one can understand the symmetries of the
theory and illuminate the relation of the critical string theory to matrix
models. The symmetry groups that arise are, roughly, the area preserving
diffeomorphisms of a two dimensional phase space that preserve the fermi
surface (of the matrix model) and the volume preserving diffeomorphisms of a
three dimensional cone. The three dimensions in question are the matrix
eigenvalue, its canonical momentum, and the time of the matrix model.

<id>
hep-th/9108005v1
<category>
hep-th
<abstract>
We discuss when and how the Verlinde dimensions of a rational conformal field
theory can be expressed as correlation functions in a topological LG theory. It
is seen that a necessary condition is that the RCFT fusion rules must exhibit
an extra symmetry. We consider two particular perturbations of the Grassmannian
superpotentials. The topological LG residues in one perturbation, introduced by
Gepner, are shown to be a twisted version of the $SU(N)_k$ Verlinde dimensions.
The residues in the other perturbation are the twisted Verlinde dimensions of
another RCFT; these topological LG correlation functions are conjectured to be
the correlation functions of the corresponding Grassmannian topological sigma
model with a coupling in the action to instanton number.

<id>
hep-th/9108006v2
<category>
hep-th
<abstract>
It is shown that, in the three-dimensional lattice gravity defined by Ponzano
and Regge, the space of physical states is isomorphic to the space of
gauge-invariant functions on the moduli space of flat $SU(2)$ connections over
a two-dimensional surface, which gives physical states in the $ISO(3)$
Chern-Simons gauge theory.

<id>
hep-th/9108007v1
<category>
hep-th
<abstract>
Starting from a given S-matrix of an integrable quantum field theory in $1+1$
dimensions, and knowledge of its on-shell quantum group symmetries, we describe
how to extend the symmetry to the space of fields. This is accomplished by
introducing an adjoint action of the symmetry generators on fields, and
specifying the form factors of descendents. The braiding relations of quantum
field multiplets is shown to be given by the universal $\CR$-matrix. We develop
in some detail the case of infinite dimensional Yangian symmetry. We show that
the quantum double of the Yangian is a Hopf algebra deformation of a level zero
Kac-Moody algebra that preserves its finite dimensional Lie subalgebra. The
fields form infinite dimensional Verma-module representations; in particular
the energy-momentum tensor and isotopic current are in the same multiplet.

<id>
hep-th/9108008v1
<category>
hep-th
<abstract>
We show that various actions of topological conformal theories that were
suggested recentely are particular cases of a general action. We prove the
invariance of these models under transformations generated by nilpotent
fermionic generators of arbitrary conformal dimension, $\Q$ and $\G$. The later
are shown to be the $n^{th}$ covariant derivative with respect to ``flat
abelian gauge field" of the fermionic fields of those models. We derive the
bosonic counterparts $\W$ and $\R$ which together with $\Q$ and $\G$ form a
special $N=2$ super $W_\infty$ algebra. The algebraic structure is discussed
and it is shown that it generalizes the so called ``topological algebra".

<id>
hep-th/9108009v1
<category>
hep-th
<abstract>
A new transverse lattice model of $3+1$ Yang-Mills theory is constructed by
introducing Wess-Zumino terms into the 2-D unitary non-linear sigma model
action for link fields on a 2-D lattice. The Wess-Zumino terms permit one to
solve the basic non-linear sigma model dynamics of each link, for discrete
values of the bare QCD coupling constant, by applying the representation theory
of non-Abelian current (Kac-Moody) algebras. This construction eliminates the
need to approximate the non-linear sigma model dynamics of each link with a
linear sigma model theory, as in previous transverse lattice formulations. The
non-perturbative behavior of the non-linear sigma model is preserved by this
construction. While the new model is in principle solvable by a combination of
conformal field theory, discrete light-cone, and lattice gauge theory
techniques, it is more realistically suited for study with a Tamm-Dancoff
truncation of excited states. In this context, it may serve as a useful
framework for the study of non-perturbative phenomena in QCD via analytic
techniques.

<id>
hep-th/9108010v1
<category>
hep-th
<abstract>
$U(1)$ zero modes in the $SL(2,R)_k/U(1)$ and $SU(2)_k/U(1)$ conformal coset
theories, are investigated in conjunction with the string black hole solution.
The angular variable in the Euclidean version, is found to have a double set of
winding. Region III is shown to be $SU(2)_k/U(1)$ where the doubling accounts
for the cut sructure of the parafermionic amplitudes and fits nicely across the
horizon and singularity. The implications for string thermodynamics and
identical particles correlations are discussed.

<id>
hep-th/9108011v1
<category>
hep-th
<abstract>
It has been shown that given a classical background in string theory which is
independent of $d$ of the space-time coordinates, we can generate other
classical backgrounds by $O(d)\otimes O(d)$ transformation on the solution. We
study the effect of this transformation on the known black $p$-brane solutions
in string theory, and show how these transformations produce new classical
solutions labelled by extra continuous parameters and containing background
antisymmetric tensor field.

<id>
hep-th/9108012v1
<category>
hep-th
<abstract>
The perturbations of string-theoretic black holes are analyzed by
generalizing the method of Chandrasekhar. Attention is focussed on the case of
the recently considered charged string-theoretic black hole solutions as a
representative example. It is shown that string-intrinsic effects greatly alter
the perturbed motions of the string-theoretic black holes as compared to the
perturbed motions of black hole solutions of the field equations of general
relativity, the consequences of which bear on the questions of the scattering
behavior and the stability of string-theoretic black holes. The explicit forms
of the axial potential barriers surrounding the string-theoretic black hole are
derived. It is demonstrated that one of these, for sufficiently negative values
of the asymptotic value of the dilaton field, will inevitably become negative
in turn, in marked contrast to the potentials surrounding the static black
holes of general relativity. Such potentials may in principle be used in some
cases to obtain approximate constraints on the value of the string coupling
constant. The application of the perturbation analysis to the case of
two-dimensional string-theoretic black holes is discussed.

<id>
hep-th/9108013v1
<category>
hep-th
<abstract>
We derive directly from the N=2 LG superpotential the differential equations
that determine the flat coordinates of arbitrary topological CFT's.

<id>
hep-th/9108014v1
<category>
hep-th
<abstract>
We study the double scaling limit of mKdV type, realized in the two-cut
Hermitian matrix model. Building on the work of Periwal and Shevitz and of
Nappi, we find an exact solution including all odd scaling operators, in terms
of a hierarchy of flows of $2\times 2$ matrices. We derive from it loop
equations which can be expressed as Virasoro constraints on the partition
function. We discover a ``pure topological" phase of the theory in which all
correlation functions are determined by recursion relations. We also examine
macroscopic loop amplitudes, which suggest a relation to 2D gravity coupled to
dense polymers.

<id>
hep-th/9108015v1
<category>
hep-th
<abstract>
A particular $U(N)$ gauge theory defined on the three dimensional
dodecahedral lattice is shown to correspond to a model of oriented
self-avoiding surfaces. Using large $N$ reduction it is argued that the model
is partially soluble in the planar limit.

<id>
hep-th/9108016v1
<category>
hep-th
<abstract>
This is a talk given by S.D. at the the workshop on Random Surfaces and 2D
Quantum Gravity, Barcelona 10-14 June 1991. It is an updated review of recent
work done by the contributors on a proposal for non-perturbatively stable 2D quantum
gravity coupled to c<1 matter, based on the flows of the (generalised) KdV
hierarchy.

<id>
hep-th/9108017v1
<category>
hep-th
<abstract>
We find new solutions to the Yang--Baxter equation in terms of the
intertwiner matrix for semi-cyclic representations of the quantum group
$U_q(s\ell(2))$ with $q= e^{2\pi i/N}$. These intertwiners serve to define the
Boltzmann weights of a lattice model, which shares some similarities with the
chiral Potts model. An alternative interpretation of these Boltzmann weights is
as scattering matrices of solitonic structures whose kinematics is entirely
governed by the quantum group. Finally, we consider the limit $N\to\infty$
where we find an infinite--dimensional representation of the braid group, which
may give rise to an invariant of knots and links.

<id>
hep-th/9108018v1
<category>
hep-th
<abstract>
We describe four types of inner involutions of the Cartan-Weyl basis
providing (for $ |q|=1$ and $q$ real) three types of real quantum Lie algebras:
$U_{q}(O(3,2))$ (quantum D=4 anti-de-Sitter), $U_{q}(O(4,1))$ (quantum D=4
de-Sitter) and $U_{q}(O(5))$. We give also two types of inner involutions of
the Cartan-Chevalley basis of $U_{q}(Sp(4;C))$ which can not be extended to
inner involutions of the Cartan-Weyl basis. We outline twelve contraction
schemes for quantum D=4 anti-de-Sitter algebra. All these contractions provide
four commuting translation generators, but only two (one for $ |q|=1$, second
for $q$ real) lead to the quantum \po algebra with an undeformed space
rotations O(3) subalgebra.

<id>
hep-th/9108019v2
<category>
hep-th
<abstract>
I review some of the recent progress in two-dimensional string theory, which
is formulated as a sum over surfaces embedded in one dimension.

<id>
hep-th/9108020v1
<category>
hep-th
<abstract>
It is shown that the scattering of spacetime axions with fivebrane solitons
of heterotic string theory at zero momentum is proportional to the Donaldson
polynomial.

<id>
hep-th/9108021v2
<category>
hep-th
<abstract>
We discuss the conformal field theory and string field theory of the NSR
superstring using a BRST operator with a nonminimal term, which allows all
bosonic ghost modes to be paired into creation and annihilation operators.
Vertex operators for the Neveu-Schwarz and Ramond sectors have the same ghost
number, as do string fields. The kinetic and interaction terms are the same for
Neveu-Schwarz as for Ramond string fields, so spacetime supersymmetry is closer
to being manifest. The kinetic terms and supersymmetry don't mix levels,
simplifying component analysis and gauge fixing.

<id>
hep-th/9108022v1
<category>
hep-th
<abstract>
This review talk focusses on some of the interesting developments in the area
of superstring compactification that have occurred in the last couple of years.
These include the discovery that ``mirror symmetric" pairs of Calabi--Yau
spaces, with completely distinct geometries and topologies, correspond to a
single (2,2) conformal field theory. Also, the concept of target-space duality,
originally discovered for toroidal compactification, is being extended to
Calabi--Yau spaces. It also associates sets of geometrically distinct manifolds
to a single conformal field theory.
  A couple of other topics are presented very briefly. One concerns conceptual
challenges in reconciling gravity and quantum mechanics. It is suggested that
certain ``distasteful allegations" associated with quantum gravity such as loss
of quantum coherence, unpredictability of fundamental parameters of particle
physics, and paradoxical features of black holes are likely to be circumvented
by string theory. Finally there is a brief discussion of the importance of
supersymmetry at the TeV scale, both from a practical point of view and as a
potentially significant prediction of string theory.

<id>
hep-th/9108023v1
<category>
hep-th
<abstract>
We extend Felder's construction of Fock space resolutions for the Virasoro
minimal models to all irreducible modules with $c\leq 1$. In particular, we
provide resolutions for the representations corresponding to the boundary and
exterior of the Kac table.

<id>
hep-th/9108024v1
<category>
hep-th
<abstract>
We compute the $S$-matrix of the Tricritical Ising Model perturbed by the
subleading magnetic operator using Smirnov's RSOS reduction of the
Izergin-Korepin model. The massive model contains kink excitations which
interpolate between two degenerate asymmetric vacua. As a consequence of the
different structure of the two vacua, the crossing symmetry is implemented in a
non-trivial way. We use finite-size techniques to compare our results with the
numerical data obtained by the Truncated Conformal Space Approach and find good
agreement.

<id>
hep-th/9108025v2
<category>
hep-th
<abstract>
We calculate three- and four-point functions in super Liouville theory
coupled to super Coulomb gas on world sheets with spherical topology. We first
integrate over the zero mode and assume that a parameter takes an integer
value. After calculating the amplitudes, we formally continue the parameter to
an arbitrary real number. Remarkably the result is completely parallel to the
bosonic case, the amplitudes being of the same form as those of the bosonic
case.

<id>
hep-th/9108026v2
<category>
hep-th
<abstract>
We construct superstring theory in two dimensional black hole background
based on supersymmetric $SU(1,1)/U(1)$ gauged Wess-Zumino-Witten model.

<id>
hep-th/9108027v2
<category>
hep-th
<abstract>
Factorization of the $N$-point amplitudes in two-dimensional $c=1$ quantum
gravity is understood in terms of short-distance singularities arising from the
operator product expansion of vertex operators after the Liouville zero mode
integration. Apart from the tachyon states, there are infinitely many
topological states contributing to the intermediate states.

<id>
hep-th/9108028v1
<category>
hep-th
<abstract>
These lectures consisted of an elementary introduction to conformal field
theory, with some applications to statistical mechanical systems, and fewer to
string theory.
  Contents:
  1. Conformal theories in d dimensions
  2. Conformal theories in 2 dimensions
  3. The central charge and the Virasoro algebra
  4. Kac determinant and unitarity
  5. Identication of m = 3 with the critical Ising model
  6. Free bosons and fermions
  7. Free fermions on a torus
  8. Free bosons on a torus
  9. Affine Kac-Moody algebras and coset constructions
  10. Advanced applications

<id>
hep-th/9109001v2
<category>
hep-th
<abstract>
We propose possible new string theories based on local world-sheet symmetries
corresponding to extensions of the Virasoro algebra by fractional spin
currents. They have critical central charges $c=6(K+8)/(K+2)$ and Minkowski
space-time dimensions $D=2+16/K$ for $K\geq2$ an integer. We present evidence
for their existence by constructing modular invariant partition functions and
the massless particle spectra. The dimension $4$ and $6$ strings have
space-time supersymmetry.

<id>
hep-th/9109002v1
<category>
hep-th
<abstract>
A review is given of work by Abhay Ashtekar and his colleagues Carlo Rovelli,
Lee Smolin, and others, which is directed at constructing a nonperturbative
quantum theory of general relativity.

<id>
math/9401222v1
<category>
math.MP
<abstract>
The immediate purpose of the paper was neither to review the basic
definitions of percolation theory nor to rehearse the general physical notions
of universality and renormalization (an important technique to be described in
Part Two). It was rather to describe as concretely as possible, although in
hypothetical form, the geometric aspects of universality, especially conformal
invariance, in the context of percolation, and to present the numerical results
that support the hypotheses. On the other hand, one ulterior purpose is to draw
the attention of mathematicians to the mathematical problems posed by the
physical notions. Some precise basic definitions are necessary simply to orient
the reader. Moreover a brief description of scaling and universality on the one
hand and of renormalization on the other is also essential in order to
establish their physical importance and to clarify their mathematical content.

<id>
math/9709219v1
<category>
math.MP
<abstract>
We establish the isomorphism between a nonlinear $\sigma$-model and the
abelian gauge theory on an arbitrary curved background, which allows us to
derive integrable models and the corresponding Lax representations from gauge
theoretical point of view. In our approach the spectral parameter is related to
the global degree of freedom associated with the conformal or Galileo
transformations of the spacetime. The B$\ddot{\rm a}$cklund transformations are
derived from Chern-Simons theory where the spectral parameter is defined in
terms of the extract compactified space dimension coordinate.

<id>
math/9712278v1
<category>
math.MP
<abstract>
The initial value problem for the Ginzburg-Landau-Schr\"odinger equation is
examined in the $\epsilon \rightarrow 0$ limit under two main assumptions on
the initial data $\phi^\epsilon$. The first assumption is that $\phi^\epsilon$
exhibits $m$ distinct vortices of degree $\pm 1$; these are described as points
of concentration of the Jacobian $[J\phi^\epsilon]$ of $\phi^\epsilon$. Second,
we assume energy bounds consistent with vortices at the points of
concentration. Under these assumptions, we identify ``vortex structures'' in
the $\epsilon \rightarrow 0$ limit of $\phi^\epsilon$ and show that these
structures persist in the solution $u^\epsilon(t)$ of $GLS_\epsilon$. We derive
ordinary differential equations which govern the motion of the vortices in the
$\epsilon \rightarrow 0$ limit. The limiting system of ordinary differential
equations is a Hamitonian flow governed by the renormalized energy of Bethuel,
Brezis and H\'elein. Our arguments rely on results about the structural
stability of vortices which are proved in a separate paper.

<id>
math/9801140v1
<category>
math.MP
<abstract>
We study the Cauchy problem for an $p$-Laplacian type of evolution system
${\mathbf H}_{t}+\g [ | \g {\mathbf H}|^{p-2} \g {\mathbf H}|]={\mathbf F}$.
This system governs the evolution of a magnetic field ${\bf H}$, where the
current displacement is neglected and the electrical resistivity is assumed to
be some power of the current density. The existence, uniqueness and regularity
of solutions to the system are established. Furthermore, it is shown that the
limit solution as the power $p\rightarrow \infty$ solves the problem of Bean's
model in the type-II superconductivity theory. The result provides us
information about how the superconductor material under the external force to
become the normal conductor and vice visa. It also provides an effective method
to find numerical solutions to Bean's model.

<id>
math-ph/9801201v1
<category>
math.MP
<abstract>
We study symmetry properties of the Schr\"odinger equation with the potential
as a new dependent variable, i.e., the transformations which do not change the
form of the class of equations. We also consider systems of the Schr\"odinger
equations with certain conditions on the potential. In addition we investigate
symmetry properties of the equation with convection term. The contact
transformations of the Schr\"odinger equation with potential are obtained.

<id>
math-ph/9801202v1
<category>
math.MP
<abstract>
We study the differential forms over the frame bundle of the based loop
space. They are stochastics in the sense that we put over this frame bundle a
probability measure. In order to understand the curvatures phenomena which
appear when we look at the Lie bracket of two horizontal vector fields, we
impose some regularity assumptions over the kernels of the differential forms.
This allows us to define an exterior stochastic differential derivative over
these forms.

<id>
math-ph/9801203v1
<category>
math.MP
<abstract>
We investigate closed ideals in the Grassmann algebra serving as bases of
Lie-invariant geometric objects studied before by E. Cartan. Especially, the E.
Cartan theory is enlarged for Lax integrable nonlinear dynamical systems to be
treated in the frame work of the Wahlquist Estabrook prolongation structures on
jet-manifolds and Cartan-Ehresmann connection theory on fibered spaces. General
structure of integrable one-forms augmenting the two-forms associated with a
closed ideal in the Grassmann algebra is studied in great detail. An effective
Maurer-Cartan one-forms construction is suggested that is very useful for
applications. As an example of application the developed Lie-invariant
geometric object theory for the Burgers nonlinear dynamical system is
considered having given rise to finding an explicit form of the associated Lax
type representation.

<id>
math-ph/9801204v1
<category>
math.MP
<abstract>
We investigate Lie symmetries of Einstein's vacuum equations in N dimensions,
with a cosmological term. For this purpose, we first write down the second
prolongation of the symmetry generating vector fields, and compute its action
on Einstein's equations. Instead of setting to zero the coefficients of all
independent partial derivatives (which involves a very complicated substitution
of Einstein's equations), we set to zero the coefficients of derivatives that
do not appear in Einstein's equations. This considerably constrains the
coefficients of symmetry generating vector fields. Using the Lie algebra
property of generators of symmetries and the fact that general coordinate
transformations are symmetries of Einstein's equations, we are then able to
obtain all the Lie symmetries. The method we have used can likely be applied to
other types of equations.

<id>
math-ph/9801205v1
<category>
math.MP
<abstract>
Quantization of BKP type equations are done through the Moyal bracket and the
formalism of pseudo-differential operators. It is shown that a variant of the
dressing operator can also be constructed for such quantized systems.

<id>
math-ph/9804010v1
<category>
math.MP
<abstract>
This paper offers a solution method that allows one to find exact values for
a large class of convergent series of rational terms. Sums of this form arise
often in problems dealing with Quantum Field Theory.

<id>
math-ph/9804012v2
<category>
math.MP
<abstract>
The quantum derivatives of $e^{-A}, A^{-1}$ and $\log A$, which play a basic
role in quantum statistical physics, are derived and their convergence is
proven for an unbounded positive operator $A$ in a Hilbert space. Using the
quantum analysis based on these quantum derivatives, a basic equation for the
entropy operator in nonequilibrium systems is derived, and Zubarev's theory is
extended to infinite order with respect to a perturbation. Using the
first-order term of this general perturbational expansion of the entropy
operator, Kubo's linear response is rederived and expressed in terms of the
inner derivation $\delta_{{\cal H}}$ for the relevant Hamiltonian ${\cal H}$.
Some remarks on the conductivity $\sigma (\omega)$ are given.

<id>
math-ph/9804013v2
<category>
math.MP
<abstract>
We introduce the fuzzy supersphere as sequence of finite-dimensional,
noncommutative $Z_{2}$-graded algebras tending in a suitable limit to a dense
subalgebra of the $Z_{2}$-graded algebra of ${\cal H}^{\infty}$-functions on
the $(2| 2)$-dimensional supersphere. Noncommutative analogues of the body map
(to the (fuzzy) sphere) and the super-deRham complex are introduced. In
particular we reproduce the equality of the super-deRham cohomology of the
supersphere and the ordinary deRham cohomology of its body on the "fuzzy
level".

<id>
math-ph/9804015v3
<category>
math.MP
<abstract>
We find the covariant deformed Heisenberg algebra and the Laplace-Beltrami
operator on the extended $h$-deformed quantum plane and solve the Schr\"odinger
equations explicitly for some physical systems on the quantum plane. In the
commutative limit the behaviour of a quantum particle on the quantum plane
becomes that of the quantum particle on the Poincar\'e half-plane, a surface of
constant negative Gaussian curvature. We show the bound state energy spectra
for particles under specific potentials depend explicitly on the deformation
parameter $h$. Moreover, it is shown that bound states can survive on the
quantum plane in a limiting case where bound states on the Poincar\'e
half-plane disappear.

<id>
math-ph/9804017v1
<category>
math.MP
<abstract>
In this paper we study Lie symmetries, Kac-Moody-Virasoro algebras,
similarity reductions and particular solutions of two different recently
introduced (2+1)-dimensional nonlinear evolution equations, namely (i)
(2+1)-dimensional breaking soliton equation and (ii) (2+1)-dimensional
nonlinear Schr\"odinger type equation introduced by Zakharov and studied later
by Strachan. Interestingly our studies show that not all integrable higher
dimensional systems admit Kac-Moody-Virasoro type sub-algebras. Particularly
the two integrable systems mentioned above do not admit Virasoro type
subalgebras, eventhough the other integrable higher dimensional systems do
admit such algebras which we have also reviewed in the Appendix. Further, we
bring out physically interesting solutions for special choices of the symmetry
parameters in both the systems.

<id>
math-ph/9804018v1
<category>
math.MP
<abstract>
In this paper we give a method to obtain Darboux transformations (DTs) of
integrable equations. As an example we give a DT of the dispersive water wave
equation. Using the Miura map, we also obtain the DT of the Jaulent-Miodek
equation. \end{abstract}

<id>
math-ph/9805001v1
<category>
math.MP
<abstract>
The motion of an incompressible fluid in Lagrangian coordinates involves
infinitely many symmetries generated by the left Lie algebra of group of volume
preserving diffeomorphisms of the three dimensional domain occupied by the
fluid. Utilizing a 1+3-dimensional Hamiltonian setting an explicit realization
of this symmetry algebra is constructed recursively. A dynamical connection is
used to split the symmetries into reparametrization of trajectories and
one-parameter family of volume preserving diffeomorphisms of fluid domain.
Algebraic structures of symmetries and Hamiltonian structures of their
generators are inherited from the same construction. A comparison with the
properties of 2D flows is included.

<id>
math-ph/9805002v1
<category>
math.MP
<abstract>
A Poisson structure on the time-extended space R x M is shown to be
appropriate for a Hamiltonian formalism in which time is no more a privileged
variable and no a priori geometry is assumed on the space M of motions.
Possible geometries induced on the spatial domain M are investigated. An
abstract representation space for sl(2,R) algebra with a concrete physical
realization by the Darboux-Halphen system is considered for demonstration. The
Poisson bi-vector on R x M is shown to possess two intrinsic infinitesimal
automorphisms one of which is known as the modular or curl vector field.
Anchored to these two, an infinite hierarchy of automorphisms can be generated.
Implications on the symmetry structure of Hamiltonian dynamical systems are
discussed. As a generalization of the isomorphism between contact flows and
their symplectifications, the relation between Hamiltonian flows on R x M and
infinitesimal motions on M preserving a geometric structure therein is
demonstrated for volume preserving diffeomorphisms in connection with
three-dimensional motion of an incompressible fluid.

<id>
math-ph/9805003v1
<category>
math.MP
<abstract>
We propose a method for the approximate computation of the Green function of
a scalar massless field subjected to potential barriers of given size and shape
in spacetime. The potential of the barriers has the form
V(phi)=xi(phi^2-phi_0^2)^2; xi is very large and phi_0 very close to zero, the
product (xi phi_0^2) being finite and small. This is equivalent to the
insertion of a suitable constraint in the functional integral for phi. The
Green function contains a double Fourier transform of the characteristic
function of the region where the potential has support.

<id>
math-ph/9805016v1
<category>
math.MP
<abstract>
We review several procedures of quantization formulated in the framework of
(classical) phase space M. These quantization methods consider Quantum
Mechanics as a "deformation" of Classical Mechanics by means of the
"transformation" of the commutative algebra of smooth functions on M in a new
non-commutative algebra. These ideas lead in a natural way to Quantum Groups as
deformation (or quantization, in a broad sense) of Poisson-Lie groups, which is
also analysed here.

<id>
math-ph/9805018v1
<category>
math.MP
<abstract>
Let ${\cal H}(x,\xi)$ be a holomorphic Hamiltonian of quadratic growth on $
R^{2n}$, $b$ a holomorphic exponentially localized observable, $H$, $B$ the
corresponding operators on $L^2(R^n)$ generated by Weyl quantization, and
$U(t)=\exp{iHt/\hbar}$. It is proved that the $L^2$ norm of the difference
between the Heisenberg observable $B_t=U(t)BU(-t)$ and its semiclassical
approximation of order ${N-1}$ is majorized by $K N^{(6n+1)N}(-\hbar
ln\hbar)^N$ for $t\in [0,T_N(\hbar)]$ where $T_N(\hbar)=-{2 ln\hbar\over
{N-1}}$. Choosing a suitable $N(\hbar)$ the error is majorized by
$C\hbar^{ln|ln\hbar|}$, $0\leq t\leq |ln\hbar|/ln|ln\hbar|$. (Here $K,C$ are
constants independent of $N,\hbar$).

<id>
math-ph/9805024v1
<category>
math.MP
<abstract>
It is shown that any dynamic equation on a configuration bundle $Q\to R$ of
non-relativistic time-dependent mechanics is associated with connections on the
affine jet bundle $J^1Q\to Q$ and on the tangent bundle $TQ\to Q$. As a
consequence, any non-relativistic dynamic equation can be seen as a geodesic
equation with respect to a (non-linear) connection on the tangent bundle $TQ\to
Q$. Using this fact, the relationship between relativistic and non-relativistic
equations of motion is studied. The geometric notions of reference frames and
relative accelerations in non-relativistic mechanics are introduced in the
terms of connections. The covariant form of non-relativistic dynamic equations
is written.

<id>
math-ph/9805027v1
<category>
math.MP
<abstract>
A formula is derived that provides generating functions for any
multi-j-symbol, such as the 3-j-symbol, the 6-j-symbol, the 9-j-symbol, etc.
The result is completely determined by geometrical objects (loops and curves)
in the graph of the the multi-j-symbol. A geometric-combinatorical
interpretation for multi-j-symbols is given.

<id>
math-ph/9806004v1
<category>
math.MP
<abstract>
The talk presented at ICMP 97 focused on the scaling limits of critical
percolation models, and some other systems whose salient features can be
described by collections of random lines. In the scaling limit we keep track of
features seen on the macroscopic scale, in situations where the short--distance
scale at which the system's basic variables are defined is taken to zero. Among
the challenging questions are the construction of the limit, and the
explanation of some of the emergent properties, in particular the behavior
under conformal maps as discussed in [LPS 94]. A descriptive account of the
project, and some related open problems, is found in ref. [A] and in [AB]
(joint work with A. Burchard) where tools are developed for establishing a
curve--regularity condition which plays a key role in the construction of the
limit. The formulation of the scaling limit as a random Web measure permits to
formulate the question of uniqueness of measure(s) describing systems of random
curves satisfying the conditions of independence, Euclidean invariance, and
regularity. The uniqueness question remains open; progress on it could shed
light on the purported universality of critical behavior and the apparent
conformal invariance of the critical measures. The random Web yields also
another perspective on some of the equations of conformal field theory which
have appeared in this context, such as the equation proposed by J. Cardy [C].

<id>
math-ph/9806009v1
<category>
math.MP
<abstract>
A typical result of the paper is the following. Let $H_\gamma=H_0 +\gamma V$
where $H_0$ is multiplication by $|x|^{2l}$ and $V$ is an integral operator
with kernel $\cos< x,y\rang le$ in the space $L_2(R^d)$. If $l=d/2+ 2k$ for
some $k= 0,1,...$, then the operator $H_\gamma$ has infinite number of negative
eigenvalues for any coupling constant $\gamma\neq 0$. For other values of $l$,
the negative spectrum of $H_\gamma$ is infinite for $|\gamma|> \sigma_l$ where
$\sigma_l$ is some explicit positive constant. In the case $\pm \gamma\in
(0,\sigma_l]$, the number $N^{(\pm)}_l$ of negative eigenvalues of $H_\gamma$
is finite and does not depend on $\gamma$. We calculate $N^{(\pm)}_l$.

<id>
math-ph/9806011v2
<category>
math.MP
<abstract>
In this work we introduce the Killing-Yano symmetry on the phase space and we
investigate the symplectic structure on the space of Killing-Yano tensors. We
perform the detailed analyze of the $n$-dimensional flat space and the
Riemaniann manifolds with constant scalar curvature. We investigate the form of
some multipole tensors, which arise in the expansion of a system of charges and
currents, in terms of second-order Killing-Yano tensors in the phase space of
classical mechanics.
  We find some relations between these tensors and the generators of dynamical
symmetries like the angular momentum, the mass-inertia tensor, the conformal
operator and the momentum conjugate Runge-Lenz vector.

<id>
math-ph/9806012v2
<category>
math.MP
<abstract>
We give a proof of the Lieb-Thirring inequality in the critical case $d=1$,
$\gamma= 1/2$, which yields the best possible constant.

<id>
math-ph/9807001v2
<category>
math.MP
<abstract>
We study charge transport driven by deformations in molecular rings and
chains. Level crossings and the associated Longuet-Higgins phase play a central
role in this theory. In molecular rings a vanishing cycle of shears pinching a
gap closure leads, generically, to diverging charge transport around the ring.
We call such behavior homeopathic. In an infinite chain such a cycle leads to
integral charge transport which is independent of the strength of deformation.
In the Jahn-Teller model of a planar molecular ring there is a distinguished
cycle in the space of uniform shears which keeps the molecule in its manifold
of ground states and pinches level crossing. The charge transport in this cycle
gives information on the derivative of the hopping amplitudes.

<id>
math-ph/9807002v2
<category>
math.MP
<abstract>
The linearized Vlasov equation for a plasma system in a uniform magnetic
field and the corresponding linear Vlasov operator are studied. The spectrum
and the corresponding eigenfunctions of the Vlasov operator are found. The
spectrum of this operator consists of two parts: one is continuous and real;
the other is discrete and complex. Interestingly, the real eigenvalues are
infinitely degenerate, which causes difficulty solving this initial value
problem by using the conventional eigenfunction expansion method. Finally, the
Vlasov equation is solved by the resolvent method.

<id>
math-ph/9807003v1
<category>
math.MP
<abstract>
In this paper all seven-vertex type solutions of the coloured Yang-Baxter
equation dependent on spectral as well as coloured parameters are given. It is
proved that they are composed of five groups of basic solutions, two groups of
their degenerate forms up to five solution transformations. Moreover, all
solutions can be claasified into two types called Baxter type and free-fermion
type.

<id>
math-ph/9807005v1
<category>
math.MP
<abstract>
The Gutzwiller semiclassical trace formula links the eigenvalues of the
Scrodinger operator ^H with the closed orbits of the corresponding classical
mechanical system, associated with the Hamiltonian H, when the Planck constant
is small ("semiclassical regime"). Gutzwiller gave a heuristic proof, using the
Feynman integral representation for the propagator of ^H. Later on
mathematicians gave rigorous proofs of this trace formula, under different
settings, using the theory of Fourier Integral Operators and Lagrangian
manifolds. Here we want to show how the use of coherent states (or gaussian
beams) allows us to give a simple and direct proof.

<id>
nucl-ex/9412001v1
<category>
nucl-ex
<abstract>
The DLS collaboration has recently completed a high statistics study of
dilepton production at the Bevalac. In particular, we have measured dielectrons
(e+e-) from p-p and p-d collisions to understand the basic dilepton production
mechanisms in the energy range from 1.05 - 4.9 GeV. These data can be used to
determine the basic processes which contribute to nucleon-nucleon dilepton
production such as hadronic bremsstrahlung, vector meson processes, and
hadronic Dalitz decay. The data show that a simple elastic bremsstrahlung
calculation is insufficient to explain the data. Theoretical models are
compared with the data. A new high statistics study of Ca-Ca at 1.05 A GeV has
been made to study the collectivity of A-A collisions.

<id>
nucl-ex/9412002v1
<category>
nucl-ex
<abstract>
We present measurements of the pion transverse momentum (p_t) spectra in
central Si-nucleus collisions in the rapidity range 2.0<y<5.0 for p_t down to
and including p_t=0. The data exhibit an enhanced pion yield at low p_t
compared to what is expected for a purely thermal spectral shape. This
enhancement is used to determine the Delta-resonance abundance at freeze-out.
The results are consistent with a direct measurement of the Delta-resonance
yield by reconstruction of proton-pion pairs and imply a temperature of the
system at freeze-out close to 140 MeV.

<id>
nucl-ex/9412003v2
<category>
nucl-ex
<abstract>
We present the results of an analysis of charged particle pseudorapidity
distributions in the central region in collisions of a Au projectile with Al,
Cu, Au, and U targets at an incident energy of 10.8~GeV/c per nucleon. The
pseudorapidity distributions are presented as a function of transverse energy
produced in the target or central pseudorapidity regions. The correlation
between charged multiplicity and transverse energy measured in the central
region, as well as the target and projectile regions is also presented. We give
results for transverse energy per charged particle as a function of
pseudorapidity and centrality.

<id>
nucl-ex/9502001v1
<category>
nucl-ex
<abstract>
A systematic study of energy spectra for light particles emitted at
midrapidity from Au+Au collisions at E=0.25-1.15 A GeV reveals a significant
non-thermal component consistent with a collective radial flow. This component
is evaluated as a function of bombarding energy and event centrality.
Comparisons to Quantum Molecular Dynamics (QMD) and Boltzmann-Uehling-Uhlenbeck
(BUU) models are made for different equations of state.

<id>
nucl-ex/9502003v2
<category>
nucl-ex
<abstract>
We have studied via in-beam $\gamma$-ray spectroscopy $^{196}$Po and
$^{198}$Po, which are the first neutron-deficient Po isotopes to exhibit a
collective low-lying structure. The ratios of yrast state energies and the E2
branching ratios of transitions from non-yrast to yrast states are indicative
of a low-lying vibrational structure. The onset of collective motion in these
isotopes can be attributed to the opening of the neutron i$_{13/2}$ orbital at
N$\approx$112 and the resulting large overlap between the two valence protons
in the h$_{9/2}$ orbital and the valence neutrons in the i$_{13/2}$ orbital.

<id>
nucl-ex/9504001v2
<category>
nucl-ex
<abstract>
The partition of decay energy between the kinetic energy of reaction products
and their Q-value of formation is obtained in a statistical derivation
appropriate to highly excited nuclei, and is shown to be in a constant ratio.
We measure the kinetic energy fraction, $R = \Sigma E_{kin}/(\Sigma E_{kin} +
\Sigma Q_0)$, over a wide range of excitation energy for well-defined systems
formed in the Cl + C reaction at 35A MeV. Relationships between excitation
energy, charged-particle multiplicity, and intermediate-mass-fragment
multiplicity, observed in this work and in recent experiments by a number of
other groups, follow from the derivation of the average kinetic energies and
Q-values.

<id>
nucl-ex/9504003v1
<category>
nucl-ex
<abstract>
The inclusive A(e,e') cross section for $x \simeq 1$ was measured on $^2$H,
C, Fe, and Au for momentum transfers $Q^2$ from 1-7 (GeV/c)$^2$. The scaling
behavior of the data was examined in the region of transition from y-scaling to
x-scaling. Throughout this transitional region, the data exhibit $\xi$-scaling,
reminiscent of the Bloom-Gilman duality seen in free nucleon scattering.

<id>
nucl-ex/9505001v2
<category>
nucl-ex
<abstract>
Prompt, in-beam $\gamma$ rays following the reaction $^{170}$Yb + 142 MeV
$^{28}$Si were measured at the ATLAS facility using 10 Compton-suppressed Ge
detectors and the Fragment Mass Analyzer. Transitions in $^{194}$Po were
identified and placed using $\gamma$-ray singles and coincidence data gated on
the mass of the evaporation residues. A level spectrum up to
J$\approx$10$\hbar$ was established. The structure of $^{194}$Po is more
collective than that observed in the heavier polonium isotopes and indicates
that the structure has started to evolve towards the more collective nature
expected for deformed nuclei.

<id>
nucl-ex/9506001v1
<category>
nucl-ex
<abstract>
The ``identical'' bands in superdeformed mercury, thallium, and lead nuclei
are interpreted as examples of orbital angular momentum rotors with the weak
spin-orbit coupling of pseudo-$SU(3)$ symmetries and supersymmetries.

<id>
nucl-ex/9506002v2
<category>
nucl-ex
<abstract>
Particles with strange quark content produced in the system 1.93 AGeV
$^{58}$Ni on $^{58}$Ni have been investigated at GSI Darmstadt with the FOPI
detector system. The correlation of these produced particles was analyzed with
respect to the reaction plane. Lambda baryons exhibit a very pronounced
sideward flow pattern which is qualitatively similar to the proton flow.
However, the kaon ($K^+$,$K^0_S$) flow patterns are significantly different
from that of the protons, and their form may be useful to restrict theoretical
models on the form of the kaon potential in the nuclear medium.

<id>
nucl-ex/9507002v1
<category>
nucl-ex
<abstract>
A new measurement of $\Delta\sigma_T$ for polarized neutrons transmitted
through a polarized proton target at 16.2 MeV has been made. A polarized
neutron beam was obtained from the $^{3}\rm{H}(d,\vec n)^{4}\rm{He}$ reaction;
proton polarization over 90\% was achieved in a frozen spin target of 20 cm$^3$
volume. The measurement yielded the value $\Delta\sigma_T=(-126\pm21\pm14)$ mb.
The result of a simple phase shift analysis for the $^3S_1-^3D_1$ mixing
parameter $\epsilon_1$ is presented and compared with the theoretical potential
model predictions.

<id>
nucl-ex/9509001v1
<category>
nucl-ex
<abstract>
In a kinematically complete measurement of the
$^{7}$Li($^{7}$Li,$\alpha$$^{6}$He)$^4$He reaction at $E_{i}$ = 8 MeV it was
observed that the $^{10}$Be excited states at 9.6 and 10.2 MeV decay by
$^{6}$He emission. The state at 10.2 MeV may be a member of a rotational band
based on the 6.18 MeV 0$^+$ state.

<id>
nucl-ex/9510002v1
<category>
nucl-ex
<abstract>
A procedure has been developed for the charge, mass and energy calibration of
ions produced in nuclear heavy ion reactions. The charge and mass
identification are based on a $\Delta$E-E technique. A computer code determines
the conversion from ADC channels into energy values, atomic number and mass of
the detected fragments by comparing with energy loss calculations through a
minimization routine. The procedure does not need prior measurements with beams
of known energy and charge. An application of this technique to the calibration
of the MULTICS apparatus is described.

<id>
nucl-ex/9510004v3
<category>
nucl-ex
<abstract>
The response of an array of plastic phoswich detectors to ions of $1\le Z\le
18$ has been measured from $E/A$=12 to 72 MeV. The detector response has been
parameterized by a three parameter fit which includes both quenching and high
energy delta-ray effects. The fits have a mean variation of $\le 4\%$ with
respect to the data.

<id>
nucl-ex/9511001v1
<category>
nucl-ex
<abstract>
In recent years several new Majoron models were invented to avoid
shortcomings of the classical models while leading to observable decay rates in
double beta experiments. We give the first experimental half life bounds on
double beta decays with new Majoron emission and derive bounds on the effective
neutrino--Majoron couplings from the data of the $^{76}Ge$ HEIDELBERG--MOSCOW
experiment. While stringent half life limits for all decay modes and the
coupling constants of the classical models were obtained, small matrix elements
and phase space integrals \cite{hir95,pae95} result in much weaker limits on
the effective coupling constants of the new Majoron models.

<id>
nucl-ex/9511002v1
<category>
nucl-ex
<abstract>
The $^{174}$Yb($^{29}$Si,5n) reaction at 148 MeV with thin targets was used
to populate high-angular momentum states in $^{198}$Po. Resulting $\gamma$ rays
were observed with Gammasphere. A weakly-populated superdeformed band of 10
$\gamma$-ray transitions was found and has been assigned to $^{198}$Po. This is
the first observation of a SD band in the $A \approx 190$ region in a nucleus
with $Z > 83$. The ${\cal J}^{(2)}$ of the new band is very similar to those of
the yrast SD bands in $^{194}$Hg and $^{196}$Pb. The intensity profile suggests
that this band is populated through states close to where the SD band crosses
the yrast line and the angular momentum at which the fission process dominates.

<id>
nucl-ex/9512001v1
<category>
nucl-ex
<abstract>
The fragmentation resulting from peripheral Au + Au collisions at an incident
energy of E = 35 MeV/nucleon is investigated. A power-law charge distribution,
$A^{-\tau}$ with $\tau \approx 2.2$, and an intermittency signal are observed
for events selected in the region of the Campi scatter plot where "critical"
behavior is expected.

<id>
nucl-ex/9512002v1
<category>
nucl-ex
<abstract>
Multifragment disintegration has been measured with a high efficiency
detection system for the reaction $Au + Au$ at $E/A = 35\ MeV$. From the event
shape analysis and the comparison with the predictions of a many-body
trajectories calculation the data, for central collisions, are compatible with
a fast emission from a unique fragment source.

<id>
nucl-ex/9512003v2
<category>
nucl-ex
<abstract>
Multifragment disintegrations, measured for central Au + Au collisions at E/A
= 35 MeV, are analyzed with the Statistical Multifragmentation Model. Charge
distributions, mean fragment energies, and two-fragment correlation functions
are well reproduced by the statistical breakup of a large, diluted and
thermalized system slightly above the multifragmentation threshold.

<id>
nucl-ex/9602001v1
<category>
nucl-ex
<abstract>
Recently, there has been considerable theoretical interest in determining
strange quark contributions to hadronic matrix elements. Such matrix elements
can be accessed through the nucleon's neutral weak form factors as determined
in parity violating electron scattering. The SAMPLE experiment will measure the
strange magnetic form factor $G_M^s$ at low momentum transfer. By combining
measurements from hydrogen and deuterium the theoretical uncertainties in the
measurement can be greatly reduced and the result will be limited by
experimental errors only. A summary of recent progress on the SAMPLE experiment
is presented.

<id>
nucl-ex/9602002v1
<category>
nucl-ex
<abstract>
Parity-violating electron scattering measurements on hydrogen and deuterium,
such as those underway at the Bates and CEBAF laboratories, require
luminosities exceeding $10^{38}$cm$^{-2}$s$^{-1}$, resulting in large beam
power deposition into cryogenic liquid. Such targets must be able to absorb 500
watts or more with minimal change in target density. A 40~cm long liquid
hydrogen target, designed to absorb 500~watts of beam power without boiling,
has been developed for the SAMPLE experiment at Bates. In recent tests with
40~$\mu$A of incident beam, no evidence was seen for density fluctuations in
the target, at a sensitivity level of better than 1\%. A summary of the target
design and operational experience will be presented.

<id>
nucl-ex/9604001v1
<category>
nucl-ex
<abstract>
Multifragment events resulting from peripheral Au + Au collisions at 35
MeV/nucleon are analysed in terms of critical behavior. The analysis of most of
criticality signals proposed so far (conditional moments of charge
distributions, Campi scatter plot, fluctuations of the size of the largest
fragment, intermittency analysis) is consistent with the occurrence of a
critical behavior of the system.

<id>
nucl-ex/9605004v2
<category>
nucl-ex
<abstract>
Design studies for a second generation Dilepton Spectrometer to be built at
the SIS accelerator of GSI are presented. The basic design parameters of this
system are specified and the different detector components for charged particle
tracking and for lepton identification are described. The geometrical
acceptance for lepton pairs is given. Results on single track momentum
resolution and on lepton pair mass resolution are reported.

<id>
nucl-ex/9605005v1
<category>
nucl-ex
<abstract>
A test of parity-conserving, time-reversal non-invariance (PC TRNI) has been
performed in 5.9 MeV polarized neutron transmission through nuclear spin
aligned holmium. The experiment searches for the T-violating five-fold
correlation via a double modulation technique - flipping the neutron spin while
rotating the alignment axis of the holmium. Relative cross sections for spin-up
and spin-down neutrons are found to be equal to within $1.2 \times 10^{-5}$
(80\% confidence). This is a two order of magnitude improvement compared to
traditional detailed balance studies of time reversal, and represents the most
precise test of PC TRNI in a dynamical process.

<id>
nucl-ex/9606001v1
<category>
nucl-ex
<abstract>
Multi-fragment decays of 129Xe, 197Au, and 238U projectiles in collisions
with Be, C, Al, Cu, In, Au, and U targets at energies between E/A = 400 MeV and
1000 MeV have been studied with the ALADIN forward-spectrometer at SIS. By
adding an array of 84 Si-CsI(Tl) telescopes the solid-angle coverage of the
setup was extended to \theta_lab = 16 degree. This permitted the complete
detection of fragments from the projectile-spectator source.
  The dominant feature of the systematic set of data is the Z_bound
universality that is obeyed by the fragment multiplicities and correlations.
These observables are invariant with respect to the entrance channel if plotted
as a function of Z_bound, where Z_bound is the sum of the atomic numbers Z_i of
all projectile fragments with Z_i \geq 2. No significant dependence on the
bombarding energy nor on the target mass is observed. The dependence of the
fragment multiplicity on the projectile mass follows a linear scaling law.
  The reasons for and the limits of the observed universality of spectator
fragmentation are explored within the realm of the available data and with
model studies. It is found that the universal properties should persist up to
much higher bombarding energies than explored in this work and that they are
consistent with universal features exhibited by the intranuclear cascade and
statistical multifragmentation models.
  PACS numbers: 25.70.Mn, 25.70.Pq, 25.75.-q

<id>
nucl-ex/9607002v1
<category>
nucl-ex
<abstract>
Signatures of critical behaviour in nuclear fragmentation are often based on
arguments from percolation theory. We demonstrate with general thermodynamic
considerations and studies of the Ising model that the reliance on percolation
as a reference model bears the risk of missing parts of the essential physics.

<id>
nucl-ex/9607003v1
<category>
nucl-ex
<abstract>
We report on temperature measurements of nuclear systems formed in the Au+Au
collisions at incident energies of 50, 100, 150, 200 and 1000 A MeV. The target
spectator matter was studied at the highest energy and the interacting zone
(participants) at the lower ones.The temperature deduced from the isotope
ratios was compared with the one deduced via the excited states population. An
unexpected disagreement was found between the two measurements.

<id>
nucl-ex/9607004v1
<category>
nucl-ex
<abstract>
More than two decades ago, the van der Waals behavior of the nucleon -
nucleon force inspired the idea of a liquid-gas phase transition in nuclear
matter. Heavy-ion reactions at relativistic energies offer the unique
possibility for studying this phase transition in a finite, hadronic system. A
general overview of this subject is given emphasizing the most recent results
on nuclear calorimetry.

<id>
nucl-ex/9607005v1
<category>
nucl-ex
<abstract>
Fission excitation functions of compound nuclei in a mass region where shell
effects are expected to be very strong are shown to scale exactly according to
the transition state prediction once these shell effects are accounted for. The
fact that no deviations from the transition state method have been observed
within the experimentally investigated excitation energy regime allows one to
assign an upper limit for the transient time of 10 zs.

<id>
nucl-ex/9607006v1
<category>
nucl-ex
<abstract>
A new, sensitive method allows one to search for the enhancement of events
with nearly equal-sized fragments as predicted by theoretical calculations
based on volume or surface instabilities. Simulations have been performed to
investigate the sensitivity of the procedure. Experimentally, charge
correlations of intermediate mass fragments emitted from heavy ion reactions at
intermediate energies have been studied. No evidence for a preferred breakup
into equal-sized fragments has been found.

<id>
nucl-th/9210001v2
<category>
nucl-th
<abstract>
This paper describes a user-friendly frontend to a Fortran program that
integrates coupled nonlinear ordinary differential equations. The user
interface is built using the NeXTstep Interface Builder, together with a
public-domain graphical palette for displaying intermediate and final results.
In running the code for a given set of equation parameters the user sees a plot
of the solutions at each stage of the iterative process. In the case of a
successful sequence of iterations, the initially discontinuous curves smooth
out as the scale parameters of the solutions are adjusted to achieve a solution
to the nonlinear equations. If the iterative process goes astray, as it often
does for a poor choice of starting scale parameters, the user has the
opportunity to stop and start over with a better choice, guided by the result
of the previous run. The ease of interaction with the equations also allows the
user to develop an intuition regarding their solutions and to explore the
parameter space for the equations much more quickly.

<id>
nucl-th/9210002v1
<category>
nucl-th
<abstract>
We consider the effect of including quark delocalization and color screening,
in the nonrelativistic quark cluster model, on baryon-baryon potentials and
phase shifts. We find that the inclusion of these additional effects allows a
good qualitative description of both.

<id>
nucl-th/9210004v1
<category>
nucl-th
<abstract>
In this report, electric quadrupole corrections to the two neutron removal
cross section measured in heavy ion collisions are estimated for $^{197}$Au and
$^{59}$Co targets. The quadrupole process is assumed to proceed primarily
through excitation of the giant isovector quadrupole resonance, which then
decays by neutron emission. For $^{59}$Co, the contribution from E2 radiation
is found to be small, while for $^{197}$Au we find the quadrupole contribution
resolves the discrepancy between experiment and the simple predictions of the
Weissacker-Williams virtual photon method.

<id>
nucl-th/9210005v1
<category>
nucl-th
<abstract>
Using separable $NN$ and $\Lambda N$-$\Sigma N$ potentials in the Faddeev
equations, we have demonstrated that the predicted enhancement in the $\Lambda
d$ cross section near the $\Sigma d$ threshold is associated with resonance
poles in the scattering amplitude. The positions of these poles, on the second
Riemann sheet of the complex energy plane, are determined by examining the
eigenvalues of the kernel of the Faddeev equations. This suggests that for a
certain class of $\Lambda N$-$\Sigma N$ potentials we can form a
$\Sigma$-hypertriton with a width of about 8 MeV.

<id>
nucl-th/9210006v1
<category>
nucl-th
<abstract>
A modified Low procedure for constructing soft-photon amplitudes has been
used to derive two general soft-photon amplitudes, a two-s-two-t special
amplitude $M^{TsTts}_{\mu}$ and a two-u-two-t special amplitude
$M^{TuTts}_{\mu}$, where s, t and u are the Mandelstam variables.
$M^{TsTts}_{\mu}$ depends only on the elastic T-matrix evaluated at four sets
of (s,t) fixed by the requirement that the amplitude be free of derivatives
($\partial$T/$\partial$s and /or $\partial$T/$\partial t$). Likewise
$M^{TuTts}_{\mu}$ depends only on the elastic T-matrix evaluated at four sets
of (u,t). In deriving these amplitudes, we impose the condition that
$M^{TsTts}_{\mu}$ and $M^{TuTts}_{\mu}$ reduce to $\bar{M}^{TsTts}_{\mu}$ and
$\bar{M}^{TuTts}_{\mu}$, respectively, their tree level approximations. The
amplitude $\bar{M}^{TsTts}_{\mu}$ represents photon emission from a sum of
one-particle t-channel exchange diagrams and one-particle s-channel exchange
diagrams, while the amplitude $\bar{M}^{TuTts} _{\mu}$ represents photon
emission from a sum of one-particle t-channel exchange diagrams and
one-particle u-channel exchange diagrams. The precise expressions for
$\bar{M}^{TsTts}_{\mu}$ and $\bar{M}^{TuTts}_{\mu}$ are determined by using the
radiation decomposition identities of Brodsky and Brown. We point out that it
is theoretically impossible to describe all bremsstrahlung processes by using
only a single class of soft-photon amplitudes. At least two different classes
are required: the amplitudes which depend on s and t or the amplitudes which
depend on u and t. When resonance effects are important, the amplitude
$M^{TsTts}_{\mu}$, not $M^{Low(st)}_{\mu}$, should be used. For processes with
strong u-channel exchange effects, the amplitude $M^{TuTts}_{\mu}$ should be
the first choice.

<id>
nucl-th/9210010v1
<category>
nucl-th
<abstract>
Once density-dependent meson masses are introduced into the nuclear many-body
problem, conventional mechanisms for saturation no longer operate. We suggest
that a loop correction, essentially the introduction of the axial vector
coupling $g_A(\rho,k)$ as function of density $\rho$ and momentum $k$, can
bring about saturation, and present schematic calculations to illustrate this.
We find that a very small density-dependence in $g_A$ gives rise to a very
large saturating effect on nuclear matter. In fact, this new saturation
mechanism turns out to be more powerful than any of the conventional
mechanisms.

<id>
nucl-th/9210011v1
<category>
nucl-th
<abstract>
The influence of relativity on the triton binding energy is investigated. The
relativistic three-dimensional version of the Bethe-Salpeter equation proposed
by Blankenbecler and Sugar (BbS) is used. Relativistic (non-separable)
one-boson-exchange potentials (constructed in the BbS framework) are employed
for the two-nucleon interaction. In a 34-channel Faddeev calculation, it is
found that relativistic effects increase the triton binding energy by about 0.2
MeV. Including charge-dependence (besides relativity), the final triton binding
energy predictions are 8.33 and 8.16 MeV for the Bonn A and B potential,
respectively.

<id>
nucl-th/9210012v1
<category>
nucl-th
<abstract>
The angular momentum, angular velocity, Kelvin circulation, and vortex
velocity vectors of a quantum Riemann rotor are proven to be either (1) aligned
with a principal axis or (2) lie in a principal plane of the inertia ellipsoid.
In the second case, the ratios of the components of the Kelvin circulation to
the corresponding components of the angular momentum, and the ratios of the
components of the angular velocity to those of the vortex velocity are analytic
functions of the axes lengths.

<id>
nucl-th/9210013v2
<category>
nucl-th
<abstract>
A particle-vibration coupling calculation based on the RPA and the cranked
shell model has been carried out for superdeformed rotational bands in
$^{193}$Hg. The result suggests that properties of single-particle motions in
superdeformed nuclei may be significantly affected by coupling effects with
low-frequency octupole vibrational modes, especially by the lowest $K=2$
octupole mode.

<id>
nucl-th/9210014v1
<category>
nucl-th
<abstract>
We present novel Monte Carlo methods for treating the interacting shell model
that allow exact calculations much larger than those heretofore possible. The
two-body interaction is linearized by an auxiliary field; Monte Carlo
evaluation of the resulting functional integral gives ground-state or thermal
expectation values of few-body operators. The ``sign problem'' generic to
quantum Monte Carlo calculations is absent in a number of cases. We discuss the
favorable scaling of these methods with nucleon numb er and basis size and
their suitability to parallel computation.

<id>
nucl-th/9210015v1
<category>
nucl-th
<abstract>
The derivation of the nucleon-nucleon force from the Skyrme model is
reexamined. Starting from previous results for the potential energy of
quasistatic solutions, we show that a calculation using the Born-Oppenheimer
approximation properly taking into account the mixing of nucleon resonances,
leads to substantial central attraction. We obtain a potential that is in
qualitative agreement with phenomenological potentials. We also study the
non-adiabatic corrections, such as the velocity dependent transition
potentials, and discuss their importance.

<id>
nucl-th/9210016v1
<category>
nucl-th
<abstract>
We calculate transverse response functions for quasi-elastic electron
scattering at high momentum transfers in a relativistic Hartree approximation
in configuration space. We treat the excitation of the $\Delta$ resonance using
its free mass and width. Good agreement with experiment is found in the dip
region.

<id>
nucl-th/9210017v1
<category>
nucl-th
<abstract>
We examine the prediction of supersymmetric quantum mechanics that bands with
identical gamma-ray energies occur in quartets. The experimental data suggest
that this scenario is actually realized in nature. In the $A=150$ mass region,
four known pairs of isospectral bands can be grouped in two quartets, while
there are indications of such patterns around $A=190$. We introduce a small
supersymmetry breaking, necessary to describe the details of the data. We
derive relations among the transition rates that can be used to test our
predictions. pacs: 21.10.Re, 11.30.Pb, 21.60.Fw, 23.20.Lv

<id>
nucl-th/9210018v1
<category>
nucl-th
<abstract>
Excited negative parity hyperon masses are calculated in a chiral bag model
in which the pion and the kaon fields are treated as perturbations. We also
calculate the hadronic widths of $\lama$ and $\lamb$ as well as the coupling
constants of the lightest $I=0$ excited hyperon to the meson-baryon channels,
and discuss how the dispersive effects of the hadronic meson-baryon decay
channels affect the excited hyperon masses. Meson cloud corrections to the
electromagnetic decay widths of the two lightest excited hyperons into ground
states $\lamz$ and $\sigz$ are calculated within the same model and are found
to be small. Our results strengthen the argument that predictions of these
hyperon radiative decay widths provide an excellent test for various quark
models of hadrons.

<id>
nucl-th/9210020v1
<category>
nucl-th
<abstract>
We show that the coalescence model for fragment formation leads to an
approximate site percolation model. Features characteristic of a percolation
model also appear in microscopic models of disassembly.

<id>
nucl-th/9211002v1
<category>
nucl-th
<abstract>
The assumption that a small point-like configuration does not interact with
nucleons leads to a new set of sum rules that are interpreted as models of the
baryon-nucleon interaction. These models are rendered semi-realistic by
requiring consistency with data for cross section fluctuations in proton-proton
diffractive collisions.

<id>
nucl-th/9211004v1
<category>
nucl-th
<abstract>
There have been suggestions to measure atomic parity nonconservation (PNC)
along an isotopic chain, by taking ratios of observables in order to cancel
complicated atomic structure effects. Precise atomic PNC measurements could
make a significant contribution to tests of the Standard Model at the level of
one loop radiative corrections. However, the results also depend upon certain
features of nuclear structure, such as the spatial distribution of neutrons in
the nucleus. To examine the sensitivity to nuclear structure, we consider the
case of Pb isotopes using various recent relativistic and non-relativistic
nuclear model calculations. Contributions from nucleon internal weak structure
are included, but found to be fairly negligible. The spread among present
models in predicted sizes of nuclear structure effects may preclude using Pb
isotope ratios to test the Standard Model at better than a one percent level,
unless there are adequate independent tests of the nuclear models by various
alternative strong and electroweak nuclear probes. On the other hand,
sufficiently accurate atomic PNC experiments would provide a unique method to
measure neutron distributions in heavy nuclei.

<id>
nucl-th/9211005v1
<category>
nucl-th
<abstract>
The Kelvin circulation is the kinematical Hermitian observable that measures
the true character of nuclear rotation. For the anisotropic oscillator, mean
field solutions with fixed angular momentum and Kelvin circulation are derived
in analytic form. The cranking Lagrange multipliers corresponding to the two
constraints are the angular and vortex velocities. Self-consistent solutions
are reported with a constraint to constant volume.

<id>
nucl-th/9211006v1
<category>
nucl-th
<abstract>
It is pointed out that the meson mixing matrix elements usually considered
responsible for the bulk of the observed few-body charge symmetry breaking are
naturally $q^2$-dependent in QCD. For $\pi^o-\eta$ mixing, using the usual
representation of the pseudoscalar fields, the leading $q^2$ dependence can be
explicitly calculated using chiral perturbation theory to one loop, the result
being a significant decrease in the magnitude of the matrix element in going
from timelike to spacelike values of $q^2$. Since it is the latter range of
$q^2$ which is relevant to NN scattering and the few-body bound state, this
result calls into serious question the standard treatment of few-body charge
symmetry breaking contributions associated with $\pi^o-\eta$ and $\rho -\omega$
mixing.

<id>
nucl-th/9211007v1
<category>
nucl-th
<abstract>
In view of persisting misunderstanding about the determination of the
pion-nucleon coupling constants in the Nijmegen multienergy partial-wave
analyses of pp, np, and pbar-p scattering data, we present additional
information which may clarify several points of discussion. We comment on
several recent papers addressing the issue of the pion-nucleon coupling
constant and criticizing the Nijmegen analyses.

<id>
nucl-th/9211009v1
<category>
nucl-th
<abstract>
We develop a nonperturbative technique in field theory to study properties of
infinite nuclear matter at zero temperature as well as at finite temperatures.
Here we dress the nuclear matter with off-mass shell pions. The techniques of
thermofield dynamics are used for finite temperature calculations. Equation of
state is derived from the dynamics of the interacting system in a self
consistent manner. The transition temperature for nuclear matter appears to be
around 15 MeV.

<id>
nucl-th/9211012v1
<category>
nucl-th
<abstract>
Single--particle spectra of $\Lambda $ and $\Sigma $ hypernuclei are
calculated within a relativistic mean--field theory. The hyperon couplings used
are compatible with the $\Lambda $ binding in saturated nuclear matter,
neutron-star masses and experimental data on $\Lambda $ levels in hypernuclei.
Special attention is devoted to the spin-orbit potential for the hyperons and
the influence of the $\rho $-meson field (isospin dependent interaction).

<id>
nucl-th/9211013v1
<category>
nucl-th
<abstract>
We calculate the chi**2 of various NN potential models with respect to the pp
scattering data. We find that only the potential models which were explicitly
fitted to the pp data give a reasonable description of these data. Most models
give a pretty large chi**2 on the very low-energy pp data, due to incorrect 1S0
phase shifts.

<id>
nucl-th/9212001v1
<category>
nucl-th
<abstract>
The excitation of a $\Delta$-isobar in a finite nucleus in charge--exchange
($^3\!$He,t) reaction is discussed in terms of a nuclear response function. The
medium effects modifying a $\Delta$- and a pion propagation were considered for
a finite size nucleus. The Glauber approach has been used for distortion of a
$^3\!$He and a triton in the initial and the final states. The effects
determining the peak positions and its width are discussed. Large displacement
width for the $\Delta$ - h excitations and considerable contribution of
coherent pion production were found for the reaction on $^{12}$C.

<id>
nucl-th/9212002v1
<category>
nucl-th
<abstract>
We find solutions to the 1+1 dimensional scalar-only linear sigma model. A
new method is used to compute 1-fermion loop contributions exactly and
agreement with published results employing other methods is excellent. A
renormalization scheme which differs from that commonly used in such
calculations but similar to that required in 1+3 dimensions is also presented.
We compare ``kink'' {\it versus} ``shallow bag'' solutions paying careful
attention to the implications of the 1-fermion loop contributions for the
stability of the former. We find that, for small fermion multiplicities,
self-consistent shallow bag solutions are always more bound than their
metastable kink counterparts. However, as the fermion multiplicity increases,
shallow bags evolve into kinks which eventually are the only self-consistent
configurations. This situation is qualitatively the same for the two
renormalization schemes considered. When we construct ``baryons'', each
containing three fermions, the kink configuration is typically more bound than
the shallow bag when 1-fermion loop contributions are included.

<id>
nucl-th/9212003v1
<category>
nucl-th
<abstract>
We study relativistic nuclear matter in the $\sigma - \omega$ model including
the ring-sum correlation energy. The model parameters are adjusted
self-consistently to give the canonical saturation density and binding energy
per nucleon with the ring energy included. Two models are considered,
mean-field-theory where we neglect vacuum effects, and the relativistic Hartree
approximation where such effects are included but in an approximate way. In
both cases we find self-consistent solutions and present equations of state. In
the mean-field case the ring energy completely dominates the attractive part of
the energy density and the elegant saturation mechanism of the standard
approach is lost, namely relativistic quenching of the scalar attraction. In
the relativistic Hartree approach the vacuum effects are included in an
approximate manner using vertex form factors with a cutoff of 1 - 2 GeV, the
range expected from QCD. Due to the cutoff, the ring energy for this case is
significantlysmaller, and we obtain self-consistent solutions which preserve
the basic saturation mechanism of the standard relativistic approach.

<id>
nucl-th/9212005v1
<category>
nucl-th
<abstract>
The rotational spectrum of $^{168}$Yb is calculated diagonalizing different
effective interactions within the basis of unperturbed rotational bands
provided by the cranked shell model. A transition between order and chaos
taking place in the energy region between 1 and 2 MeV above the yrast line is
observed, associated with the onset of rotational damping. It can be related to
the higher multipole components of the force acting among the unperturbed
rotational bands.

<id>
nucl-th/9212006v1
<category>
nucl-th
<abstract>
Fluctuations associated with stretched E2 transitions from high spin levels
in nuclei around $^{168}$Yb are investigated by a cranked shell model extended
to include residual two-body interactions. It is found that the gamma-ray
energies behave like random variables and the energy spectra show the Poisson
fluctuation, in the cranked mean field model without the residual interaction.
With two-body residual interaction included, discrete transition pattern with
unmixed rotational bands is still valid up to around 600 keV above yrast, in
good agreement with experiments. At higher excitation energy, a gradual onset
of rotational damping emerges. At 1.8 MeV above yrast, complete damping is
observed with GOE type fluctuations for both energy levels and transition
strengths(Porter-Thomas fluctuations).

<id>
nucl-th/9212007v1
<category>
nucl-th
<abstract>
It is shown that the 1S level hyperfine populations prior to muon capture
will be statistical when either target or beam are unpolarised independent of
the atomic level at which the hyperfine interaction becomes appreciable. This
assertion holds in the absence of magnetic transitions during the cascade and
is true because of minimal polarisation after atomic capture and selective
feeding during the cascade.

<id>
nucl-th/9212009v1
<category>
nucl-th
<abstract>
Cross section fluctuations in nuclear scattering are briefly reviewed in
order to show the main important features. Then chaotic scattering is
introduced by means of a very simple model. It is shown that chaoticity
produces the same kind of irregular fluctuations observed in light heavy--ion
collisions. The transition from order to chaos allows a new general framework
for a deeper understanding of reaction mechanisms.

<id>
physics/9610008v1
<category>
physics.acc-ph
<abstract>
The high energy physics advantages, disadvantages and luminosity requirements
of hadrons, of leptons and photon-photon colliders are considered. Technical
arguments for increased energy in each type of machine are presented. Their
relative size, and the implications of size on cost are discussed.

<id>
physics/9610009v1
<category>
physics.acc-ph
<abstract>
Muon Colliders have unique technical and physics advantages and disadvantages
when compared with both hadrons and electron machines. They should be regarded
as complementary. Parameters are given of a 4 TeV high luminosity muon-muon
collider, and of a 0.5 TeV demonstration machine. We discuss the various
systems in such muon collider.

<id>
physics/9611001v1
<category>
physics.acc-ph
<abstract>
This work is devoted to an examination of Stern-Gerlach forces consistent
with special relativity and is motivated by recent interest in the relativistic
Stern-Gerlach force acting on polarized protons in high-energy particle
accelerators. The equations for the orbital and spin motion of a classical
charged particle with arbitrary intrinsic magnetic dipole moment in an external
electromagnetic field are considered and by imposing the constraints of special
relativity and restricting to first order in spin (= first order $\hbar$) a
well-defined class of spin-orbit systems is obtained. All these systems can be
treated on an equal footing including such prominent cases as those considered
by Frenkel and by Good. The Frenkel case is considered in great detail because
I show that this system is identical with the one introduced by Derbenev and
Kondratenko for studying spin motion in accelerators. In particular I prove
that the spin-orbit system of Derbenev and Kondratenko is (nonmanifestly)
Poincar\'e covariant and identify the transformation properties of this system
under the Poincar\'e group. The Derbenev-Kondratenko Hamiltonian was originally
proposed as a way to combine relativistic spin precession and the Lorentz
force. The aforementioned findings now demonstrate that the
Derbenev-Kondratenko Hamiltonian also provides a legitimate framework for
handling the relativistic Stern-Gerlach force. Numerical examples based on the
Frenkel and Good cases for the HERA proton ring and electromagnetic traps are
provided.

<id>
physics/9612016v2
<category>
physics.acc-ph
<abstract>
Quite good agreement has been achieved between computer modeling and actual
performance of the Brookhaven 200 MeV Linac. We will present comparisons
between calculated and measured performance for the beam transport through the
RFQ, the 6 meter transport from RFQ to the linac and meching and transport
through the linac.

<id>
physics/9701005v2
<category>
physics.acc-ph
<abstract>
It is shown that change in transverse momentum of a relativistic particle,
crossing an accelerating cavity parallel to its axis, may be presented as an
integral over trajectory, the integrand of which is proportional to the
component of magnetic field parallel to this axis. The changes in two
transversal components of momentum are equal in value but opposite in sign. The
obtained result is compared with Panofsky-Wenzel theorem

<id>
physics/9701013v1
<category>
physics.acc-ph
<abstract>
It is shown that high accelerating gradient can be obtained in a specially
constructed system of electron (positron) bunches, moving in cold plasma,with
definite density. These combined bunch systems do not generate the wake fields
behind them and can pass through the plasma column in a periodic sequence. The
consideration is carried out numericaly and analyticaly in one dimensional
approach, (which can be applied to finite system when its transverse dimensions
are larger than plasma wave length, divided by $2\pi$). The possibilities of
the experimental tests by measuring the predicted energy gain are discussed on
the examples of Argonne Wakefield Accelerator and induction linac with typical
parameters.

<id>
physics/9701022v1
<category>
physics.acc-ph
<abstract>
One dimensional nonlinear plasma wake-fields excited by a single bunch and by
series of bunches are considered. Essential differences are brought to light
between negatively charged bunch case and positively one. The bunches with
nonuniform distributions of density are investigated. The obtained results show
dependence of excited potential electric fields on bunches parameters and
allows to choose these parameters optimal.

<id>
physics/9702016v2
<category>
physics.acc-ph
<abstract>
We consider the high energy advantages, disadvantages and luminosity
requirements of hadrons, leptons and photon-photon colliders. Technical
problems in obtaining increased energy in each type of machine are presented.
The machines relative size are also discussed.

<id>
physics/9702017v1
<category>
physics.acc-ph
<abstract>
Parameters are given of 4 TeV and 0.5 TeV (c-of-m) high luminosity muon-muon
Colliders. We discuss the various systems, starting from the proton accelerator
needed to generate the muons and proceeding through muon cooling, acceleartion
and storage in a collider ring. Detector background, polarization are analyzed.
We also look at other type of colliders (hadron, lepton and photon-photon) for
comparison. Technical problems in obtaining increased energy in each type of
machine are presented. Their relative size and probable relative costs are
discussed.

<id>
physics/9703018v1
<category>
physics.acc-ph
<abstract>
Fast wire scanners are today considered as part of standard instrumentation
in high energy synchrotrons. The extension of their use to synchrotrons working
at lower energies, where Coulomb scattering can be important and the transverse
beam size is large, introduces new complications considering beam heating of
the wire, composition of the secondary particle shower and geometrical
consideration in the detection set-up. A major problem in treating these
effects is that the creation of secondaries in a thin carbon wire by a
energetic primary beam is difficult to describe in an analytical way. We are
here presenting new results from a full Monte Carlo simulation of this process
yielding information on heat deposited in the wire, particle type and energy
spectrum of secondaries and angular dependence as a function of primary beam
energy. The results are used to derive limits for the use of wire scanners in
low energy accelerators.

<id>
physics/9704012v1
<category>
physics.acc-ph
<abstract>
The status and initial performance of a simulation program CAIN for
interaction region of linear colliders is described. The program is developed
to be applicable for e+e-, e-e-, e-gamma and gamma-gamma linear colliders. As
an example of an application, simulation of a gamma-gamma collider option of
NLC is reported.

<id>
physics/9705028v1
<category>
physics.acc-ph
<abstract>
Beam stability and halo formation in high-intensity axisymmetric 2D beams in
a uniform focusing channel are analyzed using particle-in-cell simulations. The
tune depression - mismatch space is explored for the uniform (KV) distribution
of the particle transverse-phase-space density, as well as for more realistic
ones (in particular, the water-bag distribution), to determine the stability
limits and halo parameters. The numerical results show an agreement with
predictions of the analytical model for halo formation (R.L. Gluckstern, Phys.
Rev. Letters, 73 (1994) 1247).

<id>
physics/9706036v2
<category>
physics.acc-ph
<abstract>
ANKA is a 2.5 GeV synchrotron radiation storage ring under construction at
the Forschungszentrum Karlsruhe in Germany. The injector system will consist of
a pre-injector with an end energy of 20 to 50 MeV and a 0.5 GeV booster
synchrotron. In the following three different concepts for designing the
booster synchrotron are compared.

<id>
physics/9706039v1
<category>
physics.acc-ph
<abstract>
In this paper the concept and the first successful tests towards constructing
a superconductive micro-undulator with a period length of 3.8 mm are described.
The structures were built with a commercially available NbTi wire. A full 100
periods prototype is now under construction.

<id>
physics/9709013v1
<category>
physics.acc-ph
<abstract>
In the National Spallation Neutron Source (NSNS) design, a 180 meter long
transport line connects the 1 GeV linac to an accumulator ring. The linac beam
has a current of 28 mA, pulse length of 1 ms, and 60 Hz rep rate. The high
energy transport line consists of sixteen 60 degrees FODO cells, and
accommodates a 90 degrees achromatic bend, an energy compressor, collimators,
part of injection system, and enough diagnostic devices to measure the beam
quality before injection. To reduce the uncontrolled beam losses, this line has
nine beam halo scrapers and very tight tolerances on both transverse and
longitudinal beam dynamics under space charge conditions. The design of this
line is presented.

<id>
physics/9709014v1
<category>
physics.acc-ph
<abstract>
Projections of charged particle beam current density (profiles) are
frequently used as a measure of beam position and size. In conventional
practice only two projections, usually horizontal and vertical, are measured.
This puts a severe limit on the detail of information that can be achieved. A
third projection provides a significant improvement. The Algebraic
Reconstruction Technique (ART) uses three or more projections to reconstruct
3-dimensional density profiles. At the 200 MeV H- linac, we have used this
technique to measure beam density, and it has proved very helpful, especially
in helping determine if there is any coupling present in x-y phase space. We
will present examples of measurements of current densities using this
technique.

<id>
physics/9711018v2
<category>
physics.acc-ph
<abstract>
During the last years several attempts were undertaken to decrease the period
length of undulators to the mm range. In this paper, a novel type of an
in-vacuum undulator is described which is built with superconductive wires. The
period length of this special device is 3.8 mm. In principle, it is possible to
decrease this period length even further. A 100 period long undulator has been
built and will be tested with beam in the near future.

<id>
physics/9712023v1
<category>
physics.acc-ph
<abstract>
The present status of suggested linac-ring type ep and gamma-p colliders is
reviewed. The main parameters of these machines as well as e-nucleus and
gamma-nucleus colliders are considered. It is shown that sufficiently high
luminosities may be achieved with a reasonable improvement of proton and
electron beam parameters.

<id>
physics/9802002v1
<category>
physics.acc-ph
<abstract>
We present a Hamiltonian formulation of muon dynamics in toroidal sector
solenoids (bent solenoids)

<id>
physics/9802004v1
<category>
physics.acc-ph
<abstract>
Present paper is devoted to formulation of the system of the master equations
of the problem and analytical considerations of the limiting cases, which have
an exact solutions (transversal and longitudinal flat bunches, linear approxim-
ation). Presented consideration gives some hints to the construction of proper
algorithm for numerical calculations.

<id>
physics/9802005v1
<category>
physics.acc-ph
<abstract>
Parameters are given of machines with center-of-mass (CoM) energies of 3 TeV
and 400 GeV but, besides a comment on neutrino radiation, the paper
concentrates on progress on the design of a machine to operate at a light Higgs
mass, assumed, for this study, to be 100 GeV (CoM).

<id>
physics/9803043v2
<category>
physics.acc-ph
<abstract>
An approach has been developed where the Smith-Purcell radiation (SPR), i.e.
emission of electrons moving close to a periodic structure, is treated as the
resonant diffraction radiation. Simple formulas have been designed for the SPR
intensity for a grating having perfectly conducting strips spaced by a vacuum
gap. The results have been compared with those obtained via other techniques.
It has been shown that the intensity of radiation for the said gratings for a
relativistic case sufficiently exceeds the SPR intensity for the grating made
up by a periodically deformed continuous surface.

<id>
physics/9804009v2
<category>
physics.acc-ph
<abstract>
Tracking studies have indicated that for a lattice whose elements all have a
single field multipole present, all having the same order k, the dynamic
aperture approaches a non zero limit when k becomes very large. The dynamic
aperture and other properties of the lattice, as k becomes large, will be
called the high multipole limit. It will be shown that the high multipole limit
provides a reasonable estimate of the dynamic aperture of an accelerator, and
the other properties of the high multipole limit found below are useful for
understanding the stability of the accelerator. The high multipole limit is
easily computed and it also provides an estimate of how much can be gained by
correcting the lower field multipoles. The above results will be illustrated by
tracking studies done with a simple one cell lattice, and with a RHIC lattice
having six low beta insertions.

<id>
physics/9804015v1
<category>
physics.acc-ph
<abstract>
We discuss the new layout of a cavity chain (superstructure) allowing, we
hope, significant cost reduction of the RF system of both linacs of the TESLA
linear collider. The proposed scheme increases the fill factor and thus makes
an effective gradient of an accelerator higher. We present mainly computations
we have performed up to now and which encouraged us to order the copper model
of the scheme, still keeping in mind that experiments with a beam will be
necessary to prove if the proposed solution can be used for the acceleration.

<id>
physics/9806049v1
<category>
physics.acc-ph
<abstract>
A prototype of a novel superconductive undulator with a period length of 3.8
mm is described. The undulator is 100 periods long. In the first tests
described in this paper the undulator was cooled in a helium bath and it was
shown that it can be operated as expected with a maximum current of 1400 A.
Afterwards the undulator field was measured with a miniature Hall probe with an
active area of (100x100)x10^-6 m^2. Calculated and measured field distributions
are in good agreement. A cryostat for a beam test at Mainz microtron MAMI was
built in which liquid helium cools indirectly the in-vacuum undulator. At the
moment the cryostat is tested and optimized.

<id>
physics/9807006v1
<category>
physics.acc-ph
<abstract>
Besides continued work on the parameters of a 3-4 and 0.5 TeV CoM collider,
many studies are now concentrating on a machine near 100 GeV that could be a
factory for the s-channel production of Higgs particles. We mention the
research on the various components in such muon colliders, starting from the
proton accelerator needed to generate pions from a heavy-Z target and
proceeding through the phase rotation and decay channel, muon cooling,
acceleration, storage in a ring and the collider detector. We also mention
theoretical and experimental R&D plans for the next several years that should
lead to a better understanding of the design and feasibility issues for all of
the components. This note is a summary of a report updating the progress on the
R&D since the Feasibility Study of Muon Colliders presented at the Workshop
Snowmass'96.

<id>
physics/9807028v1
<category>
physics.acc-ph
<abstract>
The 1.7-GeV 100-mA CW proton linac is now under design for the Accelerator
Production of Tritium (APT) Project. The APT linac comprises both the normal
conducting (below 211 MeV) and superconducting (SC) sections. The high current
leads to stringent restrictions on allowable beam losses (<1 nA/m), that
requires analyzing carefully all possible loss sources. While wake-field
effects are usually considered negligible in proton linacs, we study these
effects for the APT to exclude potential problems at such a high current. Loss
factors and resonance frequency spectra of various discontinuities of the
vacuum chamber are investigated, both analytically and using 2-D and 3-D
simulation codes with a single bunch as well as with many bunches. Our main
conclusion is that the only noticeable effect is the HOM heating of the 5-cell
SC cavities. It, however, has an acceptable level and, in addition, will be
taken care of by HOM couplers.

<id>
physics/9807029v1
<category>
physics.acc-ph
<abstract>
Cavity loss factors can be easily computed for ultrarelativistic beams using
time-domain codes like MAFIA or ABCI. However, for non-ultrarelativistic beams
the problem is more complicated because of difficulties with its numerical
formulation in the time domain. We calculate the loss factors of a
non-ultrarelativistic bunch and compare results with the relativistic case.

<id>
physics/9807030v1
<category>
physics.acc-ph
<abstract>
A new class of self-consistent 6-D phase space stationary distributions is
constructed both analytically and numerically. The beam is then mismatched
longitudinally and/or transversely, and we explore the beam stability and halo
formation for the case of 3-D axisymmetric beam bunches using particle-in-cell
simulations. We concentrate on beams with bunch length-to-width ratios varying
from 1 to 5, which covers the typical range of the APT linac parameters. We
find that the longitudinal halo forms first for comparable longitudinal and
transverse mismatches. An interesting coupling phenomenon - a longitudinal or
transverse halo is observed even for very small mismatches if the mismatch in
the other plane is large - is discovered.

<id>
physics/9807041v1
<category>
physics.acc-ph
<abstract>
Free Electron Lasers driven by Super-Conducting Linacs require the
generation, acceleration and transport of trains of bunches with high charge,
high repetition rate and low emittance. A numerical model already developed for
the modeling of beam dynamics in SC Linacs has been now extended to treat RF
guns with proper description of the photocathode bunch generation. The model
algorithm is based on a coupled integration of Newton and Maxwell equations
under a slowly varying envelope approximation for the time evolution of mode
amplitudes in the accelerating structure and an envelope equation description
for a set of bunch slices. In this way beam loading effects and higher order
modes excitation can be studied. The application to the TTF-FEL injector as a
particular design is presented and the optimization according to the invariant
envelope concept is discussed, in the frame of single bunch calculations
compared to the results of other multi-particle codes.

<id>
physics/9610002v1
<category>
physics.ao-ph
<abstract>
It is noted that the results of recent experiments on the enhancement of
turbulent kinetic energy (TKE) dissipation below surface waves can be stated as
follows. TKE dissipation is enhanced by a factor $15 H_{ws}/z$ at depths $0.5
H_{ws} < z < 20 H_{ws}$ with respect to the wall-layer result $\epsilon =
u_{*w}^3/\kappa z$, where $u_{*w}$ is the friction velocity in water and
$H_{ws}$ is the significant wind-sea wave height. For open ocean conditions,
this reduces in most cases to an enhancement factor $10^6 u_{*w}^2/gz \approx
U_{10}^2/gz$.

<id>
physics/9610021v1
<category>
physics.ao-ph
<abstract>
The mechanism that causes an interdecadal oscillation in a coarse resolution
sector ocean model forced by mixed boundary conditions is studied. The
oscillation is characterized by large fluctuations in convective activity and
air/sea heat exchange on a decadal time scale. When the convective activity is
large, a strengthening of the southeastward surface flow advects more
relatively fresh water from the northwestern part of the basin into the
convective area, which reduces the convective activity. Similarly, when the
convective activity is small, the flow of relatively fresh water is weak, which
enables the expansion of the convective area. The oscillation critically
depends on how the ocean circulation, and especially the surface circulation,
responds to anomalous convective activity. Horizontal boundaries turn out to
play an important role in the dynamical response of the ocean circulation. That
the dynamical reponse is essential to the oscillation is confirmed with two
simple (conceptual) models, and some idealized ocean experiments.

<id>
physics/9703026v1
<category>
physics.ao-ph
<abstract>
A comparison is carried out between two operational wave
forecasting/assimilation models for the North Sea, with the emphasis on the
assimilation schemes. One model is the WAM model, in combination with an
optimal interpolation method (OIP). The other model, DASWAM, consists of the
third generation wave model PHIDIAS in combination with an approximate
implementation of the adjoint method.
  In an experiment over the period February 19 - March 30, 1993, the models are
driven by the same wind field (HIRLAM analysis winds), and the same observation
data set is assimilated. This set consists of a) spectra from three
pitch-and-roll buoys and b) Synthetic Aperture Radar (SAR) spectra from the
ERS-1 satellite. Three analysis/forecast runs are performed: one without
assimilation, one with assimilation of buoy measurements only, and one with all
data assimilated. For validation, observations from four buoys, altimeter data
from ERS-1 and Topex-Poseidon, and scatterometer data from ERS-1 are used.
  A detailed analysis of the "Wadden Storm" (February 20-22) shows the very
different nature of the two assimilation schemes: the wave and wind field
corrections of the WAM/OIP scheme are all in the vicinity of the observations,
whereas the DASWAM adjustments are more of a global nature. The impact of some
individual buoy and SAR observations is visualized. A comparison of the
performance of the two schemes is somewhat obscured by the very different
behaviour of the two first-guess runs.
  A statistical analysis over the whole 39-day period gives the following
results. In a comparison with buoy observations it is shown that a positive
impact of wave data assimilation remains until about 12 hours in forecast in

<id>
physics/9706007v1
<category>
physics.ao-ph
<abstract>
Anomalies during an El Nino are dominated by a single, irregularly
oscillating, mode. Equatorial dynamics has been linked to delayed-oscillator
models of this mode. Usually, the El Nino mode is regarded as an unstable mode
of the coupled atmosphere system and the irregularity is attributed to noise
and possibly chaos. Here a variation on the delayed oscillator is explored. In
this stochastic-oscillator view, El Nino is a stable mode excited by noise. It
is shown that the autocorrelation function of the observed NINO3.4 index is
that of a stochastic oscillator, within the measurement uncertainty. Decadal
variations as would occur in a stochastic oscillator are shown to be comparable
to those observed, only the increase in the long-term mean around 1980 is
rather large. The observed dependence of the seasonal cycle on the variance and
the correlation is so large that it can not be attributed to the natural
variability of a stationary stochastic oscillator. So the El Ni\~{n}o
stochastic-oscillator parameters must depend on the season. A forecast model
based on the stochastic oscillator with a variance that depends on the season
has a skill that approaches that of more comprehensive statistical models: over
the period 1982-1993, the anomaly correlation is 0.65 for two-season lead
forecasts.

<id>
physics/9812040v1
<category>
physics.ao-ph
<abstract>
In a statistical analysis of more than a century of data we find a strong
connection between strong warm El Nino winter events and high spring
precipitation in a band from Southern England eastwards into Asia. This
relationship is an extension of the connection mentioned by Kiladis and Diaz
(1989), and much stronger than the winter season teleconnection that has been
the subject of other studies. Linear correlation coefficients between DJF NINO3
indices and MAM precipitation are higher than r=0.3 for individual stations,
and as high as r=0.49 for an index of precipitation anomalies around 50N from
5W to 35E. The lagged correlation suggests that south-east Asian surface
temperature anomalies may act as intermediate variables.

<id>
physics/9812046v1
<category>
physics.ao-ph
<abstract>
A warm cloud modification experiment was carried out in an area of 4800 Sq.Km
in the Pune region,India, during the 11-summer monsoon (June-September) seasons
(1973-74, 1976, 1979-86). A double-area cross-over design with area
randomization was adopted and an instrumented aircraft was used for seeding and
cloud physical measurements. Finely pulverised salt (sodium chloride) particles
were released into the monsoon clouds (cumulus and stratocumulus) during
aircraft penetrations into the clouds at a height of 200-300 m above the
cloud-base. The warm cloud responses to salt seeding are found to be critically
dependent on the cloud physical characteristics e.g., vertical thickness and
liquid water content. Clouds with vertical thickness greater than 1 km, LWC
greater than 0.5 gm/cubic m when seeded with salt particles (modal diameter 10
micro m, concentration 1 per litre of cloud air) produced increase in rainfall
of 24 per cent significant at 4 per cent level. Shallow clouds (vertical
thickness less than 1 km, LWC less than 0.5 gm/cubic m) when seeded showed
tendency for dissipation. The cloud physical observations made in not-seeded
(control) and seeded (target) clouds have provided some useful evidence to test
the applicability of the warm cloud modification hypothesis. The results of the
cloud model computations suggested that moderate convergence at the cloud-base
is essential for the cloud growth and development of precipitation in the real
world. Hygroscopic particle seeding of warm clouds under favourable dynamical
conditions (convergence at the cloud-base level) may result in the acceleration
of the collision-coalescence process resulting in the enhancement of rainfall.

<id>
physics/9907033v3
<category>
physics.ao-ph
<abstract>
The astronomical theory of Milankovitch relates the changes of Earth' past
climate to variations in insolation caused by oscillations of the orbital
parameters. However, this theory has problems to account for some major
observed phenomena of the past few million years. Here, we present an
alternative explanation for these phenomena. It is based on the idea that the
solar system until quite recently contained an additional massive object of
planetary size. This object, called Z, is assumed to have moved on a highly
eccentric orbit bound to the sun. It influenced Earth's climate through a gas
cloud of evaporated material. Calculations show that more than once during the
last 3.2 Myr it even approached the Earth close enough to provoke a significant
shift of the geographic position of the poles. The last of these shifts
terminated Earth's Ice Age epoch about 11.5 kyr ago. The origin and fate of Z
is also discussed.

<id>
physics/9909001v1
<category>
physics.ao-ph
<abstract>
We present a parabolic approximation that incorporates reflection. With this
approximation, there is no need to solve the parabolic equation for a coupled
pair of solutions consisting of the incident and reflected waves. Rather, this
approximation uses a synthetic wave whose spectral components manifest the
incident and reflected waves.

<id>
physics/9912030v1
<category>
physics.ao-ph
<abstract>
The radiation from the mixed layer into the interior of the ocean of
near-inertial oscillations excited by a passing storm in the presence of the
beta effect is reconsidered as an initial-value problem. Making use of the fact
that the mixed layer depth is much smaller than the total depth of the ocean,
the solution is obtained in the limit of an ocean that is effectively
infinitely deep. For a uniform initial condition, analytical results for the
velocity, horizontal kinetic energy density and fluxes are obtained. The
resulting decay of near-inertial mixed layer energy in the presence of the beta
effect occurs on a timescale similar to that observed.

<id>
physics/0001072v1
<category>
physics.ao-ph
<abstract>
A new simple method for the first order phase transition kinetics is
suggested. The metastable phase consumption can be imagined in frames of the
modisperse approximation for the distribution of the droplets sizes. In all
situations of the metastable phase decay this approximation leads to negligible
errors in the total number of droplets appeared in the system. An evident
advantage of the presented method is the possibility to investigate the
situation of the metastable phase decay on several sorts of heterogeneous
centers.

<id>
physics/0003009v1
<category>
physics.ao-ph
<abstract>
Small-scale oceanic motions, in combination with bottom topography, induce
mean large-scale along-isobaths flows. The direction of these mean flows is
usually found to be anticyclonic (cyclonic) over bumps (depressions). Here we
employ a quasigeostrophic model to show that the current direction of these
topographically induced large-scale flows can be reversed by the small-scale
variability. This result addresses the existence of a new bulk effect from the
small-scale activity that could have strong consequences on the circulation of
the world's ocean.

<id>
physics/0003010v1
<category>
physics.ao-ph
<abstract>
An optimal estimation inverse method is presented which can be used to
retrieve simultaneously vertical profiles of temperature and specific humidity,
in addition to surface pressure, from satellite-to-satellite radio occultation
observations of the Earth's atmosphere. The method is a non-linear, maximum
{\it a posteriori} technique which can accommodate most aspects of the real
radio occultation problem and is found to be stable and to converge rapidly in
most cases. The optimal estimation inverse method has two distinct advantages
over the analytic inverse method in that it accounts for some of the effects of
horizontal gradients and is able to retrieve optimally temperature and humidity
simultaneously from the observations. It is also able to account for
observation noise and other sources of error. Combined, these advantages ensure
a realistic retrieval of atmospheric quantities.
  A complete error analysis emerges naturally from the optimal estimation
theory, allowing a full characterisation of the solution. Using this analysis a
quality control scheme is implemented which allows anomalous retrieval
conditions to be recognised and removed, thus preventing gross retrieval
errors.
  The inverse method presented in this paper has been implemented for bending
angle measurements derived from GPS/MET radio occultation observations of the
Earth. Preliminary results from simulated data suggest that these observations
have the potential to improve NWP model analyses significantly throughout their
vertical range.

<id>
physics/0003028v1
<category>
physics.ao-ph
<abstract>
Analysis of MST radar observations at Gadanki (near Tirupati, India), for a
period of 14 months from 1995 September to 1996 November, shows that "Clear air
Turbulence" is not the primary source of high MST radar reflectivity; this
conventional idea needs to be modified. An alternative hypothesis based on the
microphysics and microdynamics associated with aerosols and water substance in
the atmosphere, is presented here.

<id>
physics/0006060v1
<category>
physics.ao-ph
<abstract>
We report on the influence of submerged bubble clouds on the remote sensing
properties of water. We show that the optical effect of bubbles on radiative
transfer and on the estimate of the ocean color is significant. We present a
global map of the volume fraction of air in water derived from daily wind speed
data. This map, together with the parameterization of the microphysical
properties, shows the possible significance of bubble clouds on the albedo of
incoming solar energy

<id>
physics/0006061v1
<category>
physics.ao-ph
<abstract>
A Monte Carlo model was used to study the scattering error of an absorption
meter with a divergent light beam and a limited acceptance angle of the
receiver. Reflections at both ends of the tube were taken into account.
Calculations of the effect of varying optical properties of water, as well as
the receiver geometry, were performed. A weighting function showing the
scattering error quantitatively as a function of angle was introduced. Some
cases of the practical interests are discussed.

<id>
physics/0006063v1
<category>
physics.ao-ph
<abstract>
We report that the submerged microbubbles are an efficient source of diffuse
radiance and may contribute to a rapid transition to the diffuse asymptotic
regime. In this asymptotic regime an average cosine is easily predictable and
measurable.

<id>
physics/0006064v1
<category>
physics.ao-ph
<abstract>
The discrete-dipole approximation (DDA) is a flexible technique for computing
scattering and absorption by targets of arbitrary geometry. In this paper we
perform systematic study of various non-stationary iterative (conjugate
gradient) methods in search for the most efficient one in order to solve the
system of equations arising in DDA. We document implementation of these methods
in our public domain code DDSCAT.5a

<id>
physics/0011027v1
<category>
physics.ao-ph
<abstract>
Several experimental results show that it is possible to extract useful phase
information from reflected GPS signals over the oceans. In this work we begin
the development of the theoretical background to account for these results and
fully understand the phenomena involved. This information will then be used to
define and carry out new experiments to evaluate the feasibility of using the
phase from reflected GPS signals for altimetric purposes and the advantages of
using interferometric combinations of the signals at different
frequencies---the PIP concept.
  We focus on the coherence properties of the signals, including the PIP
interferometric combination of phases in the different frequencies. In this
work we will concentrate on a static, 8 m high receiver (at least in regards to
the simulations), and an infinitely removed static source. As the ocean moves,
the received field will pick up a random phase. We want to understand the
behavior of this phase, as the goal is to carry out altimetric measurements
using phase ranging. We will also show that this random phase carries
geophysical information (intuitively, the bigger the significant wave height,
the larger the phase excursions).

<id>
physics/0011074v2
<category>
physics.ao-ph
<abstract>
A Lagrangian column model has been developed to simulate the mean (monthly
and annual) three-dimensional structure in ozone and nitrogen oxides
concentrations in the boundary layer within and immediately around an urban
area. Short time-scale photochemical processes of ozone, as well as emissions
and deposition to the ground are simulated. The results show that the average
surface ozone concentration in the urban area is lower than the surrounding
rural areas by typically 50%. Model results are compared with observations.

<id>
physics/0105005v1
<category>
physics.ao-ph
<abstract>
The day-night variation of cosmic rays intensity at sea level has been
observed by a simple G-M counter telescope. We preform two 5 hours counting
during the day and night and find that the pattern of variation is closely
related to the atmospheric disturbance, especially when the observation station
is being affected by meterological front or trough. Such effects may lasts for
a few days until the trough or fronts weakened. The pattern of variation may be
negatively correlated to the altitude of the 0 degC level of the atmosphere.
This is closely related to the muon decay effect in the atmosphere. The
phenomenon should be further investigation for possible application in weather
forecasting.

<id>
physics/0108035v1
<category>
physics.ao-ph
<abstract>
Time series both of microwave radiometer brightness temperature measurements
at 23.8 and 31.4 GHz and of retrievals of water vapor and liquid water path
from these brightness temperatures are evaluated using the detrended
fluctuation analysis method. As quantified by the parameter $\alpha$, this
method (i) enables identification of the time scales over which noise dominates
the time series and (ii) characterizes the temporal range of correlations in
the time series. The more common spectral analysis method is also used to
assess the data and its results are compared with those from detrended
fluctuation analysis method. The assumption that measurements should have
certain scaling properties allows the quality of the measurements to be
characterized. The additional assumption that the scaling properties of the
measurements of an atmospheric quantity are preserved in a useful retrieval
provides a means for evaluating the retrieval itself. Applying these two
assumptions to microwave radiometer measurements and retrievals demonstrates
three points. First, the retrieved water vapor path during cloudy-sky periods
can be dominated by noise on shorter than ~30~min time scales
($\alpha$-exponent = 0.1) and exhibits no scaling behavior at longer time
scales. However, correlations in the brightness temperatures and liquid water
path retrievals are found to be consistent with a power-law behavior for time
scales up to 3 hr with an $\alpha$-exponent equal to approximately 0.3, as in
other geophysical phenomena. Second, clear-sky, moist atmospheres show the
expected scaling for both measurements and retrievals of the water vapor path.
Third, during clear-sky, dry atmospheric days, instrument noise from the 31.4
GHz channel compromises the quality of the water vapor path retrieval.

<id>
physics/0110083v1
<category>
physics.ao-ph
<abstract>
A possible relationship between sunspot number and total annual precipitation
from the Izana Observatory has been found. The annual precipitation period
ranges from 1916 to 1998, thus including nearly eight 11-year solar cycles.
  When points of total precipitation for a given year at Izana are plotted on
the ordinate axis versus the yearly sunspot number on the abcisa axis three
years back from the precipitation one, nearly all of them lie in the lower left
hand corner of the diagram. This seems to indicate a relationship between the
above mentioned variables.
  If this relationship is confirmed it would permit the prediction of a maximum
annual precipitation at Izana three years in advance.

<id>
physics/0202007v1
<category>
physics.ao-ph
<abstract>
We show that the hourly rainfall rate distribution can be described by a
simple power law to a good approximation. We show that the exponent of the
distribution in tropics is universal and is equal to $1.13\pm 0.11$. At higher
latitudes the exponent increases and is found to lie in the range 1.3-1.6.

<id>
physics/0208032v1
<category>
physics.ao-ph
<abstract>
Floquet theory is used to describe the unstable spectrum at large scales of
the beta-plane equation linearized about Rossby waves. Base flows consisting of
one to three Rossby wave are considered analytically using continued fractions
and the method of multiple scales, while base flow with more than three Rossby
waves are studied numerically. It is demonstrated that the mechanism for
instability changes from inflectional to triad resonance at an O(1) transition
Rhines number Rh, independent of the Reynolds number. For a single Rossby wave
base flow, the critical Reynolds number Re^c for instability is found in
various limits. In the limits Rh --> infinity and k --> 0, the classical value
Re^c = sqrt(2) is recovered. For Rh --> 0 and all orientations of the Rossby
wave except zonal and meridional, the base flow is unstable for all Reynolds
numbers; a zonal Rossby wave is stable, while a meridional Rossby wave has
critical Reynolds number Re^c = sqrt(2). For more isotropic base flows
consisting of many Rossby waves (up to forty), the most unstable mode is purely
zonal for 2 <= Rh < infinity and is nearly zonal for Rh = 1/2, where the
transition Rhines number is again O(1), independent of the Reynolds number and
consistent with a change in the mechanism for instability from inflectional to
triad resonance.

<id>
physics/0210095v1
<category>
physics.ao-ph
<abstract>
We analyze global surface temperature data obtained at 13472 weather stations
from the year 1702 to 1990. The mean annual temperature of a station fluctuates
from year to year by typically +-0.6oC (one standard deviation). Superimposed
on this fluctuation is a linear increase of the temperature by typically 0.40oC
per century ever since reliable data is available, i.e. since 1702. The world
population has doubled from 1952 to 1990, yet we see no statistically
significant acceleration of global warming in this period. We conclude that the
effect of humankind on global warming up to 1990 is 0.0 +- 0.1oC.

<id>
physics/0211095v1
<category>
physics.ao-ph
<abstract>
The validation of NASA Global Precipitation Mission (GPM) satellite
precipitation products is important for their credibility and utility within
the larger community. This document defines GPM ground validation scientific
objectives and several programmatic components for meeting those objectives.
Multi-year, multi-sensor ground-based observation programs in a few locations
are proposed to generate local observation products and global error covariance
products. Focused measurement programs utilizing aircraft, ships, and
ground-based measurements would fill in geographic and scientific gaps not
addressed by the multi-year observing programs.

<id>
physics/0304114v2
<category>
physics.ao-ph
<abstract>
The stability of classical semi-implicit scheme, and some more advanced
iterative schemes recently proposed for Numerical Weather Prediction (NWP)
purpose is examined. In all these schemes, the solution of the centred-implicit
non-linear equation is approached by an iterative fixed-point algorithm,
preconditioned by a simple, constant in time, linear operator. A general
methodology for assessing analytically the stability of these schemes on
canonical problems for a vertically unbounded atmosphere is presented. The
proposed method is valid for all the equation systems usually employed in NWP.
However, as in earlier studies, the method can be applied only in simplified
meteorological contexts, thus overestimating the actual stability that would
occur in more realistic meteorological contexts. The analysis is performed in
the spatially-continuous framework, hence allowing to eliminate the
spatial-discretisation or the boundary conditions as possible causes of the
fundamental instabilities linked to the time-scheme itself. The general method
is then shown concretely to apply to various time-discretisation schemes and
equation-systems (namely shallow-water, and fully compressible Euler
equations). Analytical results found in the literature are recovered from the
proposed method, and some original results are presented.

<id>
physics/0306043v2
<category>
physics.ao-ph
<abstract>
We present, for the first time, spectral behaviour of aerosol optical depths
(AODs) over Manora Peak, Nainital located at an altitude of $\sim$ 2 km in the
Shivalik ranges of central Himalayas. The observations were carried out using a
Multi-Wavelength solar Radiometer during January to December 2002. The main
results of the study are extremely low AODs during winter, a remarkable
increase to high values in summer and a distinct change in the spectral
dependencies of AODs from a relatively steeper spectra during winter to a
shallower one in summer. During transparent days, the AOD values lie usually
below 0.08 while during dusty (turbid) days, it lies between 0.08 to 0.69 at
0.5 $\mu$m. The average AOD value at 0.5 $\mu$m during winters, particularly in
January and February, is $\sim 0.03\pm0.01$. The mean aerosol extinction law at
Manora Peak during 2002 is best represented by $0.10 \lambda^{-0.61}$. However
during transparent days, which almost covers 40% of the time, it is represented
by $0.02 \lambda^{-0.97}$. This value of wavelength exponent, representing
reduced coarse concentration and presence of fine aerosols, indicates that the
station measures aerosol in the free troposphere at least during part of the
year.

<id>
physics/0306069v1
<category>
physics.ao-ph
<abstract>
Using only lidar or radar an accurate cloud boundary height estimate is often
not possible. The combination of lidar and radar can give a reliable cloud
boundary estimate in a much broader range of cases. However, also this
combination with standard methods still can not measure the cloud boundaries in
all cases. This will be illustrated with data from the Clouds and Radiation
measurement campaigns, CLARA. Rain is a problem: the radar has problems to
measure the small cloud droplets in the presence of raindrops. Similarly, few
large particles below cloud base can obscure the cloud base in radar
measurements. And the radar reflectivity can be very low at the cloud base of
water clouds or in large regions of ice clouds, due to small particles.
Multiple cloud layers and clouds with specular reflections can pose problems
for lidar. More advanced measurement techniques are suggested to solve these
problems. An angle scanning lidar can, for example, detect specular
reflections, while using information from the radars Doppler velocity spectrum
may help to detect clouds during rain.

<id>
physics/0307141v1
<category>
physics.ao-ph
<abstract>
Salmon's nearly geostrophic model for rotating shallow-water flow is derived
in full spherical geometry. The model, which results upon constraining the
velocity field to the height field in Hamilton's principle for rotating
shallow-water dynamics, constitutes an important prototype of Hamiltonian
balanced models. Instead of Salmon's original approach, which consists in
taking variations of particle paths at fixed Lagrangian labels and time, Holm's
approach is considered here, namely variations are taken on Lagrangian particle
labels at fixed Eulerian positions and time. Unlike the classical
quasigeostrophic model, Salmon's is found to be sensitive to the differences
between geographic and geodesic coordinates. One consequence of this result is
that the $\beta $ plane approximation, which is included in Salmon's original
derivation, is not consistent for this class of model.

<id>
physics/9611003v1
<category>
physics.atom-ph
<abstract>
The retarded long-range potentials for hydrogen and alkali-metal atoms in
their ground states and a perfectly conducting wall are calculated. The
potentials are given over a wide range of atom-wall distances and the validity
of the approximations used is established.

<id>
physics/9611004v1
<category>
physics.atom-ph
<abstract>
It is shown that the potential for lepton-antilepton bound states (leptonium)
is the Fourier transform of the first Born approximation to the QED scattering
amplitude in an 8-component equation, while 16-component equations are
excluded. The Fourier transform is exact at all cms energies $-\infty < E <
\infty$; the resulting atomic spectrum is explicitly CPT-invariant.

<id>
physics/9611005v1
<category>
physics.atom-ph
<abstract>
We discuss the formation of antihydrogen atoms ($\bar{\rm H}$) in an
ultra-cold positron-antiproton plasma. For positron densities $n_p\agt 10^8$
cm$^{-3}$ the characteristic formation time of stable $\bar{H}$ is determined
by collisional relaxation of highly excited atoms produced in the process of
3-body Thompson recombination. Relying on the mechanisms of ``replacement
collisions'' and ``transverse collisional drift'' we find a bottleneck in the
relaxation kinetics and analyze the physical consequences of this phenomenon.

<id>
physics/9611007v1
<category>
physics.atom-ph
<abstract>
We consider the coherent population trapping phenomenon in a thermal sodium
atomic beam. We compare the different coherent population trapping schemes that
can be established on the D1 line using the Zeeman sublevels of a given ground
hyperfine state. The coherent population trapping preparation is examined by
means of a Hanle effect configuration. The efficiency of the coherent
population trapping phenomenon has been examined in presence of optical pumping
into hyperfine levels external to those of the excited transition. We show that
both the contrast and the width of the coherent population trapping resonance
strongly decrease when the optical pumping rate is increased. In the
experiment, the loss rate due to optical pumping has been controlled by means
of a laser repump of variable intensity.

<id>
physics/9611015v1
<category>
physics.atom-ph
<abstract>
We experimentally study the motion of atoms interacting with a periodically
pulsed near resonant standing wave. For discrete pulse frequencies we observe a
comb-like momentum distribution. The peaks have widths of 0.3 recoil momenta
and a spacing which is an integer multiple of the recoil momentum. The atomic
population is trapped in ground states which periodically evolve to dark states
each time the standing wave is switched on.

<id>
physics/9611019v1
<category>
physics.atom-ph
<abstract>
We create a dark optical lattice structure using a blue detuned laser field
coupling an atomic ground state of total angular momentum F simultaneously to
two excited states with angular momenta F and F-1, or F and F+1. The atoms are
trapped at locations of purely circular polarization. The cooling process
efficiently accumulates almost half of the atomic population in the lowest
energy band which is only weakly coupled to the light field. The populations of
the two lowest energy bands reaches 70%. Kinetic energies on the order of the
recoil energy are obtained by adiabatically reducing the optical potential. The
band populations are directly mapped on free particle momentum intervals by
this adiabatic release. In an experiment with subrecoil momentum resolution we
measure the band populations and find good absolute agreement with the
theoretically calculated steady state band populations.

<id>
physics/9611020v1
<category>
physics.atom-ph
<abstract>
We create a gray optical lattice structure using a blue detuned laser field
coupling an atomic ground state of angular momentum J simultaneously to two
excited states with angular momenta J and J-1. The atoms are cooled and trapped
at locations of purely circular polarization. The cooling process efficiently
accumulates almost half of the atomic population in the lowest energy band
which is only weakly coupled to the light field. Very low kinetic temperatures
are obtained by adiabatically reducing the optical potential. The dynamics of
this process is analysed using a full quantum Monte Carlo simulation. The
calculations explicitly show the mapping of the band populations on the
corresponding momentum intervals of the free atom. In an experiment with
subrecoil momentum resolution we measure the band populations and find
excellent absolut agreement with the theoretical calculations.

<id>
physics/9611021v1
<category>
physics.atom-ph
<abstract>
In the recent paper by Zhang the order $\alpha^4 R_{\infty}$ corrections to
the positronium P levels were reconsidered. Those calculations confirm our
corresponding results, except for the contribution due to the squared
spin-orbit interaction. We present here a new derivation of our previous result
for this last correction, this derivation being to our opinion both simple and
convincing.

<id>
physics/9611023v1
<category>
physics.atom-ph
<abstract>
We show that zero ejection energy of the photoelectrons is classically
impossible for hydrogen-like ions, even when field ionization occurs
adiabatically. To prove this we transform the basic equations to those
describing two 2D anharmonic oscillators. The same method yields an alternative
way to derive the anomalous critical field of hydrogen-like ions. The
analytical results are confirmed and illustrated by numerical simulations. PACS
Number: 32.80.Rm

<id>
physics/9611026v1
<category>
physics.atom-ph
<abstract>
We show that the degeneracy parameter of a trapped Bose gas can be changed
adiabatically in a reversible way, both in the Boltzmann regime and in the
degenerate Bose regime. We have performed measurements on spin-polarized atomic
hydrogen in the Boltzmann regime demonstrating reversible changes of the
degeneracy parameter (phase-space density) by more than a factor of two. This
result is in perfect agreement with theory. By extending our theoretical
analysis to the quantum degenerate regime we predict that, starting close
enough to the Bose-Einstein phase transition, one can cross the transition by
an adiabatic change of the trap shape.

<id>
physics/9611028v1
<category>
physics.atom-ph
<abstract>
In a recent experiment by Eichmann et al., polarization-sensitive
measurements of the fluorescence from two four-level ions driven by a linearly
polarized laser were made. Depending on the polarization chosen, different
degrees of interference were observed. We carry out a theoretical and numerical
study of this system, showing that the results can largely be understood by
treating the atoms as independent radiators which are synchronized by the phase
of the incident laser field. The interference and its loss may be described in
terms of the difference between coherent and incoherent driving of the various
atomic transitions in the steady-state. In the numerical simulations, which are
carried out using the Monte Carlo wave function method, we remove the
assumption that the atoms radiate independently and consider the photodetection
process in detail. This allows us to see the total interference pattern build
up from individual photodetections and also to see the effects of
superfluorescence, which become important when the atomic separation is
comparable to an optical wavelength. The results of the calculations are
compared with the experiment. We also carry out simulations in the non
steady-state regime and discuss the relationship between the visibility of the
interference pattern and which-path considerations.

<id>
physics/9612011v1
<category>
physics.atom-ph
<abstract>
The long-range interactions of two atoms, of an atom and a dielectric wall,
of an atom and a perfectly conducting wall, and of an atom between two
perfectly conducting walls are calculated, including the effects of
retardation, for Li using dynamic polarizabilities determined from highly
correlated, variationally determined wave functions.

<id>
physics/9701002v1
<category>
physics.atom-ph
<abstract>
The steady state in three-level lambda and ladder systems is studied. It is
well-known that in a lambda system this steady state is the coherent population
trapping state, independent of the presence of spontaneous emission. In
contrast, the steady state in a ladder system is in general not stable against
radiative decay and exhibits a minimum in the population of the ground state.
It is shown that incoherent population pumping destroys the stability of the
coherent population trapping state in the lambda system and suppresses a
previously discovered sharp dip in the steady state response. In the ladder
system the observed minimum disappears in the presence of an incoherent pump on
the upper transition.

<id>
physics/9701007v1
<category>
physics.atom-ph
<abstract>
Theoretical investigations of the hyperfine structure of the hydrogen
molecular ion (one electron and two protons) are discussed. The nuclear
spin-rotation interaction has been found to be of the same sign as in the
hydrogen molecule and the hyperfine transition frequencies can be accurately
predicted. With measurements of the hyperfine structure of the deuterium
molecular ion (or of HD+) it should be possible to obtain a value of the
deuteron quadrupole moment that could be compared with the values obtained from
the deuterium molecule and from nuclear theory.

<id>
physics/9702001v2
<category>
physics.atom-ph
<abstract>
Based on a three-potential formalism we propose mathematically well-behaved
Faddeev-type integral equations for the atomic three-body problem and descibe
their solutions in Coulomb-Sturmian space representation. Although the system
contains only long-range Coulomb interactions these equations allow us to reach
solution by approximating only some auxiliary short-range type potentials. We
outline the method for bound states and demonstrate its power in benchmark
calculations. We can report a fast convergence in angular momentum channels.

<id>
physics/9702002v1
<category>
physics.atom-ph
<abstract>
A simple analytical solution for the problem of multiphoton detachment from
negative ions by a linearly polarized laser field is found. It is valid in the
wide range of intensities and frequencies of the field, from the perturbation
theory to the tunneling regime, and is applicable to the excess-photon as well
as near-threshold detachment. Practically, the formulae are valid when the
number of photons is greater than two. They produce the total detachment rates,
relative intensities of the excess-photon peaks, and photoelectron angular
distributions for the hydrogen and halogen negative ions, in agreement with
those obtained in other, more numerically involved calculations in both
perturbative and non-perturbative regimes. Our approach explains the extreme
sensitivity of the multiphoton detachment probability to the asymptotic
behaviour of the bound-state wave function. Rapid oscillations in the angular
dependence of the $n$-photon detachment probability are shown to arise due to
interference of the two classical trajectories which lead to the same final
state after the electron emerges at the opposite sides of the atom when the
field is close to maximal.

<id>
physics/9702003v1
<category>
physics.atom-ph
<abstract>
In this paper we compare the results of our adiabatic theory (Gribakin and
Kuchiev, Phys. Rev. A, accepted for publication) with other theoretical and
experimental results, mostly for halogen negative ions. The theory is based on
the Keldysh approach. It shows that the multiphoton detachment rates and the
corresponding n-photon detachment cross sections depend only on the asymptotic
behaviour of the bound state radial wave function. The dependence on the
exponent is very strong. This is the main reason for the disagreement with some
previous calculations, which employed bound state wave functions with incorrect
asymptotic forms. In a number of cases our theoretical results produces best
agreement with absolute and relative experimental data.

<id>
physics/9702007v1
<category>
physics.atom-ph
<abstract>
The Gaussian Wave-Packet phase-space representation is used to show that the
expansion in powers of $\hbar$ of the quantum Liouville propagator leads, in
the zeroth order term, to results close to those obtained in the statistical
quasiclassical method of Lee and Scully in the Weyl-Wigner picture. It is also
verified that propagating the Wigner distribution along the classical
trajectories the amount of error is less than that coming from propagating the
Gaussian distribution along classical trajectories.

<id>
physics/9702027v1
<category>
physics.atom-ph
<abstract>
Radiative corrections to the 2E1 decay width of the 2s-state in the low-Z
hydrogen-like system are examined within logarithmic approximation. The
correction is found to be 2.025(1) alpha (Z alpha)^2 / pi log(Z alpha)^2 in
units of the non-relativistic rate.

<id>
physics/9702028v1
<category>
physics.atom-ph
<abstract>
Radiative corrections to the decay rate in the low-Z hydrogen-like muonic
atoms are considered. The correction arises from the Uehling potential and it
has the relative order of alpha. The numerical results are reported for the
2p-1s transition for the atom with the nuclear charge value up to Z=10.

<id>
physics/9703011v2
<category>
physics.atom-ph
<abstract>
A Feshbach resonance associated with the 1s3s4s ^{4}S state of He^{-} has
been observed in the He(1s2s ^{3}S) + e^- (\epsilon s) partial photodetachment
cross section. The residual He(1s2s ^{3}S) atoms were resonantly ionized and
the resulting He^+ ions were detected in the presence of a small background. A
collinear laser-ion beam apparatus was used to attain both high resolution and
sensitivity. We measured a resonance energy E_r = 2.959 255(7) eV and a width
\Gamma = 0.19(3) meV, in agreement with a recent calculation.

<id>
physics/9703012v1
<category>
physics.atom-ph
<abstract>
The electron affinity of tellurium has been determined to 1.970 876(7) eV.
The threshold for photodetachment of Te^-(^{2} P_{3/2}) forming neutral Te in
the ground state was investigated by measuring the total photodetachment cross
section using a collinear laser-ion beam geometry. The electron affinity was
obtained from a fit to the Wigner law in the threshold region.

<id>
physics/9703013v1
<category>
physics.atom-ph
<abstract>
We have investigated the threshold of photodetachment of Li^- leading to the
formation of the residual Li atom in the $2p ^2P$ state. The excited residual
atom was selectively photoionized via an intermediate Rydberg state and the
resulting Li^+ ion was detected. A collinear laser-ion beam geometry enabled
both high resolution and sensitivity to be attained. We have demonstrated the
potential of this state selective photodetachment spectroscopic method by
improving the accuracy of Li electron affinity measurements an order of
magnitude. From a fit to the Wigner law in the threshold region, we obtained a
Li electron affinity of 0.618 049(20) eV.

<id>
physics/9703015v1
<category>
physics.atom-ph
<abstract>
We report on the first observation of resonance structure in the total cross
section for the photodetachment of Li^-. The structure arises from the
autodetaching decay of doubly excited ^1P states of Li^- that are bound with
respect to the 3p state of the Li atom. Calculations have been performed for
both Li^- and H^- to assist in the identification of these resonances. The
lowest lying resonance is a symmetrically excited intrashell resonance. Higher
lying asymmetrically excited intershell states are observed which converge on
the Li(3p) limit.

<id>
physics/9703027v1
<category>
physics.atom-ph
<abstract>
We calculate the spectrum of a beam of atoms output from a single mode atomic
cavity. The output coupling uses an internal state change to an untrapped
state. We present an analytical solution for the output energy spectrum from a
broadband coupler of this type. An example of such an output coupler, which we
discuss in detail uses a Raman transition to produce a non-trapped state.

<id>
physics/9704003v3
<category>
physics.atom-ph
<abstract>
Recoil corrections to the atomic decay rate are considered in the order of
Zm/M . The expressions are treated exactly without any expansion over Z alpha.
The expressions obtained are valid both for muonic atoms (for which they
contribute on the level of a few percent in high Z ions) and for electronic
atoms. Explicit results for Lyman-alpha transitions for low-Z of the order
(Zm/M)(Z alpha)^2 are also presented.

<id>
physics/9704016v1
<category>
physics.atom-ph
<abstract>
A theory of the Post-Collision Interaction (PCI) is developed for the case
when an electron atom impact results in creation of two low-energy Wannier
electrons and an ion excited into an autoionizing state. The following
autoionization decay exposes the Wannier pair to the influence of PCI resulting
in variation of the shape of the line in the autoionization spectrum. An
explicit dependence of the autoionization profile on the wave function of the
Wannier pair is found. PCI provides an opportunity to study this wave function
for a wide area of distances

<id>
physics/9705005v1
<category>
physics.atom-ph
<abstract>
We develop the idea of selectively manipulating the condensate in a trapped
Bose-condensed gas, without perturbing the thermal cloud. The idea is based on
the possibility to modify the mean field interaction between atoms (scattering
length) by nearly resonant incident light or by spatially uniform change of the
trapping magnetic field. For the gas in the Thomas-Fermi regime we find
analytical scaling solutions for the condensate wavefunction evolving under
arbitrary variations of the scattering length $a$. The change of $a$ from
positive to negative induces a global collapse of the condensate, and the final
stages of the collapse will be governed by intrinsic decay processes.

<id>
physics/9705006v1
<category>
physics.atom-ph
<abstract>
Bogolyubov-De Gennes equations for the excitations of a Bose condensate in
the Thomas-Fermi regime in harmonic traps of any asymmetry and introduce a
classification of eigenstates. In the case of cylindrical symmetry we emphasize
the presence of an accidental degeneracy in the excitation spectrum at certain
values of the projection of orbital angular momentum on the symmetry axis and
discuss possible consequences of the degeneracy in the context of new
signatures of Bose-Einstein condensation.

<id>
physics/9705010v3
<category>
physics.atom-ph
<abstract>
In this paper we present some concepts in heavy ion atomic physics for the
extraction of parity violating effects. We investigate the effects of the
so-called Stark-quenching, i.e., the fast decay of a meta stable state induced
by a Stark field, and the superposition of one- and two-photon transitions in
beryllium-like heavy ions. It turns out that the discussed theoretical
phenomena for heavy ions with few electrons are beyond the scope of present day
experimental possibilities because one has to require beam energies of up to 1
TeV/A, laser intensities of up to $10^{17} {\rm W/cm^2}$ and ion currents of up
to $10^{11}$ ions per second in beryllium-like uranium. However, especially the
superposition of one- and two-photon transitions is a very interesting
phenomenon that could provide the germ of an idea to be applied in a more
favorable system.

<id>
physics/9706040v1
<category>
physics.atm-clus
<abstract>
We discuss the calculation of collective excitations in atomic clusters using
the time-dependent local density approximation. In principle there are many
formulations of the TDLDA, but we have found that a particularly efficient
method for large clusters is to use a coordinate space mesh and the algorithms
for the operators and the evolution equations that had been developed for the
nuclear time-dependent Hartree-Fock theory. The TDLDA works remarkably well to
describe the strong excitations in alkali metal clusters and in carbon
clusters. We show as an example the benzene molecule, which has two strong
features in its spectrum. The systematics of the linear carbon chains is well
reproduced, and may be understood in rather simple terms.

<id>
physics/9801002v1
<category>
physics.atm-clus
<abstract>
The life-times due to Auger-electron emission for a hole on a deep electronic
shell of neutral and charged sodium clusters are studied for different sizes.
We consider spherical clusters and calculate the Auger-transition probabilities
using the energy levels and wave functions calculated in the
Local-Density-Approximation (LDA).
  We obtain that Auger emission processes are energetically not allowed for
neutral and positively charged sodium clusters. In general, the Auger
probabilities in small Na$_N^-$ clusters are remarkably different from the
atomic ones and exhibit a rich size dependence.
  The Auger decay times of most of the cluster sizes studied are orders of
magnitude larger than in atoms and might be comparable with typical
fragmentation times.

<id>
physics/9802017v1
<category>
physics.atm-clus
<abstract>
The time-dependent local-density approximation (TDLDA) is shown to remain
accurate in describing the atomic response of IB elements under the additional
approximation of using pseudopotentials to treat the effects of core electrons.
This extends the work of Zangwill and Soven who showed the utility of the
all-electron TDLDA in the atomic response problem.

<id>
physics/9901008v1
<category>
physics.atm-clus
<abstract>
We calculate the two-photon ionization of clusters for photon energies near
the surface plasmon resonance. The results are expressed in terms of the
ionization rate of a double plasmon excitation, which is calculated
perturbatively. For the conditions of the experiment by Schlipper et al., we
find an ionization rate of the order of 0.05-0.10 fs^(-1). This rate is used to
determine the ionization probability in an external field in terms of the
number of photons absorbed and the duration of the field. The probability also
depends on the damping rate of the surface plasmon. Agreement with experiment
can only be achieved if the plasmon damping is considerably smaller than its
observed width in the room-temperature single-photon absorption spectrum.

<id>
physics/9902025v1
<category>
physics.atm-clus
<abstract>
We have theoretically studied the effect of dissociative autoionization on
the photoelectron energy spectrum in (1+2)-photon above threshold
ionization(ATI) of H2 molecules. We have considered excitation from the ground
state X-singlet-Sigma-g+(v=0,j) to the doubly excited autoionizing states of
singlet-Sigma-u+ and singlet-Pi-u+ symmetry, via the intermediate resonant
B-singlet-Sigma-u+(v=5,j) states. We have shown that the photoelectron energy
spectrum is oscillatory in nature and shows three distinct peaks above the
photoelectron energy 0.7 eV. This feature has been observed in a recent
experiment by Rottke et al, J. Phys. B, Vol. 30, p-4049 (1997).

<id>
physics/9902047v1
<category>
physics.atm-clus
<abstract>
We investigate noncollinear effects in antiferromagnetically coupled clusters
using the general, rotationally invariant form of local spin-density theory.
The coupling to the electronic degrees of freedom is treated with relativistic
non-local pseudopotentials and the ionic structure is optimized by Monte-Carlo
techniques. We find that small chromium clusters (N \le 13) strongly favor
noncollinear configurations of their local magnetic moments due to frustration.
This effect is associated with a significantly lower total magnetization of the
noncollinear ground states, ameliorating the disagreement between Stern-Gerlach
measurements and previous collinear calculations for Cr_{12} and Cr_{13}. Our
results further suggest that the trend to noncollinear configurations might be
a feature common to most antiferromagnetic clusters.

<id>
physics/9903041v1
<category>
physics.atm-clus
<abstract>
The time-dependent local density approximation is applied to the optical
response of the silver clusters, Ag_2, Ag_3, Ag_8 and Ag_9^+. The calculation
includes all the electrons beyond the closed-shell Ag^{+11} ionic core, thus
including for the first time explicitly the filled d-shell in the response. The
excitation energy of the strong surface plasmon near 4 eV agrees well with
experiment. The theoretical transition strength is quenched by a factor of 4
with respect to the pure s-electron sum rule in Ag_8 due to the d-electrons. A
comparable amount of strength lies in complex states below 6 eV excitation. The
total below 6 eV, about 50% of the s sum rule, is consistent with published
experiments.

<id>
physics/9903049v2
<category>
physics.atm-clus
<abstract>
The orbital M1 collective mode predicted for deformed clusters in a schematic
model is studied in a self-consistent random-phase-approximation approach which
fully exploits the shell structure of the clusters. The microscopic mechanism
of the excitation is clarified and the close correlation with E2 mode
established. The study shows that the M1 strength of the mode is fragmented
over a large energy interval. In spite of that, the fraction remaining at low
energy, well below the overwhelming dipole plasmon resonance, is comparable to
the strength predicted in the schematic model. The importance of this result in
view of future experiments is stressed.

<id>
physics/9907022v1
<category>
physics.atm-clus
<abstract>
We report ground state energies and structural properties for small helium
clusters (4He) containing an H- impurity computed by means of variational and
diffusion Monte Carlo methods. Except for 4He_2H- that has a noticeable
contribution from collinear geometries where the H- impurity lies between the
two 4He atoms, our results show that our 4He_NH- clusters have a compact 4He_N
subsystem that binds the H- impurity on its surface. The results for $N\geq 3$
can be interpreted invoking the different features of the minima of the He-He
and He-H- interaction potentials.

<id>
physics/9908034v1
<category>
physics.atm-clus
<abstract>
Tight-binding molecular dynamics (TBMD) is used to study the structural and
electronic properties of silver clusters. The ground state structures of Ag
clusters up to 21 atoms are optimized via TBMD combined with genetic algorithm
(GA). The detailed comparison with {\em ab initio} results on small Ag$_n$
clusters (n=3-9) proves the validity of the tight-bind model. The clusters are
found to undergo a transition from ``electronic order'' to ``atomic order'' at
n=10. This is due to s-d mixing at such size. The size dependence of electronic
properties such as density of states (DOS), s-d band separation, HOMO-LUMO gap,
and ionization potentials are discussed. Magic number behavior at Ag$_2$,
Ag$_8$, Ag$_{14}$, Ag$_{18}$, Ag$_{20}$ is obtained, in agreement with the
prediction of electronic ellipsoid shell model. It is suggested that both the
electronic and geometrical shell exist in the coinage metal clusters and they
play a significant role in determining cluster properties.

<id>
physics/9909058v1
<category>
physics.atm-clus
<abstract>
Photoelectron (PES) spectra from aluminum cluster anions (from 12 to 15
atoms) at various temperature regimes, were studied using ab-initio molecular
dynamics simulations and experimentally. The calculated PES spectra, obtained
via shifting of the simulated electronic densities of states by the
self-consistently determined values of the asymptotic exchange-correlation
potential, agree well with the measured ones, allowing reliable structural
assignments and theoretical estimation of the clusters' temperatures.

<id>
physics/0004041v4
<category>
physics.atm-clus
<abstract>
The structures and relative stabilities of doubly-charged nonstoichiometric
(CaO)$_n$Ca$^{2+}$ (n=1--29) cluster ions and of neutral stoichiometric
(MgO)$_n$ and (CaO)$_n$ (n=3,6,9,12,15,18) clusters are studied through {\em ab
initio} Perturbed Ion plus polarization calculations. The large
coordination-dependent polarizabilities of oxide anions favor the formation of
surface sites, making the critical cluster size where anions with bulk
coordination first appear larger than that found in the related case of alkali
halides. Thus, we show that there are substantial structural differences
between alkali halide and alkaline-earth oxide cluster ions, contrary to what
is suggested by the similarities in the experimental mass spectra. An
alternative interpretation of the magic numbers for the case of oxides is
proposed, which involves an explicit consideration of isomer structures
different from the ground states. A comparison with the previously studied
(MgO)$_n$Mg$^{2+}$ cluster ions shows that the emergence of bulklike structural
properties with size is slower for calcium oxide. Nevertheless, the structures
of the doubly charged clusters are rather similar for the two materials. On the
contrary, the study of the neutrals reveals interesting structural differences
between MgO and CaO, similar to those found in the case of alkali halides.

<id>
physics/0004066v1
<category>
physics.atm-clus
<abstract>
We have developed a transferable nonorthogonal tight-binding total energy
model for germanium and use it to study small clusters. The cohesive energy,
bulk modulus, elastic constants of bulk germanium can be described by this
model to considerably good extent. The calculated bulk phase diagram for
germanium agrees well with LDA results. The geometries and binding energies
found for small Ge$_n$ clusters with $n=3-10$ are very close to previous {\em
ab initio} calculations and experiments. All these results suggest that this
model can be further applied to the simulation of germanium cluster of larger
size or with longer time scale, for which {\em ab initio} methods is much more
computational expensive.

<id>
physics/0005045v1
<category>
physics.atm-clus
<abstract>
The melting-like transition in disordered sodium clusters Na_N, with N=92 and
142 is studied by using a first-principles constant-energy molecular dynamics
simulation method. Na_142, whose atoms are distributed in two (surface and
inner) main shells with different radial distances to the center of mass of the
cluster, melts in two steps: the first one, at approx. 130 K, is characterized
by a high intrashell mobility of the atoms, and the second, homogeneous
melting, at approx. 270 K, involves diffusive motion of all the atoms across
the whole cluster volume (both intrashell and intershell displacements are
allowed). On the contrary, the melting of Na_92 proceeds gradually over a very
wide temperature interval, without any abrupt step visible in the thermal or
structural melting indicators. The occurrence of well defined steps in the
melting transition is then shown to be related to the existence of a
distribution of the atoms in shells. Thereby we propose a necessary condition
for a cluster to be considered rigorously amorphouslike (totally disordered),
namely that there are no space regions of the cluster where the local value of
the atomic density is considerably reduced. Na_92 is the only cluster from the
two considered that verifies this condition, so its thermal behavior can be
considered as representative of that expected for amorphous clusters. Na_142,
on the other hand, has a discernible atomic shell structure and should be
considered instead as just partially disordered. The thermal behavior of these
two clusters is also compared to that of icosahedral (totally ordered) sodium
clusters of the same sizes.

<id>
physics/0005053v2
<category>
physics.atm-clus
<abstract>
The melting-like transition in potasium clusters K_N, with N=20, 55, 92 and
142, is studied by using an orbital-free density-functional constant-energy
molecular dynamics simulation method, and compared to previous theoretical
results on the melting-like transition in sodium clusters of the same sizes.
Melting in potasium and sodium clusters proceeds in a similar way: a surface
melting stage develops upon heating before the homogeneous melting temperature
is reached. Premelting effects are nevertheless more important and more easily
established in potasium clusters, and the transition regions spread over
temperature intervals which are wider than in the case of sodium. For all the
sizes considered, the percentage melting temperature reduction when passing
from Na to K clusters is substantially larger than in the bulk. Once those two
materials have been compared for a number of different cluster sizes, we study
the melting-like transition in Rb_55 and Cs_55 clusters and make a comparison
with the melting behavior of Na_55 and K_55. As the atomic number increases,
the height of the specific heat peaks decreases, their width increases, and the
melting temperature decreases as in bulk melting, but in a more pronounced way.

<id>
physics/0008235v1
<category>
physics.atm-clus
<abstract>
The emergence of CsCl bulk structure in (CsCl)nCs+ cluster ions is
investigated using a mixed quantum-mechanical/semiempirical theoretical
approach. We find that rhombic dodecahedral fragments (with bulk CsCl symmetry)
are more stable than rock-salt fragments after the completion of the fifth
rhombic dodecahedral atomic shell. From this size (n=184) on, a new set of
magic numbers should appear in the experimental mass spectra. We also propose
another experimental test for this transition, which explicitely involves the
electronic structure of the cluster. Finally, we perform more detailed
calculations in the size range n=31--33, where recent experimental
investigations have found indications of the presence of rhombic dodecahedral
(CsCl)32Cs+ isomers in the cluster beams.

<id>
physics/0009011v2
<category>
physics.atm-clus
<abstract>
The energetic characteristics of two-shell carbon nanoparticles ("onions")
with different shapes of second shell are calculated. The barriers of relative
rotation of shells are found to be surprisingly small; therefore, free relative
rotation of shells can take place at room temperature. The intershell
orientational melting of the nanoparticle $C_{60}@C_{240}$ is studied by
molecular dynamics. The parameters of Arrhenius formula for jump rotational
intershell diffusion are calculated. The definition of orientational melting
temperature is proposed as the temperature when the transition probability over
barrier between equivalent potential minima is equal to 1/2. The temperature of
orientational melting of the nanoparticle $C_{60}@C_{240}$ is about 60 K.

<id>
physics/0011063v1
<category>
physics.atm-clus
<abstract>
We study the thermodynamics of Na_8 and Na_{20} clusters using
multiple-histogram methods and an ab initio treatment of the valence electrons
within density functional theory. We consider the influence of various electron
kinetic-energy functionals and pseudopotentials on the canonical ionic specific
heats. The results for all models we consider show qualitative similarities,
but also significant temperature shifts from model to model of peaks and other
features in the specific-heat curves. The use of phenomenological
pseudopotentials shifts the melting peak substantially (~ 50--100 K) when
compared to ab-initio results. It is argued that the choice of a good
pseudopotential and use of better electronic kinetic-energy functionals has the
potential for performing large time scale and large sized thermodynamical
simulations on clusters.

<id>
physics/0011070v1
<category>
physics.atm-clus
<abstract>
For the energy absorption of atomic clusters as a function of the laser pulse
duration we find a similar behavior as it has been observed for metallic
clusters [K\"oller et al., Phys. Rev. Lett. {\bf 82}, 3783 (1999)]. In both
situations there exists an optimum radius $R_{o}$ of the cluster for energy
absorption. In the metallic case the existence of $R_{o}$ has been interpreted
as a consequence of the collective oscillation of a delocalized electron cloud
in resonance with the laser frequency. Here, we give evidence that in the
atomic cluster the origin of $R_{o}$ is very different. Based on field assisted
tunneling it can be related to the phenomenon of enhanced ionization as it
occurs in small molecules. The dependence of $R_{o}$ on the laser frequency
turns out to be the key quantity to distinguish the processes.

<id>
physics/0101069v1
<category>
physics.atm-clus
<abstract>
The properties of neutral and anionic Pd_N clusters were investigated with
spin-density-functional calculations. The ground state structures are
three-dimensional for N>3 and they are magnetic with a spin-triplet for 2<=N<=7
and a spin nonet for N=13 neutral clusters. Structural- and spin-isomers were
determined and an anomalous increase of the magnetic moment with temperature is
predicted for a Pd_7 ensemble. Vertical electron detachment and ionization
energies were calculated and the former agree well with measured values for
anionic Pd_N clusters.

<id>
physics/0104071v1
<category>
physics.atm-clus
<abstract>
We report on results of a theoretical study of the adsorption process of a
single carbon oxide molecule on a Platinum (111) surface. A four-component
relativistic density functional method was applied to account for a proper
description of the strong relativistic effects. A limited number of atoms in
the framework of a cluster approach is used to describe the surface. Different
adsorption sites are investigated. We found that CO is preferably adsorbed at
the top position.

<id>
physics/0106044v3
<category>
physics.atm-clus
<abstract>
We study the interaction of strong femtosecond laser pulses with the C$_{60}$
molecule employing time-dependent density functional theory with the ionic
background treated in a jellium approximation. The laser intensities considered
are below the threshold of strong fragmentation but too high for perturbative
treatments such as linear response. The nonlinear response of the model to
excitations by short pulses of frequencies up to 45eV is presented and analyzed
with the help of Kohn-Sham orbital resolved dipole spectra. In femtosecond
laser pulses of 800nm wavelength ionization is found to occur multiphoton-like
rather than via excitation of a ``giant'' resonance.

<id>
physics/0110003v1
<category>
physics.atm-clus
<abstract>
The energy variance optimization algorithm over a fixed ensemble of
configurations in variational Monte Carlo is formally identical to a problem of
fitting data: we reexamine it from a statistical maximum-likelihood point of
view. We detect the origin of the problem of convergence that is often
encountered in practice and propose an alternative procedure for optimization
of trial wave functions in quantum Monte Carlo. We successfully test this
proposal by optimizing a trial wave function for the Helium trimer.

<id>
physics/0110072v1
<category>
physics.atm-clus
<abstract>
The optical response of the lowest energy members of the C20 family is
calculated using time-dependent density functional theory within a real-space,
real-time scheme. Significant differences are found among the spectra of the
different isomers, and thus we propose optical spectroscopy as a tool for
experimental investigation of the structure of these important clusters.

<id>
physics/0112012v1
<category>
physics.atm-clus
<abstract>
New type of nonlinear (anharmonic) excitations -- bushes of vibrational modes
-- in physical systems with point or space symmetry are discussed. All infrared
active and Raman active bushes for C60 fulerene are found by means of special
group-theoretical methods.

<id>
physics/0112053v2
<category>
physics.atm-clus
<abstract>
We have investigated the lowest-energy structures and electronic properties
of the Au$_n$(n=2-20) clusters based on density functional theory (DFT) with
local density approximation. The small Au$_n$ clusters adopt planar structures
up to n=6. Tabular cage structures are preferred in the range of n=10-14 and a
structural transition from tabular cage-like structure to compact
near-spherical structure is found around n=15. The most stable configurations
obtained for Au$_{13}$ and Au$_{19}$ clusters are amorphous instead of
icosahedral or fcc-like, while the electronic density of states sensitively
depend on the cluster geometry. Dramatic odd-even alternative behaviors are
obtained in the relative stability, HOMO-LUMO gaps and ionization potentials of
gold clusters. The size evolution of electronic properties is discussed and the
theoretical ionization potentials of Au$_n$ clusters compare well with
experiments.

<id>
physics/0112054v1
<category>
physics.atm-clus
<abstract>
We study the carbon-dope aluminum clusters by using time-of-flight mass
spectrum experiments and {\em ab initio} calculations. Mass abundance
distributions are obtained for anionic aluminum and aluminum-carbon mixed
clusters. Besides the well-known magic aluminum clusters such as Al$_{13}^-$
and Al$_{23}^-$, Al$_7$C$^-$ cluster is found to be particularly stable among
those Al$_n$C$^-$ clusters. Density functional calculations are performed to
determine the ground state structures of Al$_n$C$^-$ clusters. Our results show
that the Al$_7$C$^-$ is a magic cluster with extremely high stability, which
might serve as building block of the cluster-assembled materials.

<id>
physics/0112058v1
<category>
physics.atm-clus
<abstract>
We present a quantum Monte Carlo study of the structure and energetics of
silver doped helium clusters AgHe$_n$ for $n$ up to 100. Our simulations show
the first solvation shell of the Ag atom to be composed by roughly 20 He atoms,
and to possess a structured angular distribution. Moreover, the electronic
$^2$P$_{1/2}\leftarrow ^2$S$_{1/2}$ and $^2$P$_{3/2}\leftarrow ^2$S$_{1/2}$
electronic transitions of the embedded silver impurity have been studied as a
function of the number ofhelium atoms. The computed spectra show a redshift for
$n\leq 15$ and an increasing blueshift for larger clusters, a feature
attributed to the effect of the second solvation shell of He atoms. For the
largest cluster, the computed excitation spectrum is found in excellent
agreement with the ones recorded in superfluid He clusters and bulk. No
signature of the direct formation of proposed AgHe$_2$ exciplex is present in
the computed spectra of AgHe$_{100}$.

<id>
physics/0112072v1
<category>
physics.atm-clus
<abstract>
Polarizabilities of Ge$_n$ clusters with 2 to 25 atoms are calculated using
coupled-perturbation Hartree-Fock (CPHF) and finite field (FF) method within
density functional theory. The polarizabilities of the Ge$_n$ clusters increase
rapidly in the size range of 2 to 5 atoms and then fluctuate around the bulk
value. The polarizabilities are sensitively dependent on the cluster geometries
and electronic structures. The large HOMO-LUMO gap may lead to the small
polarizability. As compared with the compact structure and diamond structure,
the prolate cluster structure corresponds to a larger polarizability.

<id>
physics/0112075v1
<category>
physics.atm-clus
<abstract>
The structural and magnetic properties of Co$_{18-m}$Cu$_m$ ($0\leq m\leq
18$) clusters are investigated with a genetic algorithm and a $spd$-band model
Hamiltonian in the unrestricted Hartree-Hock approximation respectively. In
general, Cu atoms tend to occupy the surface, while Co atoms prefer to the
interior of the clusters. Layered structures appear in some clusters with given
stoichiometric compositions. The introduction of Cu atoms leads to large
increase of the magnetic moment of Co-rich circumstance and nearly zero
magnetism of the Cu-rich ambient. The interaction between Cu and Co atoms
induces nonzero magnetic moment for Cu atoms. The total magnetic moments tend
to decrease with the increase of Cu atoms. However, some particular large
magnetic moment are found to be closely related to the structures. The
environment of Cu and Co atoms have a dominant effect on the magnetism of the
cluster.

<id>
physics/9802001v1
<category>
physics.bio-ph
<abstract>
By means of Surface Evolver (Exp. Math,1,141 1992), a software package of
brute-force energy minimization over a triangulated surface developed by the
geometry center of University of Minnesota, we have numerically searched the
non-axisymmetric shapes under the Helfrich spontaneous curvature (SC) energy
model. We show for the first time there are abundant mechanically stable
non-axisymmetric vesicles in SC model, including regular ones with intrinsic
geometric symmetry and complex irregular ones. We report in this paper several
interesting shapes including a corniculate shape with six corns, a
quadri-concave shape, a shape resembling sickle cells, and a shape resembling
acanthocytes. As far as we know, these shapes have not been theoretically
obtained by any curvature model before. In addition, the role of the
spontaneous curvature in the formation of irregular crenated vesicles has been
studied. The results shows a positive spontaneous curvature may be a necessary
condition to keep an irregular crenated shape being mechanically stable.

<id>
physics/9803048v1
<category>
physics.bio-ph
<abstract>
We perform an experimental study of the time behavior of the $\alpha$-wave
events occuring in human electroencephalographic signals. We find that the
fraction of the time spent in an $\alpha$-burst of time size $\tau$ exhibits a
scaling behavior as a function of $\tau$. The corresponding exponent is equal
to 1.75$\pm$0.13. We therefore point out the existence of a new power law
appearing in physiology. Furhtermore, we show that our experimental result may
have a possible explanation within a class of Self-Organized Critical (SOC)
models recently proposed by Boettcher and Paczuski. In particular, one of these
models, when properly re-interpreted, seems to be consistent both with our
result and a commonly accepted physiological description of the possible origin
of $\alpha$-wave events.

<id>
physics/9808044v1
<category>
physics.bio-ph
<abstract>
Based on the understanding that chemical bonds can act as tunnel junctions in
the Coulomb blockade regime, and on the technical ability to coat a DNA strand
with metal, we suggest that DNA can be used to built logical devices. We
discuss two explicit examples: a Single Electron Tunneling Transistor (SET) and
a Quantum Bit Element. These devices would be literally in the nano-meter scale
and would be able to operate at room temperature. In addition they would be
identical to each other, highly stable and would have a self assembly property.

<id>
physics/9810035v1
<category>
physics.bio-ph
<abstract>
We determined scaling laws for the numerical effort to find the optimal
configurations of a simple model potential energy surface (PES) with a perfect
funnel structure that reflects key characteristics of the protein interactions.
Generalized Monte-Carlo methods(MCM, STUN) avoid an enumerative search of the
PES and thus provide a natural resolution of the Levinthal paradox. We find
that the computational effort grows with approximately the eighth power of the
system size for MCM and STUN, while a genetic algorithm was found to scale
exponentially. The scaling behaviour of a derived lattice model is also
rationalized.

<id>
physics/9811031v1
<category>
physics.bio-ph
<abstract>
In a simplified fashion, the motion of the eyeball in its orbit consists of
rotations around a fixed point. Therefore, this motion can be described in
terms of the Euler's angles of rigid body dynamics. However, there is a
physiological constraint in the motion of the eye which reduces to two its
degrees of freedom. This paper reviews the basic features of the kinematics of
the eye and the laws governing its motion.

<id>
physics/9812048v2
<category>
physics.bio-ph
<abstract>
preprint withdrawn. A revised version of this paper, with different contributors
appears in E. Nelson, P. Wolynes, and J. Onuchic, in Optimization in
computational chemistry and molecular biology, C. Floudas and P. Pardalos
editors, (1999)). The main failing of my approach in these papers is the fact
that the Hamiltonian and order parameters are based on pair distances only, and
therefore do not break the local gauge (reflection) symmetry. Consequently, the
order parameters cannot detect the difference between (for example) a partially
compact topology and its mirror image. I take full responsibility for this
problem, the withdrawal of this preprint, and the comments made above. Erik D.
Nelson

<id>
physics/0002039v1
<category>
physics.bio-ph
<abstract>
We present a novel method for analyzing Small Angle X-ray Scattering data on
multilamellar phospholipid bilayer systems at full hydration. The method
utilizes a modified Caille' theory structure factor in combination with a
Gaussian model representation of the electron density profile such that it
accounts also for the diffuse scattering between Bragg peaks. Thus, the method
can retrieve structural information even if only a few orders of diffraction
are observed. We further introduce a new procedure to derive fundamental
parameters, such as area per lipid, membrane thickness, and number of water
molecules per lipid, directly from the electron density profile without the
need of additional volumetric measurements. The theoretical apparatus is
applied to experimental data on
1-palmitoyl-2-oleoyl-sn-glycero-3-phosphocholine and
1,2-dipalmitoyl-sn-glycero-3-phosphoethanolamine liposome preparations.

<id>
physics/0002040v1
<category>
physics.bio-ph
<abstract>
The response kinetics of liquid crystalline phosphatidylcholine bilayer
stacks to rapid, IR-laser induced temperature jumps has been studied by
millisecond time-resolved x-ray diffraction. The system reacts on the fast
temperature change by a discrete bilayer compression normal to its surface and
a lateral bilayer expansion. Since water cannot diffuse from the excess phase
into the interbilayer water region within the 2 ms duration of the laser pulse,
the water layer has to follow the bilayer expansion, by an anomalous thinning.
Structural analysis of a 20 ms diffraction pattern from the intermediate phase
indicates that the bilayer thickness remains within the limits of isothermal
equilibrium values. Both, the intermediate structure and its relaxation into
the original equilibrium L_(alpha)-phase, depend on the visco-elastic
properties of the bilayer/water system. We present an analysis of the
relaxation process by an overdamped one-dimensional oscillation model revealing
the concepts of Hooke's law for phospholipid bilayers on a supramolecular
basis. The results yield a constant bilayer repulsion and viscosity within
Hooke's regime suggesting that the hydrocarbon chains act as a buffer for the
supplied thermal energy. The bilayer compression is a function of the initial
temperature and the temperature amplitude, but is independent of the chain
length.

<id>
physics/0003101v1
<category>
physics.bio-ph
<abstract>
We discuss a two-dimensional model for the dynamics of axonemal deformations
driven by internally generated forces of molecular motors. Our model consists
of an elastic filament pair connected by active elements. We derive the dynamic
equations for this system in presence of internal forces. In the limit of small
deformations, a perturbative approach allows us to calculate filament shapes
and the tension profile. We demonstrate that periodic filament motion can be
generated via a self-organization of elastic filaments and molecular motors.
Oscillatory motion and the propagation of bending waves can occur for an
initially non-moving state via an instability termed Hopf bifurcation. Close to
this instability, the behavior of the system is shown to be independent of
microscopic details of the axoneme and the force-generating mechanism. The
oscillation frequency however does depend on properties of the molecular
motors. We calculate the oscillation frequency at the bifurcation point and
show that a large frequency range is accessible by varying the axonemal length
between 1 and 50$\mu$m. We calculate the velocity of swimming of a flagellum
and discuss the effects of boundary conditions and externally applied forces on
the axonemal oscillations.

<id>
physics/0006015v2
<category>
physics.bio-ph
<abstract>
We investigate a model where idiotypes (characterizing B-lymphocytes and
antibodies of an immune system) and anti-idiotypes are represented by
complementary bitstrings of a given length d allowing for a number of
mismatches (matching rules). In this model, the vertices of the hypercube in
dimension d represent the potential repertoire of idiotypes. A random set of
(with probability p) occupied vertices corresponds to the expressed repertoire
of idiotypes at a given moment. Vertices of this set linked by the above
matching rules build random clusters. We give a structural and statistical
characterisation of these clusters - or in other words - of the architecture of
the idiotypic network. Increasing the probability p one finds at a critical p a
percolation transition where for the first time a large connected graph occures
with probability one. Increasing p further, there is a second transition above
which the repertoire is complete in the sense that any newly introduced
idiotype finds a complementary anti-idiotype. We introduce structural
characteristics such as the mass distributions and the fragmentation rate for
random clusters, and determine the scaling behaviour of the cluster size
distribution near the percolation transition, including finite size
corrections. We find that slightly above the percolation transition the large
connected cluster (the central part of the idiotypic network) consists
typically of one highly connected part and a number of weakly connected
constituents and coexists with a number of small, isolated clusters. This is in
accordance with the picture of a central and a peripheral part of the idiotypic
network and gives some support to idealized architectures of the central part
used in recent dynamical mean field models.

<id>
physics/0006045v1
<category>
physics.bio-ph
<abstract>
A novel approach for structure alignment is presented, where the key
ingredients are: (1) An error function formulation of the problem
simultaneously in terms of binary (Potts) assignment variables and real-valued
atomic coordinates. (2) Minimization of the error function by an iterative
method, where in each iteration a mean field method is employed for the
assignment variables and exact rotation/translation of atomic coordinates is
performed, weighted with the corresponding assignment variables. The approach
allows for extensive search of all possible alignments, including those
involving arbitrary permutations. The algorithm is implemented using a C_alpha
representation of the backbone and explored on different protein structure
categories using the Protein Data Bank (PDB) and is successfully compared with
other algorithms. The approach performs very well with modest CPU consumption
and is robust with respect to choice of parameters. It is extremely generic and
flexible and can handle additional user-prescribed constraints easily.
Furthermore, it allows for a probabilistic interpretation of the results.

<id>
physics/0103042v1
<category>
physics.bio-ph
<abstract>
We study the problem of information propagation in brain microtubules. After
considering the propagation of electromagnetic waves in a fluid of permanent
electric dipoles, the problem reduces to the sine-Gordon wave equation in one
space and one time dimensions. The problem of propagation of information is
thus set.

<id>
physics/0104024v2
<category>
physics.bio-ph
<abstract>
With tangent angle perturbation approach the axial symmetry deformation of a
spherical vesicle in large under the pressure changes is studied by the
elasticity theory of Helfrich spontaneous curvature model.Three main results in
axial symmetry shape: biconcave shape, peanut shape, and one type of myelin are
obtained. These axial symmetry morphology deformations are in agreement with
those observed in lipsome experiments by dark-field light microscopy [Hotani,
J. Mol. Biol. 178, (1984) 113] and in the red blood cell with two thin
filaments (myelin) observed in living state (see, Bessis, Living Blood Cells
and Their Ultrastructure, Springer-Verlag, 1973). Furthermore, the biconcave
shape and peanut shape can be simulated with the help of a powerful software,
Surface Evolver [Brakke, Exp. Math. 1, 141 (1992) 141], in which the
spontaneous curvature can be easy taken into account.

<id>
physics/0111204v1
<category>
physics.bio-ph
<abstract>
The tuning curve of the cochlea measures how large an input is required to
elicit a given output level as a function of the frequency. It is a fundamental
object of auditory theory, for it summarizes how to infer what a sound was on
the basis of the cochlear output. A simple model is presented showing that only
two elements are sufficient for establishing the cochlear tuning curve: a
broadly tuned traveling wave, moving unidirectionally from high to low
frequencies, and a set of mechanosensors poised at the threshold of an
oscillatory (Hopf) instability. These two components suffice to generate the
various frequency-response regimes which are needed for a cochlear tuning curve
with a high slope.

<id>
physics/0206074v2
<category>
physics.bio-ph
<abstract>
Chirality is considered by many scientists to be mainly a geometric concept.
There exists also a physical aspect of chirality which is largely being
overlooked at. Two examples of mechanical devices are introduced here that
represent ``Physical Chirality''. These are a rotating water sprinkler and a
variant of Crookes' radiometer. When interacting with appropriate media, they
both choose only one mode of rotation out of two possible ones. Such a behavior
does not obey time-reversal invariance, which is regarded to be a rule in
classical mechanics. This is due to their chiral nature. Instead, they do obey
a space-time (ST) law of invariance, that is, what is rotating in the opposite
direction is the mirror-image of the given device. In a recent experiment of
Koumura et al. they discovered a similar behavior of a molecular rotor. The
possible biological significance of physical chirality is emphasized hereby,
and the conclusion is that chiral molecular systems do not reach readily
thermal equilibrium. In other words: ``Physical chirality does feed on negative
entropy'', and therefore, it may well be of crucial value to life.

<id>
physics/0207102v2
<category>
physics.bio-ph
<abstract>
We study the phenomenon of spatiotemporal stochastic resonance (STSR) in a
chain of diffusively coupled bistable oscillators. In particular, we examine
the situation in which the \textit{global} STSR response is controlled by a
\textit{locally applied signal} and reveal a wave front propagation. In order
to deepen the understanding of the system dynamics, we introduce, on the time
scale of STSR, the study of the effective statistical renormalization of a
generic lattice system. Using this technique we provide a new criterion for
STSR, and predict and observe numerically a bifurcation-like behaviour that
reflects the difference between the most probable value of the local
quasi-equilibrium density and its mean value. Our results, tested with a chain
of nonlinear oscillators, appear to possess some universal qualities and may
stimulate a deeper search for more generic phenomena

<id>
physics/0209001v1
<category>
physics.bio-ph
<abstract>
Throughout history, for reasons of health, well known personalities have
opted not to eat to satiety - to under-eat - with known benefits. Here, for the
benefits of under-eating a very simple explanation is offered and discussed in
some detail. Our distant ancestors, whose bodies we inherited, ate and weighed
a good deal less than we do. They had to run around every day to scour the
countryside for their scarce food, which kept them lean, hungry and healthy.
We, not so healthy relative over-weights, load up our shopping carts once a
week at the corner supermarket and simply eat too much for our ~50000-year-old
body design. Under-eating or not to eat one's fill appears to bring us somewhat
more in line with this design. Thereby, it causes our way of life to more
closely resemble the healthier one of our ancient forbears and enables us to
potentially live longer, to 100 and more. The putative underlying physiological
mechanisms for the health benefits of under-eating are of great interest and
therefore are touched upon here.

<id>
physics/0209018v1
<category>
physics.bio-ph
<abstract>
The phase transition from the isotropic (I) to nematic (N) liquid crystalline
suspension of F-actin of average length $3~\mu$m or above was studied by local
measurements of optical birefringence and protein concentration. Both
parameters were detected to be continuous in the transition region, suggesting
that the I-N transition is higher than 1st order. This finding is consistent
with a recent theory by Lammert, Rokhsar & Toner (PRL, 1993, 70:1650),
predicting that the I-N transition may become continuous due to suppression of
disclinations. Indeed, few line defects occur in the aligned phase of F-actin.
Individual filaments in solutions of a few mg/ml F-actin undergo fast
translational diffusion along the filament axis, whereas both lateral and
rotational diffusions are suppressed.

<id>
physics/0210109v2
<category>
physics.bio-ph
<abstract>
Chiral molecules are characterized by a specific optical rotation angle. An
experimental method was presented to dissect the temperature dependence of the
optical rotation angle with the molecular chirality of D-alanine, L-alanine and
DL-alanine crystals. Salam hypothesis predicted that quantum mechanical
cooperative and condensation phenomena may give rise to a second order phase
transition below a critical temperature linking the transformation of D-amino
acids to L-amino acids due to parity-violating energy difference. The
temperature- dependent measurement of the optical rotation angle of D-, L- and
DL-alanine crystals provided the direct evidence of the phase transition, but
denied the configuration change from D-alanine to L-alanine. New views on Salam
hypothesis are presented to demonstrate its importance in the application of
low temperature enantiomeric separation and the origin of biochirality.

<id>
physics/0212095v1
<category>
physics.bio-ph
<abstract>
The writhe of a space curve fragment is considered for various boundary
conditions. An expression for the writhe as a function of arclength for an
arbitrary space curve is obtained. The formula is built on the base of closing
the tangent indicatrix with a geodesic. The corresponding closure of a curve in
3-space is explicitly constructed. The addition rule for writhe is formulated.
A relationship connecting the writhe with the Gauss integral over the open
curve is presented. The single and double regular helical shapes are examined
as examples.

<id>
physics/0301075v1
<category>
physics.bio-ph
<abstract>
The original method of treatment of various pathological processes in human
skin covers is described. The method has a brightly expressed differential
action: destroying pathologic cells it does not render any influence on healthy
cells. The examples of application of a method for treatment of some kinds of
skin pathologies are given.

<id>
physics/0302030v1
<category>
physics.bio-ph
<abstract>
Given a complex biological or social network, how many clusters should it be
decomposed into? We define the distance $d_{i,j}$ from node $i$ to node $j$ as
the average number of steps a Brownian particle takes to reach $j$ from $i$.
Node $j$ is a global attractor of $i$ if $d_{i,j}\leq d_{i,k}$ for any $k$ of
the graph; it is a local attractor of $i$, if $j\in E_i$ (the set of
nearest-neighbors of $i$) and $d_{i,j}\leq d_{i,l}$ for any $l\in E_i$. Based
on the intuition that each node should have a high probability to be in the
same community as its global (local) attractor on the global (local) scale, we
present a simple method to uncover a network's community structure. This method
is applied to several real networks and some discussion on its possible
extensions is made.

<id>
physics/0303038v1
<category>
physics.bio-ph
<abstract>
Motivated by the motion of nematode sperm cells, we present a model for the
motion of an adhesive gel on a solid substrate. The gel polymerizes at the
leading edge and depolymerizes at the rear. The motion results from a
competition between a self-generated swelling gradient and the adhesion on the
substrate. The resulting stress provokes the rupture of the adhesion points and
allows for the motion. The model predicts an unusual force-velocity relation
which depends in significant ways on the point of application of the force.

<id>
physics/0303089v1
<category>
physics.bio-ph
<abstract>
The possible normal modes of vibration of a nearly spherical virus particle
are discussed. Two simple models for the particle are treated, a liquid drop
model and an elastic sphere model. Some estimates for the lowest vibrational
frequency are given for each model. It is concluded that this frequency is
likely to be of the order of a few GHz for particles with a radius of the order
of 50 nm.

<id>
physics/0306044v1
<category>
physics.bio-ph
<abstract>
The influence of the intramolecular anharmonicity and the strong
vibron-phonon coupling on the two-vibron dynamics in an $\alpha$-helix protein
is studied within a modified Davydov model. The intramolecular anharmonicity of
each amide-I vibration is considered and the vibron dynamics is described
according to the small polaron approach. A unitary transformation is performed
to remove the intramolecular anharmonicity and a modified Lang-Firsov
transformation is applied to renormalize the vibron-phonon interaction. Then, a
mean field procedure is realized to obtain the dressed anharmonic vibron
Hamiltonian. It is shown that the anharmonicity modifies the vibron-phonon
interaction which results in an enhancement of the dressing effect. In
addition, both the anharmonicity and the dressing favor the occurrence of two
different bound states which the properties strongly depend on the interplay
between the anharmonicity and the dressing. Such a dependence was summarized in
a phase diagram which characterizes the number and the nature of the bound
states as a function of the relevant parameters of the problem. For a
significant anharmonicity, the low frequency bound states describe two vibrons
trapped onto the same amide-I vibration whereas the high frequency bound states
refer to the trapping of the two vibrons onto nearest neighbor amide-I
vibrations.

<id>
physics/0306163v1
<category>
physics.bio-ph
<abstract>
We present a study of transport of a Brownian particle moving in periodic
symmetric potential in the presence of asymmetric unbiased fluctuations. The
particle is considered to move in a medium with periodic space dependent
friction. By tuning the parameters of the system, the direction of current
exhibit reversals, both as a function of temperature as well as the amplitude
of rocking force. We found that the mutual interplay between the opposite
driving factors is the necessary term for current reversals.

<id>
physics/0310070v1
<category>
physics.bio-ph
<abstract>
Relaxation channels for two-vibron bound states in an anharmonic alpha-helix
protein are studied. It is pointed out that the relaxation originates in the
interaction between the dressed anharmonic vibrons and the remaining phonons.
This interaction is responsible for the occurrence of transitions between
two-vibron eigenstates mediated by both phonon absorption and phonon emission.
At biological temperature, it is shown that the relaxation rate does not
significantly depends on the nature of the two-vibron state involved in the
process. Therefore, the lifetime for both bound and free states is of the same
order of magnitude and ranges between 0.1 and 1.0 ps for realistic parameters.
By contrast, the relaxation channels strongly depend on the nature of the
two-vibron states which is a consequence of the breather-like behavior of the
two-vibron bound states.

<id>
physics/0401124v1
<category>
physics.bio-ph
<abstract>
We report the phenomenon of symmetry breaking in ants escaping under panic.
Ants confined into a cell with two symmetrically located exits use both exits
in approximately equal proportions to abandon the cell in normal conditions,
but prefer one of the exits if panic is created by adding a repellent fluid.
This finding is consistent with the predictions of recent theoretical models
for the escape of humans in panic conditions, indicating that some features of
the collective behaviour of humans and ants can be similar when escaping under
panic.

<id>
physics/0401159v2
<category>
physics.bio-ph
<abstract>
Vesicles under a shear flow exhibit a tank-treading motion of their membrane,
while their long axis points with an angle < 45 degrees with respect to the
shear stress if the viscosity contrast between the interior and the exterior is
not large enough. Above a certain viscosity contrast, the vesicle undergoes a
tumbling bifurcation, a bifurcation which is known for red blood cells. We have
recently presented the full numerical analysis of this transition. In this
paper, we introduce an analytical model that has the advantage of being both
simple enough and capturing the essential features found numerically. The model
is based on general considerations and does not resort to the explicit
computation of the full hydrodynamic field inside and outside the vesicle.

<id>
physics/0403040v1
<category>
physics.bio-ph
<abstract>
Recent experiments with amyloid-beta (Abeta) peptide suggest that formation
of toxic oligomers may be an important contribution to the onset of Alzheimer's
disease. The toxicity of Abeta oligomers depends on their structure, which is
governed by assembly dynamics. Due to limitations of current experimental
techniques, a detailed knowledge of oligomer structure at the atomic level is
missing. We introduce a molecular dynamics approach to study Abeta dimer
formation: (1) we use discrete molecular dynamics simulations of a
coarse-grained model to identify a variety of dimer conformations, and (2) we
employ all-atom molecular mechanics simulations to estimate the thermodynamic
stability of all dimer conformations. Our simulations of a coarse-grained Abeta
peptide model predicts ten different planar beta-strand dimer conformations. We
then estimate the free energies of all dimer conformations in all-atom
molecular mechanics simulations with explicit water. We compare the free
energies of Abeta(1-42) and Abeta(1-40) dimers. We find that (a) all dimer
conformations have higher free energies compared to their corresponding
monomeric states, and (b) the free energy difference between the Abeta(1-42)
and the analogous Abeta(1-40) dimer conformation is not significant. Our
results suggest that Abeta oligomerization is not accompanied by the formation
of stable planar beta-strand Abeta dimers.

<id>
physics/9610015v2
<category>
physics.chem-ph
<abstract>
A new formalism for the optimal control of quantum mechanical physical
observables is presented. This approach is based on an analogous classical
control technique reported previously[J. Botina, H. Rabitz and N. Rahman, J.
chem. Phys. Vol. 102, pag. 226 (1995)]. Quantum Lagrange multiplier functions
are used to preserve a chosen subset of the observable dynamics of interest. As
a result, a corresponding small set of Lagrange multipliers needs to be
calculated and they are only a function of time. This is a considerable
simplification over traditional quantum optimal control theory[S. shi and H.
Rabitz, comp. Phys. Comm. Vol. 63, pag. 71 (1991)]. The success of the new
approach is based on taking advantage of the multiplicity of solutions to
virtually any problem of quantum control to meet a physical objective. A family
of such simplified formulations is introduced and numerically tested. Results
are presented for these algorithms and compared with previous reported work on
a model problem for selective unimolecular reaction induced by an external
optical electric field.

<id>
physics/9611012v1
<category>
physics.chem-ph
<abstract>
A model of rods with heads of variable size, which are confined to a planar
surface, is used to study the influence of the head group size on tilted phases
in Langmuir monolayers. Simple free energy considerations as well as exact zero
temperature calculations indicate that molecules with small head groups tilt
towards next nearest neighbors, and molecules with larger head groups towards
nearest neighbors. This provides a possible explanation for recent experimental
results, and for details of the generic phase diagram for fatty acid
monolayers.

<id>
physics/9611013v2
<category>
physics.chem-ph
<abstract>
An information theory model is used to construct a molecular explanation why
hydrophobic solvation entropies measured in calorimetry of protein unfolding
converge at a common temperature. The entropy convergence follows from the weak
temperature dependence of occupancy fluctuations for molecular-scale volumes in
water. The macroscopic expression of the contrasting entropic behavior between
water and common organic solvents is the relative temperature insensitivity of
the water isothermal compressibility. The information theory model provides a
quantitative description of small molecule hydration and predicts a negative
entropy at convergence. Interpretations of entropic contributions to protein
folding should account for this result.

<id>
physics/9611027v1
<category>
physics.chem-ph
<abstract>
The stretching and bending vibrations of methane are studied in the framework
of a symmetry-adapted algebraic model. The model is based on the realization of
the one-dimensional Morse potential in terms of a $U(2)$ algebra. For the 44
observed energies we obtain a fit with a r.m.s. deviation of 1.16 cm$^{-1}$
which is an order of magnitude more accurate than previous algebraic
calculations.

<id>
physics/9611029v1
<category>
physics.chem-ph
<abstract>
We apply a symmetry-adapted algebraic model to the vibrational excitations in
D_3h and T_d molecules. A systematic procedure is used to establish the
relation between the algebraic and configuration space formulations. In this
way we have identified interaction terms that were absent in previous
formulations of the vibron model. The inclusion of these new interactions leads
to reliable spectroscopic predictions. We illustrate the method for the D_3h
triatomic molecules, H_3^+, Be_3 and Na_3, and the T_d molecules, Be_4 and
CH_4.

<id>
physics/9612001v4
<category>
physics.chem-ph
<abstract>
We apply the time-dependent local density approximation (TDLDA) to calculate
dipole excitations in small carbon clusters. A strong low-frequency mode is
found which agrees well with observation for clusters C_n with n in the range
7-15. The size dependence of the mode may be understood simply as the classical
resonance of electrons in a conducting needle. For a ring geometry, the lowest
collective mode occurs at about twice the frequency of the collective mode in
the linear chain, and this may also be understood in simple terms.

<id>
physics/9612006v1
<category>
physics.chem-ph
<abstract>
We present a symmetry-adapted version of the vibron model and discuss an
application to D_{3h} triatomic molecules: H_3^+, Be_3 and Na_3^+.

<id>
physics/9701015v1
<category>
physics.chem-ph
<abstract>
A very efficient large-order perturbation theory is formulated for the
nuclear motion of a linear triatomic molecule. To demonstrate the method, all
of the experimentally observed rotational energies, with values of $J$ almost
up to 100, for the ground and first excited vibrational states of CO$_2$ and
for the ground vibrational states of N$_2$O and of OCS are calculated. All
coupling between vibration and rotation is included. The perturbation
expansions reported here are rapidly convergent. The perturbation parameter is
$D^{-1/2}$, where $D$ is the dimensionality of space. Increasing $D$ is
qualitatively similar to increasing the angular momentum quantum number $J$.
Therefore, this approach is especially suited for states with high rotational
excitation. The computational cost of the method scales only as $JN_v^{5/3}$,
where $N_v$ is the size of the vibrational basis set.

<id>
physics/9701017v1
<category>
physics.chem-ph
<abstract>
We report the investigation of the 3s <- 2p transition in the BAr2 cluster.
In a supersonic expansion of B atoms entrained in Ar, at high beam source
backing pressures we observe several features in the fluorescence excitation
spectrum which cannot be assigned to the BAr diatom. Using BAr(X, B) potential
energy curves which reproduce our experimental observations on this molecule
and an Ar-Ar interaction potential, we employ a pairwise additive model, along
with variational and diffusion Monte-Carlo treatments of the nuclear motion, to
determine the lowest vibrational state of the BAr2 cluster. A subsequent
simulation of the fluorescence excitation spectrum reproduces nearly
quantitatively the strongest feature in our experimental spectrum not
assignable to BAr. Because of the barrier in the BAr(B 2Sigma+) potential
energy curve, the 3s <- 2p transition in the BAr2 cluster is predicted to have
an asymmetric profile, as is found experimentally.

<id>
physics/9701021v1
<category>
physics.chem-ph
<abstract>
A theoretical study is made on He scattering from three fundamental classes
of disordered ad-layers: (a) Translationally random adsorbates, (b) disordered
compact islands and (c) fractal submonolayers. The implications of the results
to experimental studies of He scattering from disordered surfaces are
discussed, and a combined experimental-theoretical study is made for Ag
submonolayers on Pt(111). Some of the main theoretical findings are: (1)
Structural aspects of the calculated intensities from translationally random
clusters were found to be strongly correlated with those of individual
clusters. (2) Low intensity Bragg interference peaks appear even for scattering
from very small ad-islands, and contain information on the ad-island local
electron structure. (3) For fractal islands, just as for islands with a
different structure, the off-specular intensity depends on the parameters of
the He/Ag interaction, and does not follow a universal power law as previously
proposed in the literature. In the experimental-theoretical study of Ag on
Pt(111), we use first experimental He scattering data from low-coverage (single
adsorbate) systems to determine an empirical He/Ag-Pt potential of good
quality. Then, we carry out He scattering calculations for high coverage and
compare with experiments. The conclusions are that the actual experimental
phase corresponds to small compact Ag clusters of narrow size distribution,
translationally disordered on the surface.

<id>
physics/9702004v1
<category>
physics.chem-ph
<abstract>
An algebraic model in terms of a local harmonic boson realization was
recently proposed to study molecular vibrational spectra [Zhong-Qi Ma et al.,
Phys. Rev. A 53, 2173 (1996)]. Because of the local nature of the bosons the
model has to deal with spurious degrees of freedom. An approach to eliminate
the latter from both the Hamiltonian and the basis was suggested. We show that
this procedure does not remove all spurious components from the Hamiltonian and
leads to a restricted set of interactions. We then propose a scheme in which
the physical Hamiltonian can be systematically constructed up to any order
without the need of imposing conditions on its matrix elements. In addition, we
show that this scheme corresponds to the harmonic limit of a symmetry adapted
algebraic approach based on U(2) algebras.

<id>
physics/9702012v1
<category>
physics.chem-ph
<abstract>
The temperature dependence of the rate of the reaction CH_4+H \to CH_3+H_2 is
studied using classical collision theory with a temperature-dependent effective
potential derived from a path integral analysis. Analytical expressions are
obtained for the effective potential and for the rate constant. The rate
constant expressions use a temperature-dependent activation energy. They give
better agreement with the available experimental results than do previous
empirical fits. Since all but one of the parameters in the present expressions
are obtained from theory, rather than by fitting to experimental reaction
rates, the expressions can be expected to be more dependable than purely
empirical expressions at temperatures above 2000 K or below 350 K, where
experimental results are not available.

<id>
physics/9704027v2
<category>
physics.chem-ph
<abstract>
A system containing a pre-steady state standard (non-autocatalytic) reaction,
with multiple paths, evolves toward a kinetic state with the minimum attainable
activation free energy. Displacement of the path frequency distribution in this
transition was shown to minimise the affinity linked to this change in
activation free energy. In damping this scalar force, a standard system is
driven along a path of least action, as previously established for a system of
competing autocatalytic reactions. A kinetic source of time asymmetry arises
within the system, as the activation affinity moves the system toward the most
probable distribution of reaction paths. As the functions of state are not
changed by path displacement, a change of kinetic state cannot produce chemical
work. This generalises the notion of force to a scalar quantity responsible for
a displacement that does not yield work or heat. Spectrophotometric
observations reported on the transition to steady state kinetics during
dinitrophenyl phosphate phosphorolysis confirmed that time variations in the
activation affinity are non-positive.

<id>
physics/9705002v1
<category>
physics.chem-ph
<abstract>
Heat capacity curves as functions of temperature were calculated using Monte
Carlo methods for the series of Ne_(13-n)Ar_n clusters (0 <= n <= 13). The
clusters were modeled classically using pairwise additive Lennard-Jones
potentials. The J-walking (or jump-walking) method was used to overcome
systematic errors due to quasiergodicity. Substantial discrepancies between the
J-walking results and those obtained using standard Metropolis methods were
found. Results obtained using the atom-exchange method, another Monte Carlo
variant for multi-component systems, also did not compare well with the
J-walker results. Quench studies were done to investigate the clusters'
potential energy surfaces. Only those Ne-Ar clusters consisting predominately
of either one or the other component had lowest energy isomers having the
icosahedral-like symmetry typical of homogeneous 13-atom rare gas clusters;
non-icosahedral structures dominated the lowest-energy isomers for the other
clusters. This resulted in heat capacity curves that were very much different
than that of their homogeneous counterpart. Evidence for coexistence behavior
different than that seen in homogenous clusters is also presented.

<id>
physics/9706016v1
<category>
physics.chem-ph
<abstract>
Some aspects of direct ion transfer across the water/1,2-dichloroethane are
analyzed using a very simple model based on thermodynamic considerations. It
was concluded that ion solvation by water molecules may occur in some
particular cases in the organic phase, delivering an important contribution to
the Gibbs free energy of ion transfer between the aqueous and the organic
phase. In general terms, this particular type of transfer should be favored in
the case of highly charged small ions at interfaces with a relatively low
surface tension and a large difference between the reciprocal of the
corresponding dielectric constants.

<id>
physics/9706028v3
<category>
physics.chem-ph
<abstract>
Reported here are theoretical calculations on the triflic acid and water,
establishing molecular scale information necessary to modeling of the
structure, thermodynamics, and ionic transport of Nafion membranes. To
characterize side chain flexibility and accessibility of the acid proton, free
energies for rotation of both carbon-sulfur and sulfur-oxygen (hydroxyl) bonds
are presented. The energetic barrier to rotation of the acid proton away from
the sulfonic acid oxygen plane is substantially flattened, with barrier less
than one kcal/mol, by electrostatic solvation. The activation free energy for
acid-water proton interchange is about 4.7 kcal/mol.

<id>
physics/9708003v1
<category>
physics.chem-ph
<abstract>
The optimized random phase approximation (ORPA) for classical liquids is
re-examined in the framework of the generating functional approach to the
integral equations. We show that the two main variants of the approximation
correspond to the addition of the same correction to two different first order
approximations of the homogeneous liquid free energy. Furthermore, we show that
it is possible to consistently use the ORPA with arbitrary reference systems
described by continuous potentials and that the same approximation is
equivalent to a particular extremum condition for the corresponding generating
functional. Finally, it is possible to enforce the thermodynamic consistence
between the thermal and the virial route to the equation of state by requiring
the global extremum condition on the generating functional.

<id>
physics/9708012v1
<category>
physics.chem-ph
<abstract>
Protein structure prediction can be shown to be an NP-hard problem; the
number of conformations grows exponentially with the number of residues. The
native conformations of proteins occupy a very small subset of these, hence an
exploratory, robust search algorithm, such as a genetic algorithm (GA), is
required. The dynamics of GAs tend to be complicated and problem-specific.
However, their empirical success warrants their further study. In this paper,
guidelines for the design of genetic algorithms for protein structure
prediction are determined. To accomplish this, the performance of the simplest
genetic algorithm is investigated for simple lattice-based protein structure
prediction models (which is extendible to real-space), using energy
minimization. The study has led us to two important conclusions for
`protein-structure-prediction-genetic-algorithms'. Firstly, they require high
resolution building blocks attainable by multi-point crossovers and secondly
they require a local dynamics operator to `fine tune' good conformations.
Furthermore, we introduce a statistical mechanical approach to analyse the
genetic algorithm dynamics and suggest a convergence criterion using a quantity
analogous to the free energy of population.

<id>
physics/9708021v1
<category>
physics.chem-ph
<abstract>
The role of quantum coherence loss in mixed quantum-classical dynamical
systems is explored in the context of the theory of quantum decoherence
introduced recently by Bittner and Rossky. (J. Chem. Phys. {\bf 103}, 8130
(1995)). This theory, which is based upon the consistent histories
interpretation of quantum mechanics, introduces decoherence in the quantum
subsystem by carefully considering the relevant time and length scales over
which one must consider the effects of phase interference between alternative
histories of the classical subsystem. Such alternative histories are an
integral part of any quantum-classical computational scheme which employ
transitions between discrete quantum states; consequently, the coherences
between alternative histories have a profound effect on the transition
probability between quantum states. In this paper, we review the Bittner-Rossky
theory and detail a computational algorithm suitable for large-scale quantum
molecular dynamics simulations which implements this theory. Application of the
algorithm towards the relaxation of a photoexcited aqueous electron compare
well to previous estimates of the excited state survival time as well as to the
experimental measurements.

<id>
physics/9708022v1
<category>
physics.chem-ph
<abstract>
We present an overview of the role of quantum coherence in influencing
nonadiabatic processes in the condensed phase. Equations of motion for mixed
quantum-classical dynamics are derived from the Consistent Histories
interpretation of quantum mechanics. Application of the methods toward
computing the excited state lifetime of an excess electron in water and heavy
water provides a unique demonstration of our method.

<id>
physics/9708029v1
<category>
physics.chem-ph
<abstract>
We discuss a symmetry-adapted algebraic (or vibron) model for molecular
spectroscopy. The model is formulated in terms of tensor operators under the
molecular point group. In this way, we have identified interactions that are
absent in previous versions of the vibron model, in which the Hamiltonian is
expressed in terms of Casimir operators and their products. The inclusion of
these new interactions leads to reliable spectroscopic predictions. As an
example we study the vibrational excitations of the methane molecule, and
compare our results with those obtained in other algebraic models.

<id>
physics/9709005v1
<category>
physics.chem-ph
<abstract>
We present the APLIP process (Adiabatic Passage by Light Induced Potentials)
for the adiabatic transfer of a wave packet from one molecular potential to the
displaced ground vibrational state of another. The process uses an intermediate
state, which is only slightly populated, and a counterintuitive sequence of
light pulses to couple the three molecular states. APLIP shares many features
with STIRAP (stimulated Raman adiabatic passage), such as high efficiency and
insensitivity to pulse parameters. However, in APLIP there is no two-photon
resonance, and the main mechanism for the transport of the wave packet is a
light-induced potential. The APLIP process appears to violate the Franck-Condon
principle, because of the displacement of the wave packet, but does in fact
take place on timescales which are at least a little longer than a vibrational
timescale.

<id>
physics/9709015v1
<category>
physics.chem-ph
<abstract>
A series of calculations on the energetics of complexation of alkaline metals
with 1,10-phenanthroline are presented. It is an experimental fact that the
ordering of the free energy of transfer across the water - 1,2-dichloroethane
interphase is governed by the charge / size ratio of the diferent cations; the
larger cations showing the lower free energy of transfer. This ordering of the
free energies of transfer is reverted in the presence of 1,10-phenanthroline in
the organic phase. We have devised a thermodynamic cycle for the transfer
process and by means of ab-initio calculations have drawn the conclusion that
in the presence of phen the free energy of transfer is governed by the
stability of the PHEN/M $^{+}$complex, which explains the observed tendency
from a theoretical point of view.

<id>
physics/9709042v1
<category>
physics.chem-ph
<abstract>
Free energies of ionic solvation calculated from computer simulations exhibit
a strong system size dependence. We perform a finite-size analysis based on a
dielectric-continuum model with periodic boundary conditions. That analysis
results in an estimate of the Born ion size. Remarkably, the finite-size
correction applies to systems with only eight water molecules hydrating a
sodium ion and results in an estimate of the Born radius of sodium that agrees
with the experimental value.

<id>
physics/9711003v1
<category>
physics.chem-ph
<abstract>
The electronic structure of the hydrogen molecule in a magnetic field is
investigated for parallel internuclear and magnetic field axes. The lowest
states of the $\Pi$ manifold are studied for spin singlet and triplet$(M_s =
-1) $ as well as gerade and ungerade parity for a broad range of field
strengths $0 \leq B \leq 100 a.u.$ For both states with gerade parity we
observe a monotonous decrease in the dissociation energy with increasing field
strength up to $B = 0.1 a.u.$ and metastable states with respect to the
dissociation into two H atoms occur for a certain range of field strengths. For
both states with ungerade parity we observe a strong increase in the
dissociation energy with increasing field strength above some critical field
strength $B_c$. As a major result we determine the transition field strengths
for the crossings among the lowest $^1\Sigma_g$, $^3\Sigma_u$ and $^3\Pi_u$
states. The global ground state for $B \lesssim 0.18 a.u.$ is the strongly
bound $^1\Sigma_g$ state. The crossings of the $^1\Sigma_g$ with the
$^3\Sigma_u$ and $^3\Pi_u$ state occur at $B \approx 0.18$ and $B \approx0.39
a.u.$, respectively. The transition between the $^3\Sigma_u$ and $^3\Pi_u$
state occurs at $B \approx 12.3 a.u.$ Therefore, the global ground state of the
hydrogen molecule for the parallel configuration is the unbound $^3\Sigma_u$
state for $0.18 \lesssim B \lesssim 12.3 a.u.$ The ground state for $B \gtrsim
12.3 a.u.$ is the strongly bound $^3\Pi_u$ state. This result is of great
relevance to the chemistry in the atmospheres of magnetic white dwarfs and
neutron stars.

<id>
physics/9711020v1
<category>
physics.chem-ph
<abstract>
The electronic structure of the hydrogen molecule is investigated for the
parallel configuration. The ground states of the Sigma manifold are studied for
ungerade and gerade parity as well as singlet and triplet states covering a
broad regime of field strengths from B = 0 up to B = 100a.u. A variety of
interesting phenomena can be observed. For the ^1Sigma_g state we found a
monotonous decrease of the equilibrium distance and a simultaneously increase
of the dissociation energy with growing magnetic field strength. The ^3\Sigma_g
state is shown to develop an additional minimum which has no counterpart in
field-free space. The ^1\Sigma_u state shows a monotonous increase in the
dissociation energy with first increasing and than decreasing internuclear
distance of the minimum. For this state the dissociation channel is H_2 to H^-
+ H^+ for magnetic-field strengths B greater than 20a.u. due to the existence
of strongly bound H^- states in strong magnetic fields. The repulsive
^3\Sigma_u state possesses a very shallow van der Waals minimum for
magnetic-field strengths smaller than 1.0a.u. within the numerical accuracy of
our calculations. The ^1\Sigma_g and ^3\Sigma_u states cross as a function of B
and the ^3\Sigma_u state, which is an unbound state, becomes the ground state
of the hydrogen molecule in magnetic fields B greater than 0.2a.u. This is of
particular interest for the existence of molecular hydrogen in the vicinity of
white dwarfs. In superstrong fields the ground state is again a strongly bound
state, the ^3\Pi_u state.

<id>
physics/9711028v1
<category>
physics.chem-ph
<abstract>
A theory of vibrational energy relaxation based on a semiclassical treatment
of the quantum master equation is presented. Using new results on the
semiclassical expansion of dipole matrix elements, we show that in the
classical limit the master equation reduces to the Zwanzig energy diffusion
equation. The leading quantum corrections are determined and discussed for the
harmonic and Morse potentials.

<id>
physics/9712040v1
<category>
physics.chem-ph
<abstract>
We investigated the vibrational energies in the 2p^2A^"_2 and 3d^2E^" states
of the triatomic deuterium molecule D_3. The experiments were performed using a
fast neutral beam photoionization spectrometer recently developed at Freiburg.
A depletion type optical double-resonance scheme using two pulsed dye lasers
was applied. The measured vibrational frequencies of the 2p^2A^"_2 state of D_3
are compared to those of H_3 and to theoretical values calculated from an ab
initio potential energy surface. The data give insight into the importance of
the coupling between the valence electron and the ion core.

<id>
physics/9801009v1
<category>
physics.chem-ph
<abstract>
The question of the origin of the Gibbs-factorials is reconsidered. It is
argued, using the example of the Brownian motion, that their appearance in
classical statistical calculations cannot in general be traced back to the
symmetry postulate of quantum mechanics as often assumed.

<id>
physics/9801012v1
<category>
physics.chem-ph
<abstract>
Both the stretch and bend vibrational spectrum and the intensity of infrared
transitions in a tetrahedral molecule are studied in a U(2) algebraic model,
where the spurious states in the model Hamiltonian and the wavefunctions are
exactly removed. As an example, we apply the model to silicon tetrafluoride
SiF$_4$.

<id>
physics/9611009v2
<category>
physics.class-ph
<abstract>
It is shown that the Evans-Vigier modified electrodynamics is compatible with
the Relativity Theory.

<id>
physics/9611018v1
<category>
physics.class-ph
<abstract>
We propose a very simple but general method to construct solutions of Maxwell
equations propagating with a group velocity $v_{gr} \neq c$. Applications to
wave guides and a possible description of the known experimental evidences on
photonic tunneling are discussed.

<id>
physics/9612002v1
<category>
physics.class-ph
<abstract>
A new relativistic invariant version of nonlinear Maxwell equations is
offerred. Some properties of these equations are considered.

<id>
physics/9701023v1
<category>
physics.class-ph
<abstract>
The existence of radial solutions of a nonlinear Dirichlet problem in a ball
is translated to the language of Mechanics, i.e. to requirements on the time of
motion of a particle in an external potential and under the action of a
viscosity force. This approach reproduces existing theorems and, in principle,
provides a method for the analysis of the general case. Examples of new
theorems are given, which prove the usefulness of this qualitative method.

<id>
physics/9703029v1
<category>
physics.class-ph
<abstract>
Variables are separated in Maxwell equations by the Newman-Penrose method of
isotropic complex tetrade in the uniformly accelerated spherical coordinate
system. Particular solutions are obtained in terms of spin 1 spherical
harmonics. PACS: 03.50.De

<id>
physics/9703031v1
<category>
physics.class-ph
<abstract>
We considered the problem of the proportionality of inertial and
gravitational masses in classical mechanics. We found that the kinetic energy
of a material mass point m in a circular motion with a constant angular
velocity around another material point M depends only on its gravitational
mass. This fact, together with the known result that the straight line is a
circumference with an infinite radius, allowed us to prove the proportionality
between the inertial and gravitational masses.

<id>
physics/9708002v2
<category>
physics.class-ph
<abstract>
In the present work foundations of the law of the energy conservation and the
introduction of particles in the classical electrodynamics are discussed. We
pay attention to a logic error which takes place at an interpretation of the
Poynting's theorem as the law of conservation of energy. It was shown that the
laws of conservation of energy and momentum of the system of electromagnetic
fields and charged particles does not follow from the equations of
electrodynamics and the violation of these laws is displayed at the energy
change of particles. Particular examples are considered which make it possible
to restrict a possible kind of fields of a non-electromagnetic origin. We hope
that this work will permit to produce a more comprehensive analysis and to
stimulate the next step to the development of the foundations of the classical
and quantum electrodynamics.

<id>
physics/9708023v1
<category>
physics.class-ph
<abstract>
Some considerations of long wavelength and broadband radiation sources based
on the emission of the coherent radiation by a train of short relativistic
electron bunches moving in an open resonator along an arc-like or undulator
trajectories and some new versions of the transition radiation sources and long
wavelength sources based on storage rings are presented.

<id>
physics/9709031v1
<category>
physics.class-ph
<abstract>
Approximate asymptotic conditions on the motion of compact, electrically
charged particles are derived within the framework of general relativity using
the Einstein- Infeld-Hoffmann (EIH) surface integral method. While
superficially similar to the Abraham-Lorentz and Lorentz-Dirac (ALD) equations
of motion, these conditions differ from them in several fundamental ways. They
are not equations of motion in the usual sense but rather a set of conditions
which these motions must obey in the asymptotic future of an initial value
surface. In addition to being asymptotic, these conditions of motion are
approximate and apply, as do the original EIH equations, only to slowly moving
systems. Also, they do not admit the run- away solutions of these other
equations. As in the original EIH work, they are integrability conditions
gotten from integrating the empty-space (i.e., source free) Einstein-Maxwell
equations of general relativity over closed two-surfaces surrounding the
sources of the fields governed by these equations. No additional ad hoc
assumptions, such as the form of a force law or the introduction of inertial
reaction terms, needed to derive the ALD equations are required for this
purpose. Nor is there a need for any of the infinite mass renormalizations that
are required in deriving these other equations.

<id>
physics/9710024v1
<category>
physics.class-ph
<abstract>
Recent theoretical results show the existence of arbitrary speeds (0 <= v <
\infty) solutions of all relativistic wave equations. Some recent experiments
confirm the results for sound waves. The question arises naturally: What is the
appropriate geometry of spacetime to describe superluminal phenomena? In this
paper we present a spacetime model that incorporates the valid results of
Relativity Theory and yet describes coherently superluminal phenomena without
paradoxes.

<id>
physics/9710025v1
<category>
physics.class-ph
<abstract>
In this paper we analyze the physical meaning of sub- and superluminal
soliton-like solutions (as the X-waves) of the relativistic wave equations and
of some non-trivial solutions of the free Schr\"odinger equation for which the
concepts of phase and group velocities have a different meaning than in the
case of plane wave solutions. If we accept the strict validity of the principle
of relativity, such solutions describe objects of two essentially different
natures: carrying energy wave packets and inertia-free properly phase
vibrations. Speeds of the first-type objects can exceed the plane wave velocity
$c_*$ only inside media and are always less than the vacuum light speed $c$.
Particularly, very fast sound pulses with speeds $c_* < v < c$ have already
been launched. The second-type objects are incapable of carrying energy and
information but have superluminal speed. If we admit the possibility of a
breakdown of Lorentz invariance, pulses described, for example, by superluminal
solutions of the Maxwell equations can be generated. Only experiment will give
the final answer.

<id>
physics/9712014v1
<category>
physics.class-ph
<abstract>
We prove that, contrary to the common belief, the classical Maxwell
electrodynamics of a point-like particle may be formulated as an
infinite-dimensional Hamiltonian system. We derive well defined
quasi-Hamiltonian which possesses direct physical interpretation being equal to
the total energy of the composed (field + particle) system. The phase space of
this system is endowed with an interesting symplectic structure. We prove that
this structure is strongly non-degenerated and, therefore, enables one to
define consistent Poisson bracket for particle's and field degrees of freedom.
We stress that this formulation is perfectly gauge-invariant.

<id>
physics/9801024v4
<category>
physics.class-ph
<abstract>
The Reply to G. W. Bruhn is added.

<id>
physics/9802018v1
<category>
physics.class-ph
<abstract>
An experiment was conducted with the intent to detect the effect, if any, of
the hypothetical centricoris acceleration of freely spinning electrons by the
use of an electromagnet with its conductor firmly clamped between its two
attracting magnetic plates. It was theorized that the freely spinning electrons
were uniformly oriented by the magnetic field, and the induced centricoris
accelerations should have their combined effect on the special electromagnet.
This electromagnet was installed on a digital scale. The effect of the expected
centricoris acceleration showed up, depending on the direction of the applied
electric potential, as the about +/- 0.05 gr variation of the electromagnet's
135.2 gr weight. Another experiment confirmed that the observed weight
variations must be attributed to the centricoris acceleration, to the lateral
drifts of the conductor's free electrons.

<id>
physics/9802037v2
<category>
physics.class-ph
<abstract>
Grasers based on a stimulated emission of gravitational radiation by
relativistic charged particle beams in external fields and on a conversion of
laser radiation into gravitational one in the magnetic fields as well as
detectors are discussed. A scheme of the gravitational radiation not
accompanied by an useless inaccessible by a value average power of the
electromagnetic radiation and a stimulation of the conversion of gravitons into
photons in gravitational detectors by an open resonator are considered.

<id>
physics/9804025v1
<category>
physics.class-ph
<abstract>
We studied the dynamics of surfacea and wake charges induced by a charged
particle traversing a dielectric slab. It is shown that after the crossing of
the slab first boundary, the induced on the slab surface charge (image charge)
is transformed into the wake charge, which overflows to the second boundary
when the particle crosses it. It is also shown, that the polarization of the
slab is of an oscillatory nature, and the net induced charge in a slab remains
zero at all stages of the motion.

<id>
physics/9805003v1
<category>
physics.class-ph
<abstract>
In the Comment [Eur. J. Phys., to be published] on our paper [Eur. J. Phys.
19 (1998) 1-6], the contributor declares that we claim that the ratio of inertial
mass to gravitational mass can be derived ex nihilo and that our paper was
published "by mistake". In this "Reply" we dispute the point of view of the
contributor.

<id>
physics/9807032v1
<category>
physics.class-ph
<abstract>
New, gauge-independent, second-order Lagrangian for the motion of classical,
charged test particles is used to derive the corresponding Hamiltonian
formulation. For this purpose a Hamiltonian description of the theories derived
from the second-order Lagrangian is presented. Unlike in the standard approach,
the canonical momenta arising here are explicitely gauge-invariant and have a
clear physical intepretation. The reduced symplectic form is equaivalent to the
Souriau's form. This approach illustrates a new method of deriving equations of
motion from field equations.

<id>
physics/9807038v1
<category>
physics.class-ph
<abstract>
New, gauge-independent, second-order Lagrangian for the motion of classical,
charged test particles is proposed. It differs from the standard,
gauge-dependent, first order Lagrangian by boundary terms only. A new method of
deriving equations of motion from field equations is developed. When applied to
classical electrodynamics, this method enables us to obtain unambigously the
above, second order Lagrangian from the general energy-momentum conservation
principle.

<id>
physics/9807055v1
<category>
physics.class-ph
<abstract>
It is demonstrated that any two reference frames (RFs), which are uniformly
and rectilinearly moving relative to each other, can be adjusted via (possibly
anisotropic) rescaling and re-synchronization so that the resulting pair of RFs
is Lorentzian; this statement remains true if the word "Lorentzian" is replaced
by "Galilean" or "Riemannian", i.e., if a finite positive value of $c^2$ is
replaced by $\infty$ or by a negative real number. In this particular sense,
the Lorentzian, as well as Galilean or Riemannian, phenomenon turns out to be
merely a matter of an arbitrary choice of appropriate rescaling and
re-synchronization of any given pair of RFs. Generalizations and refinements of
this result are obtained, including universal generalized Lorentzian adjustment
via rescaling and re-synchronization of arbitrarily large families of RFs.
Alternatively, the generalized Lorentzian property of a pair of RFs is shown to
be a consequence of reciprocity and isotropy, with no adjustment needed in this
case. The universality of light and of the corresponding Lorentzian property of
the spacetime is questioned. Waves of transformation of spacetime are
introduced, which have in a certain sense a more universal character than
electromagnetic or gravitational waves.

<id>
physics/9808007v2
<category>
physics.class-ph
<abstract>
We study, both classically and quantum-mechanically, the problem of a neutral
particle with spin, moving in one-dimension in an inhomogeneous magnetic field.
This problem serves for us as a toy model to study the trapping of neutral
particles. We identify K, the ratio between the precessional frequency of the
particle and its vibration frequency, as the relevant parameter of the problem.
Classicaly, we find that when the magnetic moment is antiparallel to the
direction of the magnetic field, the particle is trapped provided that K<0.5.
We also find that viscous friction, be it translational or precessional,
destabilizes the system. Quantum-mechanically, we study the problem of spin
half particle in the same field. Treating K as a small parameter for the
perturbation from the adiabatic Hamiltonian, we find the lifetime of the
particle in its trapped ground-state.

<id>
physics/9808042v1
<category>
physics.class-ph
<abstract>
Classical Stokes' drift is the small time-averaged drift velocity of
suspended non-diffusing particles in a fluid due to the presence of a wave. We
consider the effect of adding diffusion to the motion of the particles, and
show in particular that a non-zero time-averaged drift velocity exists in
general even when the classical Stokes' drift is zero. Our results are obtained
from a general procedure for calculating ensemble-averaged Lagrangian mean
velocities for motion that is close to Brownian, and are verified by numerical
simulations in the case of sinusoidal forcing.

<id>
physics/9809026v1
<category>
physics.class-ph
<abstract>
An accelerated classical point charge radiates at the Larmor power rate
$2e^2a^2/3$, leading to the expectation of an associated radiation reaction
force. The famous Abraham-Lorentz-Dirac proposal is plagued with difficulties.
Here we note a simple, amazing, and apparently overlooked fact: an accelerated
charge is also always absorbing power at exactly the Larmor rate. Implications
for radiation reaction and the particle motion are considered. Our analysis
supports Kijowski's recent proposal.

<id>
physics/9810037v1
<category>
physics.class-ph
<abstract>
There are reasons to believe that implications of a certain paradox
introduced by Ginzburg and related problems have not been fully recognized.
Pertinent issues remain open and unresolved. There are instances when the
current widely used theory yields results having no physical meaning. As a
result, the current widely used theory is not equipped to cope with some of the
most fundamental problems in physics. In the present investigation, a new
formulation is proposed based on certain non-Maxwellian quantities which occur
whenever there is absorption. In the proposed formulation, the inconsistencies
and contradictions disclosed by the Ginzburg Paradox have been removed.

<id>
physics/9811019v1
<category>
physics.class-ph
<abstract>
It is proved that in classical electrodynamics the uniform rectilinear motion
of charged "rigid" sphere with self-action (Sommerfeld model) is stable.

<id>
physics/9812012v1
<category>
physics.class-ph
<abstract>
Some recent experiments, performed at Berkeley, Cologne, Florence and Vienna
led to the claim that something seems to travel with a speed larger than the
speed c of light in vacuum. Various other experimental results seem to point in
the same direction: For instance, localized wavelet-type solutions of Maxwell
equations have been found, both theoretically and experimentally, that travel
with Superluminal speed. Even muonic and electronic neutrinos --it has been
proposed-- might be "tachyons", since their square mass appears to be negative;
not to mention the apparent Superluminal expansions observed in the core of
quasars and, recently, in the so-called galactic microquasars. In the first
part of this paper we verify, on the basis of the numerical solution of Maxwell
equations, that waves propagating down a microwave guide can travel with
Superluminal group velocity, just confirming some of the previously mentioned
experimental results. Then, we have to face the question of Superluminal
motions within the theory of Special Relativity. It is not widely recognized
that all such theoretical and experimental results do not place relativistic
causality in jeopardy. For instance, it is possible (at least in microphysics)
to solve also the known causal paradoxes, devised for "faster than light"
motion. Here we show, in detail and rigorously, how to solve the oldest causal
paradox, originally proposed by Tolman, which is the kernel of many further
tachyon paradoxes. The key to the solution is a careful application of tachyon
mechanics, as it unambiguously follows from Special Relativity. [Subj-classes:
Classical Physics, Special Relativity, Optics, General Physics, Microwaves,
Evanescent Waves, Tunnelling Photons]

<id>
physics/9812053v1
<category>
physics.class-ph
<abstract>
It recently has been demonstrated that signals conveyed by evanescent modes
can travel faster than light. In this report some special features of signals
are introduced and investigated, for instance the fundamental property that
signals are frequency band limited.
  Evanescent modes are characterized by extraordinary properties: Their energy
is {\it negative}, they are not directly measurable, and the evanescent region
is not causal since the modes traverse this region instantaneously. The study
demonstrates the necessity of quantum mechanical principles in order to
interprete the superluminal signal velocity of classical evanescent modes.

<id>
physics/9901049v1
<category>
physics.class-ph
<abstract>
In the paper [B.M.Bolotovskii, S.N.Stoliarov, Uspekhi Fizicheskich Nauk,
v.162, No 3, p.195, 1992] the energy conservation law was applied to a problem
of radiation of a charged particle in an external electromagnetic field. The
contributors consecutively and mathematically strictly solved the problem but
received wrong result. They derived an expression which includes a change of
the energies of the electromagnetic fields accompanying the moving particle
corresponding to the initial and final velocity of the particle. The energy of
the field accompanying the particle is the energy of the particle of the
electromagnetic origin. It should not enter the solution of the problem. The
origin of the mistake made by the contributors is discussed in this comment.

<id>
physics/9902008v1
<category>
physics.class-ph
<abstract>
In the framework of the classical Maxwell-Lorentz electrodynamics the energy
conservation law is reconsidered.

<id>
physics/9902018v3
<category>
physics.class-ph
<abstract>
Equation of motion of Sommerfeld sphere in the one-dimensional potential
hole, produced by two equal charges on some distance from each other, is
numerically investigated. Two types of solutions are found: (i) damping
oscillations, (ii) oscillations without damping (radiationless motion).
Solutions with growing amplitude ("climbing-up-the-wall solution") for chosen
initial conditions were not founded.

<id>
physics/9610003v1
<category>
physics.comp-ph
<abstract>
The spectral test of random number generators (R.R. Coveyou and R.D.
McPherson, 1967) is generalized. The sequence of random numbers is analyzed
explicitly not just via their n-tupel distributions. The generalized analysis
of many generators becomes possible due to a theorem on the harmonic analysis
of multiplicative groups of residue class rings. We find that the mixed
multiplicative generator with power of two modulus does not pass the extended
test with an ideal result. Best qualities has a new generator with the
recursion formula X(k+1)=a*X(k)+c*int(k/2) mod 2^d. We discuss the choice of
the parameters a, c for very large moduli 2^d and present an implementation of
the suggested generator with d=256, a=2^128+2^64+2^32+62181, c=(2^160+1)*11463.

<id>
physics/9610004v1
<category>
physics.comp-ph
<abstract>
The spectral test of random number generators (R.R. Coveyou and R.D.
McPherson, 1967) is generalized. The sequence of random numbers is analyzed
explicitly, not just via their n-tupel distributions. We find that the mixed
multiplicative generator with power of two modulus does not pass the extended
test with an ideal result. Best qualities has a new generator with the
recursion formula X(k+1)=a*X(k)+c*int(k/2) mod 2^d. We discuss the choice of
the parameters a, c for very large moduli 2^d and present an implementation of
the suggested generator with d=256, a=2^128+2^64+2^32+62181, c=(2^160+1)*11463.

<id>
physics/9610019v1
<category>
physics.comp-ph
<abstract>
The discrete class algorithm presented in this paper is an efficient
simulation tool for stochastic processes governed by a reasonably small set of
transition rates. The algorithm is presented, its performance compared to
prevailing methods and applications to epitaxial growth and neuronal models are
sketched.
  Source code is available from the contributor's WWW-site.

<id>
physics/9702021v2
<category>
physics.comp-ph
<abstract>
One of shortcomings of stepwise interval methods is the following. The
intervals determining the solution of a system are often expanded in the course
of time irrespective of the method and step used (the {\em Moore effect}). We
introduce the notion of general {\em interval spaces} and study the
infinitesimal Moore effect (IME) in these spaces. We obtain the local
conditions of absence of the IME in terms of Jacobi matrices field. The
relation between the absence of IME and simultaneous dissipativity of the
Jacobi matrices is established. We study simultaneously dissipative operators
in $\Bbb{R}^n$. A linear operator $A$ is {\em dissipative} with respect to a
norm $\|...\|$ if $\| \exp (At) \| \leq 1$ at all $t \geq 0$. For each norm,
the dissipative operator form a closed convex cone. An operator $A$ is {\em
stable dissipative} if it belongs to the interior of this cone. The family of
linear operators $\{A_\alpha \}$ is called {\em simultaneously dissipative}, if
there exists a norm with respect to which all the operators are dissipative. We
studied general properties of such families. For example, let the family
$\{A_\alpha \}$ be finite and generate a nilpotent Lee algebra and let for each
$A_\alpha$ there exist a norm with respect to which it is dissipative. Then
$\{A_\alpha \}$ is simultaneously dissipative. Let the family $\{A_\alpha \}$
be compact and generate solvable Lee algebra, and let the spectrum of each
operator $A_\alpha $ lie in the open left half-plane. Then $\{A_\alpha \}$ is
simultaneously stable dissipative, i.e. there exists a norm with respect to
which all $A_\alpha $ are stable dissipative. We study the conditions of
simultaneous dissipativity of the matrices of rank one and discussed their
application to equations of {\em mass action law} kinetics.

<id>
physics/9703024v1
<category>
physics.comp-ph
<abstract>
We analyze the structure of the periodic trajectories of the K-system
generator of pseudorandom numbers on rational sublattice which coincides with
the Galois field. The period of the trajectories increases as a function of
lattice size and the dimension of the K-matrix. We emphasize the connection of
this approach with the one which is based on primitive matrices over Galois
fields.

<id>
physics/9705041v1
<category>
physics.comp-ph
<abstract>
We present an efficient method for anealing 3d metallic grains using Gradient
Weighted Moving Finite Elements (GWMFE). The initial grain microstructure is
generated from Monte-Carlo evolution of a an effective discrete model on the
initial (frozen) unstructured grid generated for the external geometry of the
GWMFE calculation.

<id>
physics/9708009v3
<category>
physics.comp-ph
<abstract>
The rescaled range statistical analysis (R/S) is proposed as a new method to
detect correlations in pseudorandom number generators used in Monte Carlo
simulations. In an extensive test it is demonstrated that the RS analysis
provides a very sensitive method to reveal hidden long run and short run
correlations. Several widely used and also some recently proposed pseudorandom
number generators are subjected to this test. In many generators correlations
are detected and quantified.

<id>
physics/9709023v1
<category>
physics.comp-ph
<abstract>
Comptonization is the process in which photon spectrum changes due to
multiple Compton scatterings in the electronic plasma. It plays an important
role in the spectral formation of astrophysical X-ray and gamma-ray sources.
There are several intrinsic limitations for the analytical method in dealing
with the Comptonization problem and Monte Carlo simulation is one of the few
alternatives. We describe an efficient Monte Carlo method that can solve the
Comptonization problem in a fully relativistic way. We expanded the method so
that it is capable of simulating Comptonization in the media where electron
density and temperature varies discontinuously from one region to the other and
in the isothermal media where density varies continuously along photon paths.
The algorithms are presented in detail to facilitate computer code
implementation. We also present a few examples of its application to the
astrophysical research.

<id>
physics/9710028v1
<category>
physics.comp-ph
<abstract>
Monte Carlo (MC) methods for numerical integration seem to be embarassingly
parallel on first sight. When adaptive schemes are applied in order to enhance
convergence however, the seemingly most natural way of replicating the whole
job on each processor can potentially ruin the adaptive behaviour. Using the
popular VEGAS-Algorithm as an example an economic method of semi-micro
parallelization with variable grain-size is presented and contrasted with
another straightforward approach of macro-parallelization. A portable
implementation of this semi-micro parallelization is used in the xloops-project
and is made publicly available.

<id>
physics/9711019v1
<category>
physics.comp-ph
<abstract>
A method to compute the bound state eigenvalues and eigenfunctions of a
Schr\"{o}dinger equation or a spinless Salpeter equation with central
interaction is presented. This method is the generalization to the
three-dimensional case of the Fourier grid Hamiltonian method for
one-dimensional Schr\"{o}dinger equation. It requires only the evaluation of
the potential at equally spaced grid points and yields the radial part of the
eigenfunctions at the same grid points. It can be easily extended to the case
of coupled channel equations and to the case of non-local interactions.

<id>
physics/9802009v1
<category>
physics.comp-ph
<abstract>
We present a comparative study of the application of modern eigenvalue
algorithms to an eigenvalue problem arising in quantum physics, namely, the
computation of a few interior eigenvalues and their associated eigenvectors for
the large, sparse, real, symmetric, and indefinite matrices of the Anderson
model of localization. We compare the Lanczos algorithm in the 1987
implementation of Cullum and Willoughby with the implicitly restarted Arnoldi
method coupled with polynomial and several shift-and-invert convergence
accelerators as well as with a sparse hybrid tridiagonalization method. We
demonstrate that for our problem the Lanczos implementation is faster and more
memory efficient than the other approaches. This seemingly innocuous problem
presents a major challenge for all modern eigenvalue algorithms.

<id>
physics/9802014v1
<category>
physics.comp-ph
<abstract>
An iterative algorithm is presented for solving the RPA equations of linear
response. The method optimally computes the energy-weighted moments of the
strength function, allowing one to match the computational effort to the
intrinsic accuracy of the basic mean-field approximation, avoiding the problem
of solving very large matrices. For local interactions, the computational
effort for the method scales with the number of particles N_p as O(N_p^3).

<id>
physics/9803011v1
<category>
physics.comp-ph
<abstract>
Wavelets are a powerful new mathematical tool which offers the possibility to
treat in a natural way quantities characterized by several length scales. In
this article we will show how wavelets can be used to solve partial
differential equations which exhibit widely varying length scales and which are
therefore hardly accessible by other numerical methods. As a benchmark
calculation we solve Poisson's equation for a 3-dimensional Uranium dimer. The
length scales of the charge distribution vary by 4 orders of magnitude in this
case. Using lifted interpolating wavelets the number of iterations is
independent of the maximal resolution and the computational effort therefore
scales strictly linearly with respect to the size of the system.

<id>
physics/9804019v1
<category>
physics.comp-ph
<abstract>
We propose here a new model of encryption of binary data taking advantage of
the complexity associated with delayed dynamics. In this scheme, the encryption
process is a coupling dynamics with various time delays between different bits
in the original data. It is shown that decoding of the encrypted data is
extremely difficult without a complete knowledge of coupling manner with
associated delays for all bits of the data.

<id>
physics/9805002v1
<category>
physics.comp-ph
<abstract>
The main formalisms of partial level densities (PLD) used in preequilibrium
nuclear reaction models, based on the equidistant spacing model (ESM), are
considered. A collection of FORTRAN77 functions for PLD calculation by using 14
formalisms for the related partial-state densities is provided and 28 sample
cases (73 versions) are described. The results are given in graphic form too.
Composite (recommended) formulas, which include the optional use of various
corrections, i.e. the advanced pairing and shell correction in addition to the
Pauli effect, and average energy-dependent single-particle level densities for
the excited particles and holes, are also given. The formalism comprises the
density of particle-hole bound states, and the effects of an exact correction
for the Pauli-exclusion principle are considered. Keywords: Partial nuclear
level density; Nuclear level density; Single-particle level density;
Equidistant-spacing model; Preequilibrium emission; Nuclear reactions

<id>
physics/9805011v1
<category>
physics.comp-ph
<abstract>
The exchange-correlation energy in Kohn-Sham density functional theory is
expressed as a functional of the electronic density and the Kohn-Sham orbitals.
An alternative to Kohn-Sham theory is to express the energy as a functional of
the reduced first-order density matrix or equivalently the natural orbitals. In
the former approach the unknown part of the functional contains both a kinetic
and a potential contribution whereas in the latter approach it contains only a
potential energy and consequently has simpler scaling properties. We present an
approximate, simple and parameter-free functional of the natural orbitals,
based solely on scaling arguments and the near satisfaction of a sum rule. Our
tests on atoms show that it yields on average more accurate energies and charge
densities than the Hartree Fock method, the local density approximation and the
generalized gradient approximations.

<id>
physics/9806033v1
<category>
physics.comp-ph
<abstract>
In the past many papers have appeared which simulated surface growth with
different growth models. The results showed that, if models differed only
slightly in their `growth' rules, the resulting surfaces may belong to
different universality classes, i.e. they are described by different
differential equations. In the present paper we describe a mapping of ``growth
rules'' to differential operators and give plausibility arguments for this
mapping. We illustrate the validity of our theory by applying it to published
results.

<id>
physics/9807017v2
<category>
physics.comp-ph
<abstract>
In previous papers [Phys. Rev. A {\bf 41}, 4501 (1990), Phys. Rev. E {\bf
18}, 3178 (1993)], simple equilibrium expressions were obtained for nonlinear
Burnett coefficients. A preliminary calculation of a 32 particle Lennard-Jones
fluid was presented in the previous paper. Now, sufficient resources have
become available to address the question of whether nonlinear Burnett
coefficients are finite for soft spheres. The hard sphere case is known to have
infinite nonlinear Burnett coefficients (ie a nonanalytic constitutive
relation) from mode coupling theory. This paper reports a molecular dynamics
caclulation of the third order nonlinear Burnett coefficient of a Lennard-Jones
fluid undergoing colour flow, which indicates that this term is diverges in the
thermodynamic limit.

<id>
physics/9808030v1
<category>
physics.comp-ph
<abstract>
The `lid' algorithm performs an exhaustive exploration of neighborhoods of
local energy minima of energy landscapes. This paper describes an
implementation of the algorithm, including issues of parallel performance and
scalability. To illustrate the versatility of the approach and to stress the
common features present in landscapes of quite different systems, we present
selected results for 1) a spin glass, 2) a ferromagnet, 3) a covalent network
model for glassy systems, and 4) a polymer. The exponential nature of the local
density of states found in these systems and its relation to the ordering
transition is briefly commented upon.

<id>
physics/9810003v1
<category>
physics.comp-ph
<abstract>
We have studied the driving forces governing reconstructions on polar GaN
surfaces employing first-principles total-energy calculations. Our results
reveal properties not observed for other semiconductors, as for example a
strong tendency to stabilize Ga-rich surfaces. This mechanism is shown to have
important consequences on various surface properties: Novel and hitherto
unexpected structures are stable, surfaces may become metallic although GaN is
a wide-bandgap semiconductor, and the surface energy is significantly higher
than for other semiconductors. We explain these features in terms of the small
lattice constant of GaN and the unique bond strength of nitrogen molecules.

<id>
physics/9810015v1
<category>
physics.comp-ph
<abstract>
We present a C program to compute by successive approximations the regular
order reduction of a large class of ordinary differential equations, which
includes evolution equations in electrodynamics and gravitation. The code may
also find the regular order reduction of delay-differential equations.

<id>
physics/9812003v1
<category>
physics.comp-ph
<abstract>
An analytical-numeric calculation method of extremely complicated integrals
is presented. These integrals appear often in magnet soliton theory. The
appropriate analytical continuation and a corresponding integration contour
allow to reduce the calculation of wide class of integrals to a numeric search
of integrand denominator roots (in a complex plane) and a subsequent residue
calculations. The acceleration of series convergence of residue sum allows to
reach the high relative accuracy limited only by roundoff error in case when
$10\div 15$ terms are taken into account. The circumscribed algorithm is
realized in the C program and tested on the example allowing analytical
solution. The program was also used to calculate some typical integrals that
can not be expressed through elementary functions. In this case the control of
calculation accuracy was made by means of one-dimensional numerical integration
procedure.

<id>
physics/9901027v1
<category>
physics.comp-ph
<abstract>
A new algorithm for numerical integration of the rigid-body equations of
motion is proposed. The algorithm uses the leapfrog scheme and the quantities
involved are angular velocities and orientational variables which can be
expressed in terms of either principal axes or quaternions. Due to specific
features of the algorithm, orthonormality and unit norms of the orientational
variables are integrals of motion, despite an approximate character of the
produced trajectories. It is shown that the method presented appears to be the
most efficient among all known algorithms of such a kind.

<id>
physics/9901033v1
<category>
physics.comp-ph
<abstract>
The objective of this article is to study the behavior of electromagnetic
field under X-ray diffraction by time-dependent deformed crystals. Derived
system of differential equations looks like the Takagi equations in the case of
non-stationary crystals. This is a system of multidimensional first-order
hyperbolic equations with complex time-dependent coefficients. Efficient
difference schemes based on the multicomponent modification of the alternating
direction method are proposed. The stability and convergence of devised schemes
are proved. Numerical results are shown for the case of an ideal crystal, a
crystal heated uniformly according to a linear law and a time-varying bent
crystal. Detailed numerical studies indicate the importance of consideration
even small crystal changes.

<id>
physics/9903008v1
<category>
physics.comp-ph
<abstract>
We investigate a novel stochastic technique for the global optimization of
complex potential energy surfaces (PES) that avoids the freezing problem of
simulated annealing by allowing the dynamical process to tunnel energetically
inaccessible regions of the PES by way of a dynamically adjusted nonlinear
transformation of the original PES. We demonstrate the success of this
approach, which is characterized by a single adjustable parameter, for three
generic hard minimization problems.

<id>
physics/9903044v1
<category>
physics.comp-ph
<abstract>
In this paper we demonstrate that multi-modal Probability Distribution
Functions (PDFs) may be efficiently sampled using an algorithm originally
developed for numerical integrations by Monte-Carlo methods. This algorithm can
be used to generate an input PDF which can be used as an independence sampler
in a Metropolis-Hastings chain to sample otherwise troublesome
distributions.Some examples in one two and five dimensions are worked out.

<id>
physics/9905035v2
<category>
physics.comp-ph
<abstract>
It is shown that MS Fortran-77 compilers allow to construct recursive
subroutines. The recursive one-dimensional adaptive quadrature subroutine is
considered in particular. Despite its extremely short body (only eleven
executable statements) the subroutine proved to be very effective and
competitive. It was tested on various rather complex integrands. The
possibility of function calls number minimization by choosing the optimal
number of Gaussian abscissas is considered. The proposed recursive procedure
can be effectively applied for creating more sophisticated quadrature codes
(one- or multi-dimensional) and easily incorporated into existing programs.

<id>
physics/9908008v3
<category>
physics.comp-ph
<abstract>
For plane-wave and many-spiral states of the experimentally based Luo-Rudy 1
model of heart tissue in large (8 cm square) domains, we show that an explicit
space-time-adaptive time-integration algorithm can achieve an order of
magnitude reduction in computational effort and memory - but without a
reduction in accuracy - when compared to an algorithm using a uniform
space-time mesh at the finest resolution. Our results indicate that such an
explicit algorithm can be extended straightforwardly to simulate quantitatively
large-scale three-dimensional electrical dynamics over the whole human heart.

<id>
physics/9909002v1
<category>
physics.comp-ph
<abstract>
A logical explanation as to why the choice of the Pian-Sumihara basis (as a
linear basis to approximate stress) leads to greater efficiency in enhanced
strain problems, is presented. An Airy stress function and the consequent
selective simplification resulting from the differentiation of an implied,
single, parent approximating polynomial, are the essence of this argument.

<id>
physics/9909042v1
<category>
physics.comp-ph
<abstract>
A simple and explicit technique for the numerical solution of the
two-particle, time-dependent Schr\"{o}dinger equation is assembled and tested.
The technique can handle interparticle potentials that are arbitrary functions
of the coordinates of each particle, arbitrary initial and boundary conditions,
and multi-dimensional equations. Plots and animations are given here and on the
World Wide Web of the scattering of two wavepackets in one dimension.

<id>
physics/9701026v2
<category>
physics.data-an
<abstract>
Gaussian processes are a natural way of defining prior distributions over
functions of one or more input variables. In a simple nonparametric regression
problem, where such a function gives the mean of a Gaussian distribution for an
observed response, a Gaussian process model can easily be implemented using
matrix computations that are feasible for datasets of up to about a thousand
cases. Hyperparameters that define the covariance function of the Gaussian
process can be sampled using Markov chain methods. Regression models where the
noise has a t distribution and logistic or probit models for classification
applications can be implemented by sampling as well for latent values
underlying the observations. Software is now available that implements these
methods using covariance functions with hierarchical parameterizations. Models
defined in this way can discover high-level properties of the data, such as
which inputs are relevant to predicting the response.

<id>
physics/9706015v1
<category>
physics.data-an
<abstract>
Conventional statistics begins with a model, and assigns a likelihood of
obtaining any particular set of data. The opposite approach, beginning with the
data and assigning a likelihood to any particular model, is explored here for
the case of points drawn randomly from a continuous probability distribution. A
scalar field theory is used to assign a likelihood over the space of
probability distributions. The most likely distribution may be calculated,
providing an estimate of the underlying distribution and a convenient graphical
representation of the raw data. Fluctuations around this maximum likelihood
estimate are characterized by a robust measure of goodness-of-fit. Its
distribution may be calculated by integrating over fluctuations. The resulting
method of data analysis has some advantages over conventional approaches.

<id>
physics/9706025v1
<category>
physics.data-an
<abstract>
Bayesian inference --- although becoming popular in physics and chemistry ---
is hampered up to now by the vagueness of its notion of prior probability. Some
of its supporters argue that this vagueness is the unavoidable consequence of
the subjectivity of judgements --- even scientific ones. We argue that priors
can be defined uniquely if the statistical model at hand possesses a symmetry
and if the ensuing confidence intervals are subjected to a frequentist
criterion. Moreover, it is shown via an example taken from recent experimental
nuclear physics, that this procedure can be extended to models with broken
symmetry.

<id>
physics/9707002v1
<category>
physics.data-an
<abstract>
We consider noise-driven exit from a domain of attraction in a
two-dimensional bistable system lacking detailed balance. Through analog and
digital stochastic simulations, we find a theoretically predicted bifurcation
of the most probable exit path as the parameters of the system are changed, and
a corresponding nonanalyticity of the activation energy. We also investigate
the extent to which the bifurcation is related to the local breaking of
time-reversal invariance.

<id>
physics/9710015v1
<category>
physics.data-an
<abstract>
BAYES-LIN is an extension of the LISP-STAT object-oriented statistical
computing environment, which adds to LISP-STAT some object prototypes
appropriate for carrying out local computation via message-passing between
clique-tree nodes of Bayes linear belief networks. Currently the BAYES-LIN
system represents a rather low-level set of tools for a back-end computational
engine, together with diagnostic graphics for understanding the effects of
adjustments on the moral graph. A GUI front end, allowing interactive
formulation of DAG models could be easily added, but is currently missing from
the system. This document provides a very brief introduction to the system, by
means of a work-through of two example computations, followed by a list of
variables, functions, objects and methods provided by the system.

<id>
physics/9712022v1
<category>
physics.data-an
<abstract>
The characterization of non-stationary signals requires joint time and
frequency information. However, time (t) and frequency (omega) being
non-commuting variables there cannot be a joint probability density in the
(t,omega) plane and the time-frequency distributions, that have been proposed,
have difficult interpretation problems arising from negative or complex values
and spurious components. As an alternative we propose to obtain time-frequency
information by looking at the marginal distributions along rotated directions
in the (t,omega) plane. The rigorous probability interpretation of the marginal
distributions avoids all interpretation ambiguities. Applications to signal
analysis and signal detection are discussed as well as an extension of the
method to other pairs of non-commuting variables.

<id>
physics/9712035v1
<category>
physics.data-an
<abstract>
Principal component analysis (PCA) algorithms use neural networks to extract
the eigenvectors of the correlation matrix from the data. However, if the
process is non-Gaussian, PCA algorithms or their higher order generalisations
provide only incomplete or misleading information on the statistical properties
of the data. To handle such situations we propose neural network algorithms,
with an hybrid (supervised and unsupervised) learning scheme, which constructs
the characteristic function of the probability distribution and the transition
functions of the stochastic process. Illustrative examples are presented, which
include Cauchy and Levy-type processes

<id>
physics/9712041v2
<category>
physics.data-an
<abstract>
Completely automatic and adaptive non-parametric inference is a pie in the
sky. The frequentist approach, best exemplified by the kernel estimators, has
excellent asymptotic characteristics but it is very sensitive to the choice of
smoothness parameters. On the other hand the Bayesian approach, best
exemplified by the mixture of gaussians models, is optimal given the observed
data but it is very sensitive to the choice of prior. In 1984 the contributor
proposed to use the Cross-Validated gaussian kernel as the likelihood for the
smoothness scale parameter h, and obtained a closed formula for the posterior
mean of h based on Jeffreys's rule as the prior. The practical operational
characteristics of this bayes' rule for the smoothness parameter remained
unknown for all these years due to the combinatorial complexity of the formula.
It is shown in this paper that a version of the metropolis algorithm can be
used to approximate the value of h producing remarkably good completely
automatic and adaptive kernel estimators. A close study of the form of the
cross validated likelihood suggests a modification and a new approach to
Bayesian Non-parametrics in general.

<id>
physics/9805018v1
<category>
physics.data-an
<abstract>
A method to approximate continuous multi-dimensional probability density
functions (PDFs) using their projections and correlations is described. The
method is particularly useful for event classification when estimates of
systematic uncertainties are required and for the application of an unbinned
maximum likelihood analysis when an analytic model is not available. A simple
goodness of fit test of the approximation can be used, and simulated event
samples that follow the approximate PDFs can be efficiently generated. The
source code for a FORTRAN-77 implementation of this method is available.

<id>
physics/9807018v1
<category>
physics.data-an
<abstract>
It is well known that the output of a Neural Network trained to disentangle
between two classes has a probabilistic interpretation in terms of the
a-posteriori Bayesian probability, provided that a unary representation is
taken for the output patterns. This fact is used to make Neural Networks
approximate probability density functions from examples in an unbinned way,
giving a better performace than ``standard binned procedures''. In addition,
the mapped p.d.f. has an analytical expression.

<id>
physics/9809035v1
<category>
physics.data-an
<abstract>
Conventional classical confidence intervals in specific cases are unphysical.
A solution to this problem has recently been published by Feldman and Cousins.
We show that there are cases where the new approach is not applicable and that
it does not remove the basic deficiencies of classical confidence limits.

<id>
physics/9810018v1
<category>
physics.data-an
<abstract>
A parametric method similar to autoregressive spectral estimators is proposed
to determine the probability density function (pdf) of a random set. The method
proceeds by maximizing the likelihood of the pdf, yielding estimates that
perform equally well in the tails as in the bulk of the distribution. It is
therefore well suited for the analysis short sets drawn from smooth pdfs and
stands out by the simplicity of its computational scheme. Its advantages and
limitations are discussed.

<id>
physics/9811003v1
<category>
physics.data-an
<abstract>
An adaptive kernel method in the Bayesian framework together with a new
simulation program for Rutherford backscattering spectroscopy (RBS) have been
applied to the analysis of RBS data. Even in the case of strongly overlapping
RBS peaks a depth profile reconstruction without noise fitting has been
achieved. The adaptive kernel method leads to the simplest depth profile
consistent with the data. Erosion and redeposition rates of carbon divertor
plates in the fusion experiment ASDEX Upgrade could be determined by
RBS-analysis of thin film probes before and after exposition to plasma
discharges.

<id>
physics/9812030v1
<category>
physics.data-an
<abstract>
A new method of background subtraction is presented which uses the concept of
a signal estimator to construct a confidence level which is always conservative
and which is never better than e^-s. The new method yields stronger exclusions
than the Bayesian method with a flat prior distribution.

<id>
physics/9812036v3
<category>
physics.data-an
<abstract>
A suggestion is made for improving the Feldman Cousins method of estimating
signal counts in the presence of background. The method concentrates on finding
essential information about the signal and ignoring extraneous information
about background. An appropriate method is found which uses the condition that
the number of background events obtained does not exceed the total number of
events obtained. Several alternative approaches are explored.

<id>
physics/9901002v1
<category>
physics.data-an
<abstract>
Noise induced jumping between meta-stable states in a potential depends on
the structure of the noise. For an $\alpha$-stable noise, jumping triggered by
single extreme events contributes to the transition probability. This is also
called Levy flights and might be of importance in triggering sudden changes in
geophysical flow and perhaps even climatic changes. The steady state statistics
is also influenced by the noise structure leading to a non-Gibbs distribution
for an $\alpha$-stable noise.

<id>
physics/9901050v2
<category>
physics.data-an
<abstract>
This note deals with a multivariate stochastic approach to forecast the
behaviour of a cyclic time series. Particular attention is devoted to the
problem of the prediction of time behaviour of sunspot numbers for the current
23th cycle. The idea is to consider the previous known n cycles as n particular
realizations of a given stochastic process. The aim is to predict the future
behaviour of the current n+1th realization given a portion of the curve and the
structure of the previous n realizations. The model derived is based on the
cross-correlations between the current n+1th realization and the previous n
ones and the solution of the related least squares problem. As example we
applied the method to smoothed monthly sunspots numbers from SIDC archives, in
order to predict the behaviour of the current 23th solar cycle.

<id>
physics/9902043v1
<category>
physics.data-an
<abstract>
The features of the HEMAS code are presented. The results of the comparison
between the Monte Carlo expectation and the experimental data are shown.

<id>
physics/9903029v2
<category>
physics.data-an
<abstract>
The definition of nonequilibrium entropy is provided for the general
nonequilibrium processes by connecting thermodynamics with statistical physics,
and the principle of entropy increment in the nonequilibrium processes is also
proved in the paper. The result shows that the definition of nonequilibrium
entropy is not unique.

<id>
physics/9906010v1
<category>
physics.data-an
<abstract>
The interpretation of new particle search results involves a confidence level
calculation on either the discovery hypothesis or the background-only ("null")
hypothesis. A typical approach uses toy Monte Carlo experiments to build an
expected experiment estimator distribution against which an observed
experiment's estimator may be compared. In this note, a new approach is
presented which calculates analytically the experiment estimator distribution
via a Fourier transform, using the likelihood ratio as an ordering estimator.
The analytic approach enjoys an enormous speed advantage over the toy Monte
Carlo method, making it possible to quickly and precisely calculate confidence
level results.

<id>
physics/9906012v1
<category>
physics.data-an
<abstract>
The problem of modeling forest tree growth curves with an artificial neural
network (NN) is examined. The NN parametric form is shown to be a suitable
model if each forest tree plot is assumed to consist of several differently
growing sub-plots. The predictive Bayesian approach is used in estimating the
NN output.
  Data from the correlated curve trend (CCT) experiments are used. The NN
predictions are compared with those of one of the best parametric solutions,
the Schnute model. Analysis of variance (ANOVA) methods are used to evaluate
whether any observed differences are statistically significant. From a
Frequentist perspective the differences between the Schnute and NN approach are
found not to be significant. However, a Bayesian ANOVA indicates that there is
a 93% probability of the NN approach producing better predictions on average.

<id>
physics/9906064v1
<category>
physics.data-an
<abstract>
An expression is proposed for determining the error caused on entropy
estimates by finite sample effects. This expression is based on the Ansatz that
the ranked distribution of probabilities tends to follow an empirical Zipf law.

<id>
physics/9909033v1
<category>
physics.data-an
<abstract>
Publication bias arises whenever the probability that a study is published
depends on the statistical significance of its results. This bias, often called
the file-drawer effect since the unpublished results are imagined to be tucked
away in researchers' file cabinets, is potentially a severe impediment to
combining the statistical results of studies collected from the literature.
With almost any reasonable quantitative model for publication bias, only a
small number of studies lost in the file-drawer will produce a significant
bias. This result contradicts the well known Fail Safe File Drawer (FSFD)
method for setting limits on the potential harm of publication bias, widely
used in social, medical and psychic research. This method incorrectly treats
the file drawer as unbiased, and almost always misestimates the seriousness of
publication bias. A large body of not only psychic research, but medical and
social science studies, has mistakenly relied on this method to validate
claimed discoveries. Statistical combination can be trusted only if it is known
with certainty that all studies that have been carried out are included. Such
certainty is virtually impossible to achieve in literature surveys.

<id>
physics/9911077v1
<category>
physics.data-an
<abstract>
Nonparametric Bayesian approaches based on Gaussian processes have recently
become popular in the empirical learning community. They encompass many
classical methods of statistics, like Radial Basis Functions or various
splines, and are technically convenient because Gaussian integrals can be
calculated analytically. Restricting to Gaussian processes, however, forbids
for example the implemention of genuine nonconcave priors. Mixtures of Gaussian
process priors, on the other hand, allow the flexible implementation of complex
and situation specific, also nonconcave "a priori" information. This is
essential for tasks with, compared to their complexity, a small number of
available training data. The paper concentrates on the formalism for Gaussian
regression problems where prior mixture models provide a generalisation of
classical quadratic, typically smoothness related, regularisation approaches
being more flexible without having a much larger computational complexity.

<id>
physics/9912034v1
<category>
physics.data-an
<abstract>
We give an overview of track fitting using the Kalman filter method in the
NOMAD detector at CERN, and emphasize how the wealth of by-product information
can be used to analyze track breakpoints (discontinuities in track parameters
caused by scattering, decay, etc.). After reviewing how this information has
been previously exploited by others, we describe extensions which add power to
breakpoint detection and characterization. We show how complete fits to the
entire track, with breakpoint parameters added, can be easily obtained from the
information from unbroken fits. Tests inspired by the Fisher F-test can then be
used to judge breakpoints. Signed quantities (such as change in momentum at the
breakpoint) can supplement unsigned quantities such as the various chisquares.
We illustrate the method with electrons from real data, and with Monte Carlo
simulations of pion decays.

<id>
physics/0001031v1
<category>
physics.data-an
<abstract>
Roe and Woodroofe (RW) have suggested that certain conditional probabilities
be incorporated into the ``unified approach'' for constructing confidence
intervals, previously described by Feldman and Cousins (FC). RW illustrated
this conditioning technique using one of the two prototype problems in the FC
paper, that of Poisson processes with background. The main effect was on the
upper curve in the confidence belt. In this paper, we attempt to apply this
style of conditioning to the other prototype problem, that of Gaussian errors
with a bounded physical region. We find that the lower curve on the confidence
belt is also moved significantly, in an undesirable manner.

<id>
physics/0003002v1
<category>
physics.data-an
<abstract>
We evaluate the exact equilibrium distribution of gas molecules adsorbed on
an active surface with an infinite number of attachment sites. Our result is a
Poisson distribution having mean $X = {\mu P P_s \over P_e}$, with $\mu$ the
mean gas density, $ P_s$ the sticking probability, $P_e$ the evaporation
probability in a time interval $\tau$, and $P$ Smoluchowski's exit probability
in time interval $\tau$ for the surface in question. We then solve for the case
of a finite number of attachment sites using the mean field approximation,
recovering in this case the Langmuir isotherm.

<id>
physics/0003006v1
<category>
physics.data-an
<abstract>
Using the electric and coupling approaches, we derive a series of results
concerning the mixing times for the stratified random walk on the d-cube,
inspired in the results of Chung and Graham (1997) Stratified random walks on
the n-cube.

<id>
physics/0003086v1
<category>
physics.data-an
<abstract>
Three independent techniques are used to separate fine structure from the
absorption spectra, the background function in which is approximated by (i)
smoothing spline. We propose a new reliable criterion for determination of
smoothing parameter and the method for raising of stability with respect to
k_min variation; (ii) interpolation spline with the varied knots; (iii) the
line obtained from bayesian smoothing. This methods considers various prior
information and includes a natural way to determine the errors of XAFS
extraction. Particular attention has been given to the estimation of
uncertainties in XAFS data. Experimental noise is shown to be essentially
smaller than the errors of the background approximation, and it is the latter
that determines the variances of structural parameters in subsequent fitting.

<id>
physics/0003087v1
<category>
physics.data-an
<abstract>
The problem of error analysis is addressed in stages beginning with the case
of uncorrelated parameters and proceeding to the Bayesian problem that takes
into account all possible correlations when a great deal of prior information
about the accessible parametr space is available. The formulas for the standard
deviations and deviations with arbitrary confidence levels are derived.
Underestimation of the errors of XAFS-function extraction is shown to be a
source of unjustified optimistic errors of fitting parameters. The applications
of statistical chi^2- and F-tests to the fitting problems are also discussed.

<id>
physics/9610007v1
<category>
physics.flu-dyn
<abstract>
When an asymmetric bubble collapses it generally produces a well defined high
velocity jet. This is remarkable because one might expect such a collapse to
produce a complex or chaotic flow rather than an ordered one. I present a
dimensional argument for the ubiquity of jets from collapsing bubbles, and
model the aspherical collapse of a bubble with pieces of Rayleigh's solution
for spherical collapse and its cylindrical analogue. This model explains the
ubiquity of jet formation in aspherical collapse, and predicts the shape and
velocity profile of the resulting jet. These predictions may be tested in the
laboratory or by numerical calculation. An application to solid spall is
suggested.

<id>
physics/9705020v1
<category>
physics.flu-dyn
<abstract>
A problem of great concern in aviation and submarine propulsion is the
control of the boundary layer and, in particular, the methods to extend the
laminar region as a means to decrease noise and fuel consumption. In this paper
we study the flow of air along an airfoil when a layer of ionized gas and a
longitudinal electric field are created in the boundary layer region. By
deriving scaling solutions and more accurate numerical solutions we discuss the
possibility of achieving significant boundary layer control for realistic
physical parameters. Practical design formulas and criteria are obtained. We
also discuss the perspectives for active control of the laminar-to-turbulent
transition fluctuations by electromagnetic field modulation.

<id>
physics/9705024v1
<category>
physics.flu-dyn
<abstract>
The stability of shear flows of electrically conducting fluids, with respect
to finite amplitude three-dimensional localized disturbances is considered. The
time evolution of the fluid impulse integral, characterizing such disturbances,
for the case of low magnetic Reynolds number is obtained by integrating
analytically the vorticity equation. Analysis of the resulted equation reveals
a new instability criterion.

<id>
physics/9705036v1
<category>
physics.flu-dyn
<abstract>
We investigate a two-dimensional network simulator capable of modeling
different time dependencies in two-phase drainage displacements. In particular,
we focus on the temporal evolution of the pressure due to capillary and viscous
forces and the time dependence of the interface between the two liquids. The
dynamics of the capillary effect are taken into account and we report on high
accuracy pressure measurements. Moreover, the simulator includes important
features in drainage, like burst dynamics of the invading fluid and
simultaneous flow of two liquids in a section of a tube. The validity of the
model is checked by comparing simulation results with well known experimental
properties in drainage displacement.

<id>
physics/9706005v1
<category>
physics.flu-dyn
<abstract>
We have calculated the general dispersion relationship for surface waves on a
ferrofluid layer of any thickness and viscosity, under the influence of a
uniform vertical magnetic field. The amplification of these waves can induce an
instability called peaks instability (Rosensweig instability). The expression
of the dispersion relationship requires that the critical magnetic field and
the critical wavenumber of the instability depend on the thickness of the
ferrofluid layer. The dispersion relationship has been simplified into four
asymptotic regimes: thick or thin layer and viscous or inertial behaviour. The
corresponding critical values are presented. We show that a typical parameter
of the ferrofluid enables one to know in which regime, viscous or inertial, the
ferrofluid will be near the onset of instability.

<id>
physics/9708007v1
<category>
physics.flu-dyn
<abstract>
We present a simple viscous theory of free-surface flows in boundary layers,
which can accommodate regions of separated flow. In particular this yields the
structure of stationary hydraulic jumps, both in their circular and linear
versions, as well as structures moving with a constant speed. Finally we show
how the fundamental hydraulic concepts of subcritical and supercritical flow,
originating from inviscid theory, emerge at intermediate length scales in our
model.

<id>
physics/9710011v1
<category>
physics.flu-dyn
<abstract>
The possibility is considered that turbulence is described by differential
equations for which uniqueness fails maximally, at least in some limit. The
inviscid Burgers equation, in the context of Onsager's suggestion that
turbulence should be described by a negative absolute temperature, is such a
limit. In this picture, the onset of turbulence coincides with the
proliferation of singularities which characterizes the failure of uniqueness.

<id>
physics/9712015v1
<category>
physics.flu-dyn
<abstract>
Stability criterion for the surface gravity capillary waves in a flowing
two-layered fluid system with viscous dissipation is investigated. It is seen
that the dissipative instability of negative energy waves is absent,- contrary
to what earlier contributors have concluded. Their error is identified to arise from
an erroneous choice of the dissipation law, in which the wave profile velocity
is wrongly equated to the particle velocity. Our corrected dissipation law is
also shown to restore Galilean invariance to the stability condition of the
system.

<id>
physics/9712050v2
<category>
physics.flu-dyn
<abstract>
When time reversal is broken the viscosity tensor can have a non vanishing
odd part. In two dimensions, and only then, such odd viscosity is compatible
with isotropy. Elementary and basic features of odd viscosity are examined by
considering solutions of the wave and Navier-Stokes equations for hypothetical
fluids where the stress is dominated by odd viscosity.

<id>
physics/9801035v1
<category>
physics.flu-dyn
<abstract>
In this paper we discuss a theoretical model for the interfacial profiles of
progressive non-linear waves which result from introducing a triangular
obstacle, of finite height, attached to the bottom below the flow of a
stratified, ideal, two layer fluid, bounded from above by a rigid boundary. The
derived equations are solved by using a nonlinear perturbation method. The
dependence of the interfacial profile on the triangular obstacle size, as well
as its dependence on some flow parameters, such as the ratios of depths and
densities of the two fluids, have been studied.

<id>
physics/9803027v1
<category>
physics.flu-dyn
<abstract>
We study low-speed flows of a highly compressible, single-phase fluid in the
presence of gravity, for example in a regime appropriate for modeling recent
space-shuttle experiments on fluids near the liquid-vapor critical point. In
the equations of motion, we include forces due to capillary stresses that arise
from a contribution made by strong density gradients to the free energy. We
derive formally simplified sets of equations in a low-speed limit analogous to
the zero Mach number limit in combustion theory.
  When viscosity is neglected and gravity is weak, the simplified system
includes: a hyperbolic equation for velocity, a parabolic equation for
temperature, an elliptic equation related to volume expansion, an
integro-differential equation for mean pressure, and an algebraic equation (the
equation of state). Solutions are determined by initial values for the mean
pressure, the temperature field, and the divergence-free part of the velocity
field. To model multidimensional flows with strong gravity, we offer an
alternative to the anelastic approximation, one which admits stratified fluids
in thermodynamic equilibrium, as well as gravity waves but not acoustic waves.

<id>
physics/9806028v1
<category>
physics.flu-dyn
<abstract>
Renormalization group has enjoyed successes in other areas of statistical
physics. However, its application to turbulence faces several technical
difficulties, which have had to be circumvented by uncontrolled approximations.
Indeed, in view of the deterministic nature of the Navier-Stokes equations, it
is clear that the operation of averaging out the high-wavenumber modes while
keeping the low-wavenumber modes constant, cannot be done rigorously and in
itself can only be an approximation.
  With points like this in mind, we have recently adopted direct numerical
simulation as a tool for probing the basic feasibility of using RG techniques
to reduce the number of degrees of freedom requiring to be numerically
simulated. In this paper, we present some of the first results of this
approach.

<id>
physics/9806029v1
<category>
physics.flu-dyn
<abstract>
In this paper we consider the properties of the internal partitions of the
nonlinear term, obtained when a filter with a sharp cutoff is introduced in
wavenumber space. We see what appears to be some degree of independence of the
choice of the position of the cutoff wavenumber for both instantaneous and
time-integrated partitioned nonlinearities. We also investigate the basic idea
of an eddy-viscosity model for subgrid terms and have found that while phase
modelling will be very poor, amplitude modelling can be far more successful.

<id>
physics/9809024v1
<category>
physics.flu-dyn
<abstract>
The linear stability of inviscid, incompressible, two-dimensional, plane
parallel, shear flow was considered over a century ago by Rayleigh, Kelvin, and
others. A principal result on the subject is Rayleigh's celebrated inflection
point theorem {R80}, which states that for an equilibrium flow to be unstable,
the equilibrium velocity profile must contain an inflection point. That is, if
the velocity profile is given by $U(y)$, where $y$ is the cross-stream
coordinate, then there must be a point, $y=y_I$, for which $U''(y_I)=0$. Much
later, in 1950, Fj{\o}rtoft {F50} generalized the theorem by showing that,
moreover, if there is one inflection point, then $U'''(y_I)/U'(y_I)<0$ is
required for instability (see {Bar} for further extensions). Both Rayleigh's
Theorem and Fj{\o}rtoft's subsequent generalization are necessary conditions
for instability, but they are not sufficient. That is, even though an
equilibrium profile may contain a vorticity minimum, it is not necessarily
unstable. The point of this paper is to derive, for a large class of
equilibrium velocity profiles, a condition that is necessary and sufficient for
instability.

<id>
physics/9812002v1
<category>
physics.flu-dyn
<abstract>
We found the energetic stability criteria of non-equilibrium gase plain
flows.

<id>
physics/9812008v1
<category>
physics.flu-dyn
<abstract>
The classical problem of the fluid mechanics is the problem of a supersonic
motion around a thin body was generalized to the case of non-equilibrium gas.
The drag and lift force coefficients were founded. It is shown that the drag
and lift force coefficients in the acoustically active supersonic flow are both
decreased

<id>
physics/9901014v1
<category>
physics.flu-dyn
<abstract>
A model for the formation of cavitation nuclei in liquids has recently been
presented with basis in interfacial liquid tension at non-planar solid surfaces
of concave form. In the present paper investigations of water-solid interfaces
by atomic force microscopy are reported to illuminate experimentally effects of
interfacial liquid tension. The results support that such tension occurs and
that voids develop at solid-liquid interfaces.

<id>
physics/9902003v3
<category>
physics.flu-dyn
<abstract>
The two-dimensional flow of viscous incompressible liquid in a square cavity
with a free boundary and differentially heated vertical sides is considered in
the present work. The influence of gravitational and thermocapillary convection
on temperature and velocity fields is studied in large range of dimensionless
parameters and similarity criteria using equations in a Boussinesq
approximation. Limiting cases of dimensionless parameters are analyzed
numerically.

<id>
physics/9902071v1
<category>
physics.flu-dyn
<abstract>
We address the dynamics of a drop with viscosity $\lambda \eta$ breaking up
inside another fluid of viscosity $\eta$. For $\lambda=1$, a scaling theory
predicts the time evolution of the drop shape near the point of snap-off which
is in excellent agreement with experiment and previous simulations of Lister
and Stone. We also investigate the $\lambda$ dependence of the shape and
breaking rate.

<id>
physics/9903017v1
<category>
physics.flu-dyn
<abstract>
When two drops of radius $R$ touch, surface tension drives an initially
singular motion which joins them into a bigger drop with smaller surface area.
This motion is always viscously dominated at early times. We focus on the
early-time behavior of the radius $\rmn$ of the small bridge between the two
drops. The flow is driven by a highly curved meniscus of length $2\pi \rmn$ and
width $\Delta\ll\rmn$ around the bridge, from which we conclude that the
leading-order problem is asymptotically equivalent to its two-dimensional
counterpart. An exact two-dimensional solution for the case of inviscid
surroundings [Hopper, J. Fluid Mech. ${\bf 213}$, 349 (1990)] shows that
$\Delta \propto \rmn^3$ and $\rmn \sim (t\gamma/\pi\eta)\ln [t\gamma/(\eta
R)]$; and thus the same is true in three dimensions. The case of coalescence
with an external viscous fluid is also studied in detail both analytically and
numerically. A significantly different structure is found in which the outer
fluid forms a toroidal bubble of radius $\Delta \propto \rmn^{3/2}$ at the
meniscus and $\rmn \sim (t\gamma/4\pi\eta) \ln [t\gamma/(\eta R)]$. This basic
difference is due to the presence of the outer fluid viscosity, however small.
With lengths scaled by $R$ a full description of the asymptotic flow for
$\rmn(t)\ll1$ involves matching of lengthscales of order $\rmn^2, \rmn^{3/2}$,
\rmn$, 1 and probably $\rmn^{7/4}$.

<id>
physics/9903047v1
<category>
physics.flu-dyn
<abstract>
A number of simplified dynamical problems is studied in an attempt to clarify
some of the mechanisms leading to turbulence and the existing proposals to
control this transition. A simplified set of boundary layer equations displays
a solution that corresponds to the rolls and streaks instability and exhibits
its streamwise localized nature. The effect of random phases as a device to
control the transfer of energy to the small scales is studied both for the
shell and the boundary layer models. In spite of the simplified nature of the
models, they also provide some insight on the prospects for active turbulence
control by external body forces.

<id>
physics/9904030v1
<category>
physics.flu-dyn
<abstract>
An approximate method to compute mean velocity profiles in turbulent flows is
developed. This approach is based on the equation connecting the Reynolds
stress and mean velocity. By using the measured values of pressure drop and
average (bulk) flow velocity, such characteristics of a turbulent flow as the
mean velocity and Reynolds stress distributions were calculated and compared
with the experimental measurements in different pipes and channels for various
fluids and gases. These show good agreement for a wide range of Reynolds
numbers.

<id>
physics/9904044v1
<category>
physics.flu-dyn
<abstract>
We report the first extensive experimental observation of the two-dimensional
enstrophy cascade, along with the determination of the high order vorticity
statistics. The energy spectra we obtain are remarkably close to the Kraichnan
Batchelor expectation. The distributions of the vorticity increments, in the
inertial range, deviate only little from gaussianity and the corresponding
structure functions exponents are indistinguishable from zero. It is thus shown
that there is no sizeable small scale intermittency in the enstrophy cascade,
in agreement with recent theoretical analyses.

<id>
physics/9905012v1
<category>
physics.flu-dyn
<abstract>
The instability of a streak and its nonlinear evolution are investigated by
direct numerical simulation (DNS) for plane Poiseuille flow at Re=3000. It is
suggested that there exists a traveling-wave solution (TWS). The TWS is
localized around one of the two walls and notably resemble to the coherent
structures observed in experiments and DNS so far. The phase space structure
around this TWS is similar to a saddle point. Since the stable manifold of this
TWS is extended close to the quasi two dimensional (Q2D) energy axis, the
approaching process toward the TWS along the stable manifold is approximately
described as the instability of the streak (Q2D flow) and the succeeding
nonlinear evolution. Bursting corresponds to the escape from the TWS along the
unstable manifold. These manifolds constitute part of basin boundary of the
turbulent state.

<id>
physics/9907041v1
<category>
physics.flu-dyn
<abstract>
The skin of Tursiops Truncatus is corrugated with small, quasi-periodic
ridges running circumferentially about the torso. These ridges extend into the
turbulent boundary layer and affect the evolution of coherent structures. The
development and evolution of coherent structures over a surface is described by
the formation and dynamics of vortex filaments. The dynamics of these filaments
over a flat, non-ridged surface is determined analytically, as well as through
numerical simulation, and found to agree with the observations of coherent
structures in the turbulent boundary layer. The calculation of the linearized
dynamics of the vortex filament, successful for the dynamics of a filament over
a flat surface, is extended and applied to a vortex filament propagating over a
periodically ridged surface. The surface ridges induce a rapid parametric
forcing of the vortex filament, and alter the filament dynamics significantly.
A consideration of the contribution of vortex filament induced flow to energy
transport indicates that the behavior of the filament induced by the ridges can
directly reduce surface drag by up to 8%. The size, shape, and distribution of
cutaneous ridges for Tursiops Truncatus is found to be optimally configured to
affect the filament dynamics and reduce surface drag for swimming velocities
consistent with observation.

<id>
physics/9908006v1
<category>
physics.flu-dyn
<abstract>
We had observed a couple of new phenomena in which, two liquid bulks in
contact with each other do not coalesce. The main reason seems to be the motion
of the surface in the place of contact, which forms an air film between the
bulk boundaries. The surface velocities is also estimated and showed to be near
each other and also near the surface velocities in the similar phenomena
reported before.

<id>
physics/9911002v1
<category>
physics.flu-dyn
<abstract>
A flux of ideal fluid coupled to perturbation is investigated by
nonperturbative methods of the quantum field theory. Asymptotic behavior of the
flux coupled to perturbation turns out to be similiar to that of superfluids.

<id>
physics/9911003v1
<category>
physics.flu-dyn
<abstract>
The response of inviscid incompressible unbounded fluid subject to a
localized external perturbation is studed. The physically relevant hypotheses
on the mode coupling mechanisma is justified by renormalization group method.
The scaling laws for the scalar and vector velocity potentials are derived. The
spectrum of energy of perturbed fluid versus the distance apart from
perturbation is computed.

<id>
physics/9911078v1
<category>
physics.flu-dyn
<abstract>
Combining direct computations with invariance arguments, Taylor's
constitutive equation for an emulsion can be extrapolated to high shear rates.
We show that the resulting expression is consistent with the rigorous limits of
small drop deformation and that it bears a strong similarity to an a priori
unrelated rheological quantity, namely the dynamic (frequency dependent) linear
shear response. More precisely, within a large parameter region the nonlinear
steady-state shear viscosity is obtained from the real part of the complex
dynamic viscosity, while the first normal stress difference is obtained from
its imaginary part. Our experiments with a droplet phase of a binary polymer
solution (alginate/caseinate) can be interpreted by an emulsion analogy. They
indicate that the predicted similarity rule generalizes to the case of
moderately viscoelastic constituents that obey the Cox-Merz rule.

<id>
physics/9912009v1
<category>
physics.flu-dyn
<abstract>
A fluid jet with a finite angular velocity is subject to centripetal forces
in addition to surface tension forces. At fixed angular momentum, centripetal
forces become large when the radius of the jet goes to zero. We study the
possible importance of this observation for the pinching of a jet within a
slender jet model. A linear stability analysis shows the model to break down at
low viscosities. Numerical simulations indicate that angular momentum is
expelled from the pinch region so fast that it becomes asymptotically
irrelevant in the limit of the neck radius going to zero.

<id>
physics/9612004v1
<category>
physics.gen-ph
<abstract>
A compact analysis of development and prospects in the study of the
tunnelling evolution is given. A new systematization of various approaches to
defining tunnelling times in the light of time as a quantum mechanical
observable is proposed. The problem of superluminal group velocities, without
violations of special relativity, is also taken in account. Then a particular
attention is devoted to the presentation of new results on the analogy between
particle and photon tunnelling and analysis of the causality validity during
tunnelling. [PACS nos. 03.40.Kf, 03.50.De, 41.20.Jb, 41.20.Bt, 42.25.Bs,
03.30.+p, 03.65.-w].

<id>
physics/9612010v1
<category>
physics.gen-ph
<abstract>
A new, very different physical model of the universe is proposed. Its virtues
include unifying relativity and quantum mechanics, and particles with de
Broglie waves. It also appears to provide a truly unified physical basis for
electromagnetic, gravitational and nuclear forces. The basic system is a
four-dimensional Euclidean space, containing an array of nonlinear "flow"
waves. These repeat in one dimension, called [phi]. As in Newtonian mechanics,
time is treated as an additional, unidirectional parameter describing the
evolution of the system. Nevertheless, this wave system is shown be inherently
relativistic. Further self-organizing patterns arise within the overall wave
structure. Called "wavicles," these have intrinsic quantized fields, "spin,"
and rest energy, and represent elementary particles. Relativistic expressions
are derived for particle behavior in scalar, vector and gravitational
potentials. Proper representations of these potentials, based on the wave
fields and associated [phi] flows of wavicles, are also obtained. As in the
causal quantum mechanics of de Broglie and Bohm, wavicles exist continuously
and follow definite, stochastic trajectories. Although experimentally
equivalent to Einstein's special relativity, this theory differs fundamentally
from his general relativity and the associated Big Bang model. According to
Linde, the latter predict a large-scale space-time curvature roughly 60 orders
of magnitude greater than observed values. Here a flat large-scale geometry is
predicted, in agreement with the observed distribution of galaxies. This theory
is also consistent with recent observations pertaining to the age of the
universe.

<id>
physics/9701003v1
<category>
physics.gen-ph
<abstract>
We identify a number of problematic aspects of current classical and quantum
theories of antimatter; we introduce a new mathematical formalism which is an
antiautomorphic image of that of matter equivalent to charge conjugation at the
operator level, but applicable from Newton's equations to quantum mechanics; we
show that the emerging new theory of antimatter recovers known experimental
data on electroweak interactions; we finally identity the following predictions
of the theory: 1) reversal in the field of matter of the gravitational
curvature (antigravity) for stable antiparticles and their bound states, such
as the anti-hydrogen atom; 2) conventional (attractive) gravity for a bound
state of an elementary particle and its antiparticle, such as the positronium;
and 3) prediction that the anti- hydrogen atom emits a new photon which
coincides with the conventional photon for all electroweak interactions but
experiences repulsion in the gravitational field of matter.

<id>
physics/9702026v1
<category>
physics.gen-ph
<abstract>
If textbook Lorentz invariance is actually a property of the equations
describing a sector of matter above some critical distance scale, several
sectors of matter with different critical speeds in vacuum can coexist and an
absolute rest frame (the vacuum rest frame, possibly related to the local rest
frame of the expanding Universe) may exist without contradicting the apparent
Lorentz invariance felt by "ordinary" particles (particles with critical speed
in vacuum equal to c , the speed of light). The real geometry of space-time
will then be different from standard Lorentz invariance, and the Poincare
relativity principle will be a local (in space and time), approximate sectorial
property. It seems natural to assume that particles with critical speed in
vacuum different from c are superluminal. We illustrate such a scenario using
as an example a spinorial space-time where the modulus of the spinor,
associated to the time variable, is the size of an expanding Universe. Several
properties of superluminal particles, and of matter without a universal
relativity principle, are discussed in view of experimental applications. If
the vacuum rest frame is close to that suggested by the cosmic microwave
background, experimental searches for superluminal particles on earth should
mainly contemplate a laboratory speed range around 10E3 c , even for very high
energy superluminal cosmic rays. The detectability of several consequences of
the new scenario is briefly discussed.

<id>
physics/9703006v1
<category>
physics.gen-ph
<abstract>
In the framework of linear transformations between inertial systems, there
are no physical reasons for assuming any anisotropy in the one-way velocity of
light.

<id>
physics/9703020v1
<category>
physics.gen-ph
<abstract>
If textbook Lorentz invariance is actually a property of the equations
describing a sector of matter above some critical distance scale, several
sectors of matter with different critical speeds in vacuum can coexist and an
absolute rest frame (the vacuum rest frame, possibly related to the local rest
frame of the expanding Universe) may exist without contradicting the apparent
Lorentz invariance felt by "ordinary" particles (particles with critical speed
in vacuum equal to $c$ , the speed of light). Sectorial Lorentz invariance,
reflected by the fact that all particles of a given dynamical sector have the
same critical speed in vacuum, will then be an expression of a fundamental
sectorial symmetry (e.g. preonic grand unification or extended supersymmetry)
protecting a parameter of the equations of motion. We study the breaking of
Lorentz invariance in such a scenario, with emphasis on mixing between the
"ordinary" sector and a superluminal sector, and discuss with examples the
consequences of existing data. The sectorial universality of the value of the
high-energy speed in vacuum, even exact, does not necessarily imply that
Lorentz invariance is not violated and does not by itself exclude the
possibility to produce superluminal particles at accelerators or to find them
in experiments devoted to high-energy cosmic rays. Similarly, the stringent
experimental bounds on Lorentz symmetry violation at low energy cannot be
extrapolated to high-energy phenomena. Several basic questions related to
possible effects of Lorentz symmetry violation are discussed, and potential
signatures are examined.

<id>
physics/9703023v2
<category>
physics.gen-ph
<abstract>
Einstein rejected the differential law of energy-momentum conservation $
T^{\mu\nu}_{;\nu} = 0 $. In doing so, he violated the principle of general
covariance. Here, we prove the conservation law $ T^{\mu\nu}_{;\nu} = 0 $ and
discuss its significance for general relativity.

<id>
physics/9703025v1
<category>
physics.gen-ph
<abstract>
The baryon overdensity and the matching of the big bang explosion energy with
gravitation can be solved by a cyclical baryonic bounce model with correction
to the stress-energy tensor. Subtracting accretion energy from the CMBR allows
enough baryons in nucleosynthesis to close the universe. Collapse to infinite
density states must be prevented by energy losses at supranuclear densities. As
long as the Einstein tensor is coupled to the stress-energy tensor, any quantum
correction must involve an energy sink.

<id>
physics/9704014v1
<category>
physics.gen-ph
<abstract>
We outline the basic principles and the needed experiments for a conceivable
new recycling of nuclear waste by the power plants themselves to avoid its
transportation and storage to a (yet unknown) dumping area. Details are
provided in an adjoining paper and in patents pending.

<id>
physics/9704015v1
<category>
physics.gen-ph
<abstract>
We present a new realization of relativistic hadronic me- chanics and its
underlying iso-Poincar'e symmetry specifically constructed for nuclear physics
which: 1) permits the representation of nucleons as ex- tended, nonspherical
and deformable charge distributions with alterable mag- netic moments yet
conventional angular momentum and spin; 2) results to be a nonunitary
``completion'' of relativistic quantum mechanics much along the EPR argument;
yet 3) is axiom-preserving, thus preserves conventional quantum laws and the
axioms of the special relativity. We show that the proposed new formalism
permits the apparently first exact representation of the total magnetic moments
of new-body nuclei under conventional physical laws. We then point out that, if
experimentally confirmed the alterability of the intrinsic characteristics of
nucleons would imply new forms of recycling nuclear waste by the nuclear power
plants in their own site, thus avoiding its transportation and storage in a
(yet unidentified) dumping area. A number of possible, additional basic
advances are also indicated, such as: new un- derstanding of nuclear forces
with nowel nonlinear, nonlocal and nonunitary terms due to mutual penetrations
of the hyperdense nucleons; consequential new models of nuclear structures; new
magnetic confinement of the controlled fusion taking into account the possible
alterability of the intrinsic magnetic moments of nucleons at the initiation of
the fusion process; new sources of en- ergy based on subnuclear processes; and
other possible advances. The paper ends with the proposal of three experiments,
all essential for the continuation of scientific studies and all of basic
character, relatively moderate cost and full feasibility in any nuclear
physical laboratory.

<id>
physics/9704017v1
<category>
physics.gen-ph
<abstract>
If textbook Lorentz invariance is actually a property of the equations
describing a sector of the excitations of vacuum above some critical distance
scale, several sectors of matter with different critical speeds in vacuum can
coexist and an absolute rest frame (the vacuum rest frame) may exist without
contradicting the apparent Lorentz invariance felt by "ordinary" particles
(particles with critical speed in vacuum equal to $c$ , the speed of light).
The sectorial Lorentz symmetry may be only a low-energy limit, in the same way
as the relation $\omega $ (frequency) = $c_s$ (speed of sound) $k$ (wave
vector) holds for low-energy phonons in a crystal. We study the consequences of
such a scenario, using an ansatz inspired by the Bravais lattice as a model for
some vacuum properties. It then turns out that: a) the Greisen-Zatsepin-Kuzmin
cutoff on high-energy cosmic protons and nuclei does no longer apply; b)
high-momentum unstable particles have longer lifetimes than expected with exact
Lorentz invariance, and may even become stable at the highest observed cosmic
ray energies or slightly above. Some cosmological implications of superluminal
particles are also discussed.

<id>
physics/9704024v1
<category>
physics.gen-ph
<abstract>
We generalize the geometry of Santilli's locally anisotropic and
inhomogeneous isospaces to the geometry of vector isobundles provided with
nonlinear and distinguished isoconnections and isometric structures. We
present, apparently for the first time, the isotopies of Lagrange, Finsler and
Kaluza--Klein spaces. We also continue the study of the interior, locally
anisotropic and inhomogeneous gravitation by extending the isoriemannian
space's constructions and presenting a geometric background for the theory of
isofield interactions in generalized isolagrange and isofinsler spaces.

<id>
physics/9705001v1
<category>
physics.gen-ph
<abstract>
An inspection of the contemporary physics literature reveals that, while
matter is treated at all levels of study, from Newtonian mechanics to quantum
field theory, antimatter is solely treated at the level of second quantization.
For the purpose of initiating the restoration of full equivalence in the
treatments of matter and antimatter in due time, in this paper we present a
classical representation of antimatter which begins at the primitive Newtonian
level with expected images at all subsequent levels. By recalling that charge
conjugation of particles into antiparticles is anti-automorphic, the proposed
theory of antimatter is based on a new map, called isoduality, which is also
anti-automorphic, yet it is applicable beginning at the classical level and
then persists at the quantum level. As part of our study, we present novel
anti-isomorphic isodual images of the Galilean, special and general
relativities and show the compatibility of their representation of antimatter
with all available classical experimental knowledge, that on electromagnetic
interactions. We also identify the prediction of antigravity for antimatter in
the field of matter (or vice-versa) without any claim on its validity, and
defer its resolution to specific experiments. To avoid a prohibitive length,
the paper is restricted to the classical treatment which had not been
sufficiently treated until now. Studies on operator profiles, such as the
equivalence of isoduality and charge conjugation and the implication of the
isodual theory in particle physics, are conducted in a separate paper.

<id>
physics/9705007v7
<category>
physics.gen-ph
<abstract>
A theoretical framework supported by literature reported experimental
evidence (Homes, Harshman along with Voyager, Hubble and EGRET space platforms
and others) is presented which indicates that superconductivity is a self
energy phenomenon and congruent with the concept of the Charge Conjugation,
Parity Change and Time Reversal (CPT) theorem. A resonant symmetric structure
is proposed as an extension of Bardeen Cooper and Schrieffer (BCS) theory,
which suspends Lorentz transforms at superluminal velocities in the context of
the de Broglie hypothesis. A momentum and energy conserving (elastic) CPT
resonant structural lattice scalable over 15 orders of magnitude from nuclear
to universe dimensions and associated superconducting theory is postulated
whereby nuclear (quark) weak and strong forces, electromagnetic and
gravitational forces are mediated by a particle of resonant velocity
transformed mass (mt) (110.123 x electron mass or 56 Mev/c2), The universe mass
and density are based on an isotropic homogeneous media filling the vacuum of
and could be considered a candidate for dark matter/energy. The model predicts
a deceleration value consistent with observed Pioneer 10 and 11 deep space
translational and rotational deceleration and consistent with the notion that,
An object moving through momentum space will slow down.

<id>
physics/9705014v2
<category>
physics.gen-ph
<abstract>
Can the Past be restored? Poincar\'e and Costa de Beauregard showed that the
past is not restored statistically. This follows from Bayes formula. In given
note two new Principles are postulated which forbid the restoration of the
Past.

<id>
physics/9705015v1
<category>
physics.gen-ph
<abstract>
We submit a classical unification of the special and general relativities via
the new isominkowskian geometry in which the two relativities are
differentiated by the basic unit. We then show that the unification admits an
operator image in which gravitation verifies the abstract axioms of
relativistic quantum mechanics under a universal symmetry which is isomorphic
to the Poincar\'{e} symmetry. The compliance of the unification with available
experimental data is indicated. This study has been permitted by the recent
achievement of sufficient mathematical maturity in memoir$^{3f}$, axiomatic
consistency in memoir$^{3t}$ and generalized symmetry principles in
memoir$^{4v}$. More detailed studies are presented in the forthcoming
paper$^{3u}$.

<id>
physics/9705016v1
<category>
physics.gen-ph
<abstract>
We recall that the Minkowskian geometry possesses basic units of space and
time which are invariant under the Poincar\'{e} symmetry. We then show that, by
comparison, the Riemannian geometry possesses space-time units which are not
invariant under the symmetries of the Riemannian line element, thus causing
evident physical ambiguities. We therefore introduce a novel formulation of
general relativity in the isominkowskian geometry which is an axiom-preserving
lifting of the conventional Minkowskian geometry but which nevertheless admits
all possible Riemannian metrics thanks to a (positive-definite) $4 \times 4$
generalization of the basic unit. We construct the universal symmetry of the
isominkowskian line elements called isopoincar\'{e} symmetry}, prove that it is
locally isomorphic to the conventional Poincar\'{e} symmetry, and show that, in
this way, conventional Riemannian metrics and related field equations can be
expressed with respect to invariant generalized units of space and time. We
then show that the isominkowskian geometry and related isopoincar\'{e} symmetry
permit: I) A classical geometric unification of the general and special
relativities for matter into a formulation called isospecial relativity in
which the former occurs for generalized units while admitting the latter as a
particular case for conventional units; II) A novel operator formulation of
gravity for matter based on the abstract axioms of relativistic quantum
mechanics, thus showing hope for a possible resolution of the ambiguities in
current theories of quantum gravity; and III) A novel classical and operator
formulation of antimatter which is an antiautomorphic image of the preceding
formulations for matter constructed via a map called isoduality. The

<id>
physics/9705018v1
<category>
physics.gen-ph
<abstract>
In this note we outline the history of q-deformations; indicate their
physical shortcomings; suggest their apparent resolution via an invariant
formulation based on a new mathematics of genotopic type; and point out their
expected physical significance once formulated in an invariant way.

<id>
physics/9705019v2
<category>
physics.gen-ph
<abstract>
The hypothesis about possible existence of new class of particles able to
travel faster then light as a source of dark matter, recently formulated by L.
Gonzalez-Mesters, is analyzed. To this end the general geometrical model for
several kinds of matter with different Lorentziann structures coexisting on the
same manifold is introduced and the local energy density in cosmological
reference frame is calculated in two particular cases. It is shown that the
local energy density is positive in both considered cases and hence such models
really may describe cosmological dark matter. Nevertheless, some problems may
appear during the construction of the cosmological models or the models of the
compact objects. Moreover, the simplest generalization of the model lead to
some variants of vector-tensor theory of gravitation with preferable reference
frame which contradict to observations.

<id>
physics/9705030v1
<category>
physics.gen-ph
<abstract>
We generalize the geometry of Santilli's locally anisotropic and
inhomogeneous isospaces to the geometry of vector isobundles provided with
nonlinear and distinguished isoconnections and isometric structures. We
present, apparently for the first time, the isotopies of Lagrange, Finsler and
Kaluza-Klein spaces. We also continue the study of the interior, locally
anisotropic and inhomogeneous gravitation by extending the isoriemannian
space's constructions and presenting a geometric background for the theory of
isofield interactions in generalized isolagrange and isofinsler spaces.

<id>
physics/9705031v1
<category>
physics.gen-ph
<abstract>
Special relativity has been tested at low energy with great accuracy, but its
extrapolation to very high-energy phenomena is much less well established.
Introducing a critical distance scale, a , below 10E-25 cm (the wavelength
scale of the highest-energy observed cosmic rays) allows to consider models,
compatible with standard tests of special relativity, where a small violation
of Lorentz symmetry (a can, for instance, be the Planck length) produces
dramatic effects on the properties of high-energy cosmic rays. Not only the
Greisen-Zatsepin-Kuzmin (GZK) cutoff on very high-energy protons and nuclei
does no longer apply, but particles which are unstable at low energy (neutron,
several nuclei, some hadronic resonances like the Delta++...) would become
stable at very high energy. The muon would also become stable or very long
lived at very high energy if one of the two neutrinos associated to the light
charged leptons (electron, muon) has a mass. Similar considerations apply to
the tau lepton. We discuss several possible scenarios originating these
phenomena, as well as the cosmic ray energy range (well below the energy scale
associated to the fundamental length) and experiments where they could be
detected. Observable effects are predicted for the highest-energy cosmic rays.

<id>
physics/9705032v1
<category>
physics.gen-ph
<abstract>
Lorentz symmetry has been tested at low energy with great accuracy, but its
extrapolation to very high-energy phenomena is much less well established. We
expect a possible breaking of Lorentz symmetry to be a very high energy and
very short distance phenomenon, compatible with existing data. If textbook
special relativity is only an approximate property of the equations describing
a sector of matter above some critical distance scale, superluminal sectors of
matter may exist related to new degrees of freedom not yet discovered
experimentally. The new superluminal particles ("superbradyons") would have
positive mass and energy, and behave kinematically like "ordinary" particles
(those with critical speed in vacuum equal to c , the speed of light) apart
from the difference in critical speed (we expect c_i >> c , where c_i is the
critical speed of a superluminal sector of matter). At speed v > c , they are
expected to release "Cherenkov" radiation ("ordinary" particles) in vacuum. If
superluminal particles exist, they could provide most of the cosmic (dark)
matter and produce very high-energy cosmic rays compatible with unexplained
discoveries reported in the literature. We discuss: a) the possible relevance
of superluminal matter to the composition, sources and spectra of high-energy
cosmic rays; b) signatures and experiments allowing to possibly explore such
effects.

<id>
physics/9705043v1
<category>
physics.gen-ph
<abstract>
YBa2Cu3O7 high temperature superconductor samples were weighed on an
electronic balance during a warming cycle beginning at 77 degrees K. The
experiment was configured so that the YBa2Cu3O7 material was weighed along with
a magnet, a target mass, and liquid Nitrogen coolant. The weights were captured
during Nitrogen evaporation. Results indicated unexpected variations in the
system weight that appear as a function of temperature and possibly other
parameters.

<id>
physics/9706001v1
<category>
physics.gen-ph
<abstract>
In a preceeding paper alternative reflections on gravitation were developed.
There it was assumed that the primary interaction between two masses is not of
attractive but of repulsive nature. The repulsive force results from the impuls
transfer produced by the gravitational radiation which is emitted and absorbed
by both masses. The observed attractive force between the two masses according
to Newton`s law of gravitation, however, is a secondary effect and a
consequence of the existence of all the masses in the universe. The mutual
screening of the gravitational radiation of all masses of the universe by the
two masses under consideration leads to the gravitational attraction between
them. The balance between primary, repulsive and secondary, attractive forces
can stabilize highly concentrated spherical mass accumulations with a linear
dependence of their mass on the square of their diameter. Such objects can
really be observed in the universe in the form of globular star clusters,
elliptical galaxies and spherical clusters of galaxies. The scatter of the data
of every group is rather large. But the collection of the objects of all three
groups, reaching from the smallest globular star cluster to the largest
spherical cluster of galaxies, with masses differing by almost 12 orders of
magnitude, clearly shows the proposed mass-diameter relation.

<id>
physics/9706002v1
<category>
physics.gen-ph
<abstract>
It is assumed that the primary interaction between two masses m1 and m2 is
not attractive as postulated by Newton's law of gravitation, but repulsive.
Both m1 and m2 emit and absorb gravitational radiation. Corresponding to the
laws of optics the absorption is connected with an impulse transfer that
produces the repulsive force. If, however, m1 and m2 are embedded in the
gravitational radiation produced by all the masses of the universe the
absorption by m1 and m2 leads to a reduction of the intensity of the
gravitational radiation between them, thus creating an attractive force exactly
as described by Newton's law. The so called universal gravitational constant is
no constant. It locally depends on the arrangement of the masses in the
universe. It can accept high values which are usually explained by the
existence of dark matter. Due to the primary forces of repulsive nature between
all masses the expansion of the universe is an intrinsic property. The balance
between primary, repulsive and secondary, attractive gravitational forces can
stabilize highly concentrated mass accumulations as they are observed in
globular clusters and the bulges of galaxies.

<id>
physics/9706011v1
<category>
physics.gen-ph
<abstract>
The existence of precise particle trajectories in any quantum state is
accounted for in a consistent way by allowing delocalization of the particle
charge. The relativistic mass of the particle remains within a small volume
surrounding a singularity moving along the particle trajectory. The singularity
is the source of an electric displacement field. The field induces a
polarization charge in the vacuum and this charge is equated with the charge of
the particle. Under dynamic conditions a distributed charge density rho(x,t) is
induced in the vacuum. The volume integral of the charge density is equal to
the charge of the particle and is rigorously conserved. The charge density is
derived from a complex-valued physical field psi(x,t) such that rho(x,t) =
|psi(x,t)|^2. The position probability density is equated with the mean charge
density. The mean field psi(x,t) for many sample realizations with a given
energy E and potential V(x) is the sum of the individual fields. In order for
the sum to be non-zero, the components in the spectral decompositions of the
individual fields must be spatially coherent. The particle has a spin frequency
given by Planck's relation hv = T - V + mc^2, where T is the kinetic energy,
determined from the momentum p and V is a quantum potential such that E = T + V
is conserved. The instantaneous phase of the spin is given by the phase of
exp(ikx) in the spectral decomposition a(k) of psi(x,t). It is spatially
coherent, due to the dependence on x. The momentum probability distribution is
given by the squared magnitude of the coefficients a(k). The Schrodinger
equation is derived by requiring local conservation of mean energy.

<id>
physics/9706012v2
<category>
physics.gen-ph
<abstract>
We introduce a dual lifting of unified gauge theories, the first
characterized by the isotopies, which are axiom- preserving maps into broader
structures with positive-definite generalized units used for the representation
of matter under the isotopies of the Poincare' symmetry, and the second
characterized by the isodualities, which are anti-isomorphic maps with
negative-definite generalized units used for the representation of antimatter
under the isodualities of the Poincare' symmetry. We then submit, apparently
for the first time, a novel grand unification with the inclusion of gravity for
matter embedded in the generalized positive-definite units of unified gauge
theories while gravity for antimatter is embedded in the isodual isounit. We
then show that the proposed grand unification provides realistic possibilities
for a resolution of the axiomatic incompatibilities between gravitation and
electroweak interactions due to curvature, antimatter and the fundamental
space-time symmetries.

<id>
physics/9706022v1
<category>
physics.gen-ph
<abstract>
We discuss the implications of a recently proposed pattern of Lorentz
symmetry violation on very high-energy cross sections. As a consequence of the
breaking of local Lorentz invariance by the introduction of a fundamental
length, $a$ , the kinematics is modified and the properties of final states are
fundamentally different in collider-like (two incoming particles with equal,
opposite momenta with respect to the vacuum rest frame) and fixed-target (one
of the incoming particles at rest with respect to the vacuum rest frame)
situations. In the first case, the properties of the allowed final states are
similar to relativistic kinematics, as long as the relevant wave vectors are
much smaller than the critical wave vector scale $a^{-1}$ . But, if one of the
incoming particles is close to rest in the vacuum rest frame, energy
conservation reduces the final-state phase space at very high energy and can
lead to a sharp fall of cross sections starting at incoming-particle wave
vectors well below the inverse of the fundamental length. Then, the Froissart
bound may cease to be relevant, as total cross sections seem to become much
smaller than it would be allowed by local, Lorentz-invariant, field theory.
Important experimental implications of the new scenario are found for
cosmic-ray astrophysics and for very high-energy cosmic rays reaching the
earth.

<id>
physics/9706032v1
<category>
physics.gen-ph
<abstract>
Special relativity has been tested at low energy with great accuracy, but
these results cannot be extrapolated to the very high-energy region.
Introducing a critical distance scale, $a$ , below 10E-25 cm (the wavelength
scale of the highest-energy observed cosmic rays) allows to consider models,
compatible with standard tests of special relativity, where a small violation
of Lorentz symmetry ($a$ can, for instance, be the Planck length, around 10E-33
cm) produces dramatic effects on the interaction properties of very high-energy
particles. Lorentz symmetry violation may potentially solve all the basic
problems raised by the highest-energy cosmic rays (origin and energy,
propagation...). Furthermore, superluminal sectors of matter may exist and
release very high-energy ordinary particles or directly produce very
high-energy cosmic-ray events with unambiguous signatures in very large
detectors. We discuss these phenomena, as well as the cosmic-ray energy range
(well below the energy scale associated to the fundamental length) and
experiments where they could be detected and studied.

<id>
physics/9706033v2
<category>
physics.gen-ph
<abstract>
The spacetime singularity in relativistic cosmology is cancelled by using an
additional variable. That is, the singularity-free models for an expanding
universe are obtained from general relativity.

<id>
physics/9806031v1
<category>
physics.geo-ph
<abstract>
Based on waveform data from a profile of aftershocks following the
north-south trace of the June 28, 1992 Landers rupture across the Mojave
desert, we construct a new velocity model for the Mojave region which features
a thin, slow crust. Using this model, we obtain source parameters, including
depth and duration, for each of the aftershocks in the profile, and in
addition, any significant (M>3.7) Joshua Tree--Landers aftershock between
April, 1992 and October, 1994 for which coherent TERRAscope data were
available. In all, we determine source parameters and stress-drops for 45
significant (M_w > 4) earthquakes associated with the Joshua Tree and Landers
sequences, using a waveform grid-search algorithm. Stress drops for these
earthquakes appear to vary systematically with location, with respect to
previous seismic activity, proximity to previous rupture (i.e., with respect to
the Landers rupture), and with tectonic province. In general, for areas north
of the Pinto Mountain fault, stress-drops of aftershocks located off the faults
involved with the Landers rupture are higher than those located on the fault,
with the exception of aftershocks on the newly recognized Kickapoo (Landers)
fault. Stress drops are moderate south of the Pinto Mountain fault, where there
is a history of seismic swarms but no single through-going fault. In contrast
to aftershocks in the eastern Transverse ranges, and related to the 1992 Big
Bear, California, sequence, Landers events show no clear relationship between
stress-drop and depth. Instead, higher stress-drop aftershocks appear to
correlate with activity on nascent faults, or those which experienced
relatively small slip during mainshock rupture.

<id>
physics/9806041v1
<category>
physics.geo-ph
<abstract>
The regional phase Lg is used to estimate location and magnitude for sources
closer than 1500 km. The complexity of Lg waveforms makes it difficult to
consistently determine Lg arrival time, thus affecting source location with a
single station or array. This study tests an automatic method for timing Lg
arrivals using wavelet transforms to decompose the Lg signal into its
components localized both in time and scale. A Continuous Wavelet Transform
(CWT) using a Daubechies order two (db2) wavelet is applied to 10 seconds of
raw data, containing the start of Lg. Initial positioning of the window is
obtained using the standard Lg travel time tables. The coefficients at scale 8
from the db2 decomposition are squared and the resulting time series is
represented by an approximation of the 4'th level Discrete Wavelet Transform
(DWT) using a Haar wavelet. A threshold detector is then applied to the
resulting time series to determine the Lg arrival time. The method was tested
using well located earthquakes (USGS) and explosions from known mines (mb less
than 4.0), recorded on the vertical components at TXAR (Lajitas, Texas) and
PDAR (Pinedale, Wyoming) arrays. The Lg arrival time was automatically picked
with a standard deviation of less than 1.5 seconds (less than 10 km location
error) for well known locations. Location errors are larger with the increase
in distance and smaller with the increase in signal to noise ratio of events.

<id>
physics/9811024v1
<category>
physics.geo-ph
<abstract>
The possibility of an increasing gravitational constant $G$ and its
implication on the Earth's history are discussed. The model is consistent with
geophysical and astronomical data. The number of days in early epochs predicted
by the model is in agreement with those obtained from fossil corals. The model
predicts that the Earth was initially cold and is gradually heating up during
its course of evolution. The exact law for the variation of $G$ is still to be
obtained from the variation of the
  Earth's parameters with cosmic expansion.

<id>
physics/9903013v1
<category>
physics.geo-ph
<abstract>
Ionospheric tomography using GPS data has been reported in the literature and
even the application to radar altimeter calibration was succesfully carried out
in a recent work. We here present a new software tool, called Global
Ionospheric Stochastic Tomography software (GIST), and its powerful capability
for ingesting GPS data from different sources (ground stations, receivers on
board LEO for navigation and occultation purposes) and other data such as
altimetry data to yield global maps with dense coverage and inherent
calibration of the instruments. We show results obtained including 106 IGS
ground stations, GPS/MET low rate occultation data, TOPEX/POSEIDON GPS data
from the navigation antenna and NASA Radar Altimeter with the additional
benefit of a direct estimation of the NRA bias. The possibility of ingesting
different kinds of ionospheric data into the tomographic model suggest a way to
accurately monitor the ionosphere with direct application to single frequency
instrument calibration.

<id>
physics/9909023v1
<category>
physics.geo-ph
<abstract>
Examples of heavy mineral placer deposits are presented in which wave
reflection, refraction, diffraction and resonance would appear to have played a
major concentrating role. Their geometry is compared with the computer
generated patterns predicted for the reflection, refraction and diffraction of
surface waves moving over fairly simple, idealised bathymetries. Much of this
work is founded on the idea that similar sediments document equivalent (or once
equivalent) flow-tractional environments.
  Most of the examples could be satisfactorily explained in this fashion. It
may therefore be possible to ignore the exact physics of the boundary layer,
longshore and tidal return currents etc. at the scale on which these examples
occur, leaving the way open for the qualitative use of results obtained using
the likes of the mild slope wave equation. A ``Monte Carlo'' approach based on
wave induced tractions should therefore succeed in elucidating presently known
heavy mineral placer deposits and, consequently, in predicting other deposits
which remain as yet undiscovered.

<id>
physics/9911016v1
<category>
physics.geo-ph
<abstract>
Researchers of seismic waves may construct a new seismographic recording
adding one seismometer to each component of a conventional seismic station. The
two identical conventional seismometers are set up in position of perpendicular
and are connected in parallel feeding one recording device (digital or analog).
This use of the seismometers (which they may be both horizontal or, one is
vertical) is called "two seismometers seismograph" or simply "2S-S".
  2S-seismograph performs new capabilities: 1.-it cause to a higher gain which
is based on directly ground motion energy from the two orthogonal components of
signals, 2.-it has a much smoother response curve than that of the single use
of seismometer,3.-because of this smoothing, we are able to apply a higher
level of static magnification which cause to widening the response at its both
ends, therefore, 2S-System enable to work with a larger dynamic range
frequency, 4.- it has a directional and motional filtering property which may
be used in some cases advantageously, The contribution of "1", "2", "3" and "4"
correspond to unique instrumental improvements for which seismography are ever
needed.
  Data which are obtained from the 2S-Ss have also more advantageous properties
comparing with even that of ARRAY's: 5.-it is possible to record signals with
their larger plane components all the time by a second 2S-S connected with the
opposite ends, 6.-seismic wave types (P,S,R,L) can often be recorded separately
on a separated 2S-seismogram since researchers usually deal with a known area
of research, 7.-some implicit weak signals, which can not be readable as a
phase on the conventional seismograms, become recorded newly and readably by
the 2S-Ss

<id>
physics/0005047v1
<category>
physics.geo-ph
<abstract>
This article is the first in a series of three papers investigating the
detailed geometry of river networks. Large-scale river networks mark an
important class of two-dimensional branching networks, being not only of
intrinsic interest but also a pervasive natural phenomenon. In the description
of river network structure, scaling laws are uniformly observed. Reported
values of scaling exponents vary suggesting that no unique set of scaling
exponents exists. To improve this current understanding of scaling in river
networks and to provide a fuller description of branching network structure, we
report here a theoretical and empirical study of fluctuations about and
deviations from scaling. We examine data for continent-scale river networks
such as the Mississippi and the Amazon and draw inspiration from a simple model
of directed, random networks. We center our investigations on the scaling of
the length of sub-basin's dominant stream with its area, a characterization of
basin shape known as Hack's law. We generalize this relationship to a joint
probability density and show that fluctuations about scaling are substantial.
We find strong deviations from scaling at small scales which can be explained
by the existence of linear network structure. At intermediate scales, we find
slow drifts in exponent values indicating that scaling is only approximately
obeyed and that universality remains indeterminate. At large scales, we observe
a breakdown in scaling due to decreasing sample space and correlations with
overall basin shape. The extent of approximate scaling is significantly
restricted by these deviations and will not be improved by increases in network
resolution.

<id>
physics/0005048v1
<category>
physics.geo-ph
<abstract>
The structure of a river network may be seen as a discrete set of nested
sub-networks built out of individual stream segments. These network components
are assigned an integral stream order via a hierarchical and discrete ordering
method. Exponential relationships, known as Horton's laws, between stream order
and ensemble-averaged quantities pertaining to network components are observed.
We extend these observations to incorporate fluctuations and all higher moments
by developing functional relationships between distributions. The relationships
determined are drawn from a combination of theoretical analysis, analysis of
real river networks including the Mississippi, Amazon and Nile, and numerical
simulations on a model of directed, random networks. Underlying distributions
of stream segment lengths are identified as exponential. Combinations of these
distributions form single-humped distributions with exponential tails, the sums
of which are in turn shown to give power law distributions of stream lengths.
Distributions of basin area and stream segment frequency are also addressed.
The calculations identify a single length-scale as a measure of size
fluctuations in network components. This article is the second in a series of
three addressing the geometry of river networks.

<id>
physics/0005049v1
<category>
physics.geo-ph
<abstract>
River networks serve as a paradigmatic example of all branching networks.
Essential to understanding the overall structure of river networks is a
knowledge of their detailed architecture. Here we show that sub-branches are
distributed exponentially in size and that they are randomly distributed in
space, thereby completely characterizing the most basic level of river network
description. Specifically, an averaged view of network architecture is first
provided by a proposed self-similarity statement about the scaling of drainage
density, a local measure of stream concentration. This scaling of drainage
density is shown to imply Tokunaga's law, a description of the scaling of side
branch abundance along a given stream, as well as a scaling law for stream
lengths. This establishes the scaling of the length scale associated with
drainage density as the basic signature of self-similarity in river networks.
We then consider fluctuations in drainage density and consequently the numbers
of side branches. Data is analyzed for the Mississippi River basin and a model
of random directed networks. Numbers of side streams are found to follow
exponential distributions as are stream lengths and inter-tributary distances
along streams. Finally, we derive the joint variation of side stream abundance
with stream length, affording a full description of fluctuations in network
structure. Fluctuations in side stream numbers are shown to be a direct result
of fluctuations in stream lengths. This is the last paper in a series of three
on the geometry of river networks.

<id>
physics/0007006v1
<category>
physics.geo-ph
<abstract>
We developed a new technology for global detection of atmospheric
disturbances, on the basis of phase measurements of the total electron content
(TEC) using an international GPS networks. Temporal dependencies of TEC are
obtained for a set of spaced receivers of the GPS network simultaneously for
the entire set of visible satellites. These series are subjected to filtering
in the selected range of oscillation periods using known algorithms for
spatio-temporal analysis of signals. An "instantaneous" ionospheric response to
the sudden commencement of a strong magnetic storm of April 6, 2000 was
detected. On the dayside of the Earth the largest value of the net response
amplitude was found to be of order 0.8*10^16 m^-2 (1--2 % of the background TEC
value), and the delay with respect to the SC in mid-latitudes was about 200 s.
In higher latitudes the delay goes as long as 15 min. On the nightside these
values are 0.2*10^16 m^-2 and 30 min, respectively. The velocity of the
traveling disturbance from the middle to high latitudes on the dayside as well
as from the dayside to the nightside was about 10-20 km/s.

<id>
physics/0007024v1
<category>
physics.geo-ph
<abstract>
We present the results derived from measuring fundamental parameters of the
ionospheric response to the August 11, 1999 total solar eclipse. Our study is
based on using the data from about 70 GPS stations located in the neighbourhood
of the eclipse totality phase in Europe. The eclipse period was characterized
by a low level of geomagnetic disturbance (Dst - variation from -10 to -20 nT),
which alleviated significantly the problem of detecting the ionospheric
response to the eclipse. Our analysis revealed a well-defined effect of a
decrease (depression) of the total electron content (TEC) for all GPS stations.
The delay between minimum TEC values with respect to the totality phase near
the eclipse path increased gradually from 4 min in Greenwich longitude (10:40
UT, LT) to 8 min at the longitude 16 degrees (12:09 LT). The depth and duration
of the TEC depression were found to be 0.2-0.3 TECU and 60 min, respectively.
The results obtained in this study are in good agreement with earlier
measurements and theoretical estimates.

<id>
physics/0007026v1
<category>
physics.geo-ph
<abstract>
This contributor suggests the concept of a new technology for global detection
(GLOBDET) of atmospheric disturbances of natural and technogenic origin, on the
basis of phase measurements of the total electron content (TEC) in the
ionosphere using an international GPS network. Temporal dependencies of TEC are
obtained for a set of spaced receivers of the GPS network simultaneously for
the entire set of "visible" (over a given time interval) GPS satellites (up to
5-10 satellites). These series are subjected to filtering in the selected range
of oscillation periods using algorithms for spatio-temporal analysis of signals
of non-equidistant GPS phased antenna arrays which are adequate to the detected
disturbance. An analysis is made of the possibilities of using the GLOBDET when
detecting the ionospheric response of solar flares. In this case it is best to
make the coherent summation of the filtered series of TEC. A powerful impulsive
flares of July 29, 1999 and December 28, 1999 were chosen to illustrate the
practical implementation of the proposed method.

<id>
physics/0007036v1
<category>
physics.geo-ph
<abstract>
This paper presents data from first GPS measurements of global response of
the ionosphere to solar flares of September 23, 1998 and July 29, 1999. The
analysis used novel technology of a global detection of ionospheric effects
from solar flares (GLOBDET) as developed by one of the contributors (Afraimovich E.
L.). The essence of the method is that use is made of appropriate filtering and
a coherent processing of variations in total electron content (TEC) in the
ionosphere which is determined from GPS data, simultaneously for the entire set
of visible (over a given time interval) GPS satellites at all stations used in
the analysis. It was found that fluctuations of TEC, obtained by removing the
linear trend of TEC with a time window of about 5 min, are coherent for all
stations and beams to the GPS satellites on the dayside of the Earth. The time
profile of TEC responses is similar to the time behavior of hard X-ray emission
variations during flares in the energy range 25-35 keV if the relaxation time
of electron density disturbances in the ionosphere of order 50-100 s is
introduced. No such effect on the nightside of the Earth has been detected yet.

<id>
physics/0007041v2
<category>
physics.geo-ph
<abstract>
We investigate the form and dynamics of shock-acoustic waves generated by
earthquakes. We use the method for detecting and locating the sources of
ionospheric impulsive disturbances, based on using data from a global network
of receivers of the GPS navigation system and requiring no a priori information
about the place and time of associated effects. The practical implementation of
the method is illustrated by a case study of earthquake effects in Turkey
(August 17, and November 12, 1999), in Southern Sumatera (June 4, 2000), and
off the coast of Central America (January 13, 2001). It was found that in all
instances the time period of the ionospheric response is 180-390 s, and the
amplitude exceeds by a factor of two as a minimum the standard deviation of
background fluctuations in total electron content in this range of periods
under quiet and moderate geomagnetic conditions. The elevation of the wave
vector varies through a range of 20-44 degree, and the phase velocity
(1100-1300 m/s) approaches the sound velocity at the heights of the ionospheric
F-region maximum. The calculated (by neglecting refraction corrections)
location of the source roughly corresponds to the earthquake epicenter. Our
data are consistent with the present views that shock-acoustic waves are caused
by a piston-like movement of the Earth surface in the zone of an earthquake
epicenter.

<id>
physics/0007043v1
<category>
physics.geo-ph
<abstract>
This paper is concerned with the form and dynamics of shock-acoustic waves
(SAW) generated during rocket launchings. We have developed a method for
determining SAW parameters (including angular characteristics of the wave
vector, and the SAW phase velocity, as well as the direction towards the
source) using GPS-arrays whose elements can be chosen out of a large set of
GPS-stations of the global GPS network. The application of the method is
illustrated by a case study of ionospheric effects from launchings of launch
vehicles (LV) Proton and Space Shuttle from space-launch complexes Baikonur and
Kennedy Space Center (KSC) in 1998 and 1999 (a total of five launchings). The
study revealed that, in spite of a difference of LV characteristics, the
ionospheric response for all launchings had the character of an N - wave
corresponding to the form of a shock wave, regardless of the disturbance source
(rocket launchings, industrial explosions). The SAW period T is 270--360 s, and
the amplitude exceeds the standard deviation of TEC background fluctuations in
this range of periods under quiet and moderate geomagnetic conditions by
factors of 2 to 5 as a minimum. The angle of elevation of the SAW wave vector
varies from 30 degree to 60 degree, and the SAW phase velocity (900-1200 m/s)
approaches the sound velocity at heights of the ionospheric F-region maximum.

<id>
physics/0009027v4
<category>
physics.geo-ph
<abstract>
We have investigated a dependence of the relative density of phase slips in
the GPS navigation system on the disturbance level of the Earth's
magnetosphere. The study is based on using Internet-available selected data
from the global GPS network, with the simultaneously handled number of
receiving stations ranging from 160 to 323. The analysis used four days from
the period 1999-2000, with the values of the geomagnetic field disturbance
index Dst from 0 to -300 nT. During strong magnetic storms, the relative
density of phase slips on mid latitudes exceeds the one for magnetically quiet
days by one-two orders of magnitude as a minimum, and reaches a few and (for
some of the GPS satellites) even ten percent of the total density of
observations. Furthermore, the level of phase slips for the GPS satellites
located on the sunward side of the Earth was by a factor of 5-10 larger
compared with the opposite side of the Earth. The high positive correlation of
an increase in the density of phase slips and the intensity of ionospheric
irregularities during geomagnetic disturbances as detected in this study points
to the fact that the increase is slips is caused by the scattering of the GPS
signal from ionospheric irregularities.

<id>
physics/0010083v3
<category>
physics.geo-ph
<abstract>
The composition of the lower mantle can be investigated by examining
densities and seismic velocities of compositional models as functions of depth.
In order to do this it is necessary to know the volumes and thermoelastic
properties of the compositional constituents under lower mantle conditions. We
determined the thermal equation of state (EOS) of MgSiO3 perovskite using the
nonempirical variational induced breathing (VIB) interatomic potential with
molecular dynamics simulations at pressures and temperatures of the lower
mantle. We fit our pressure-volume-temperature results to a thermal EOS of the
form P(V,T) = P0(V,T0) + Delta Pth(T), where T0 = 300 K and P0 is the
isothermal Universal EOS. The thermal pressure Delta Pth can be represented by
a linear relationship Delta Pth = a + b T. We find V0 = 165.40 A^3, KT0 = 273
GPa, K'T0 = 3.86, a = -1.99 GPa, and b = 0.00664 GPa K^-1 for pressures of
0-140 GPa and temperatures of 300-3000 K. By fixing V0 to the experimentally
determined value of 162.49 A^3 and calculating density and bulk sound velocity
profiles along a lower mantle geotherm we find that the lower mantle cannot
consist solely of (Mg,Fe)SiO3 perovskite with XMg ranging from 0.9-1.0. Using
pyrolitic compositions of 67 vol % perovskite (XMg = 0.93-0.96) and 33 vol %
magnesiowustite (XMg = 0.82-0.86), however, we obtained density and velocity
profiles that are in excellent agreement with seismological models for a
reasonable geotherm.

<id>
physics/0011065v1
<category>
physics.geo-ph
<abstract>
I think it important that pilots, the aircraft industry, air safety staff,
accident investigators, should all be aware of the possibility of the hazard
described in the accompanying article, and also that research shold be directed
to establish the severity of this hazard, and find ways of diminishing the
risk.

<id>
physics/0012006v1
<category>
physics.geo-ph
<abstract>
In this paper an attempt is made to verify the hypothesis on the role of
geomagnetic disturbances as a factor determining the intensity of traveling
ionospheric disturbances (TIDs). To improve the statistical validity of the
data, we have used the based on the new GLOBDET technology method involving a
global spatial averaging of disturbance spectra of the total electron content
(TEC). To characterize the TID intensity quantitatively, we suggest that a new
global index of the degree of disturbance should be used, which is equal to the
mean value of the rms variations in TEC within the selected range of spectral
periods (of 20-60 min in the present case). It was found that power spectra of
daytime TEC variations in the range of 20-60 min periods under quiet conditions
have a power-law form, with the slope index k = -2.5. With an increase of the
level of magnetic disturbance, there is an increase in total intensity of TIDs,
with a concurrent kink of the spectrum caused by an increase in oscillation
intensity in the range of 20-60 min. It was found that an increase in the level
of geomagnetic activity is accompanied by an increase in total intensity of
TEC; however, it correlates not with the absolute level of Dst, but with the
value of the time derivative of Dst (a maximum correlation coefficient reaches
-0.94). The delay of the TID response of the order of 2 hours is consistent
with the view that TIDs are generated in auroral regions, and propagate
equatorward with the velocity of about 300-400 m/s.

<id>
physics/0103043v1
<category>
physics.geo-ph
<abstract>
An explicit expression for P-wave velocity is proposed to develop a novel
tomographic technique in a spherically symmetric model of the Earth (MZY). The
distribution of the P velocity structure in the mantle is determined using only
34 P- and 2 PcP-observed traveltimes. By applying a non-linear inversion, the
P-residuals in the range between 0 and 100 degrees are minimised up to a
maximum value of 0.015 s. Furthermore, from the high quality computation of PcP
traveltimes, with residuals much better than 0.13 s, it is possible to infer
the existence of a brief low velocity layer in the D" region. This is then
followed by a gradual increasing in the velocity profile towards the core,
which begins at a depth of 2893.9 km.

<id>
physics/0104023v1
<category>
physics.geo-ph
<abstract>
By using the maximum entropy principle with Tsallis entropy and under the
assumption that the gouge plays an active role in the triggering of
earthquakes, we obtain a functional dependence for energy distribution function
for earthquakes which fits very well with observations in the region of small
energies. This distribution function is related to the size distribution
function of fragments in the gouge.

<id>
physics/0104070v1
<category>
physics.geo-ph
<abstract>
Basic properties of the mid-latitude large-scale traveling ionospheric
disturbances (LS TIDs) during the maximum phase of a strong magnetic storm of
6-8 April 2000 are shown. Total electron content (TEC) variations were studied
by using data from GPS receivers located in Russia and Central Asia. The
nightglow response to this storm at mesopause and termospheric altitudes was
also measured by optical instruments FENIX located at the observatory of the
Institute of Solar-Terrestrial Physics, (51.9 deg. N, 103.0 deg. E) and MORTI
located at the observatory of the Institute of Ionosphere (43.2 deg. N, 77.0
deg. E). Observations of the O (557.7 nm, 630.0 nm, 360-410 nm, and 720-830 nm)
emissions originating from atmospheric layers centered at altitudes of 90 km,
97 km, and 250 km were carried out at Irkutsk and of the O_2 (866.5 nm)
emission originating from an atmospheric layer centered at altitude of 95 km
was carried out at Almaty. Variations of the f_0F2 and virtual altitude of the
F2 layer were measured at Almaty as well. An analysis of data was performed for
the time interval 17.00-21.00 UT comprising a maximum of the Dst derivative.
Results have shown that the storm-induced solitary large-scale wave with
duration of 1 hour and with the front width of 5000 km moved equatorward with
the velocity of 200 ms-1 to a distance of no less than 1000 km. The TEC
disturbance, basically displaying an electron content depression in the maximum
of the F2 region, reveals a good correlation with growing nightglow emission,
the temporal shift between the TEC and emission variation maxima being
different for different altitudes.

<id>
physics/0107030v2
<category>
physics.geo-ph
<abstract>
In this paper the interrelation between geomagnetic pulsations and variations
in frequency Doppler shift (Fd) of the ionosphere-reflected radio signal is
under investigation. The experiment on simultaneous recording of Fd variations
and geomagnetic pulsations was organised at high latitude station in Norilsk
(geomagnetic latitude and longitude 64.2 N, 160.4 E, L=5.3) during
Febrary-April of 1995-98. Thirty cases of simultaneous recording of duration
from 10 min to two hour were analysed: 6 cases of simultaneous recording of
variations Fd and regular geomagnetic pulsations Pc5; and 25 cases of recording
of Fd variations and irregular pulsations Pi2.
  On the basis of experimental results, the following conclusions have been
drawn: a) Hydromagnetic waves in the range of regular Pc5 pulsations, when
interacting with the ionospheric F2 layer, make the main contribution to
short-period Fd variations. The possible mechanism of Fd variations are
oscillations of electron density, associated with distribution of a
hydromagnetic wave in an ionosphere. b) There exists an unquestionable
interrelation between Fd variations of the sporadic E layer-reflected radio
signal and irregular Pi2 pulsations, but for some reasons it is traced poorly.

<id>
physics/0107031v2
<category>
physics.geo-ph
<abstract>
In March and August/September 1995, February 1996, and in March-April 1998,
observations of the inhomogeneous structure of the high-latitude ionosphere
were carried out at Norilsk (geomagnetic latitude and longitude are 64.2 N and
160.4 E, and L=5.2). Small-scale irregularities (with the lifetime of several
seconds,and the spatial scale less than 5-7 km), and medium-size wave
irregularities(with the period of 10-50 min, and the horizontal size of tens
and hundreds of kilometres) of the ionospheric F layer were investigated under
different geophysical conditions. A total of 300 hours of observations was
recorded, including 250 reflections from the F2 layer, and the other
reflections from the sporadic E layer.
  The diurnal variations of inhomogeneous structure parameters in March and
April is obtained. Dependence of some ionospheric irregularity parameters on
geomagnetic activity is presented.

<id>
physics/0107077v1
<category>
physics.geo-ph
<abstract>
We analyzed effects of elasticity on the dynamics of fluids in porous media
by studying a flow of a Maxwell fluid in a tube, which oscillates
longitudinally and is subject to oscillatory pressure gradient. The present
study investigates novelties brought about into the classic Biot's theory of
propagation of elastic waves in a fluid-saturated porous solid by inclusion of
non-Newtonian effects that are important, for example, for hydrocarbons. Using
the time Fourier transform and transforming the problem into the frequency
domain, we calculated: (A) the dynamic permeability and (B) the function
$F(\kappa)$ that measures the deviation from Poiseuille flow friction as a
function of frequency parameter $\kappa$. This provides a more complete theory
of flow of Maxwell fluid through the longitudinally oscillating cylindrical
tube with the oscillating pressure gradient, which has important practical
applications. This study has clearly shown transition from dissipative to
elastic regime in which sharp enhancements (resonances) of the flow are found.

<id>
physics/0107078v2
<category>
physics.geo-ph
<abstract>
The present study investigates novelties brought about into the classic
Biot's theory of propagation of elastic waves in a fluid-saturated porous solid
by inclusion of non-Newtonian effects that are important, for example, for
hydrocarbons. Based on our previous results (Tsiklauri and Beresnev: 2001,
Phys. Rev. E, 63, 046304), we have investigated the propagation of rotational
and dilatational elastic waves, through calculating their phase velocities and
attenuation coefficients as a function of frequency. We found that the
replacement of an ordinary Newtonian fluid by a Maxwell fluid in the
fluid-saturated porous solid results in: (a) an overall increase of the phase
velocities of both the rotational and dilatational waves. With the increase of
frequency these quantities tend to a fixed, higher, as compared to the
Newtonian limiting case, level which is not changing with the decrease of the
Deborah number $\alpha$. (b) the overall decrease of the attenuation
coefficients of both the rotational and dilatational waves. With the increase
of frequency these quantities tend to a progressively lower, as compared to the
Newtonian limiting case, levels as $\alpha$ decreases. (c) Appearance of
oscillations in all physical quantities in the deeply non-Newtonian regime.

<id>
physics/0110048v1
<category>
physics.geo-ph
<abstract>
We discuss the question as to how the magnetospheric energy source feeds the
ionospheric current system.
  It is shown that a consistent application and further development of Kennel's
ideas makes it possible to successfully solve the magnetosphere-ionosphere
coupling problem in regard to the formation of auroral electrojets by steady
volume currents generated in the magnetosphere by the magnetospheric MHD
generator in the case of a simple model which, nevertheless, retains the
essential features of the reality. It is concluded that the whole of the
complicated magnetospheric ''design'' only acts to redistribute, in space and
time, currents and energy fluxes which must be supplied by external sources to
feed the dissipative processes in the ionosphere.

<id>
physics/0110063v1
<category>
physics.geo-ph
<abstract>
Results derived from analysing the ionosphere response to faint and bright
solar flares are presented. The analysis used technology of a global detection
of ionospheric effects from solar flares as developed by the contributors, on the
basis of phase measurements of the total electron content (TEC) in the
ionosphere using an international GPS network. The essence of the method is
that use is made of appropriate filtering and a coherent processing of
variations in the TEC which is determined from GPS data, simultaneously for the
entire set of visible GPS satellites at all stations used in the analysis. This
technique is useful for identifying the ionospheric response to faint solar
flares (of X-ray class C) when the variation amplitude of the TEC response to
separate line-on-sight to GPS satellite is comparable to the level of
background fluctuations. The dependence of the TEC variation response amplitude
on the flares location on the Sun is investigated.

<id>
physics/0201047v1
<category>
physics.geo-ph
<abstract>
We discuss the measurements of the main parameters of the ionospheric
response to the total solar eclipse of June 21, 2001. This study is based on
using the data from three stations of the global GPS network located in the
area of the totality band in Africa. This period was characterized by a low
level of geomagnetic disturbance (the Dst-index varied from -6 to 22 nT), which
alleviated greatly the problem of detecting the ionospheric response to
eclipse. An analysis revealed a clearly pronounced effect of a decrease
(depression) of the total electron content (TEC) for all GPS stations. The
delay between the smallest value of the TEC with respect to eclipse totality
was 9-37 min. The depth and duration of the TEC depression were 0.5-0.9 TECU
and 30-67 min, respectively. The results obtained in this study are in good
agreement with earlier measurements and theoretical estimations.

<id>
physics/0202052v1
<category>
physics.geo-ph
<abstract>
Using the geomagnetic storm of July 15, 2000 as an example, we investigated
the dependence of GPS navigation system performance on the nightside at
mid-latitudes on the level of geomagnetic disturbance. The investigation was
based on the data from the global GPS system available through the Internet. It
was shown that the number of GPS phase slips increases with the increasing
level of disturbance and that there is a good correlation between the rate of
Dst-variation and the frequency of slips. It was further shown that the
relative frequency of slips has also a clearly pronounced aspect dependence.
Phase slips of the GPS signal can be caused by the scattering from small-scale
irregularities of the ionospheric E-layer. Phase slip characteristics are
indicative of Farley-Buneman instabilities as a plausible physical mechanism
that is responsible for the formation of geomagnetic field-aligned
irregularities. Using simultaneous measurements of backscatter signal
characteristics from the Irkutsk incoherent scatter radar and existing models
for such irregularities, we estimated the order of magnitude of the expected
phase fluctuations of the GPS signal at a few degrees.

<id>
physics/9710007v1
<category>
physics.hist-ph
<abstract>
In two respects Ludwig Boltzmann was a pioneer of quantum mechanics. First
because in his statistical interpretation of the second law of thermodynamics
he introduced the theory of probability into a fundamental law of physics and
thus broke with the classical prejudice, that fundamental laws have to be
strictly deterministic. Even Max Planck had not been ready to accept
Boltzmann's statistical methods until 1900. With Boltzmann's pioneering work
the probabilistic interpretation of quantum mechanics had already a precedent.
In fact in a paper in 1897 Boltzmann had already suggested to Planck to use his
statistical methods for the treatment of black body radiation. The second
pioneering step towards quantum mechanics was Boltzmann's introduction of
discrete energy levels. Boltzmann used this method already in his 1872 paper on
the H-theorem. One may ask whether Boltzmann considered this procedure only as
a mathematical device or whether he attributed physical significance to it. In
this connection Ostwald reports that when he and Planck tried to convince
Boltzmann of the superiority of purely thermodynamic methods over atomism at
the Halle Conference in 1891 Boltzmann suddenly said: ``I see no reason why
energy shouldn't also be regarded as divided atomically.'' Finally I would like
to mention, that Boltzmann in his lectures on Natural Philosophy in 1903
already anticipated the equal treatment of space coordinates and time
introduced in the theory of special relativity. Furthermore in the lectures by
Boltzmann and his successor Fritz Hasen\"ohrl in Vienna the students learned
already about noneuclidean geometry, so that they could immediately start to
work when Einstein's general theory of relativity had been formulated.

<id>
physics/9802039v1
<category>
physics.hist-ph
<abstract>
Quantum electrodynamics is the well-accepted theory. However, we feel it is
useful to look at formalisms that provide alternative ways to describe light,
because in the recent years the development of quantum field theories based
primarily on the gauge principle has encountered considerable difficulties.
There is a wide variety of generalized theories and they are characterized
mainly by the introduction of additional parameters and/or longitudinal modes
of electromagnetism. The Majorana-Oppenheimer form of electrodynamics, the
Sachs' theory of Elementary Matter, the analysis of the action-at-a-distance
concept, presented recently by Chubykalo and Smirnov-Rueda, and the analysis of
the claimed `longitudinality' of the antisymmetric tensor field after
quantization are reviewed in this essay. We also list recent advances in the
Weinberg 2(2J+1) formalism (which is built on First Principles) and in the
Majorana theory of neutral particles. These may serve as starting points for
constructing a quantum theory of light.

<id>
physics/9803005v1
<category>
physics.hist-ph
<abstract>
This paper gives a short review of the history of statistical physics
starting from D. Bernoulli's kinetic theory of gases in the 18th century until
the recent new developments in nonequilibrium kinetic theory in the last
decades of this century. The most important contributions of the great
physicists Clausius, Maxwell and Boltzmann are sketched. It is shown how the
reversibility and the recurrence paradox are resolved within Boltzmann's
statistical interpretation of the second law of thermodynamics. An approach to
classical and quantum statistical mechanics is outlined. Finally the progress
in nonequilibrium kinetic theory in the second half of this century is sketched
starting from the work of N.N. Bogolyubov in 1946 up to the progress made
recently in understanding the diffusion processes in dense fluids using
computer simulations and analytical methods.

<id>
physics/9807046v1
<category>
physics.hist-ph
<abstract>
Although we accept that Physics is, as a last resort, an experimental
science, the relationship between theory and experiment is far away from being
trivial. Any experiment is always explained within a determinate theoretical
context and, at the same time, an experiment can give suggestions for theories
or even can bring new theoretical challenges. Thus, we cannot say without
ambiguity when an experiment is a crucial one.

<id>
physics/9808033v2
<category>
physics.hist-ph
<abstract>
The passage of particles through matter is one of the principal ways to
investigate nature. In this article, we would like to outline the most
important stages in the development of the theory about the stopping power.

<id>
physics/9808034v1
<category>
physics.hist-ph
<abstract>
During the 80's, some experiments and the repetitions of old ones, lead to
the hypothesis of a fifth force. Nevertheless, a more accurate research was not
able to confirm this hypothesis. This article wants to go over again the most
important steps of the event.

<id>
physics/9810023v5
<category>
physics.hist-ph
<abstract>
Ettore Majorana, perhaps the greatest Italian theoretical physicist of this
century (Enrico Fermi compared him to Galilei and Newton), disappeared
misteriously from Naples in 1938, when he was 31. In the first part of this
work we outline his scientific personality (on the basis of letters, documents,
testimonies collected by us in about twenty years) and the significance of some
parts of his publications. In the second part of this paper we set forth some
brief information about the unpublished scientific manuscripts left by
E.Majorana and known to us till this moment (most of which are deposited at the
"Domus Galilaeana" in Pisa, Italy), and present a preliminary Catalogue of them
prepared in collaboration with M.Baldo and R.Mignani. [The present material is
mainly taken from our book "Il Caso Majorana: Epistolario, Documenti,
Testimonianze" (Mondadori, Milan, 1987,1991; Di Renzo, Rome, 2000-2008): We
address to such a book (c/o www.direnzo.it, "Arcobaleno" series) all the
readers interested in more and deeper information; as well as, for more
technical topics, to the subsequent volumes reproducing e.g. part of the
scientific manuscripts left unpublished by Ettore Majorana: see, for instance,
the e-print arXiv:0709.1183v1[physics.hist-ph].]

<id>
physics/9810042v1
<category>
physics.hist-ph
<abstract>
The question of the contributorship of Shakespeare's plays has long been debated.
The two leading contenders are W. Shakspere (1564-1616) and Edward de Vere the
13th Earl of Oxford (1550-1604). Here I note that Shakespeare's references to
important events and discoveries in astronomy and geophysics in 1572 and 1600,
but not to similarly important events of 1604, 1609 and 1610, especially given
Shakespeare's frequent references to and knowledge of the physical sciences,
might be able to shed some light on the contributorship question.

<id>
physics/9901037v2
<category>
physics.hist-ph
<abstract>
During the past one hundred years three related elementary particles - the
electron, the muon, and the tau - were discovered by very different scientific
techniques. The contributor, who received the Wolf Prize and the Nobel Prize for the
discovery of the tau, uses this history to discuss certainty and uncertainty in
the practice of science. While the emphasis is on the practice of scientific
research, the paper also explains for the non-physicist some basic ideas in
elementary particle science.

<id>
physics/9903023v1
<category>
physics.hist-ph
<abstract>
We review the history of the road to a manifestly covariant perturbative
calculus within quantum electrodynamics from the early semi-classical results
of the mid-twenties to the complete formalism of Stueckelberg in 1934. We chose
as our case study the calculation of the cross-section of the Compton effect.
We analyse Stueckelberg's paper extensively. This is our first contribution to
a study of his fundamental contributions to the theoretical physics of
twentieth century.

<id>
physics/9907032v1
<category>
physics.hist-ph
<abstract>
Original English Summary. - A systematic method of constructing potentials,
for which the one-variable Schroedinger equation can be solved in terms of the
hypergeometric (HGM) function, is presented. All the potentials, obtained by
energy-independent transformations of the HGM equation, are determined together
with eigenvalues and eigenfunctions. A class of potentials derived from the
confluent HGM equation is found by means of a limit process

<id>
physics/9908003v1
<category>
physics.hist-ph
<abstract>
One of Darboux's seminal results is archived here

<id>
physics/9908019v1
<category>
physics.hist-ph
<abstract>
Crum's seminal result of 1955 is archived here

<id>
physics/9909061v2
<category>
physics.hist-ph
<abstract>
Seminal result of Delsarte is archived here

<id>
physics/9910003v1
<category>
physics.hist-ph
<abstract>
Schroedinger's famous quadruple of factorizations of the hypergeometric
equation is archived here

<id>
physics/0002009v1
<category>
physics.hist-ph
<abstract>
Short note by Marcel Brillouin on the representation of the mass point in
general relativity.

<id>
physics/0006065v2
<category>
physics.hist-ph
<abstract>
It is argued that properties of Democritus' atoms parallel those of volume
forms in differential geometry. This kind of atoms has not "size" of finite
magnitude.
  -----
  Se arguye que las propiedades de los atomos de Democrito son paralelas a las
de sus formas de volumen en geometria diferencial. Este tipo de atomos no tiene
tamanno de magnitud finita.

<id>
physics/0008012v2
<category>
physics.hist-ph
<abstract>
Experimental tests on `time dilation' began in 1938 with Ives and Stilwell's
work of the transverse Doppler effect due to atoms in inertial flight. Rossi
and Hall (1941) inaugurated the era of fast moving elementary particles that
dominated the scene until the discovery of the Mossbauer effect (1957). This
discovery suggested the use of photons emitted without recoil in crystalline
solids for testing both time dilation and gravitational red shift. Finally,
around 1970, Hafele and Keating dealt again with time dilation by sending
macroscopic atomic clocks around the Earth. The interpretations of these
experiments by experimenters have been characterized by the use of additional
hypotheses not necessary for the formal development of the theories under test
(the idea that all clocks measure proper time) or hypotheses completely
extraneous to the theories themselves (the idea that atoms are clocks). If
these assumptions are dropped, it turns out that the only experiments
concerning time dilation are those performed with elementary particles in
inertial flight. The historical and epistemological implications are discussed.

<id>
physics/0008229v1
<category>
physics.hist-ph
<abstract>
Special relativity was discovered at the eve of the century, but finds its
roots in the 19th century efforts to understand the optics and electromagnetism
of moving bodies. These roots are reviewed in Parts 1 and 2, the latter being
specially devoted to the works of Lorentz and of Poincare up to 1904. Part 3
contains a detailed comparison of the works of Einstein and of Poincare in
1905. It is shown that both contributors succeeded in constructing a coherent and
fully relativistic theory, although their ideas about the ether were radically
different. In Part 4, the question of the respective merits of the three
potential fathers of special relativity (i.e. Lorentz, Poincare, Einstein) is
discussed again, at the light of the preceding analysis and of the "post-1905"
developments of the theory.

<id>
physics/0101010v1
<category>
physics.hist-ph
<abstract>
Alonso de la Veracruz conducted a physical study of image, regarding the
activities of Soul in which image is produced, as organic operations. This
research was particularly important since, at that time, image was
systematically used to propagate the European culture in New Spain. Besides,
Alonso uses the visual radius to criticize the magical attitude popular in
Renaissance, denying the far sight and fascinations abilities attributed to
witches. Also, Alonso Guti\'errez applies the Aristotelian physics to deny the
healing powers attributed to the kings of France and England, secularizing this
way the gallic monarchies, and several other superstitions of that time. This
way, the physics of image developed by Alonso can be considered as close to the
rationalism of Descartes inasmuch as Guti\'errez criticizes the magical view of
Renaissance and introduces geometrical elements to elucidate physical problems.

<id>
physics/0101039v1
<category>
physics.hist-ph
<abstract>
In recent years, a change in attitude in particle physics has led to our
understanding current quantum field theories as effective field theories
(EFTs). The present paper is concerned with the significance of this EFT
approach, especially from the viewpoint of the debate on reductionism in
science. In particular, it is a purpose of this paper to clarify how EFTs may
provide an interesting case-study in current philosophical discussion on
reduction, emergence and inter-level relationships in general.

<id>
physics/0107009v2
<category>
physics.hist-ph
<abstract>
The principle that celestial bodies must move on circular orbits or on paths
resulting from the composition of circular orbits has been assumed as a
constant guide in the astronomical thougth of the peoples facing the
Mediterranean sea as from the second century B.C. until the beginning of the
XVII century. The mathematical model based on such an assumption, the theory of
epicycles in all its versions and modifications, has been taken as a scheme for
all astronomical calculations during at least eighteen centuries, from
Hipparcus to Kepler. As it is known, in Astronomia Nova (1609), Kepler succeded
to establish the two laws which after him were named the first and the second
Kepler's laws. The revolution he performed by giving up the circle paradigm is
of fundamental importance and represents the indispensable premise to Newtonian
theory. This revolution, the result of what Kepler called his "war" against
Mars, is usually underestimated if one considers the break carried out with
respect to the pre-Kepler celestial kinematics. In "Astronomia Nova" we assist
to the attempt of Kepler to get for Mars an orbit consistent with the results
of Tycho Brahe's observations. Passing through several phases and step by step
discarding the hypoteses wich gave results in contrast with observations,
Kepler arrived at establishing that the orbit of Mars around the Sun is an
ellipse. Here an esposition is given of this work of analysis by Kepler wich
is, in the history of science, one of the early examples of rigorous setting up
of a model which correctly explains the experimental results.

<id>
physics/0110028v1
<category>
physics.hist-ph
<abstract>
This paper has been translated into German and will be included, in a
somewhat altered form, in a book Sterne der Zertrummerung, Marietta Blau,
Wegbereiterin der Moderne Teilchenphysik, Brigitte Strohmaier and Robert
Rosner, eds., Boehlau Verlag, Wien.

<id>
physics/0110031v3
<category>
physics.hist-ph
<abstract>
Galileo's realization that nature is not scale invariant, motivating his
subsequent discovery of scaling laws, is traced to two lectures he gave on the
geography of Dante's Inferno.

<id>
physics/0201028v1
<category>
physics.hist-ph
<abstract>
In October 1934 Fermi discovered that neutrons became particularly effective
in rendering the elements radioactive after being slowed down by hydrogenous
substances. His was described as an absolutely unpredictable scientific
discovery. In this paper I report and discuss the knowledge about neutrons
properties already available the previous year, which throws a new light onto a
discovery too often depicted as "accidental".

<id>
physics/0204057v2
<category>
physics.hist-ph
<abstract>
Sir Arthur Eddington is considered one of the greatest astrophysicist of the
twentieth century and yet he gained a stigma when, in the 1930s, he embarked on
a quest to develop a unified theory of gravity and quantum mechanics. His
attempts ultimately proved fruitless and he was unfortunately partially shunned
by some physicists in the latter portion of his career. In addition some
historians have been less than kind to him regarding this portion of his work.
However, detailed analysis of how this work got started shows that Eddington's
theories were not as outlandish as they are often purported to be. His entire
theory rested on the use of quantum mechanical methods of uncertainty in the
reference frames of relativity. Though the work was ultimately not fruitful, in
hindsight it did foreshadow several later results in physics and his methods
were definitely rigorous. In addition, his philosophy regarding determinism and
uncertainty was actually fairly orthodox for his time. This work begins by
looking at Eddington's life and philosophy and uses this as a basis to explore
his work with uncertainty.

<id>
physics/0206076v2
<category>
physics.hist-ph
<abstract>
Since 1945 Canada has had a nuclear power industry based on reactor design
which uses natural uranium and heavy water. The tortuous and improbable
sequence of events which led to this situation is examined.

<id>
physics/0207026v2
<category>
physics.hist-ph
<abstract>
The cyclic model of the universe has an old history in India. It has held the
preeminent position regarding the origins of the universe, and it is described
in astronomical texts, Puranic encyclopaedias, and philosophical literature.
Within the current cycle, which is supposed to have begun several billion years
ago, are smaller cycles of pole changes and extinctions on earth. Salient
features of the cyclic universe model are presented here.

<id>
physics/0207043v1
<category>
physics.hist-ph
<abstract>
We discuss the extent to which the visibility of the heavens was a necessary
condition for the development of science, with particular reference to the
measurement of time. Our conclusion is that while astronomy had significant
importance, the growth of most areas of science was more heavily influenced by
the accuracy of scientific instruments, and hence by current technology.

<id>
physics/0207115v3
<category>
physics.hist-ph
<abstract>
Insofar as Sir Karl Raimund Popper's writings deal with political statements,
they are evident; yet insofar as they deal with scientific issues, they are
misleading. If applied to the concrete implementation of science, such as
distribution of research funds and (peer) review, they would seriously impede
progress.

<id>
physics/9703004v1
<category>
physics.ins-det
<abstract>
The detection efficiency and response function of a Si(Li) detector element
for the SIXA spectrometer have been determined in the 500 eV to 5 keV energy
range using synchrotron radiation emitted at a bending magnet of the electron
storage ring BESSY, which is a primary radiation standard. The agreement
between the measured spectrum and the model calculation is better than 2%.
  PACS: 95.55.Ka; 07.85.Nc; 29.40.Wk; 85.30.De
  Keywords: Si(Li) detectors, X-ray spectrometers, detector calibration, X-ray
response, spectral lineshape

<id>
physics/9706024v1
<category>
physics.ins-det
<abstract>
A ring imaging Cherenkov counter, to be read out by four 100-channel PMTs, is
a key element of the BRAHMS experiment. We report here the most recent results
obtained tested at the BNL AGS using several radiator gases, including the
heavy fluorocarbon C4F10. Ring radii were measured for different particles
(pions, muons, and electrons) for momenta ranging from 2 to 12 GeV/c employing
pure C4F10 as radiator.

<id>
physics/9708032v1
<category>
physics.ins-det
<abstract>
GaAs Schottky diode detectors have been fabricated upon Low Pressure Vapour
Phase Epitaxial GaAs. The devices were characterised before and after a $1.25
\times 10^{14}$~cm$^{-2}$ 24GeV/c proton fluence. The as fabricated Ti-GaAs
barrier height was measured, via two electrical methods, to be $0.81\pm0.005$
and $0.85\pm0.01$~eV and a space charge density of $2.8 \pm 0.2 \times
10^{14}$~cm$^{-3}$ was determined. The current was greater than that expected
for an ideal barrier with the excess attributed to generation current from the
bulk. The charge collection efficiency, determined from front alpha
illumination and 60 keV gamma irradiation, was inexcess of 95% at 50V reverse
bias. After irradiation the reverse current, measured for a bias of 200V at
20$^{o}$~C, increased from 90~nA to 1500~nA due to radiation induced generation
centres. Deep levels were showed to be present using capacitance techniques.
The charge collection of the device determined from front alpha illumination
fell to $32\pm5$% at a reverse bias of 200V.

<id>
physics/9708033v1
<category>
physics.ins-det
<abstract>
Semi-insulating, undoped, Liquid Encapsulated Czochralski (SI-U LEC) GaAs
detectors have been irradiated with 1MeV neutrons, 24GeV/c protons, and
300MeV/c pions. The maximum fluences used were 6, 3, and 1.8~10$^{14}$
particles/cm$^{2}$ respectively. For all three types of irradiation the charge
collection efficiencies (cce) of the detector are reduced due to the reduction
in the electron and hole mean free paths. Pion and proton irradiations produce
a greater reduction in cce than neutron irradiation with the pions having the
greatest effect. The effect of annealing the detectors at room temperature, at
200$^{o}$C and at 450$^{o}$C with a flash lamp have been shown to reduce the
leakage current and increase the cce of the irradiated detectors. The
flash-lamp anneal produced the greatest increase in the cce from 26% to 70% by
increasing the mean free path of the electrons. Two indium-doped samples were
irradiated with 24GeV/c protons and demonstrated no improvement over SI U GaAs
with respect to post-irradiation cce.

<id>
physics/9708034v1
<category>
physics.ins-det
<abstract>
The present understanding of the charge collection in GaAs detectors with
respect to the materials used and its processing are discussed. The radiation
induced degradation of the charge collection efficiency and the leakage current
of the detectors are summarised. The status of strip and pixel detectors for
the ATLAS experiment are reported along with the latest results from GaAs X-ray
detectors for non-high energy physics applications.

<id>
physics/9709001v1
<category>
physics.ins-det
<abstract>
The performance of 16 and 64 channel photomultipliers coupled to
scintillating fibres has been tested. The devices are sensitive to single
photoelectrons, show little gain losses for magnetic fields up to 100 Gauss and
have moderate optical cross-talk. The maximum channel to channel gain
variations reach a factor two for the 16 channel version and a factor of four
for the 64 channel PM. The measurements and simulations indicate that the
photomultipliers are well suited for the light detection in fibre trackers.

<id>
physics/9709027v1
<category>
physics.ins-det
<abstract>
We describe the electromagnetic calorimeter built for the GRAAL apparatus at
the ESRF. Its monitoring system is presented in detail. Results from tests and
the performance obtained during the first GRAAL experiments are given. The
energy calibration accuracy and stability reached is a small fraction of the
intrinsic detector resolution.

<id>
physics/9709034v1
<category>
physics.ins-det
<abstract>
The motivation for investigating the use of GaAs as a material for detecting
particles in experiments for High Energy Physics (HEP) arose from its perceived
resistance to radiation damage. This is a vital requirement for detector
materials that are to be used in experiments at future accelerators where the
radiation environments would exclude all but the most radiation resistant of
detector types.

<id>
physics/9709035v1
<category>
physics.ins-det
<abstract>
Thick epitaxial layers have been grown using Low Pressure Vapour Phase
Epitaxy techniques with low free carrier concentrations . This type of material
is attractive as a medium for X-ray detection, because of its high conversion
efficiency for X-rays in the medically interesting energy range.

<id>
physics/9709041v1
<category>
physics.ins-det
<abstract>
Full size single-sided GaAs microstrip detectors with integrated coupling
capacitors and bias resistors have been fabricated on 3'' substrate wafers.
PECVD deposited SiO_2 and SiO_2/Si_3N_4 layers were used to provide coupling
capacitaces of 32.5 pF/cm and 61.6 pF/cm, respectively. The resistors are made
of sputtered CERMET using simple lift of technique. The sheet resistivity of 78
kOhm/sq. and the thermal coefficient of resistance of less than 4x10^-3 /
degree C satisfy the demands of small area biasing resistors, working on a wide
temperature range.

<id>
physics/9710009v1
<category>
physics.ins-det
<abstract>
Cathode electrodes of the Si(Li) detector elements of the SIXA X-ray
spectrometer array are formed by gold-palladium alloy contact layers. The
equivalent thickness of gold in one element was measured by observing the
characteristic L-shell X-rays of gold excited by monochromatised synchrotron
radiation with photon energies above the L3 absorption edge of gold. The
results obtained at 4 different photon energies below the L2 edge yield an
average value of 22.4(35) nm which is consistent with the earlier result
extracted from detection efficiency measurements.
  PACS: 29.40.Wk; 85.30.De; 07.85.Nc; 95.55.Ka
  Keywords: Si(Li) detectors, X-ray spectrometers, X-ray fluorescence, detector
calibration, gold electrodes, synchrotron radiation

<id>
physics/9801005v1
<category>
physics.ins-det
<abstract>
Using scintillator tile technology several square meters of plastic
scintillator are read out by only two photomultiplier with a time precision of
about 1.5 nsec.
  Two examples are discussed to build a detector based on this technology to
search for cosmic muons and neutrinos.

<id>
physics/9801033v1
<category>
physics.ins-det
<abstract>
The E864 experiment at BNL requires a beam counter and multiplicity detector
system that can perform at an incident beam rate of 10^7 Au ions per second. We
have developed and tested a 150 micrometer thick quartz Cherenkov beam counter
and a scintillator based multiplicity-trigger counter during the first run of
this experiment in 1994. We obtained a time resolution of 78 ps for the beam
counter at an incident beam rate 5 x 10^5 Hz and 100 ps at a rate of 1 x 10^7
Hz. Pulse height discrimination is used to obtain a minimum bias and a 10%
centrality trigger from the multiplicity detectors. The multiplicity counter
has a time resolution of 250 ps.

<id>
physics/9803021v1
<category>
physics.ins-det
<abstract>
In this work we describe a parallel geometry gaseous detector constituted by
a thin (0.6 mm) amplifying gap preceded by a drift region. The gap is delimited
by a metal plate and by a wire mesh. The detector can count up to a rate of
10^7 Hz/mm2 at gain 1000 and retains at lower counting rates the high gains
(close to 10^6) that are typical of parallel geometry detectors. Additionally
it was found that in the thin amplifying gap the discharges seem to be
self-quenched and that full sparks do not develop. This fact, together with the
robustness of the electrode materials, makes this an extremely resilient
high-rate detector.

<id>
physics/9805035v1
<category>
physics.ins-det
<abstract>
The general purpose spherical nonmagnetic detector (SND) is now taking data
at VEPP-2M $e^+e^-$ collider in BINP (Novosibirsk) in the centre of mass energy
range of $0.2 \div 1.4$ GeV. The energy calibration of the NaI(Tl) calorimeter
of the SND detector with cosmic muons is described. Using this method, the
energy resolution of $5.5 % (\sigma)$ for 500 MeV photons was achieved.

<id>
physics/9806014v1
<category>
physics.ins-det
<abstract>
Calibration of the three layer NaI(Tl) spherical calorimeter of the SND
detector using electron -- positron scattering events is described. Energy
resolution of $5 % (FWHM/2.36)$ for 500 MeV photons was achieved.

<id>
physics/9807039v1
<category>
physics.ins-det
<abstract>
Gas electron multipliers (GEMs) have been overcoated with a high resistivity
10e14 - 10e15 Ohms / square amorphous carbon layer. The coating avoids charging
up of the holes and provides a constant gain immediately after switching on
independent of the rate. The gain uniformity across the GEM is improved.
Coating opens the possibility to produce thick GEMs of very high gain.

<id>
physics/9809027v1
<category>
physics.ins-det
<abstract>
We propose a method for in-situ measurement of the length of kilometer size
Fabry-Perot cavities in laser gravitational wave detectors. The method is based
on the vernier, which occurs naturally when the laser incident on the cavity
has a sideband. By changing the length of the cavity over several wavelengths
we obtain a set of carrier resonances alternating with sideband resonances.
  From the measurement of the separation between the carrier and a sideband
resonance we determine the length of the cavity. We apply the technique to the
measurement of the length of a Fabry-Perot cavity in the Caltech 40m
Interferometer and discuss the accuracy of the technique.

<id>
physics/9810004v1
<category>
physics.ins-det
<abstract>
New developments in HPD design are presented, triggered by applications in
high energy physics and astrophysics. The presented HPD designs are based on
three innovations. (i) In order to achieve the highest possible surface
coverage in a RICH detector, we introduced a photoelectron focussing method
which is efficient to the periphery of the photocathode. (ii) To prevent
positive ion feedback in HPDs, we introduced a permanent potential barrier in
front of the anode. (iii) To replace a transmittive by a reflective
photocathode, we arrived at a conceptually new HPD design with surprisingly
good imaging characteristics, high quantum efficiency and low cost.

<id>
physics/9811005v1
<category>
physics.ins-det
<abstract>
The apparatus description for control of the time parameters of
photomultipliers with high time resolution is described. For generation of
ultrashort light flashes have been used sonoluminescence effect -- emission of
the light flashes which is appearing at the stressed interior of collapsing air
microbubble by sound wave in water.

<id>
physics/9811013v1
<category>
physics.ins-det
<abstract>
In order to study photomultiplier's short-term gain stability at high
counting rate, we constructed an LED pulsed light source and its output monitor
system. For the monitor system, we employed a photon counting method using a
photomultiplier as a monitor photon detector. It is found that the method
offers a simple way to monitor outputs from a pulsed light source and that,
together with an LED light source, it provides a handy way to investigate
photomultiplier's rate effects.

<id>
physics/9811028v1
<category>
physics.ins-det
<abstract>
A general solution to the positive ion feedback problem in Hybrid Photon
Detectors (HPD), photo multipliers (PM) and other similar detectors was found
in the insertion of a permanent electrostatic potential barrier which prevents
drift of positive ions from the anode, or the first dynode, towards the
photocathode. In this paper we present the method as applied to the Intevac
HPD.

<id>
physics/9811034v1
<category>
physics.ins-det
<abstract>
A method was presented of profiling the magnetic field, with a zero vector of
magnetic flux density $B_{r}$ and strong gradient G, enabling conditions for
the Stern-Gerlach spectroscopy to be accomplished in condensed matter (the
method has been called magnetic field gradient spectroscopy - MGRS), as well
the previously proposed magnetic resonance spectroscopy for selected energy
states (SSMRS). Magnetic field distribution in a model electromagnet were shown
when condition $B_{r}= 0$ for resultant magnetic field was satisfied within the
sample

<id>
physics/9812001v1
<category>
physics.ins-det
<abstract>
Polarization characteristics of the gamma beam obtained by the Compton back
scattering of laser photons on high energy electrons are evaluated by
Monte-Carlo simulations.
  It is assumed that outgoing photons are tagged; the energy dispersion of the
tagging photons and emittance of the initial electrons are taken into account.
Dependence of the final photon polarization parameters on measured photon
energy is obtained. It is shown that polarization of final photons is
decreasing with change for the worth of the tagging energy resolution.
Calculations have been applied for the storage ring SIBERIA-2 at Kurchatov
Institute. The obtained results indicate a reasonability for construction of
gamma-polarimeters on existing and planned facilities for the on-line
measurement of the final photon beam polarization.

<id>
physics/9812015v1
<category>
physics.ins-det
<abstract>
An analog integrated circuit has been designed, in a BiCMOS 0.8 micron
technology, for the feasability study of the signal processing of the AMS RICH
photomultiplier tubes. This low power, three channel gated integrator includes
its own gate and no external analog delay is requiered. It processes PMT pulses
over a dynamic range of more than 100. A logic output that indicates whether
the analog charge has to be considered is provided. This gated integrator is
used with a compact DSP based acquisition system in a 132 channels RICH
prototype. The charge calibration of each channel is carried out using a LED.
The pedestal measurement is performed on activation of a dedicated input. The
noise contribution study of the input RC network and amplifiers is presented.

<id>
physics/9812018v1
<category>
physics.ins-det
<abstract>
We have implemented a cost-effective design for the readout electronics of
both the anode wires and the cathode pads of large area proportional wire
chambers for the HERA-B muon system based on the ASD-08 integrated circuit. To
control and monitor the large number of readout channels, we have built a
distributed control system based on Philips Semiconductors' I2C bus and
microcontrollers. To date we have installed about 10800 channels of muon
chambers and electronics. The average single channel noise occupancy is less
than 10**-5, and the detectors have been operated with target interaction rates
as high as 70 MHz.

<id>
physics/9812025v1
<category>
physics.ins-det
<abstract>
Imaging Hybrid Photon Detectors (HPD) have been developed for integration in
large area Cherenkov detectors for high energy physics and astrophysics. The
presented designs - developed particularly for the experiments MAGIC, LHCb and
AQUA-RICH - comprise very good imaging properties, protection against positive
ion feedback and(or) minimum dead area. The underlying innovations are
discussed in some detail.

<id>
physics/9812027v1
<category>
physics.ins-det
<abstract>
We present a procedure for the design and construction of a passive,
multipole, mechanical high-stop vibration isolator. The isolator, consisting of
a stack of metal disks connected by thin wires, attenuates frequencies in the
kilohertz range, and is suited to both vacuum and cryogenic environments. We
derive an approximate analytical model and compare its predictions for the
frequencies of the normal modes to those of a finite element analysis. The
analytical model is exact for the modes involving only motion along and
rotation about the longitudinal axis, and it gives a good approximate
description of the transverse modes. These results show that the high-frequency
behavior of a multi-stage isolator is well characterized by the natural
frequencies of a single stage. From the single-stage frequency formulae, we
derive relationships among the various geometrical parameters of the isolator
to guarantee equal attenuation in all degrees of freedom. We then derive
expressions for the attenuation attainable with a given isolator length, and
find that the most important limiting factor is the elastic limit of the spring
wire material. For our application, which requires attenuations of 250 dB at 1
kHz, our model specifies a six-stage design using brass disks of approximately
2 cm in both radius and thickness, connected by 3 cm steel wires of diameters
ranging from 25 to 75 microns. We describe the construction of this isolator in
detail, and compare measurements of the natural frequencies of a single stage
with calculations from the analytical model and the finite element package. For
translations along and rotations about the longitudinal axes, all three results
are in agreement to within 10% accuracy.

<id>
physics/9812042v1
<category>
physics.ins-det
<abstract>
The electronic system developed for the SpaCal lead/scintillating-fibre
calorimeters of the H1 detector in operation at the HERA ep collider is
described in detail and the performance achieved during H1 data-taking is
presented. The 10 MHz bunch crossing rate of HERA puts severe constraints on
the requirements of the electronics. The energy and time readout are performed
respectively with a 14-bit dynamic range and with a resolution of about 0.4 ns.
The trigger branch consists of a nanosecond-resolution calorimetric
time-of-flight for background rejection and an electron trigger based on analog
`sliding windows'. The on-line background rejection currently achieved is
o(10**6). The electron trigger allows a low energy trigger threshold to be set
at about 0.50 +/- 0.08 (RMS) GeV with an efficiency >99.9%. The energy and time
performance of the readout and trigger electronics is based on a
newly-developed low noise (sigma_noise ca. 0.4 MeV) wideband (f < 200 MHz)
preamplifier located at the output of the photomultipliers which are used for
the fibre light readout in the ca. 1 Tesla magnetic field of H1.

<id>
physics/9901018v1
<category>
physics.ins-det
<abstract>
A new type of field emission display(FED) based on an edge-enhance electron
emission from metal-insulator-semiconductor (MIS) thin film structure is
proposed. The electrons produced by an avalanche breakdown in the semiconductor
near the edge of a top metal electrode are initially injected to the thin film
of an insulator with a negative electron affinity (NEA), and then are injected
into vacuum in proximity to the top electrode edge. The condition for the
deep-depletition breakdown near the edge of the top metal electrode is
analytically found in terms of ratio of the insulator thickness to the maximum
(breakdown) width of the semiconductor depletition region: this ratio should be
less than 2/(3 \pi - 2) = 0.27. The influence of a neighboring metal electrode
and an electrode thickness on this condition are analyzed. Different practical
schemes of the proposed display with a special reference to M/CaF_2/Si
structure are considered.

<id>
physics/9902030v1
<category>
physics.med-ph
<abstract>
We propose and analyze in detail a method to measure the in-air spatial
spread parameter of clinical electron beams. Measurements are performed at the
center of the beam and below the adjustable collimators sited in asymmetrical
configuration in order to avoid the distortions due to the presence of the
applicator. The main advantage of our procedure lies in the fact that the dose
profiles are fitted by means of a function which includes, additionally to the
Gaussian step usually considered, a background which takes care of the dose
produced by different mechanisms that the Gaussian model does not account for.
As a result, the spatial spread is obtained directly from the fitting procedure
and the accuracy permits a good determination of the angular spread. The way
the analysis is done is alternative to that followed by the usual methods based
on the evaluation of the penumbra width. Besides, the spatial spread found
shows the quadratic-cubic dependence with the distance to the source predicted
by the Fermi-Eyges theory. However, the corresponding values obtained for the
scattering power are differing from those quoted by ICRU nr. 35 by a factor ~2
or larger, what requires of a more detailed investigation.

<id>
physics/9904061v2
<category>
physics.med-ph
<abstract>
The conflict in Yugoslavia has been a source of great concern due to the
radiological and toxic hazard posed by the alleged presence of depleted uranium
in NATO weapons. In the present study some worst-case scenaria are assumed in
order to assess the risk for Yugoslavia and its neighboring countries . The
risk is proved to be negligible for the neighboring countries while for
Yugoslavia itself evidence is given that any increase in total long-term cancer
mortality will be so low that it will remain undetected. Local radioactive
hotspots such as DU weapons fragments and abandoned battle tanks, fortified or
contaminated with DU, constitute a post-war hazard which is not studied in this
article.

<id>
physics/9911017v1
<category>
physics.med-ph
<abstract>
A reliable spectral analysis requires sampling rate at least twice as large
as the frequency bound, otherwise the analysis will be unreliable and plagued
with aliasing distortions. The RR samplings do not satisfy the above
requirements and therefore their spectral analysis might be unreliable.
  In order to demonstrate the feasibility of aliasing in RR spectral analysis,
we have done an experiment which have shown clearly how the aliasing was
developed. In the experiments, one of us (A.G) had kept his high breathing rate
constant with the aid of metronome for more than 5 minutes. The breathing rate
was larger than one-half the heart rate. Very accurate results were obtained
and the resulting aliasing well understood. To our best knowledge this is the
first controlled experiment of this kind coducted on humans.
  We compared the RR spectral analysis with the spectrum of the ECG signals
from which the RR intervals were extracted. In the significant for RR analysis
frequencies (below one-half Hertz) significant differences were observed.
  In conclusion we recommend to study the spectral analysis of the ECG signal
in the free of aliasing frequency range.

<id>
physics/0003094v3
<category>
physics.med-ph
<abstract>
There are a number of constraints which limit the current and voltages which
can be applied on a multiple drive electrical imaging system. One obvious
constraint is to limit the maximum Ohmic power dissipated in the body. Current
patterns optimising distinguishability with respect to this constraint are
singular functions of the difference of transconductance matrices with respect
to the power norm. (the optimal currents of Isaacson). If one constrains the
total current ($L^1$ norm) the optimal patterns are pair drives. On the other
hand if one constrains the maximum current on each drive electrode (an
$L^\infty$ norm), the optimal patterns have each drive channel set to the
maximum source or sink current value. In this paper we consider appropriate
safety constraints and discuss how to find the optimal current patterns with
those constraints.

<id>
physics/0103047v1
<category>
physics.med-ph
<abstract>
This work is an exposure assessment for a population living in an area
contaminated by use of depleted uranium (DU) weapons. RESRAD 5.91 code is used
to evaluate the average effective dose delivered from 1, 10, 20 cm depths of
contaminated soil, in a residential farmer scenario. Critical pathway and group
are identified in soil inhalation or ingestion and children playing with the
soil, respectively. From available information on DU released on targeted
sites, both critical and average exposure can leave to toxicological hazards;
annual dose limit for population can be exceeded on short-term period (years)
for soil inhalation. As a consequence, in targeted sites cleaning up must be
planned on the basis of measured concentration, when available, while special
cautions have to be adopted altogether to reduce unaware exposures, taking into
account the amount of the avertable dose.

<id>
physics/0109054v1
<category>
physics.med-ph
<abstract>
The uncertainty relationship in MRI is shown. The result of uncertainty
relationship is compared with other factors influencing the resolution of MRI.
Our estimations show that the uncertainty relationship is of no significance in
practice.

<id>
physics/0210071v2
<category>
physics.med-ph
<abstract>
It is shown that the radiological burden due to the battlefield use of circa
400 tons of depleted-uranium munitions in Iraq (and of about 40 tons in
Yugoslavia) is comparable to that arising from the hypothetical battle-field
use of more than 600 kt (respectively 60 kt) of high-explosive equivalent
pure-fusion fourth-generation nuclear weapons.
  Despite the limited knowledge openly available on existing and future nuclear
weapons, there is sufficient published information on their physical principles
and radiological effects to make such a comparison. In fact, it is shown that
this comparison can be made with very simple and convincing arguments so that
the main technical conclusions of the paper are undisputable -- although it
would be worthwhile to supplement the hand calculations presented in the paper
by more detailed computer simulations in order to consolidate the conclusions
and refute any possible objections.

<id>
physics/0211064v1
<category>
physics.med-ph
<abstract>
We report the development of a scalar quantization approach that helps build
tables of decision and reconstruction levels for any probability density
function (pdf). Several example pdf's are used for illustration:
  Uniform, Gaussian, Laplace, one-sided Rayleigh, and Gamma (One sided and
double-sided symmetrical). The main applications of the methodology are
principally aimed at Multiresolution Image compression where generally the
Stretched Exponential pdf is encountered. Specialising to this important case,
we perform quantization and information entropy calculations from selected
medical MRI (Magnetic Resonance Imaging) pictures of the human brain. The image
histograms are fitted to a Stretched exponential model and the corresponding
entropies are compared.

<id>
physics/0302097v2
<category>
physics.med-ph
<abstract>
The relaxation effects in the perturbed angular correlation spectra of
indium-111 human carbonic anhydrase I (HCA I) are the result of chemical
transmutation and/or the complex Auger cascades that follow the electron
capture decay of indium-111. Time differential K X ray coincidence perturbed
angular correlation (PAC) spectroscopy shows that these relaxation effects are
independent of the Auger cascade intensity. This suggests that chemical
transmutation is responsible for the relaxation effects, and that bond breaking
and damage product formation around the decay site resulting from localized
energy deposition by Auger and Coster-Kronig electrons probably occur in the
microsecond time regime. Numerical simulations of chemical transmutation
relaxation effects in the time differential PAC spectrum of indium-111 HCA I
are also presented.

<id>
physics/0304019v1
<category>
physics.med-ph
<abstract>
Purpose of this work is the development of an automatic system which could be
useful for radiologists in the investigation of breast cancer. A breast
neoplasia is often marked by the presence of microcalcifications and massive
lesions in the mammogram: hence the need for tools able to recognize such
lesions at an early stage. GPCALMA (Grid Platform Computer Assisted Library for
MAmmography), a collaboration among italian physicists and radiologists, has
built a large distributed database of digitized mammographic images (at this
moment about 5500 images corresponding to 1650 patients). This collaboration
has developed a CAD (Computer Aided Detection) system which, installed in an
integrated station, can also be used for digitization, as archive and to
perform statistical analysis. With a GRID configuration it would be possible
for the clinicians tele- and co-working in new and innovative groupings
('virtual organisations') and, using the whole database, by the GPCALMA tools
several analysis can be performed. Furthermore the GPCALMA system allows to be
abreast of the CAD technical progressing into several hospital locations always
with remote working by GRID connection. We report in this work the results
obtained by the GPCALMA CAD software implemented with a GRID connection.

<id>
physics/0306198v1
<category>
physics.med-ph
<abstract>
The use of an automatic system for the analysis of mammographic images has
proven to be very useful to radiologists in the investigation of breast cancer,
especially in the framework of mammographic-screening programs. A breast
neoplasia is often marked by the presence of microcalcification clusters and
massive lesions in the mammogram: hence the need for tools able to recognize
such lesions at an early stage. In the framework of the GPCALMA (GRID Platform
for Computer Assisted Library for MAmmography) project, the co-working of
italian physicists and radiologists built a large distributed database of
digitized mammographic images (about 5500 images corresponding to 1650
patients) and developed a CAD (Computer Aided Detection) system, able to make
an automatic search of massive lesions and microcalcification clusters. The CAD
is implemented in the GPCALMA integrated station, which can be used also for
digitization, as archive and to perform statistical analyses. Some GPCALMA
integrated stations have already been implemented and are currently on clinical
trial in some italian hospitals. The emerging GRID technology can been used to
connect the GPCALMA integrated stations operating in different medical centers.
The GRID approach will support an effective tele- and co-working between
radiologists, cancer specialists and epidemiology experts by allowing remote
image analysis and interactive online diagnosis.

<id>
physics/0307098v1
<category>
physics.med-ph
<abstract>
The purpose of this study is the evaluation of the variation of performance
in terms of sensitivity and specificity of two radiologists with different
experience in mammography, with and without the assistance of two different CAD
systems. The CAD considered are SecondLookTM (CADx Medical Systems, Canada),
and CALMA (Computer Assisted Library in MAmmography). The first is a commercial
system, the other is the result of a a research project, supported by INFN
(Istituto Nazionale di Fisica Nucleare, Italy); their characteristics have been
already reported in literature. To compare the results with and without these
tools, a dataset composed by 70 images of patients with cancer (biopsy proven)
and 120 images of healthy breasts (with a three years follow up) has been
collected. All the images have been digitized and analysed by two CAD, then two
radiologists with respectively 6 and 2 years of experience in mammography
indipendently made their diagnosis without and with, the support of the two CAD
systems. In this work sensitivity and specificity variation, the Az area under
the ROC curve, are reported. The results show that the use of a CAD allows for
a substantial increment in sensitivity and a less pronounced decrement in
specificity. The extent of these effects depends on the experience of the
readers and is comparable for the two CAD considered.

<id>
physics/0307099v1
<category>
physics.med-ph
<abstract>
The CALMA (Computer Assisted Library for MAmmography) project is a five years
plan developed in a physics research frame in collaboration between INFN
(Istituto Nazionale di Fisica Nucleare) and many Italian hospitals. At present
a large database of digitized mammographic images (more than 6000) was
collected and a software based on neural network algorithms for the search of
suspicious breast lesions was developed. Two tools are available: a
microcalcification clusters hunter, based on supervised and unsupervised
feedforward neural network, and a massive lesions searcher, based on a hibrid
approach. Both the algorithms analyzed preprocessed digitized images by high
frequency filters. Clinical tests were performed to evaluate sensitivity and
specificity of the system, considering the system as alone and as secon reader.
Results show that the system is ready to be implemented by medical industry.
The CALMA project, just ended, has its natural development in the GPCALMA (Grid
Platform for CALMA) project, where distributed users join common resources
(images, tools, statistical analysis).

<id>
physics/0310150v1
<category>
physics.med-ph
<abstract>
The dosimetry for radiocolloid therapy of cystic craniopharyngiomas is
investigated. Analytical calculations based on the Loevinger and the Berger
formulae for electrons and photons, respectively, are compared with Monte Carlo
simulations. The role of the material of which the colloid introduced inside
the craniopharyngioma is made of as well as that forming the cyst wall is
analyzed. It is found that the analytical approaches provide a very good
description of the simulated data in the conditions where they can be applied
(i.e., in the case of a uniform and infinite homogeneous medium). However, the
consideration of the different materials and interfaces produces a strong
reduction of the dose delivered to the cyst wall in relation to that predicted
by the Loevinger and the Berger formulae.

<id>
physics/0310151v2
<category>
physics.med-ph
<abstract>
We review developments, issues and challenges in Electrical Impedance
Tomography (EIT), for the 4th Workshop on Biomedical Applications of EIT,
Manchester 2003. We focus on the necessity for three dimensional data
collection and reconstruction, efficient solution of the forward problem and
present and future reconstruction algorithms. We also suggest common pitfalls
or ``inverse crimes'' to avoid.

<id>
physics/0311110v1
<category>
physics.med-ph
<abstract>
Monte Carlo calculations using the codes PENELOPE and GEANT4 have been
performed to characterize the dosimetric parameters of the new 20 mm long
catheter based $^{32}$P beta source manufactured by Guidant Corporation. The
dose distribution along the transverse axis and the two dimensional dose rate
table have been calculated. Also, the dose rate at the reference point, the
radial dose function and the anisotropy function were evaluated according to
the adapted TG-60 formalism for cylindrical sources. PENELOPE and GEANT4 codes
were first verified against previous results corresponding to the old 27 mm
Guidant $^{32}$P beta source. The dose rate at the reference point for the
unsheathed 27 mm source in water was calculated to be $0.215 \pm 0.001$ cGy
s$^{-1}$ mCi$^{-1}$, for PENELOPE, and $0.2312 \pm 0.0008$ cGy s$^{-1}$
mCi$^{-1}$, for GEANT4. For the unsheathed 20 mm source these values were
$0.2908 \pm 0.0009$ cGy s$^{-1}$ mCi$^{-1}$ and $0.311 \pm 0.001$ cGy s$^{-1}$
mCi$^{-1}$, respectively. Also, a comparison with the limited data available on
this new source is shown. We found non negligible differences between the
results obtained with PENELOPE and GEANT4.

<id>
physics/0312019v1
<category>
physics.med-ph
<abstract>
GePEToS is a simulation framework developed over the last few years for
assessing the instrumental performance of future PET scanners. It is based on
Geant4, written in Object-Oriented C++ and runs on Linux platforms. The
validity of GePEToS has been tested on the well-known Siemens ECAT EXACT HR+
camera. The results of two application examples are presented : the design
optimization of a liquid Xe micro-PET camera dedicated to small animal imaging
as well as the evaluation of the effect of a strong axial magnetic field on the
image resolution of a Concorde P4 micro-PET camera.

<id>
physics/0312097v1
<category>
physics.med-ph
<abstract>
This paper describes the algorithm and examines the performance of an IMRT
beam-angle optimization (BAO) system. In this algorithm successive sets of beam
angles are selected from a set of predefined directions using a fast simulated
annealing (FSA) algorithm. An IMRT beam-profile optimization is performed on
each generated set of beams. The IMRT optimization is accelerated by using a
fast dose calculation method that utilizes a precomputed dose kernel. A compact
kernel is constructed for each of the predefined beams prior to starting the
FSA algorithm. The IMRT optimizations during the BAO are then performed using
these kernels in a fast dose calculation engine. This technique allows the IMRT
optimization to be performed more than two orders of magnitude faster than a
similar optimization that uses a convolution dose calculation engine.

<id>
physics/0401130v1
<category>
physics.med-ph
<abstract>
Optical pumping of He-3 produces large (hyper) nuclear-spin polarizations
independent of the magnetic resonance imaging (MRI) field strength. This allows
lung MRI to be performed at reduced fields with many associated benefits, such
as lower tissue susceptibility gradients and decreased power absorption rates.
Here we present results of 2D imaging as well as accurate 1D gas diffusion
mapping of the human lung using He-3 at very low field (3 mT). Furthermore,
measurements of transverse relaxation in zero applied gradient are shown to
accurately track pulmonary oxygen partial pressure, opening the way for novel
imaging sequences.

<id>
physics/0402052v1
<category>
physics.med-ph
<abstract>
In this work we develop a new method of diagnosing the nervous system
diseases and a new approach in studying human gait dynamics with the help of
the theory of discrete non-Markov random processes. The stratification of the
phase clouds and the statistical non-Markov effects in the time series of the
dynamics of human gait are considered. We carried out the comparative analysis
of the data of four age groups of healthy people: children (from 3 to 10 year
olds), teenagers (from 11 to 14 year oulds), young people (from 21 up to 29
year oulds), elderly persons (from 71 to 77 year olds) and Parkinson patients.
The full data set are analyzed with the help of the phase portraits of the four
dynamic variables, the power spectra of the initial time correlation function
and the memory functions of junior orders, the three first points in the
spectra of the statistical non-Markov parameter. The received results allow to
define the predisposition of the probationers to deflections in the central
nervous system caused by Parkinson's disease. We have found out distinct
differencies between the five submitted groups. On this basis we offer a new
method of diagnostics and forecasting Parkinson's disease.

<id>
physics/0403041v2
<category>
physics.med-ph
<abstract>
Our interest has been to study the effect that scattered radiation has on
contrast, signal-to-noise ratio and thickness reconstruction in digital
mammographies. Using the GEANT code we have performed Monte-Carlo simulations
of 25 kVp Mo/Mo photons, through a breast phantom which contains a 0.2-1.0 mm
thick microcalcifications incident on a 20x106 $mm^{2}$ pixelized detector. The
data have been analyzed assuming 6 different shapes of the incident beam: a
0.2x0.2 $mm^{2}$ ``narrow'' beam, 4 different 20 mm long scanning beams of
various widths, and a 20x100 $mm^{2}$ beam with no scatter reduction mechanisms
(NSR) . Since the image of a point depends on scattered photons which passed up
to 2 cm away from the object (for 4 cm thick phantom), we identify the
background definition as a main source of systematic uncertainty in the image
quality analysis. We propose the use of two dimensional functions (a polynomial
for the background and Gaussians for the signal) for total photon transmission
description. Our main results indicate the possible calcification thickness
reconstruction with an accuracy of the order of 6% using 3 mm wide scanning
beam. Signal-to-noise ratio with the 3 mm wide beam gets improved by 20% with
respect to NSR, a figure similar to that obtained with the narrow beam.
Thickness reconstruction is shown to be an alternative to signal-to-noise ratio
for microcalcification detection.

<id>
physics/0403054v1
<category>
physics.med-ph
<abstract>
The impact of field shape optimization is studied for prostate type geometry.
For this study, 76 and 81 Gy plans were generated. Dose distributions for
wedged plans and Intensity Modulated (IM) plans for three and seven fields were
compared for a quadratic cost function. For wedged plans, a Simulated Annealing
Algorithm (SAA) was used to optimize gantry angles, wedge angles, beam weights
and field shapes. Two kinds of wedged plans were generated: 1) field sizes were
determined by the requirement of full target coverage in the beam's-eye-view
(fixed fields) and 2) the field shape, in particular at the critical organ
target overlap region was also among the variables optimized. For IM plans the
SAA was used to optimize gantry angles and a conjugate gradient algorithm was
used to optimize the IM beam fluences. Both the field shape optimized wedged
plans and IM plans had significantly superior dose area histograms of the
target, rectum and the bladder and cost function values compared to the fixed
field optimized wedged plans.

<id>
physics/0403095v1
<category>
physics.med-ph
<abstract>
Monte Carlo simulations using the code PENELOPE have been performed to test a
simplified model of the source channel geometry of the Leksell
GammaKnife$^{\circledR}$. The characteristics of the radiation passing through
the treatment helmets are analysed in detail. We have found that only primary
particles emitted from the source with polar angles smaller than 3$^{\rm o}$
with respect to the beam axis are relevant for the dosimetry of the Gamma
Knife. The photons trajectories reaching the output helmet collimators at
$(x,y,z=236 {\rm mm})$, show strong correlations between $\rho=(x^2+y^2)^{1/2}$
and their polar angle $\theta$, on one side, and between $\tan^{-1}(y/x)$ and
their azimuthal angle $\phi$, on the other. This enables us to propose a
simplified model which treats the full source channel as a mathematical
collimator. This simplified model produces doses in excellent agreement with
those found for the full geometry. In the region of maximal dose, the relative
differences between both calculations are within 3%, for the 18 and 14 mm
helmets, and 10%, for the 8 and 4 mm ones. Besides, the simplified model
permits a strong reduction (larger than a factor 15) in the computational time.

<id>
physics/0406054v1
<category>
physics.med-ph
<abstract>
Results of systematic measurements of Sr-90 activity concentrations in milk
for the period 1961 - 2001 are summarized. An exponential decline of
radioactivity followed the moratorium on atmospheric nuclear testing. The
highest activity of Sr-90 deposited by fallout, being 1060 Bq/m2, was recorded
in 1963, while the peak Sr-90 activity concentration in milk, 1.42 +/-0.17
Bq/L, was recorded in 1964. The values in year 2001 for fallout deposition and
milk were 7.7 Bq/m2 and 0.07 +/- 0.03 Bq/L, respectively. The reactor accident
at Chernobyl caused higher Sr-90 levels only in 1986. Sr-90 fallout activity
affects milk activity, the coefficient of correlation between Sr-90 fallout
activity and Sr-90 activity concentrations in milk being 0.80. The transfer
coefficient from fallout deposition to milk was estimated to be 2.5 mBqy/L per
Bq/m2. The dose incurred by milk consumption was estimated for the Croatian
population, the annual collective effective dose in 2001 being approximately
2.0 man-Sv.

<id>
physics/0406062v1
<category>
physics.med-ph
<abstract>
To accurately verify the dose of intensity-modulated radiation therapy
(IMRT), we have used a global optimization method to investigate a new
dose-verification algorithm. In practical application of this quality assurance
(QA) procedure, verification of the dose using calculated and measured dose
distributions involves a subtle problem in the region of high dose gradient.
Consideration of systematic errors shows that the large dose differences in
high-dose-gradient regions are due to the unexpected shift of measuring
devices. We have proposed an optimization algorithm to correct this error, and
an optimization method to minimize the average dose difference has been used in
this study. The relationship between the dose-verification procedure and the
applied optimization algorithm is explained precisely. Optimization
dramatically reduced the difference between measured and calculated dose
distributions in all cases investigated. The obtained results support the
relevance of our explanations for the problem in the high-dose-gradient region.
We have described this dose-verification procedure for IMRT and
intensity-modulated radiosurgery. Through this study we have also developed an
intuitive reporting method that is statistically reasonable.

<id>
physics/0406142v1
<category>
physics.med-ph
<abstract>
A simplification of the source channel geometry of the Leksell Gamma
Knife$^{\circledR}$, recently proposed by the contributors and checked for a single
source configuration (Al-Dweri et al 2004), has been used to calculate the dose
distributions along the $x$, $y$ and $z$ axes in a water phantom with a
diameter of 160~mm, for different configurations of the Gamma Knife including
201, 150 and 102 unplugged sources. The code PENELOPE (v. 2001) has been used
to perform the Monte Carlo simulations. In addition, the output factors for the
14, 8 and 4~mm helmets have been calculated. The results found for the dose
profiles show a qualitatively good agreement with previous ones obtained with
EGS4 and PENELOPE (v. 2000) codes and with the predictions of
GammaPlan$^{\circledR}$. The output factors obtained with our model agree
within the statistical uncertainties with those calculated with the same Monte
Carlo codes and with those measured with different techniques. Owing to the
accuracy of the results obtained and to the reduction in the computational time
with respect to full geometry simulations (larger than a factor 15), this
simplified model opens the possibility to use Monte Carlo tools for planning
purposes in the Gamma Knife$^{\circledR}$.

<id>
physics/0406157v1
<category>
physics.med-ph
<abstract>
Cardiomagnetometry is a growing field of noninvasive medical diagnostics that
has triggered a need for affordable high-sensitivity magnetometers. Optical
pumping magnetometers are promising candidates satisfying that need since it
was demonstrated that they can map the heart magnetic field. For the
optimization of such devices theoretical limits on the performance as well as
an experimental approach is presented. The promising result is a intrinsic
magnetometric sensitivity of 63 fT / Hz^1/2 a measurement bandwidth of 140 Hz
and a spatial resolution of 28 mm.

<id>
physics/0410082v1
<category>
physics.med-ph
<abstract>
A breast neoplasia is often marked by the presence of microcalcifications and
massive lesions in the mammogram: hence the need for tools able to recognize
such lesions at an early stage. Our collaboration, among italian physicists and
radiologists, has built a large distributed database of digitized mammographic
images and has developed a Computer Aided Detection (CADe) system for the
automatic analysis of mammographic images and installed it in some Italian
hospitals by a GRID connection. Regarding microcalcifications, in our CADe
digital mammogram is divided into wide windows which are processed by a
convolution filter; after a self-organizing map analyzes each window and
produces 8 principal components which are used as input of a neural network
(FFNN) able to classify the windows matched to a threshold. Regarding massive
lesions we select all important maximum intensity position and define the ROI
radius. From each ROI found we extract the parameters which are used as input
in a FFNN to distinguish between pathological and non-pathological ROI. We
present here a test of our CADe system, used as a second reader and a
comparison with another (commercial) CADe system.

<id>
physics/0410084v1
<category>
physics.med-ph
<abstract>
The GPCALMA (Grid Platform for Computer Assisted Library for MAmmography)
collaboration involves several departments of physics, INFN sections, and
italian hospitals. The aim of this collaboration is developing a tool that can
help radiologists in early detection of breast cancer. GPCALMA has built a
large distributed database of digitised mammographic images (about 5500 images
corresponding to 1650 patients) and developed a CAD (Computer Aided Detection)
software which is integrated in a station that can also be used for acquire new
images, as archive and to perform statistical analysis. The images are
completely described: pathological ones have a consistent characterization with
radiologist's diagnosis and histological data, non pathological ones correspond
to patients with a follow up at least three years. The distributed database is
realized throught the connection of all the hospitals and research centers in
GRID tecnology. In each hospital local patients digital images are stored in
the local database. Using GRID connection, GPCALMA will allow each node to work
on distributed database data as well as local database data. Using its database
the GPCALMA tools perform several analysis. A texture analysis, i.e. an
automated classification on adipose, dense or glandular texture, can be
provided by the system. GPCALMA software also allows classification of
pathological features, in particular massive lesions analysis and
microcalcification clusters analysis. The performance of the GPCALMA system
will be presented in terms of the ROC (Receiver Operating Characteristic)
curves. The results of GPCALMA system as "second reader" will also be
presented.

<id>
physics/0410085v1
<category>
physics.med-ph
<abstract>
The next generation of High Energy Physics (HEP) experiments requires a GRID
approach to a distributed computing system and the associated data management:
the key concept is the Virtual Organisation (VO), a group of distributed users
with a common goal and the will to share their resources. A similar approach is
being applied to a group of Hospitals which joined the GPCALMA project (Grid
Platform for Computer Assisted Library for MAmmography), which will allow
common screening programs for early diagnosis of breast and, in the future,
lung cancer. HEP techniques come into play in writing the application code,
which makes use of neural networks for the image analysis and proved to be
useful in improving the radiologists' performances in the diagnosis. GRID
technologies allow remote image analysis and interactive online diagnosis, with
a potential for a relevant reduction of the delays presently associated to
screening programs. A prototype of the system, based on AliEn GRID Services, is
already available, with a central Server running common services and several
clients connecting to it. Mammograms can be acquired in any location; the
related information required to select and access them at any time is stored in
a common service called Data Catalogue, which can be queried by any client. The
result of a query can be used as input for analysis algorithms, which are
executed on nodes that are in general remote to the user (but always local to
the input images) thanks to the PROOF facility. The selected approach avoids
data transfers for all the images with a negative diagnosis (about 95% of the
sample) and allows an almost real time diagnosis for the 5% of images with high
cancer probability.

<id>
physics/9703003v1
<category>
physics.optics
<abstract>
A numerical study of the properties of Gaussian pulses propagating in planar
waveguide under the combined effect of positive Kerr-type nonlinearity,
diffraction in planar waveguides and anomalous or normal dispersion, is
presented. It is demonstrated how the relative strength of dispersion and
diffraction, the strength of nonlinearity and the initial spatial and temporal
pulse chirps effect on the parameters of pulse compression, such as the maximal
compression factor and the distance to the point of maximal compression.

<id>
physics/9704005v1
<category>
physics.optics
<abstract>
We report on a theoretical and numerical investigation of the switching of
power in new hybrid models of nonlinear coherent couplers consisting of optical
slab waveguides with various orders of nonlinearity. The first model consists
of two guides with second-order instead of the usual third-order
susceptibilities as typified by the Jensen coupler. This second-order system is
shown to have a power self-trapping transition at a critical power greater than
the third-order susceptibility coupler. Next, we consider a mixed coupler
composed of a second-order guide coupled to a third-order guide and show that,
although it does not display a rigorous self-trapping transition, for a
particular choice of parameters it does show a fairly abrupt trapping of power
at a lower power than in the third-order coupler. By coupling this mixed
nonlinear pair to a third, purely linear guide, the power trapping can be
brought to even lower levels and in this way a satisfactory switching profile
can be achieved at less than one sixth the input power needed in the Jensen
coupler.

<id>
physics/9707016v1
<category>
physics.optics
<abstract>
It is noted that the Jones-matrix formalism for polarization optics is a
six-parameter two-by-two representation of the Lorentz group. It is shown that
the four independent Stokes parameters form a Minkowskian four-vector, just
like the energy-momentum four-vector in special relativity. The optical filters
are represented by four-by-four Lorentz-transformation matrices. This
four-by-four formalism can deal with partial coherence described by the Stokes
parameters. A four-by-four matrix formulation is given for decoherence effects
on the Stokes parameters, and a possible experiment is proposed. It is shown
also that this Lorentz-group formalism leads to optical filters with a symmetry
property corresponding to that of two-dimensional Euclidean transformations.

<id>
physics/9707024v1
<category>
physics.optics
<abstract>
We study the electromagnetic scattering by multilayered biperiodic aggregates
of dielectric layers and gratings of conducting plates. We show that the
characteristic lengths of such structures provide a good control of absorption
bands. The influence of the physical parameters of the problem (sizes,
impedances) is discussed.

<id>
physics/9710021v1
<category>
physics.optics
<abstract>
The propagation of an electromagnetic pulse in a plasma is studied for pulse
durations that are comparable to the plasma period. When the carrier frequency
of the incident pulse is much higher than the plasma frequency, the pulse
propagates without distortion at its group speed. When the carrier frequency is
comparable to the plasma frequency, the pulse is distorted and leaves behind it
an electromagnetic wake.

<id>
physics/9712029v1
<category>
physics.optics
<abstract>
We present scattering from many body systems in a new light. In place of the
usual van Hove treatment, (applicable to a wide range of scattering processes
using both photons and massive particles) based on plane waves, we calculate
the scattering amplitude as a space-time integral over the scattering sample
for an incident wave characterized by its correlation function which results
from the shaping of the wave field by the apparatus. Instrument resolution
effects - seen as due to the loss of correlation caused by the path differences
in the different arms of the instrument are automatically included and analytic
forms of the resolution function for different instruments are obtained. The
intersection of the moving correlation volumes (those regions where the
correlation functions are significant) associated with the different elements
of the apparatus determines the maximum correlation lengths (times) that can be
observed in a sample, and hence, the momentum (energy) resolution of the
measurement. This geometrical picture of moving correlation volumes derived by
our technique shows how the interaction of the scatterer with the wave field
shaped by the apparatus proceeds in space and time. Matching of the correlation
volumes so as to maximize the intersection region yields a transparent,
graphical method of instrument design. PACS: 03.65.Nk, 3.80 +r, 03.75, 61.12.B

<id>
physics/9712051v2
<category>
physics.optics
<abstract>
In this paper we extend for the case of Maxwell equations the "X-shaped"
solutions previously found in the case of scalar (e.g., acoustic) wave
equations. Such solutions are localized in theory, i.e., diffraction-free and
particle-like (wavelets), in that they maintain their shape as they propagate.
In the electromagnetic case they are particularly interesting, since they are
expected to be Superluminal. We address also the problem of their practical,
approximate production by finite (dynamic) radiators. Finally, we discuss the
appearance of the X-shaped solutions from the purely geometric point of view of
the Special Relativity theory.
  [PACS nos.: 03.50.De; 1.20.Jb; 03.30.+p; 03.40.Kf; 14.80.-j.
  Keywords: X-shaped waves; localized solutions to Maxwell equations;
Superluminal waves; Bessel beams; Limited-dispersion beams; electromagnetic
wavelets; Special Relativity; Extended Relativity].

<id>
physics/9802032v3
<category>
physics.optics
<abstract>
This paper has been withdrawn by the contributors until some changes are made.

<id>
physics/9803013v1
<category>
physics.optics
<abstract>
The effect of dispersion or diffraction on zero-velocity solitons is studied
for the generalized massive Thirring model describing a nonlinear optical fiber
with grating or parallel-coupled planar waveguides with misaligned axes. The
Thirring solitons existing at zero dispersion/diffraction are shown numerically
to be separated by a finite gap from three isolated soliton branches. Inside
the gap, there is an infinity of multi-soliton branches. Thus, the Thirring
solitons are structurally unstable. In another parameter region (far from the
Thirring limit), solitons exist everywhere.

<id>
physics/9804003v1
<category>
physics.optics
<abstract>
We theoretically study reflection of light by a phase-conjugating mirror
preceded by a partially reflecting normal mirror. The presence of a suitably
chosen normal mirror in front of the phase conjugator is found to greatly
enhance the total phase-conjugate reflected power, even up to an order of
magnitude. Required conditions are that the phase-conjugating mirror itself
amplifies upon reflection and that constructive interference of light in the
region between the mirrors takes place. We show that the phase-conjugate
reflected power then exhibits a maximum as a function of the transmittance of
the normal mirror.

<id>
physics/9804006v1
<category>
physics.optics
<abstract>
Reliable control of the deposition process of optical films and coatings
frequently requires monitoring of the refractive index profile throughout the
layer. In the present work a simple in situ approach is proposed which uses a
WKBJ matrix representation of the optical transfer function of a single thin
film on a substrate. Mathematical expressions are developed which represent the
minima and maxima envelopes of the curves transmittance-vs-time and
reflectance-vs-time. The refractive index and extinction coefficient depth
profiles of different films are calculated from simulated spectra as well as
from experimental data obtained during PECVD of silicon-compound films.
Variation of the deposition rate with time is also evaluated from the position
of the spectra extrema as a function of time. The physical and mathematical
limitations of the method are discussed.

<id>
physics/9804037v1
<category>
physics.optics
<abstract>
A new definition for the electromagnetic field velocity is proposed. The
velocity depends on the physical fields.

<id>
physics/9805015v1
<category>
physics.optics
<abstract>
We have fabricated light emitting diodes (LEDs) with Schottky contacts on
Si-nanocrystals formed by simple techniques as used for standard Si devices.
Orange electroluminescence (EL) from these LEDs could be seen with the naked
eye at room temperature when a reverse bias voltage was applied. The EL
spectrum has a major peak with a photon energy of 1.9 eV and a minor peak with
a photon energy of 2.2 eV. Since the electrons and holes are injected into the
radiative recombination centers related to nanocrystals through avalanche
breakdown, the voltage needed for a visible light emission is reduced to 4.0 -
4.5 V, which is low enough to be applied by a standard Si transistor.

<id>
physics/9805030v1
<category>
physics.optics
<abstract>
A general model is presented for coupling of high-$Q$ whispering-gallery
modes in optical microsphere resonators with coupler devices possessing
discrete and continuous spectrum of propagating modes. By contrast to
conventional high-Q optical cavities, in microspheres independence of high
intrinsic quality-factor and controllable parameters of coupling via evanescent
field offer variety of regimes earlier available in RF devices. The theory is
applied to the earlier-reported data on different types of couplers to
microsphere resonators and complemented by experimental demonstration of
enhanced coupling efficiency (about 80%) and variable loading regimes with
Q>10^8 fused silica microspheres.

<id>
physics/9806003v2
<category>
physics.optics
<abstract>
The mechanism of DC-Electric-Field-Induced Second-Harmonic (EFISH) generation
at weakly nonlinear buried Si(001)-SiO$_2$ interfaces is studied experimentally
in planar Si(001)-SiO$_2$-Cr MOS structures by optical second-harmonic
generation (SHG) spectroscopy with a tunable Ti:sapphire femtosecond laser. The
spectral dependence of the EFISH contribution near the direct two-photon $E_1$
transition of silicon is extracted. A systematic phenomenological model of the
EFISH phenomenon, including a detailed description of the space charge region
(SCR) at the semiconductor-dielectric interface in accumulation, depletion, and
inversion regimes, has been developed. The influence of surface quantization
effects, interface states, charge traps in the oxide layer, doping
concentration and oxide thickness on nonlocal screening of the DC-electric
field and on breaking of inversion symmetry in the SCR is considered. The model
describes EFISH generation in the SCR using a Green function formalism which
takes into account all retardation and absorption effects of the fundamental
and second harmonic (SH) waves, optical interference between field-dependent
and field-independent contributions to the SH field and multiple reflection
interference in the SiO$_2$ layer. Good agreement between the phenomenological
model and our recent and new EFISH spectroscopic results is demonstrated.
Finally, low-frequency electromodulated EFISH is demonstrated as a useful
differential spectroscopic technique for studies of the Si-SiO$_2$ interface in
silicon-based MOS structures.

<id>
physics/9806043v1
<category>
physics.optics
<abstract>
The new mechanism for obtaining a nonlinear phase shift has been proposed and
the schemes are described for its implementation. As it is shown, the
interference of two waves with intensity-dependent amplitude ratio coming from
the second harmonic generation should produce the nonlinear phase shift. The
sign and amount of nonlinear distortion of a beam wavefront is dependent of the
relative phase of the waves that is introduced by the phase element. Calculated
value of $n_2^{eff}$ exceeds that connected with cascaded quadratic
nonlinearity, at the same conditions.

<id>
physics/9807014v2
<category>
physics.optics
<abstract>
We analyze the guiding problem in a realistic photonic crystal fiber using a
novel full-vector modal technique, a biorthogonal modal method based on the
nonselfadjoint character of the electromagnetic propagation in a fiber.
Dispersion curves of guided modes for different fiber structural parameters are
calculated along with the 2D transverse intensity distribution of the
fundamental mode. Our results match those achieved in recent experiments, where
the feasibility of this type of fiber was shown.

<id>
physics/9807033v2
<category>
physics.optics
<abstract>
A new method for investigation of x-ray propagation in a rough narrow
dielectric waveguide is proposed on the basis of the numerical integration of
the quazioptical equation. In calculations a model rough surface was used with
the given statistical properties. It was shown that the losses in the narrow
waveguides strongly depend on the wall roughness and on the input angle. The
losses are not zero even at zero input angle if the width of the waveguide is
smaller or about 1 mkm. The effect is accounted for as the influence of
diffraction. The angular spread of the transmitted X-ray radiation is much more
narrow than the Fresnel angle of the total external reflection.

<id>
physics/9808022v1
<category>
physics.optics
<abstract>
We study a generalized notion of two-mode squeezing for the Stokes and
anti-Stokes fields in a model of a cavity Raman laser, which leads to a
significant reduction in decoherence or quantum noise. The model comprises a
loss-less cavity with classical pump, unsaturated medium and arbitrary
homogeneous broadening and dispersion. Allowing for arbitrary linear
combinations of the two modes in the definition of quadrature variables, we
find that there always exists a combination of the two output modes which
exhibits quadrature squeezing with noise reduction below the vacuum level. The
number of noise photons for this combination mode is proportional to the square
root of the number of Stokes noise photons.

<id>
physics/9808023v1
<category>
physics.optics
<abstract>
We study the effects of higher order transversal modes in a model of a
singly-resonant OPO, using both numerical solutions and mode expansions
including up to two radial modes. The numerical and two-mode solutions predict
lower threshold and higher conversion than the single-mode solution at negative
dispersion. Relative power in the zero order radial mode ranges from about 88%
at positive and small negative dispersion to 48% at larger negative dispersion,
with most of the higher mode content in the first mode, and less than 2% in
higher modes.

<id>
physics/9808026v1
<category>
physics.optics
<abstract>
This paper presents a detailed numerical study of the effect of focusing on
the conversion efficiency of low-loss singly-resonant parametric oscillators
with collinear focusing of pump and signal. Results are given for the maximal
pump depletion and for pump power levels required for various amounts of
depletion, as functions of pump and signal confocal parameters, for kI/kP=0.33
and 0.50. It is found that the ratio of pump depletion to maximal depletion as
a function of the ratio of pump power to threshold power agrees with the
plane-wave prediction to within 5%, for a wide range of focusing conditions.
The observed trends are explained as resulting from intensity and phase
dependent mechanisms.

<id>
physics/9809014v1
<category>
physics.optics
<abstract>
Via solution of appropriate variational problem it is shown that light beams
with Gaussian spatial profile and sufficiently short duration provide maximal
destruction of global coherence under nonlinear self-modulation.

<id>
physics/9809038v1
<category>
physics.optics
<abstract>
The dynamics of Fabry-Perot cavity with suspended mirrors is described. The
suspended mirrors are nonlinear oscillators interacting with each other through
the laser circulating in the cavity. The degrees of freedom decouple in normal
coordinates, which are the position of the center of mass and the length of the
cavity. We introduce two parameters and study how the dynamics changes with
respect to these parameters. The first parameter specifies how strong the
radiation pressure is. It determines whether the cavity is multistable or not.
The second parameter is the control parameter, which determines location of the
cavity equilibrium states. The equilibrium state shows hysteresis if the
control parameter varies within a wide range. We analyze stability of the
equilibrium states and identify the instability region. The instability is
explained in terms of the effective potential: the stable states correspond to
local minima of the effective potential and unstable states correspond to local
maxima. The minima of the effective potential defines the resonant frequencies
for the oscillations of the cavity length. We find the frequencies, and analyze
how to tune them. Multistability of the cavity with a feedback control system
is analyzed in terms of the servo potential. The results obtained in this paper
are general and apply to all Fabry-Perot cavities with suspended mirrors.

<id>
physics/9810020v3
<category>
physics.optics
<abstract>
We show that an azimuthally-periodically-modulated bright ring "necklace"
beam can self-trap in self-focusing Kerr media and can exhibit stable
propagation for very large distances. These are the first bright (2+1) D beams
to exhibit stable self-trapping in a system described by the cubic (2+1) D
Nonlinear Schrodinger Equation (NLSE).

<id>
physics/9811008v1
<category>
physics.optics
<abstract>
We present a new class of micro lasers based on nanoporous molecular sieve
host-guest systems. Organic dye guest molecules of
1-Ethyl-4-(4-(p-Dimethylaminophenyl)-1,3-butadienyl)-pyridinium Perchlorat were
inserted into the 0.73-nm-wide channel pores of a zeolite AlPO$_4$-5 host. The
zeolitic micro crystal compounds where hydrothermally synthesized according to
a particular host-guest chemical process. The dye molecules are found not only
to be aligned along the host channel axis, but to be oriented as well. Single
mode laser emission at 687 nm was obtained from a whispering gallery mode
oscillating in a 8-$\mu$m-diameter monolithic micro resonator, in which the
field is confined by total internal reflection at the natural hexagonal
boundaries inside the zeolitic microcrystals.

<id>
physics/9811054v3
<category>
physics.optics
<abstract>
We report a quantum ring-like toroidal cavity naturally formed in a
vertical-cavity-like active microdisk plane due to Rayleigh's band of
whispering gallery modes. The $\sqrt{T}$-dependent redshift and a square-law
property of microampere-range threshold currents down to 2 $\mu$A are
consistent with a photonic quantum wire view, due to whispering gallery
mode-induced dimensional reduction.

<id>
physics/9812044v1
<category>
physics.optics
<abstract>
The effect of capture of X-ray beam into narrow submicron capillary was
investigated with account for diffraction and decay of coherency by roughness
scattering in transitional boundary layer. In contrast to well-known
Andronov-Leontovich approach the losses do not vanish at zero gliding angle and
scale proportional to the first power of roughness amplitude for small gliding
angles. It was shown that for small correlation radius of roughness the
scattering decay of coherency can be made of the same order as absorption decay
of lower channeling modes to produce angular collimation of X-ray beams.
Estimates were given for optimum capillary length at different roughness
amplitudes for angular sensitivity of X-ray transmission and chenneling effects
that can be usefull for designing of detector systems.

<id>
physics/9902027v1
<category>
physics.optics
<abstract>
We report the measurement of the photons flux produced in parametric
down-conversion, performed in photon counting regime with actively quenched
silicon avalanche photodiodes as single photon detectors. Measurements are done
with the detector in a well defined geometrical and spectral situation. By
comparison of the experimental data with the theory, a value for the second
order susceptibilities of the non linear crystal can be inferred.

<id>
physics/9902038v1
<category>
physics.optics
<abstract>
In a frame of quasi-crystal approximation the dispersion equations are
obtained for the wave vector of a coherent electromagnetic wave propagating in
a media which contains a random set of parallel dielectric cylinders with
possible overlapping. The results are compared with that for the case when a
regularity at the cylinder placement exists.

<id>
physics/9903005v1
<category>
physics.optics
<abstract>
Accurate calculation of internal and surface scattering losses in fused
silica microspheres is done. We show that in microspheres internal scattering
is partly inhibited as compared to losses in the bulk material. We pay
attention on the effect of frozen thermodynamical capillary waves on surface
roughness. We calculate also the value of mode splitting due to backscattering
and other effects of this backscattering.

<id>
physics/9702024v1
<category>
physics.ed-ph
<abstract>
To meet National Standards recommended by the National Research Council for
high school physics, inservice teachers must be integrated into the physics
community. They must be empowered by access to resources of the physics
community and by sustained support for their professional development. To that
end, university and college physics departments must assume an essential role
in establishing and maintaining the necessary local infrastructure.
Nevertheless, this can be done within their current systems at negligible cost
by forming partnerships with local alliances of inservice teachers. Practical
details have been worked out, and working partnerships have been established in
several localities. This effort should be extended to a national infrastructure
for reform of physics teaching in community colleges as well as in high
schools. Most teachers are eager to participate in teaching reform.

<id>
physics/9704006v1
<category>
physics.ed-ph
<abstract>
The thermodynamics of an ideal gas enclosed in a box of volume a1 x a2 x a3
at temperature T is considered. The canonical partition function of the system
is expressed in terms of complete elliptic integrals of the first kind, whose
argument obeys a transcendental equation. For high and low temperatures we
derive explicitly the main finite-volume corrections to the standard
thermodynamic quantities.

<id>
physics/9705003v1
<category>
physics.ed-ph
<abstract>
The purpose of this paper is to make clear the difference between rigid and
undeformable bodies in Relativity. The error of confusing these two concepts
has survived up to the present day treatises. We hope it will not persist in
the XXI century treatises. The large majority of relativists do not know the
formulae for the relativistic elasticity of rigid bodies (Mc Crea 1952,Brotas
1968). The paradoxes of the rotating disk and of the 3-degrees of freedom of
rigidies bodies in Relativity are in the domain of relativistic elasticity.

<id>
physics/9705025v1
<category>
physics.ed-ph
<abstract>
I discuss issue of how to adjust recitation grades given by different
instructors in a large course, taking into account and correcting for
differences in standards among the instructors, while preserving the effects of
differences in average student performance among the recitation sections.

<id>
physics/9803023v1
<category>
physics.ed-ph
<abstract>
If potential energy is the timelike component of a four-vector, then there
must be a corresponding spacelike part which would logically be called the
potential momentum. The potential four-momentum consisting of the potential
momentum and the potential energy taken together is just the gauge field of the
associated force times the charge associated with that force. The canonical
momentum is the sum of the ordinary and potential momenta. Refraction of matter
waves by a discontinuity in a gauge field can be used to explore the effects of
gauge fields at an elementary level. Using this tool it is possible to show how
the Lorentz force law of electromagnetism follows from gauge theory. The
resulting arguments are accessible to students at the level of the introductory
calculus-based physics course and tie together classical and quantum mechanics,
relativity, gauge theory, and electromagnetism. The resulting economy of
presentation makes it easier to include modern physics in the one year course
normally available for teaching introductory physics.

<id>
physics/9804007v1
<category>
physics.ed-ph
<abstract>
This paper is a lab tutorial (and a theory primer) for the integral quantum
Hall effect experiment as conducted at Applied Solid State Physics, University
of Bochum, Germany.

<id>
physics/9804018v1
<category>
physics.ed-ph
<abstract>
In this article we give an introduction to the Fock quantization of the
Maxwell field. At the classical level, we treat the theory in both the
covariant and canonical phase space formalisms. The approach is general since
we consider arbitrary (globally-hyperbolic) space-times. The Fock quantization
is shown to be equivalent to the definition of a complex structure on the
classical phase space. As examples, we consider stationary space-times as well
as ordinary Minkowski space-time. The account is pedagogical in spirit and is
tailored to beginning graduate students. The paper is selfcontained and is
intended to fill an existing gap in the literature.

<id>
physics/9806037v1
<category>
physics.ed-ph
<abstract>
Section 7 of Einstein's 1905 electrodynamics paper gives frequency-shift and
aberration formulae that together describe an elongated ellipsoidal wavefront.
A Lorentz contraction of this ellipsoid solves most (but not all) of the
associated relativistic problems.

<id>
physics/9807015v2
<category>
physics.ed-ph
<abstract>
Simple signal-propagation effects make receding objects seem contracted and
approaching objects seem elongated. These effects are theoretically
photographable, and are proportional in strength to the frequency-change in the
object's emitted light. In a one-dimensional version of the "barn-pole"
experiment, a "moving" object's photographed image can appear doubly
length-dilated according to fixed-aether theory, but only singly length-dilated
according to SR. This expected difference might be charitably described as a
form of photographable Lorentz contraction.

<id>
physics/9808040v1
<category>
physics.ed-ph
<abstract>
Recently, some discussions arose as to the definition of charge and the value
of the density of charge in stationary-current-carrying conductors. We stress
that the problem of charge definition comes from a misunderstanding of the
usual definition. We provide some theoretical elements which suggest that
positive and negative charge densities are equal in the frame of the positive
ions.

<id>
physics/9811029v1
<category>
physics.ed-ph
<abstract>
We present an elementary discussion of two basic properties of angular
displacements, namely, the anticommutation of finite rotations and the
commutation of infinitesimal rotations, and show how commutation is achieved as
the angular displacements get smaller and smaller.

<id>
physics/9901021v2
<category>
physics.ed-ph
<abstract>
Some facts about 4-spinors listed and discussed. None, well perhaps some, of
the work is original. However, locating formulas in other places has proved a
time-consuming process in which one must always worry that the formulas found
in any given source assume the other metric (I use {-1,-1,-1,+1}) or assume
some other unexpected preconditions. Here I list some formulas valid in general
representations first, then formulas using a chiral representation are
displayed, and finally formulas in a special reference frame (the rest frame of
the `current' j) in the chiral representation are listed. Some numerical and
algebraic exercises are provided.

<id>
physics/9903033v2
<category>
physics.ed-ph
<abstract>
In this article we explore the phenomena of nonequilibrium stochastic process
starting from the phenomenological Brownian motion. The essential points are
described in terms of Einstein's theory of Brownian motion and then the theory
extended to Langevin and Fokker-Planck formalism. Then the theory is applied to
barrier crossing dynamics, popularly known as Kramers' theory of activated rate
processes. The various regimes are discussed extensively and Smoluchowski
equation is derived as a special case. Then we discuss some of the aspects of
Master equation and two of its applications.

<id>
physics/9904003v1
<category>
physics.ed-ph
<abstract>
We discuss some paradoxes arising due to the gauge-dependence of canonical
variables in mechanics.

<id>
physics/9905017v1
<category>
physics.ed-ph
<abstract>
We show how some geometric elements of the path of a particle moving in a
plane -- the osculating circle and its radius of curvature -- can be used to
construct the parabolic trajectory of projectiles in motion under gravity.

<id>
physics/9905018v1
<category>
physics.ed-ph
<abstract>
We discuss how a class of difficult kinematic problems can play an important
role in an introductory course in stimulating students' reasoning on more
complex physical situations. The problems presented here have an elementary
analysis once certain symmetry features of the motion are revealed. We also
explore some unexpected directions these problems lead us.

<id>
physics/9905023v2
<category>
physics.ed-ph
<abstract>
Two Italian non-profit organizations (Set and Galea) are working together to
realize a center (Cdcs) supported by the town administration of Sesto
Fiorentino (a little town not far from Florence, Italy), for the divulgation of
the scientific culture in the schools and among people. This is their project:
the key idea is to let the non-profit organizations intervent in the public
school system by creating cooperative structures in which young scientists can
work.

<id>
physics/9905056v1
<category>
physics.ed-ph
<abstract>
Using the post-Gaussian trial functions, we calculate the variational
solutions to the quantum-mechanical anharmonic oscillator. We evaluate not only
the ground state but also some excited energies, and compare them with
numerical results.

<id>
physics/9906066v2
<category>
physics.ed-ph
<abstract>
First Internet graduate course on Classical Mechanics in Spanish
(Castellano). This is about 80% of the material I covered during the
January-June 1999 semester at IFUG in the Mexican city of Leon. English and
Romanian versions are in (slow) progress and hopefully will be arXived. For a
similar course on Quantum Mechanics, see physics/9808031

<id>
physics/9908053v1
<category>
physics.ed-ph
<abstract>
You will find here a number of (mostly) elementary physics problems dealing
mainly with uniform motion kinematics. In preparing this collection I have
tried to create original situations that could help bring motivation to an
introductory course. You are welcome to suggest improvements and ... provide
solutions!

<id>
physics/9909035v1
<category>
physics.ed-ph
<abstract>
This is the English version of a friendly graduate course on Classical
Mechanics, containing about 80% of the material I covered during the
January-June 1999 semester at IFUG in the Mexican city of Leon. For the Spanish
version, see physics/9906066

<id>
physics/9910041v1
<category>
physics.ed-ph
<abstract>
Education is a prerequisite to master the challenges of space science and
technology. Efforts to understand and control space science and technology are
necessarily intertwined with social expressions in the cultures where science
and technology is carried out. The United Nations is leading an effort to
establish regional Centres for Space Science and Technology Education in major
regions on Earth. The status of the establishment of such institutions in Asia
and the Pacific, Africa, Latin America and the Caribbean, Western Asia, and
Eastern Europe is briefly described in this article.

<id>
physics/9911071v1
<category>
physics.ed-ph
<abstract>
We address the question "How do students make sense of Physics from the point
of view of constituting physics knowledge?". A phenomenographic study is
described as a result of which we present six qualitatively different ways in
which students experience the first year of Physics. The variation is analysed
in terms of the structure of experience, the nature of knowlege and an ethical
aspect related to the identification of contributority. Three of these ways of
experiencing the first year are considered to be unproductive in terms of
making sense of physics, while the other three support, to an increasing
degree, the formation of a well-grounded physics knowledge object. Implications
for practice are considered.

<id>
physics/9912017v1
<category>
physics.ed-ph
<abstract>
We obtain the Casimir effect for the massless scalar field in one dimension
based on the analogy between the quantum field and the continuum limit of an
infinite set of coupled harmonical oscillators.

<id>
physics/9912031v2
<category>
physics.ed-ph
<abstract>
We discuss the boundary effects on a quantum system by examining the problem
of a hydrogen atom in a spherical well. By using an approximation method which
is linear in energy we calculate the boundary corrections to the ground-state
energy and wave function. We obtain the asymptotic dependence of the
ground-state energy on the radius of the well.

<id>
physics/9912037v1
<category>
physics.ed-ph
<abstract>
One of the main activities in science teaching, and in particular in Physics
teaching, is not only the discussion of both modern problems and problems which
solution is an urgent matter. It means that the picture of an active and alive
science should be transmitted to the students, mainly to the College students.
A central point in this matter is the issue which characterizes the Fundamental
Laws of Nature. In this work we emphasize that this sort of laws may exist in
areas which are different from those usually considered. In this type of
discussion it is neither possible nor desirable to avoid the historical
perspective of the scientific development.

<id>
physics/9912046v1
<category>
physics.ed-ph
<abstract>
1. Connection between integration and differentiation/ Gauss theorem.
Coordinate-free definitions: gradient, divergence, curl. Stokes theorem. 2.
Elements of continuum mechanics/ Continuity equation. Stress tensor. Euler
equation. Hydrodynamics. Elasticity.

<id>
physics/0002021v1
<category>
physics.ed-ph
<abstract>
Submitted to "Notes and Discussions" in American Journal of Physics.

<id>
physics/0002025v1
<category>
physics.ed-ph
<abstract>
The object of this laboratory work: to explore dependence mass point
oscillatory motion parameters in the following cases:
  - without resistance (free oscillations);
  - the resistance force is proportional to the velocity vector;
  - the resistance force is proportional to the velocity squared.
  Used equipment: the work have doing on a personal computer. The oscillatory
motion simulation is carried out by the numerical solution of system of
differential equations. This equations describe a motion of a particle under an
elastic force action and exterior forces (resistance force) with initial values
and parameters being entered during the dialogue with the computer.

<id>
physics/0003012v1
<category>
physics.ed-ph
<abstract>
A pedagological introduction to effective field theory is presented.

<id>
physics/9904013v1
<category>
physics.soc-ph
<abstract>
The Debrecen workshop was one of a number held in preparation for the
UNESCO-ICSU World Conference on Science, which will be held in Budapest, June
1999. A report representing the views of the workshop, prepared for that
conference and containing a number of recommended actions, is included with
this summary. The workshop affirmed the ongoing importance of physics for its
own sake and as part of our culture, as a key element in our increasingly
unified science and as an essential contributor to the solution of
environmental and energy problems. The problems faced by physics as an activity
and as an educational subject were discussed and actions for both society as a
whole and the physics community itself were put forward.

<id>
physics/9908036v1
<category>
physics.soc-ph
<abstract>
A statistical study on 565 works of art of different great painters was done
and it was calculated the ratio of the 2 sides of a paintings. Assuming that
all the painters under discussion enter in a statistics with equal weights it
is shown that the average value obtained for the ratio of the sides is 1.34.
This value, determined experimentally is significantly different from the value
of the Golden Section F=1.618, which is a theoretical ratio, obtained from an
abstract, mathematical theory, which supposedly ought to impress on a painting
a supreme harmony.

<id>
physics/0007040v2
<category>
physics.soc-ph
<abstract>
The Stanford Linear Accelerator Center (SLAC) and Deutsches Elektronen
Synchrotron (DESY) libraries have been comprehensively cataloguing the High
Energy Particle Physics (HEP) literature online since 1974. The core database,
SPIRES-HEP, now indexes over 400,000 research articles, with almost 50% linked
to fulltext electronic versions (this site now has over 15 000 hits per day).
This database motivated the creation of the first site in the United States for
the World Wide Web at SLAC. With this database and the invention of the Los
Alamos E-print archives in 1991, the HEP community pioneered the trend to
"paperless publishing" and the trend to paperless access; in other words, the
"virtual library." We examine the impact this has had both on the way
scientists research and on paper-based publishing. The standard of work
archived at Los Alamos is very high. 70% of papers are eventually published in
journals and another 20% are in conference proceedings. As a service to
contributors, the SPIRES-HEP collaboration has been ensuring that as much
information as possible is included with each bibliographic entry for a paper.
Such meta-data can include tables of the experimental data that researchers can
easily use to perform their own analyses as well as detailed descriptions of
the experiment, citation tracking, and links to full-text documents.

<id>
physics/0201012v3
<category>
physics.soc-ph
<abstract>
Doing research is fighting, what any other thing the human being could do?
Fight against powers or to get powers, that depends on us. Science can be a
revolution or deadlocked idleness. Still waters, without hitting the stones
along their history, trend to form bogs.

<id>
physics/0203076v1
<category>
physics.soc-ph
<abstract>
We present the statistics of the significant nouns and adjectives of social
impact figuring in the nominations of the Nobel prizes in Physics and Chemistry
over the period of the awards from 1901 to 2001

<id>
physics/0208046v3
<category>
physics.soc-ph
<abstract>
Scientific publishing is in a transition between the old paper-bound, static
forms and the new electronic media with its interactive, dynamic possibilities.
This takes place in the context of imploding library budgets and exploding
magazine costs. The scientists as contributors, reviewers and editors of scientific
journals are exposed to an increased pressure by the their administrations and
the public towards quantification, objectification and certification of
scientific achievements. The ``publication roulette'' resulting from
low-quality editorial procedures often amounts to malign censorship, which not
only is experienced as a frustration by the contributors, but is also delaying and
hampering the progress of science. It also leads to a waste of funds under the
cover of pseudo-objectivity and pseudo-legitimacy of financial decisions.
Different solutions are outlined and discussed. As concerns scientific
publishing, an e-print service should be established, which, in continuation of
existing e-servers such as arxiv.org, is operated either directly by the United
Nations Educational, Scientific and Cultural Organization, or by an
international consortium. In order to become generally accepted by the
scientists, certification criteria must be provided, which would make it
possible to successfully pursue a scientific career besides the traditional
peer reviewed print publications.

<id>
physics/0301059v7
<category>
physics.soc-ph
<abstract>
The only military application in which depleted-uranium (DU) alloys
out-perform present-day tungsten alloys is long-rod penetration into a main
battle-tank's armor. However, this advantage is only on the order of 10%, and
it disappears when the comparison is made in terms of actual lethality of
complete anti-tank systems instead of laboratory-type steel penetration
capability. Therefore, new micro- and nano-engineered tungsten alloys may soon
out-perform DU alloys, enabling the production of tungsten munition which will
be better than existing uranium munition, and whose overall life-cycle cost
will be lower, due to the absence of the problems related to the radioactivity
of uranium. The reasons why DU weapons have been introduced and used are
analysed from the perspective that their radioactivity must have played an
important role in the decision making process. It is found that DU weapons
belong to the diffuse category of low-radiological-impact nuclear weapons to
which emerging types of low-yield (i.e., fourth generation) nuclear explosives
also belong. It is concluded that the battlefield use of DU during the 1991
Gulf War, which broke a 46-year-long taboo against the intentional use or
induction of radioactivity in combat, has created a military and legal
precedent which has trivialized the combat use of radioactive materials, and
therefore made the use of nuclear weapons more probable.

<id>
physics/0304058v1
<category>
physics.soc-ph
<abstract>
The compositional scheme of a Bronze Age sword, found near the town of
Giurgiu in Romania has been determined by the method of particle-induced X-ray
emission (PIXE), at the tandem accelerator of the National Institute for
Physics and Nuclear Engineering from Bucharest, Magurele, Romania. The results
of the analyses and the comparison with the composition of other swords from
the same geographic area, the Danubian plane from Bulgaria and Transylvania
regions, show that the sword from Giurgiu could be relatively associated with
the swords from Bulgaria, having also the same stylistic, temporal and
geographical similitude.

<id>
physics/0305016v1
<category>
physics.soc-ph
<abstract>
It is increasingly important to support the large numbers of scientists
working in remote areas and having low -bandwidth access to the Internet. This
will continue to be the case for years to come since there is evidence from
PingER performance measurements that the, so-called, digital divide is not
decreasing. In this work, we review the collaborative work of The Abdus Salam
International Center for Theoretical Physics (ICTP) in Trieste -a leading
organization promoting science dissemination in the developing world- and SLAC
in Stanford, to monitor by PingER, Universities and Research Institutions all
over the developing world following the recent Recommendations of Trieste to
help bridge the digital divide. As a result, PingER's deployment now covers the
real-time monitoring of worldwide Internet performance and, in particular, West
and Central Africa for the first time. We report on theresults from the ICTP
sites and quantitatively identify regions with poor performance, identify
trends, discuss experiences and future work.

<id>
physics/0307127v2
<category>
physics.soc-ph
<abstract>
This paper primarily considers the potential effects of a single
high-altitude nuclear burst on the U.S. power grid. A comparison is made
between EMP and natural phenomena such as lightning. This paper concludes that
EMP is no more harmful to the power grid than its counterparts in nature. An
upper limit of the electric field of the very fast, high-amplitude EMP is
derived from first principles. The resulting values are significantly lower
than the commonly presented values. Additional calculations show that the
ionization produced by a nuclear burst severely attenuates the EMP.

<id>
physics/0307146v1
<category>
physics.soc-ph
<abstract>
We utilize anthropic reasoning to demonstrate that we are typical observers
of our reference class under a self-sampling assumption by investigating the
definition of what a civilization is. With reference to the conflict between
such reasoning and the observational lack of extra-terrestrial intelligent
life, we conclude that a part of our theoretical understanding of the Universe
will be at fault.

<id>
physics/0310113v1
<category>
physics.soc-ph
<abstract>
In this Note a social network model for opinion formation is proposed in
which a person connected to $q$ partners pays an {\em attention} $1/q$ to each
partner. The mutual attention between two connected persons $i$ and $j$ is
taken equal to the geometric mean $1/\sqrt{q_iq_j}$. Opinion is represented as
usual by an Ising spin $s=\pm 1$ and mutual attention is given through a
two-spin coupling $J_{ij} = J Q/\sqrt{q_iq_j}$, $Q$ being the average
connectivity in the network. Connectivity diminishes attention and only persons
with low connectivity can pay special attention to each other leading to a
durable common (or opposing) opinion. The model is solved in "mean-field"
approximation and a critical "temperature" $T_c$ proportional to $JQ$ is found,
which is independent of the number of persons $N$, for large $N$.

<id>
physics/0310161v1
<category>
physics.soc-ph
<abstract>
This is a lecture on the ethics and role of science in promoting rational and
objective thinking in society. It was delivered by Prof. Shyamal Sengupta of
Kolkata, India. Prof. Sengupta, who passed away recently, has inspired
generations of Indian physicists by his rational viewpoints on science and by
teaching his students the importance of a scientific outlook. We, some of his
former students dedicate this small corner of the archive to his fond memory.

<id>
physics/0403023v2
<category>
physics.soc-ph
<abstract>
We have all read that: (1) organized medicine was laughing at the germ theory
and refused to wash its hands in the late 1800's while women died of childbed
fever and other patients of wound sepsis (scientists in general, including
physicists, were among those who died); and (2) during the 1900's, and still
today, organized medicine is laughing at the essential nutrient theory. We now
hear the medical quality assurance boards enforce MANY MISTAKEN VIEWS and DO
NOT recognize that: "xenobiotics can't be substituted for essential nutrients;
ascorbic acid is NOT a vitamin and hence, is needed in multi-gram (not minute
amounts) for optimum health; there is great harm in a diet deriving half its
calories from refined carbohydrates as is common in the US today; dietary
correction is possible at a stage when surgery is contemplated for a cardiac
patient; mercury dental fillings and even the high-copper variety most widely
used are extremely unsafe;" etc. The purpose of this manuscript is to elicit
coordinated collective effort of physicists, other scientists and the Union of
Concerned Scientists to educate the public and end this tyranny of organized
medicine whose methods are reported to cause much M&M (over a million
unnecessary US deaths at a cost of many billion dollars annually). The
biochemistry of these errors would be trivial to understand for physicists and
other scientists (who are all still going innocently to slaughter among those
dying needlessly, even today).

<id>
physics/0406117v1
<category>
physics.soc-ph
<abstract>
The UN/ESA Workshops on Basic Space Science is a long-term effort for the
development of astronomy and regional and international co-operation in this
field on a world wide basis, particularly in developing nations. The first four
workshops in this series (India 1991, Costa Rica and Colombia 1992, Nigeria
1993, and Egypt 1994) addressed the status of astronomy in Asia and the
Pacific, Latin America and the Caribbean, Africa, and Western Asia,
respectively. One major recommendation that emanated from the first four
workshops was that small astronomical facilities should be established in
developing nations for research and education programmes at the university
level and that such facilities should be networked. Subsequently, material for
teaching and observing programmes for small optical telescopes were developed
or recommended and astronomical telescope facilities have been inaugurated at
UN/ESA Workshops on Basic Space Science in Sri Lanka (1995), Honduras (1997),
and Jordan (1999). UN/ESA Workshops on Basic Space Science in Germany (1996),
France (2000), Mauritius (2001), Argentina (2002), and P.R. China (2004)
emphasised the particular importance of astrophysical data systems and the
virtual observatory concept for the development of astronomy on a world wide
basis. Since 1996, the workshops are contributing to the development of the
World Space Observatory concept. Achievements of the series of workshops are
briefly summarised in this report.

<id>
physics/0408066v1
<category>
physics.soc-ph
<abstract>
There is increasing circumstantial evidence that the cuprate superconductors,
and correlated-electron materials generally, defy simple materials
categorization because of their proximity to one or more continuous
zero-temperature phase transitions. This implies that the fifteen-year
confusion about the cuprates is not fundamental at all but simply
overinterpreted quantum criticality--an effect that seems mysterious by virtue
of its hypersensitivity to perturbations, i.e. to sample imperfections in
experiment and small modifications of approximation schemes in theoretical
modeling, but is really just an unremarkable phase transition of some kind
masquerading as something important, a sheep in wolf's clothing. This
conclusion is extremely difficult for most physicists even to think about
because it requires admitting that an identifiable physical phenomenon might
cause the scientific method to fail in some cases. For this reason I have
decided to explain the problem in a way that is nonthreatening, easy to read,
and fun--as a satire modeled after a similar piece of Lewis Carroll's I once
read. My story is humorous fiction. Any similarity of the characters to living
persons is accidental. My apologies to Henry W. Longfellow.

<id>
physics/0501160v1
<category>
physics.soc-ph
<abstract>
The Heider balance is a state of a group of people with established mutual
relations between them. These relations, friendly or hostile, can be measured
in the Bogardus scale of the social distance. In previous works on the Heider
balance, these relations have been described with integers 0 and $\pm1$.
Recently we have proposed real numbers instead. Also, differential equations
have been used to simulate the time evolution of the relations, which were
allowed to vary within a given range. In this work, we investigate an influence
of this allowed range on the system dynamics. As a result, we have found that a
narrowing of the range of relations leads to a large delay in achieving the
Heider balance. Another point is that a slight shift of the initial
distribution of the social distance towards friendship can lead to a total
elimination of hostility.

<id>
physics/0502005v1
<category>
physics.soc-ph
<abstract>
The Japanese shareholding network at the end of March 2002 is studied. To
understand the characteristics of this network intuitively, we visualize it as
a directed graph and an adjacency matrix. Especially detailed features of
networks concerned with the automobile industry sector are discussed by using
the visualized networks. The shareholding network is also considered as an
undirected graph, because many quantities characterizing networks are defined
for undirected cases. For this undirected shareholding network, we show that a
degree distribution is well fitted by a power law function with an exponential
tail. The exponent in the power law range is gamma=1.8. We also show that the
spectrum of this network follows asymptotically the power law distribution with
the exponent delta=2.6. By comparison with gamma and delta, we find a scaling
relation delta=2gamma-1. The reason why this relation holds is attributed to
the local tree-like structure of networks. To clarify this structure, the
correlation between degrees and clustering coefficients is considered. We show
that this correlation is negative and fitted by the power law function with the
exponent alpha=1.1. This guarantees the local tree-like structure of the
network and suggests the existence of a hierarchical structure. We also show
that the degree correlation is negative and follows the power law function with
the exponent nu=0.8. This indicates a degree-nonassortative network, in which
hubs are not directly connected with each other. To understand these features
of the network from the viewpoint of a company's growth, we consider the
correlation between the degree and the company's total assets and age. It is
clarified that the degree and the company's total assets correlate strongly,
but the degree and the company's age have no correlation.

<id>
physics/0502046v1
<category>
physics.soc-ph
<abstract>
Several models (including the widely used Sznajd model) have been proposed in
order to describe the social phenomenon of consensus formation. The objective
of the present paper is to supplement the simulations based on these models
with a ``real world simulation''; it considers a situation which can be
considered as an ideal laboratory for analyzing consensus formation, namely the
contributorization or prohibition of using cell phones while driving. This is a
convenient laboratory for several reasons (i) The issue was raised in similar
terms in all industrialized countries, a circumstance that facilitates
comparative analysis by providing a {\it set of observations} (as opposed to
single observations generated, for instance, in election contests). (ii) This
is a situation where we happen to know the rule a ``rational'' agent should
follow. (iii) Because the issue is a matter of life and death, the phenomenon
can be considered as fairly robust with respect to various, endogenous or
exogenous, sources of noise. (iv) The relevant data are available on the
Internet and in newspaper data bases. \qL Our observations strongly suggest
that there is a missing variable in most consensus formation simulations. In
its conclusion, the paper calls for a large scale effort for identifying,
documenting and analyzing other real-world cases of consensus formation.

<id>
physics/0502047v1
<category>
physics.soc-ph
<abstract>
The paper explores the connection between short-range social ties (i.e. links
with close relatives) and the occurrence of suicide. The objective is to
discriminate between a model based on social ties and a model based on
psychological traumas. Our methodological strategy is to focus on instances
characterized by the severance of some social ties. We consider several
situations of this kind. (i) Prisoners in the first days after their
incarceration. (ii) Prisoners in solitary confinement. (iii) Prisoners who are
transferred from one prison to another. (iv) Prisoners in closed versus open
prisons. (v) Prisoners in the weeks following their release. (vi) Immigrants in
the years following their relocation. (vii) Unmarried versus married people.
  Furthermore, in order to test the impact of major shocks we consider the
responses in terms of suicides to the following shocks. (i) The attack of
September 11, 2001 in Manhattan. (ii) The Korean War. (iii) The two world wars.
(iv) The Great Depression in the United States. (v) The hyperinflation episode
of 1923 in Germany. Major global traumatic shocks such as 9/11 or wars have no
influence on suicide rates once changing environment conditions have been
controlled for.
  Overall, it turns out that the observations have a natural interpretation in
terms of short-range ties. In contrast, the trauma model seems unable to
adequately account for many observations.

<id>
physics/0502067v3
<category>
physics.soc-ph
<abstract>
The population dynamics underlying the diffusion of ideas hold many
qualitative similarities to those involved in the spread of infections. In
spite of much suggestive evidence this analogy is hardly ever quantified in
useful ways. The standard benefit of modeling epidemics is the ability to
estimate quantitatively population average parameters, such as interpersonal
contact rates, incubation times, duration of infectious periods, etc. In most
cases such quantities generalize naturally to the spread of ideas and provide a
simple means of quantifying sociological and behavioral patterns. Here we apply
several paradigmatic models of epidemics to empirical data on the advent and
spread of Feynman diagrams through the theoretical physics communities of the
USA, Japan, and the USSR in the period immediately after World War II. This
test case has the advantage of having been studied historically in great
detail, which allows validation of our results. We estimate the effectiveness
of adoption of the idea in the three communities and find values for parameters
reflecting both intentional social organization and long lifetimes for the
idea. These features are probably general characteristics of the spread of
ideas, but not of common epidemics.

<id>
physics/0502082v1
<category>
physics.soc-ph
<abstract>
Complex systems can be characterized by classes of equivalency of their
elements defined according to system specific rules. We propose a generalized
preferential attachment model to describe the class size distribution. The
model postulates preferential growth of the existing classes and the steady
influx of new classes. We investigate how the distribution depends on the
initial conditions and changes from a pure exponential form for zero influx of
new classes to a power law with an exponential cutoff form when the influx of
new classes is substantial. We apply the model to study the growth dynamics of
pharmaceutical industry.

<id>
physics/0502083v1
<category>
physics.soc-ph
<abstract>
We analyze a set of three databases at different levels of aggregation (i) a
database of approximately $10^6$ publications of 247 countries in the period
between 1980--2001. (ii) A database of 508 academic institutions from European
Union (EU) and 408 institutes from USA in the 11 year period between during
1991--2001. (iii) A database comprising of 2330 Flemish contributors in the period
1980--2000. At all levels of aggregation we find that the mean annual growth
rates of publications is independent of the number of publications of the
various units involved. We also find that the standard deviation of the
distribution of annual growth rates decays with the number of publications as a
power law with exponent $\approx 0.3$. These findings are consistent with those
of recent studies of systems such as the size of R&D funding budgets of
countries, the research publication volumes of US universities, and the size of
business firms.

<id>
physics/0502118v3
<category>
physics.soc-ph
<abstract>
Numerical simulations are reported on the Bonabeau model on a fully connected
graph, where spatial degrees of freedom are absent. The control parameter is
the memory factor f. The phase transition is observed at the dispersion of the
agents power h_i. The critical value f_C shows a hysteretic behavior with
respect to the initial distribution of h_i. f_C decreases with the system size;
this decrease can be compensated by a greater number of fights between a global
reduction of the distribution width of h_i. The latter step is equivalent to a
partial forgetting.

<id>
physics/0502144v1
<category>
physics.soc-ph
<abstract>
Using a bit-string model similar to biological simulations, the competition
between different languages is simulated both without and with spatial
structure. We compare our agent-based work with differential equations and the
competing bit-string model of Kosmidis et al.

<id>
physics/0503007v1
<category>
physics.soc-ph
<abstract>
The traditional class of elliptical distributions is extended to allow for
asymmetries. A completely robust dispersion matrix estimator (the `spectral
estimator') for the new class of `generalized elliptical distributions' is
presented. It is shown that the spectral estimator corresponds to an
M-estimator proposed by Tyler (1983) in the context of elliptical
distributions. Both the generalization of elliptical distributions and the
development of a robust dispersion matrix estimator are motivated by the
stylized facts of empirical finance. Random matrix theory is used for analyzing
the linear dependence structure of high-dimensional data. It is shown that the
Marcenko-Pastur law fails if the sample covariance matrix is considered as a
random matrix in the context of elliptically distributed and heavy tailed data.
But substituting the sample covariance matrix by the spectral estimator
resolves the problem and the Marcenko-Pastur law remains valid.

<id>
physics/0503017v1
<category>
physics.soc-ph
<abstract>
We investigate the financial network in the Korean stock exchange (KSE)
market, using both numerical simulations and scaling arguments. We estimate the
cross-correlation on the stock price exchanges of all companies listed on the
the Korean stock exchange market, where all companies are fully connected via
weighted links, by introducing a weighted random graph. The degree distribution
and the edge density are discussed numerically from the market graph, and the
statistical analysis for the degree distribution of vertices is particularly
found to approximately follow the power law.

<id>
physics/0503023v1
<category>
physics.soc-ph
<abstract>
Large scale simulations of the movements of people in a ``virtual'' city and
their analyses are used to generate new insights into understanding the dynamic
processes that depend on the interactions between people. Models, based on
these interactions, can be used in optimizing traffic flow, slowing the spread
of infectious diseases or predicting the change in cell phone usage in a
disaster. We analyzed cumulative and aggregated data generated from the
simulated movements of 1.6 million individuals in a computer (pseudo
agent-based) model during a typical day in Portland, Oregon. This city is
mapped into a graph with $181,206$ nodes representing physical locations such
as buildings. Connecting edges model individual's flow between nodes. Edge
weights are constructed from the daily traffic of individuals moving between
locations. The number of edges leaving a node (out-degree), the edge weights
(out-traffic), and the edge-weights per location (total out-traffic) are fitted
well by power law distributions. The power law distributions also fit subgraphs
based on work, school, and social/recreational activities. The resulting
weighted graph is a ``small world'' and has scaling laws consistent with an
underlying hierarchical structure. We also explore the time evolution of the
largest connected component and the distribution of the component sizes. We
observe a strong linear correlation between the out-degree and total
out-traffic distributions and significant levels of clustering. We discuss how
these network features can be used to characterize social networks and their
relationship to dynamic processes.

<id>
physics/0503061v1
<category>
physics.soc-ph
<abstract>
This work presents a model that allows the study of research specialties
through the manifestations of the specialty's social and epistemological
processes in a collection of journal papers. Collections of papers are modeled
as coupled bipartite networks interlinking 7 types of entities. Matrix-based
link weight functions are introduced to calculate weighted bipartite networks
and weighted unipartite co-occurrence networks in the collection of papers.
These weight calculation methods, when used in conjunction with unweighted
bipartite growth models, produce simple growth models for weighted networks in
collections of papers.

<id>
physics/0503074v1
<category>
physics.soc-ph
<abstract>
We present empirical evidence that the range of random time series associated
with the tangled nature model of evolution exhibits a devils staircase like
behavior characterized by logarithmic trend and the universal multi-affine
spectrum of scaling exponents xi_c of q leq q_c moments of q-order
height-height correlations, whereas for q > q_c the moments behaves
logarithmically.

<id>
physics/9610005v1
<category>
physics.plasm-ph
<abstract>
It was recently demonstrated that static, resistive, magnetohydrodynamic
equilibria, in the presence of spatially-uniform electrical conductivity, do
not exist in a torus under a standard set of assumed symmetries and boundary
conditions. The difficulty, which goes away in the ``periodic straight cylinder
approximation,'' is associated with the necessarily non-vanishing character of
the curl of the Lorentz force, j x B. Here, we ask if there exists a spatial
profile of electrical conductivity that permits the existence of zero-flow,
axisymmetric r esistive equilibria in a torus, and answer the question in the
affirmative. However, the physical properties of the conductivity profile are
unusual (the conductivity cannot be constant on a magnetic surface, for
example) and whether such equilibria are to be considered physically possible
remains an open question.

<id>
physics/9610006v1
<category>
physics.plasm-ph
<abstract>
Resistive steady states in toroidal magnetohydrodynamics (MHD), where Ohm's
law must be taken into account, differ considerably from ideal ones. Only for
special (and probably unphysical) resistivity profiles can the Lorentz force,
in the static force-balance equation, be expressed as the gradient of a scalar
and thus cancel the gradient of a scalar pressure. In general, the Lorentz
force has a curl directed so as to generate toroidal vorticity. Here, we
calculate, for a collisional, highly viscous magnetofluid, the flows that are
required for an axisymmetric toroidal steady state, assuming uniform scalar
resistivity and viscosity. The flows originate from paired toroidal vortices
(in what might be called a ``double smoke ring'' configuration), and are
thought likely to be ubiquitous in the interior of toroidally driven
magnetofluids of this type. The existence of such vortices is conjectured to
characterize magnetofluids beyond the high-viscosity limit in which they are
readily calculable.

<id>
physics/9611017v1
<category>
physics.plasm-ph
<abstract>
The condition for potential description of the wake waves,generated by flat
or cylindrical driving electron bunch in cold plasma is derived.
  The two-dimensional nonlinear equation for potential valid for small values
of that is obtained and solved by the separation of variables. Solutions in the
form of cnoidal waves,existing behind the moving bunch at small values of
vertical coordinate,are obtained.In particular,at some boundary
conditions,corresponding to blow-out regime in the underdense plasma,the
solution represents by a solitary nonlinear wave.
  Approximate solution is also obtained using the method of multiple sacales.
  The indications are obtained that the dependense of the amplitudes on
longitudinal coordinate determines essentially,even in the first
approximation,by driving bunch charge distribution.The wake wave amplitude can
increase at some conditions along the longitudinal distance from the rear part
of the bunch.

<id>
physics/9701018v1
<category>
physics.plasm-ph
<abstract>
We study a partially ionized hydrogen plasma by means of quantum molecular
dynamics, which is based on wave packets. We introduce a new model which
distinguishes between free and bound electrons. The free electrons are modelled
as Gaussian wave packets with fixed width. For the bound states the 1s-wave
function of the hydrogen atom is assumed. In our simulations we obtain
thermodynamic properties in the equilibrium such as the internal energy and the
degree of ionization. The degree of ionization is in good agreement with
theoretical predictions. The thermodynamic functions agree well with results
from quantum statistics for 10000K < T < 40000K.

<id>
physics/9704010v1
<category>
physics.plasm-ph
<abstract>
The properties of a partially ionized plasma in a long cylindrical tube
subject to a uniform axial electric field are investigated. The plasma is
maintained by an external ionizing source balanced by bulk and surface
recombinations. Collisions between neutrals, whose density greatly exceeds the
density of charged particles, and of neutrals with ions are sufficiently
effective for their velocity distribution to be close to a Maxwellian with the
same uniform temperature, independent of the external field. The behavior of
the plasma is described by a collisional two-fluid scheme with charge
neutrality in the interior of the tube. Approximate nonlinear equations for the
hydrodynamical moments are obtained from a Boltzmann equation in which
electron-neutral, electron-ion and electron-electron collisions are all
important. It is found that under certain circumstances the current, and the
temperature of the electrons undergo a drastic change, with hysteresis, as the
electric field is varied.

<id>
physics/9705037v1
<category>
physics.plasm-ph
<abstract>
It is proved that (a) the solutions of the ideal magnetohydrodynamic
equation, which describe the equlibrium states of a cylindrical plasma with
purely poloidal flow and arbitrary cross sectional shape [G. N.
Throumoulopoulos and G. Pantis, Plasma Phys. and Contr. Fusion 38, 1817 (1996)]
are also valid for incompressible equlibrium flows with the axial velocity
component being a free surface quantity and (b) for the case of isothermal
incompressible equilibria the magnetic surfaces have necessarily circular cross
section.

<id>
physics/9706027v1
<category>
physics.plasm-ph
<abstract>
We study, globaly in time, the velocity distribution $f(v,t)$ of a spatially
homogeneous system that models a system of electrons in a weakly ionized
plasma, subjected to a constant external electric field $E$. The density $f$
satisfies a Boltzmann type kinetic equation containing a full nonlinear
electron-electron collision term as well as linear terms representing
collisions with reservoir particles having a specified Maxwellian distribution.
We show that when the constant in front of the nonlinear collision kernel,
thought of as a scaling parameter, is sufficiently strong, then the $L^1$
distance between $f$ and a certain time dependent Maxwellian stays small
uniformly in $t$. Moreover, the mean and variance of this time dependent
Maxwellian satisfy a coupled set of nonlinear ODE's that constitute the
``hydrodynamical'' equations for this kinetic system. This remain true even
when these ODE's have non-unique equilibria, thus proving the existence of
multiple stabe stationary solutions for the full kinetic model. Our approach
relies on scale independent estimates for the kinetic equation, and entropy
production estimates. The novel aspects of this approach may be useful in other
problems concerning the relation between the kinetic and hydrodynamic scales
globably in time.

<id>
physics/9706041v2
<category>
physics.plasm-ph
<abstract>
We propose a new and effective method to find plasma oscillatory and wave
modes. It implies searching a pair of poles of two-dimensional (in coordinate
$x$ and time $t$) Laplace transform of self-consistent plasma electric field
$E(x,t) \to E_{p_1p_2}$, where $p_1 \equiv -i \omega$, $p_2 \equiv i k$ are
Laplace transform parameters, that is determining a pair of zeros of the
following equation $$\frac1{E_{p_1p_2}} = 0 .$$ This kind of conditional
equation for searching double poles of $E_{p_1p_2}$ we call ``general
dispersion equation'', so far as it is used to find the pair values
($\omega^{(n)}, k^{(n)}$), $n=1, 2, ...$ . It differs basically from the
classic dispersion equation $\epsilon_l(\omega,k) = 0$ (and is not its
generalization), where $\epsilon_l$ is longitudinal dielectric susceptibility,
its analytical formula being derived according to Landau analytical
continuation. In distinction to $\epsilon_l$, which is completely plasma
characteristic, the function $E_{p_1p_2}$ is defined by initial and boundary
conditions and allows one to find all the variety of asymptotical plasma modes
for each concrete plasma problem. In this paper we demonstrate some
possibilities of applying this method to the simplest cases of collisionless
ion-electron plasma and to electron plasma with collisions described by a
collision-relaxation term $-\nu f^{(1)}$.

<id>
physics/9708017v1
<category>
physics.plasm-ph
<abstract>
The propagation of a streamer near an insulating surface under the influence
of a transverse magnetic field is theoretically investigated. In the weak
magnetic field limit it is shown that the trajectory of the streamer has a
circular form with a radius that is much larger than the cyclotron radius of an
electron. The charge distribution within the streamer head is strongly
polarized by the Lorentz force exerted perpendicualr to the streamer velocity.
A critical magnetic field for the branching of a streamer is estimated. Our
results are in good agreement with available experimental data.

<id>
physics/9708018v1
<category>
physics.plasm-ph
<abstract>
The idea of new diagnostics method for the small-scale irregular structures
of magnetically confined plasma is suggested in the present paper. The method
can be based on measurements of intensity attenuation of the normal sounding
waves. Anomalous attenuation arises due to multiple scattering effects
investigated earlier for ionospheric radio propagation. It has been shown that
multiple scattering regime can realize in a tokamak plasma. Calculations of
normal sounding wave anomalous attenuation in a tokamak plasma have been
carried out. This quantity is large enough to be registered experimentally.

<id>
physics/9709004v1
<category>
physics.plasm-ph
<abstract>
The problem of radio wave reflection from an optically thick plane monotonous
layer of magnetized plasma is considered at present work. The plasma electron
density irregularities are described by spatial spectrum of an arbitrary form.
The small-angle scattering approximation in the invariant ray coordinates is
suggested for analytical investigation of the radiation transfer equation. The
approximated solution describing spatial-and-angular distribution of radiation
reflected from a plasma layer is obtained. The obtained solution can be
applied, for example, to the ionospheric radio wave propagation.

<id>
physics/9709007v1
<category>
physics.plasm-ph
<abstract>
The impact of an equilibrium radial electric field $E $ on negative-energy
perturbations (NEPs) (which are potentially dangerous because they can lead to
either linear or nonlinear explosive instabilities) in cylindrical equilibria
of magnetically confined plasmas is investigated within the framework of
Maxwell-drift kinetic theory. It turns out that for wave vectors with a
non-vanishing component parallel to the magnetic field the conditions for the
existence of NEPs in equilibria with E=0 [G. N. Throumoulopoulos and D.
Pfirsch, Phys. Rev. E 53, 2767 (1996)] remain valid, while the condition for
the existence of perpendicular NEPs, which are found to be the most important
perturbations, is modified. For $|e_i\phi|\approx T_i$ ($\phi$ is the
electrostatic potential) and $T_i/T_e > \beta_c\approx P/(B^2/8\pi)$ ($P$ is
the total plasma pressure), a case which is of operational interest in magnetic
confinement systems, the existence of perpendicular NEPs depends on $e_\nu E$,
where $e_\nu$ is the charge of the particle species $\nu$. In this case the
electric field can reduce the NEPs activity in the edge region of tokamaklike
and stellaratorlike equilibria with identical parabolic pressure profiles, the
reduction of electron NEPs being more pronounced than that of ion NEPs.

<id>
physics/9709018v1
<category>
physics.plasm-ph
<abstract>
Within generalized linear response theory, an expression for the dielectric
function is derived which is consistent with standard approaches to the
electrical dc conductivity. Explicit results are given for the first moment
Born approximation. Some exact relations as well as the limiting behaviour at
small values of the wave number and frequency are investigated.

<id>
physics/9709019v1
<category>
physics.plasm-ph
<abstract>
A self-consistent determination of the spectral function and the self-energy
of electrons in a hot and dense plasma is reported. The self-energy is
determined within the approximation of the screened potential. It is shown,
that the quasi-particle concept is not an adequate concept for hot and dense
plasmas, since the width of the spectral function has to be considered. As an
example, the solar core plasma is discussed. An effective quasi-particle
picture is introduced and results for the solar core plasma as well as for ICF
plasmas are presented.

<id>
physics/9709021v1
<category>
physics.plasm-ph
<abstract>
A multiple-moment approach to the dielectric function of a dense non-ideal
plasma is treated beyond RPA including collisions in Born approximation. The
results are compared with the perturbation expansion of the Kubo formula. Sum
rules as well as Ward identities are considered. The relations to optical
properties as well as to the dc electrical conductivity are pointed out.

<id>
physics/9709022v1
<category>
physics.plasm-ph
<abstract>
The equilibrium of a resistive axisymmetric plasma with purely toroidal flow
surrounded by a conductor is investigated within the framework of the nonlinear
magnetohydrodynamic theory. It is proved that a) the poloidal current density
vanishes and b) apart from an idealized case the pressure profile should vanish
on the plasma boundary. For the cases of isothermal magnetic surfaces,
isentropic magnetic surfaces and magnetic surfaces with constant density, the
equilibrium states obey to an elliptic partial differential equation for the
poloidal magnetic flux function, which is identical in form to the
corresponding equation governing ideal equilibria. The conductivity, which can
not be either uniform or a surface quantity, results however in a restriction
of the possible classes of equilibrium solutions, e.g., for the cases
considered the only possible equilibria with Spitzer conductivity are of
cylindrical shape.

<id>
physics/9711002v1
<category>
physics.plasm-ph
<abstract>
Magnetosonic waves are intensively studied due to their importance in space
plasmas and also in fusion plasmas where they are used in particle acceleration
and heating experiments. In the present paper we investigate the magnetosonic
waves propagating in a multi-ion-species plasma perpendicular to an external
magnetic field. Due to the presence of several ion species, this mode splits
into two branches: high- and low-frequency modes. This opens a new channel of
nonlinear interactions (between these two modes), and qualitatively changes the
picture of turbulence in the long-wave region. Using the exact kinetic
approach, we derive a general system describing the propagation of nonlinearly
coupled high- and low-frequency waves. This system includes the KdV,
Boussinesq, and Zakharov equations as limiting cases. Solitary solutions of the
system of coupled equations are obtained.

<id>
physics/9712013v1
<category>
physics.plasm-ph
<abstract>
To calculate linear oscillations and waves in dynamics of gas and plasma one
uses as a rule the old classical method of dispersion equation for complex
frequencies $\omega$ and wave numbers $k$: $\epsilon(\omega,k)=0$. This method
appears to be inapplicable, f.e., in the case of waves in Maxwellian
collisionless plasma when dispersion equation has no solutions. By means of
some refined sophistication L.~Landau in 1946 has suggested in this case
actually to replace the dispersion equation with another one, having a specific
solution (``Landau damping'') and being now widely used in plasma physics.
Recently we have suggested a quite new universal method of two-dimensional
Laplace transformation (in coordinate $x$ and time $t$ for plane wave case),
that allows to obtain asymptotical solutions of original Vlasov plasma
equations as inseparable sets of coupled oscillatory modes (but not a single
wave like $\exp(-i\omega t+i k x)$). The mode parameters are defined in this
case by double-poles ($\omega_n$,$k_n$) of Laplace image $E(\omega_n,k_n)$ of
electrical field $E(x,t)$. This method allows one to obtain the whole set of
oscillatory modes for every concrete problem. It leads to some new ideology in
the theory of plasma oscillations, which are considered as a combination of
coupled oscillatory modes (characterized by pairs ($\omega_n$,$k_n$) and
amplitudes) and depend not only on the intrinsic plasma parameters, but also on
mutually dependent self-consistent initial and boundary conditions and on
method of plasma oscillations excitation.

<id>
physics/9801015v3
<category>
physics.plasm-ph
<abstract>
A thin and dense plasma layer is created when a sufficiently strong laser
pulse impinges on a solid target. The nonlinearity introduced by the
time-dependent electron density leads to the generation of harmonics. The pulse
duration of the harmonic radiation is related to the risetime of the electron
density and thus can be affected by the shape of the incident pulse and its
peak field strength. Results are presented from numerical
particle-in-cell-simulations of an intense laser pulse interacting with a thin
foil target. An analytical model which shows how the harmonics are created is
introduced. The proposed scheme might be a promising way towards the generation
of attosecond pulses.
  PACS number(s): 52.40.Nk, 52.50.Jm, 52.65.Rr

<id>
physics/9802011v1
<category>
physics.plasm-ph
<abstract>
New types of symmetry for the Rayleigh equation are found. For small Atwood
number, an analytic solution is obtained for a smoothly varying density
profile. It is shown that a transition layer with a finite width can undergo
some kind of stratification.

<id>
physics/9802026v1
<category>
physics.plasm-ph
<abstract>
Transverse quasilinear relaxation of the cyclotron-Cherenkov instability in
the inhomogeneous magnetic field of pulsar magnetospheres is considered. We
find quasilinear states in which the kinetic cyclotron-Cherenkov instability of
a beam propagating through strongly magnetized pair plasma is saturated by the
force arising in the inhomogeneous field due to the conservation of the
adiabatic invariant. The resulting wave intensities generally have nonpower law
frequency dependence, but in a broad frequency range can be well approximated
by the power law with the spectral index -2. The emergent spectra and fluxes
are consistent with the one observed from pulsars.

<id>
physics/9803032v1
<category>
physics.plasm-ph
<abstract>
The problem of radio wave reflection from an optically thick plane monotonous
layer of magnetized plasma is considered at present work. The plasma electron
density irregularities are described by spatial spectrum of an arbitrary form.
The small-angle scattering approximation in the invariant ray coordinates is
suggested for analytical investigation of the radiation transfer equation. The
approximated solution describing spatial-and-angular distribution of radiation
reflected from a plasma layer has been obtained. The obtained solution has been
investigated numerically for the case of the ionospheric radio wave
propagation. Two effects are the consequence of multiple scattering: change of
the reflected signal intensity and anomalous refraction.

<id>
physics/9804008v3
<category>
physics.plasm-ph
<abstract>
For the first time the emission of the radiative dissociation continuum of
the hydrogen molecule ($a^{3}\Sigma_{g}^{+} \to b^{3}\Sigma_{u}^{+}$ electronic
transition) is proposed to be used as a source of information for the
spectroscopic diagnostics of non-equilibrium plasmas. The detailed analysis of
excitation-deactivation kinetics, rate constants of various collisional and
radiative transitions and fitting procedures made it possible to develop two
new methods of diagnostics of: (1) the ground $X^{1}\Sigma_{g}^{+}$ state
vibrational temperature $T_{\text{vib}}$ from the relative intensity
distribution, and (2) the rate of electron impact dissociation
$(d[\mbox{H$_{2}$}]/dt)_{\text{diss}}$ from the absolute intensity of the
continuum. A known method of determination of $T_{\text{vib}}$ from relative
intensities of Fulcher-$\alpha$ bands was seriously corrected and simplified
due to the revision of $d \to a$ transition probabilities and cross sections of
$d \gets X$ electron impact excitation. General considerations are illustrated
with examples of experiments in pure hydrogen capillary-arc and H$_{2}$+Ar
microwave discharges.

<id>
physics/9804029v1
<category>
physics.plasm-ph
<abstract>
The results of a theoretical investigation on the stopping power of ion pair
in a magnetized electron plasma are presented, with particular emphasis on the
two-ion correlation effects. The analysis is based on the assumptions that the
magnetic field is classically strong ($\lambda_B\ll a_c\ll \lambda_D$, where
$\lambda_B$, $a_c$ and $\lambda_D$ are respectively the electron de Broglie
wavelength, Larmor radius and Debye length) and that the velocity of the two
ions is identical and fixed. The stopping power and % vicinage function in a
plasma are computed by retaining two-ion correlation effects and is compared
with the results of the individual-projectile approximation.

<id>
physics/9804031v2
<category>
physics.plasm-ph
<abstract>
The influence of motion of ions and electron temperature on nonlinear
one-dimensional plasma waves with velocity close to the speed of light in
vacuum is investigated. It is shown that although the wavebreaking field weakly
depends on mass of ions, the nonlinear relativistic wavelength essentially
changes. The nonlinearity leads to the increase of the strong plasma
wavelength, while the motion of ions leads to the decrease of the wavelength.
Both hydrodynamic approach and kinetic one, based on Vlasov-Poisson equations,
are used to investigate the relativistic strong plasma waves in a warm plasma.
The existence of relativistic solitons in a thermal plasma is predicted.

<id>
physics/9804033v1
<category>
physics.plasm-ph
<abstract>
We study the long time behavior of light particles, e.g. an electron swarm in
which Coulomb interactions are unimportant, subjected to an external field and
elastic collisions with an inert neutral gas. The time evolution of the
velocity and position distribution function is described by a linear Boltzmann
equation (LBE). The small ratio of electron to neutral masses, $\epsilon$,
makes the energy transfer between them very inefficient. We show that under
suitable scalings the LBE reduces, in the limit $\epsilon \to 0$, to a formally
exact equation for the speed (energy) and position distribution of the
electrons which contains mixed spatial and speed derivatives. When the system
is spatially homogeneous this equation reduces to and thus justifies, for
$\epsilon$ small enough, the commonly used ``two-term'' approximation.

<id>
physics/9804039v1
<category>
physics.plasm-ph
<abstract>
Electromagnetic wave propagation through cold collision free plasma is
studied using the nonlinear perturbation method. It is found that the equations
can be reduced to the modified Kortweg-de Vries equation.

<id>
physics/9805009v1
<category>
physics.plasm-ph
<abstract>
The energy loss by a test gyratory particle in a cold plasma in the presence
of homogeneous magnetic field is considered. Analytical and numerical results
for the rate of energy loss are presented. This is done for strong and weak
fields (i. e., when the electron cyclotron frequency is either higher, or
smaller than the plasma frequency), and in case, when the test particle
velocity is greater than the electron thermal velocity. It is shown that the
rate of energy loss may be much higher than in the case of motion through
plasma in the absence of magnetic field.

<id>
physics/9805017v1
<category>
physics.plasm-ph
<abstract>
Relaxed States of a slightly resistive and turbulent magnetized plasma is
obtained by invoking the principle of minimum dissipation which leads to curl
curl curl B = \lambda B . A solution of the above equation is accomplished
using the analytic continuation of the Chandrasekhar-Kendall eigenfunctions in
the complex domain. The new features of this theory is to show (i) a single
fluid can relax to an MHD equilibrium which can support pressure gradient even
without a flow, (ii) field reversal (RFP) in states that are not force free.

<id>
physics/9807005v2
<category>
physics.plasm-ph
<abstract>
The influence of a constant uniform magnetic field on the thermodynamic
properties of a partially ionized hydrogen plasma is studied. Using the method
of Green' s function various interaction contributions to the thermodynamic
functions are calculated. The equation of state of a quantum magnetized plasma
is presented within the framework of a low density expansion up to the order
e^4 n^2 and, additionally, including ladder type contributions via the bound
states in the case of strong magnetic fields (2.35*10^{5} T << B << 2.35*10^{9}
T). We show that for high densities (n=10^{27-30} m^{-3}) and temperatures
T=10^5 - 10^6 K typical for the surface of neutron stars nonideality effects
as, e.g., Debye screening must be taken into account.

<id>
physics/9702005v1
<category>
physics.pop-ph
<abstract>
Book Review of Quantum Field Theory by Lewis H. Ryder. An observation on
Ryder's derivation of Dirac equation is made. The review ends as, "A rare
combination of a thorough understanding and appreciation of the essential
logical structure of quantum field theory and deep pedagogic skills have
intermingled to create a masterpiece on the elementary introduction to quantum
field theory in less than five hundred pages... . Without reservations, I give
my strongest recommendation to every beginning student of physics to acquire
and read Quantum Field Theory by L. H. Ryder."

<id>
physics/9702014v1
<category>
physics.pop-ph
<abstract>
Aging refers to the property of two-time correlation functions to decay very
slowly on (at least) two time scales. This phenomenon has gained recent
attention due to experimental observations of the history dependent relaxation
behavior in amorphous materials (``Glasses'') which pose a challenge to
theorist. Aging signals the breaking of time-translational invariance and the
violation of the fluctuation dissipation theorem during the relaxation process.
But while the origin of aging in disordered media is profound, and the
discussion is clad in the language of a well-developed theory, systems as
simple as a random walk near a wall can exhibit aging. Such a simple walk
serves well to illustrate the phenomenon and some of the physics behind it.

<id>
physics/9703014v1
<category>
physics.pop-ph
<abstract>
Problems concerning with application of quantum rules on classical phenomena
have been widely studied, for which lifted up the idea about quantization and
uncertainty principle. Energy quantization on classical example of simple
harmonic oscillator has been reviewed in this paper.

<id>
physics/9704002v2
<category>
physics.pop-ph
<abstract>
Review of the two volume set "The Quantum Theory of Fields" by S. Weinberg is
presented.

<id>
physics/9704022v1
<category>
physics.pop-ph
<abstract>
The mathematical model of J. Keller for predicting World Record race times,
based on a simple differential equation of motion, predicted quite well the
records of the day. One of its shortcoming is that it neglects to account for a
sprinter's energy loss around a curve, a most important consideration
particularly in the 200m--400m. An extension to Keller's work is considered,
modeling the aforementioned energy loss as a simple function of the centrifugal
force acting on the runner around the curve. Theoretical World Record
performances for indoor and outdoor 200m are discussed, and the use of the
model at 300m is investigated. Some predictions are made for possible 200m
outdoor and indoor times as run by Canadian 100m WR holder Donovan Bailey,
based on his 100m final performance at the 1996 Olympic Games in Atlanta.

<id>
physics/9704023v1
<category>
physics.pop-ph
<abstract>
Using the model framework for curve running as described in physics/9704022,
the possible outcome of the 150m "One to One" showdown between sprinters
Donovan Bailey and Michael Johnson are discussed (to be run 01 Jun 1997 at
Skydome in Toronto, Ontario, Canada).

<id>
physics/9705004v1
<category>
physics.pop-ph
<abstract>
A quick and fun investigation into the effects of correcting for wind
assistance/resistance and drag on 100m sprint performances. Considered are
World 100m rankings, as well as Canadian rankings

<id>
physics/9706023v1
<category>
physics.pop-ph
<abstract>
Recently, a simple model was derived to account for a sprinter's energy loss
around a curve, based on previous sprint models for linear races. This paper
offers a quick test of the model's precision by comparing split times from
Donovan Bailey's 150m ``Challenge of Champions'' race at Skydome on June 1st,
1997. The discrepancy in the track configuration which almost prompted Bailey
to drop from the race is also addressed.

<id>
physics/9709017v1
<category>
physics.pop-ph
<abstract>
In a previous article (physics/9705004), the best 100m performances in track
and field were determined by accounting for wind effects and atmospheric drag.
In this report, I seek to ascertain what was the "fastest" 100m final through a
statistical analysis of the competitors' times. Considered are the 100 finals
from the 1984-1996 Olympic Games, the 1983-1997 World Championships, and the
1996 Grand Prix Final from Lausanne, SWI, in which Frank Fredericks (Namibia)
recorded the only sub-9.90s time ever into a head-wind (9.86s, -0.4 m/s).

<id>
physics/9709020v1
<category>
physics.pop-ph
<abstract>
In the wake of numerous injuries and Donovan Bailey's media-stamped "sub-par"
100m victory of 10.03s at the 1997 Canadian Track and Field Championships, a
medal hope at the 1997 World Championships seems grim for the Olympic Champion
and World Record holder. However, when one considers that the 10.03s was run
into a hefty head-wind (-2.1 m/s), a simple correction for this effect can help
to restore Canada's faith in their sprint hero -- not to mention rewrite the
record books again.

<id>
physics/9710003v1
<category>
physics.pop-ph
<abstract>
The 100m world rankings of the 1997 outdoor track and field competition
season are reviewed, subject to corrections for wind effects and atmospheric
drag. The rankings and times are compared with those of 1996, and the
improvements of each athlete over the course of a year are discussed.
Additionally, the athletes' wind-corrected 50m and 60m splits from the 1997
IAAF World Championships are compared to the 1997 indoor world rankings over
the same distances, as an attempt to predict possible performances for the
coming 1998 indoor season.

<id>
physics/9803034v1
<category>
physics.pop-ph
<abstract>
Track and field world records have risen and fallen throughout the history of
the sport. A recent rash of record-breaking performances has prompted the
question: "How good can we get?". This article offers a review of several
attempts to answer this question, based on mathematical modeling of key
physiological processes. The predictions are compared with present-day world
records, and a discussion of the future of athletics ensues...

<id>
physics/9811004v1
<category>
physics.pop-ph
<abstract>
This article briefly reviews the quantum Hall effect and the contributions of
the winners of the 1998 Nobel Prize in Physics.

<id>
physics/9902049v1
<category>
physics.pop-ph
<abstract>
We measure the difference in the scattering probability when an experiment
scattering longitudinally polarized 221 MeV protons from liquid hydrogen is
replaced by its mirror image. The result depends on the interplay between the
weak and strong interactions in the interesting region near the surface of the
proton. The experiment is technically very challenging and requires elaborate
precautions to measure and correct for various sources of systematic error.

<id>
physics/9906040v2
<category>
physics.pop-ph
<abstract>
The most dramatic developments in theoretical physics in the next millennium
are likely to come when we make progress on so far unresolved foundational
questions. In this essay I consider two of the deepest problems confronting us,
the measurement problem in quantum theory and the problem of relating
consciousness to the rest of physics. I survey some recent promising ideas on
possible solutions to the measurement problem and explain what a proper
physical understanding of consciousness would involve and why it would need new
physics.

<id>
physics/9910015v1
<category>
physics.pop-ph
<abstract>
This is a Physics World features paper on Chen-Tajima proposal to detect
Unruh radiation by means of high-intensity lasers (PRL 83, 256 (1999)).

<id>
physics/9911036v1
<category>
physics.pop-ph
<abstract>
At the recent 1999 World Athletics Championships in Sevilla, Spain, Canada's
Bruny Surin matched Donovan Bailey's National and former World Record 100 m
mark of 9.84 s. The unofficial times for each, as read from the photo-finish,
were 9.833 s and 9.835 s respectively. Who, then, is the fastest Canadian of
all time? A possible solution is offered, accounting for drag effects resulting
from ambient tail-winds and altitude.

<id>
physics/0007042v1
<category>
physics.pop-ph
<abstract>
A quasi-physical model (having both physical and mathematical roots) of
sprint performances is presented, accounting for the influence of drag
modification via wind and altitude variations. The race time corrections for
both men and women sprinters are discussed, and theoretical estimates for the
associated drag areas are presented. The corrections are consistent with
constant-wind estimates of previous contributors, however those for variable wind
are more accentuated for this model. As a practical example, the nullified
World Record and 1988 Olympic 100 m race of Ben Johnson is studied, and
compared with the present World Record of 9.79 s.

<id>
physics/0102039v1
<category>
physics.pop-ph
<abstract>
Based on a mathematical simulation which reproduces accurate split and
velocity profiles for the 100 and 200 metre sprints, the magnitudes of altitude
and mixed wind/altitude-assisted performances as compared to their sea-level
equivalents are presented. It is shown that altitude-assisted times for the 200
metre are significantly higher than for the 100 metre, suggesting that the
``legality'' of such marks perhaps be reconsidered.

<id>
physics/0106053v1
<category>
physics.pop-ph
<abstract>
The dark Universe corresponds to the period between the thermal decoupling
matter-radiation and the formation of the first objects. During this epoq,
molecules can appeared. In this talk, I will recall the nucleosynthesis
processes, then the chemistry of the primordial Universe. I will discuss the
influence of the primordial molecules on the protocloud evolution .

<id>
physics/0201056v1
<category>
physics.pop-ph
<abstract>
The aim of this note is to give a short and popular review of the ideas which
led to my model of magnetic monopoles (hep-ph/9708394) and my prediction of the
second kind of electromagnetic radiation. I will also point out the many and
far-reaching consequences if these magnetic photon rays would be confirmed.

<id>
physics/0206093v1
<category>
physics.pop-ph
<abstract>
Many of the most familiar features of our everyday environment, and some of
our basic notions about it, stem from Relativistic Quantum Field Theory (RQFT).
We argue in particular that the origin of common names, verbs, adjectives such
as full and empty, the concepts of identity, similarity, Plato's Universals,
natural numbers, and existence versus non-existence can be traced to the
space-time and gauge symmetries and quantum properties embodied in RQFT. These
basic tools of human thought cannot arise in a universe strictly described by
classical Physics based on Planck's constant being exactly equal to zero.

<id>
physics/0207008v3
<category>
physics.pop-ph
<abstract>
Over the last decade, cosmological observations have attained a level of
precision which allows for detailed comparison with theoretical predictions. In
this paper, we briefly review some studies of the current and prospected
constraints imposed by measurements of the cosmic microwave background
radiation, of the mass and of the expansion of the Universe.

<id>
physics/0208096v2
<category>
physics.pop-ph
<abstract>
A brief description of the path-breaking evidence for the observation of
neutrino oscillations at the Sudbury Neutrino Observatory is presented and the
experimental principles and theory thereof are briefly discussed.

<id>
physics/0209015v1
<category>
physics.pop-ph
<abstract>
We show that the instant motion of particle should be essentially
discontinuous and random. This gives the logical basis of discontinuous motion.
Since what quantum mechanics describes is the discontinuous motion of
particles, this may also answer the question 'why the quantum?'.

<id>
physics/0210088v2
<category>
physics.pop-ph
<abstract>
Quantum theory has been proved as an outstanding mystery in modern science.
The predictions of science have turned out to be probabilistic. The principle
of determinism has failed. For systems like weather, earthquakes, rolling dices
etc... and of course human behavior it has proved impossible, for science, to
describe a state of the system accurately for a long time into the future.
Moreover, modern cosmology has to rely on philosophical assumptions. It is
argued -- by taking into account of the views of learned scientists and
philosophers -- that modern science can never explain everything and it is a
hard fact to discover the ``Theory Of Everything''. On the other hand, modern
scientific technology have damaged our environment and posed serious threats
for the future development of science. All these facts and results put a big
question-mark (?) on the grass-root level working of science. In fact, all
scientific researches are based upon ordinary sense perception, which keeps the
outer physical universe as a separate entity, that is something quite
independent of the observer. Basically, it is the observer -- the knower --
which makes perception possible. Astonishingly, the knower -- human mind -- is
not included in our scientific theories. One should not forget human-being
(human mind) is a part of nature and an essential component of our
observations, so it's inclusion in scientific theories is must in order to
recover modern science from the drastic state of trauma.

<id>
physics/0311037v1
<category>
physics.pop-ph
<abstract>
I give a brief non-technical review of "Quantum Gravity Phenomenology" and in
particular I describe some studies which should soon allow to establish
valuable data-based constraints on the short-distance structure of spacetime.

<id>
physics/0406116v1
<category>
physics.pop-ph
<abstract>
The World Space Observatory/Ultraviolet (WSO/UV) represents a new mission
implementation model for large space missions for astrophysics. The process has
been brought up to enable, fully scientific needs driven, a logic to be applied
to the demands for large collection powers required to undertake space missions
which are complementary to the continuously increasing sensitivity of
ground-based telescopes. One of the assumptions associated with the idea of a
WSO is to avoid the excessive complexity required for multipurpose missions.
Although there may exist purely technological or programmatic policy issues,
which would suggest such more complex missions to be more attractive, many
other aspects, which do not need to be explored in this report, may argue
against such a mission model. Following this precept and other reasons, the
first implementation model for a WSO has been done for the ultraviolet domain
WSO/UV. WSO/UV is a follow-up project of the UN/ESA Workshops on Basic Space
Science, organised annually since 1991.

<id>
physics/0410056v3
<category>
physics.pop-ph
<abstract>
We have performed angle-resolved measurements of spontaneous-emission spectra
from laser dyes and quantum dots in opal and inverse opal photonic crystals.
Pronounced directional dependencies of the emission spectra are observed:
angular ranges of strongly reduced emission adjoin with angular ranges of
enhanced emission. It appears that emission from embedded light sources is
affected both by the periodicity and by the structural imperfections of the
crystals: the photons are Bragg diffracted by lattice planes and scattered by
unavoidable structural disorder. Using a model comprising diffuse light
transport and photonic band structure, we quantitatively explain the
directional emission spectra. This provides detailed understanding of the
transport of spontaneously emitted light in real photonic crystals, which is
essential in the interpretation of quantum-optics in photonic band-gap crystals
and for applications wherein directional emission and total emission power are
controlled.

<id>
physics/0503150v1
<category>
physics.pop-ph
<abstract>
This is a semipopular introduction to the Special and General Theory of
Relativity, with special emphasis on the geometrical aspects of both theories
and their physical implications.

<id>
physics/9701001v1
<category>
physics.space-ph
<abstract>
We study the pickup ion dynamics and mechanism of multiple reflection and
acceleration at the structured quasi-perpendicular supercritical shock. The
motion of the pickup ions in the shock is studied analytically and numerically
using the test particle analysis in the model shock front. The analysis shows
that slow pickup ions may be accelerated at the shock ramp to high energies.
The maximum ion energy is determined by the fine structure of the
electro-magnetic field at the shock ramp and decreases when the angle between
magnetic field and shock normal decreases. Evolution of pickup ion distribution
across the nearly-perpendicular shock and pickup ion spectrum is also studied
by direct numerical analysis.

<id>
physics/9702009v1
<category>
physics.space-ph
<abstract>
We determine a simple expression for the ramp width of a collisionless fast
shock, based upon the relationship between the noncoplanar and main magnetic
field components. By comparing this predicted width with that measured during
an observation of a shock, the shock velocity can be determined from a single
spacecraft. For a range of low-Mach, low-beta bow shock observations made by
the ISEE-1 and -2 spacecraft, ramp widths determined from two-spacecraft
comparison and from this noncoplanar component relationship agree within 30%.
When two-spacecraft measurements are not available or are inefficient, this
technique provides a reasonable estimation of scale size for low-Mach shocks.

<id>
physics/9803026v1
<category>
physics.space-ph
<abstract>
We present the discovery of an object in the Pleiades open cluster, named
Teide 2, with optical and infrared photometry which place it on the cluster
sequence slightly below the expected substellar mass limit. We have obtained
low- and high-resolution spectra that allow us to determine its spectral type
(M6), radial velocity and rotational broadening; and to detect H$_\alpha$ in
emission and Li I 670.8 nm in absorption. All the observed properties strongly
support the membership of Teide 2 into the Pleiades. This object has an
important role in defining the reappearance of lithium below the substellar
limit in the Pleiades. The age of the Pleiades very low-mass members based on
their luminosities and absence or presence of lithium is constrained to be in
the range 100--120 Myr.

<id>
physics/9910042v1
<category>
physics.space-ph
<abstract>
During the second half of the twentieth century, expensive observatories are
being erected at La Silla (Chile), Mauna Kea (Hawai), Las Palmas (Canary
Island), and Calar Alto (Spain), to name a view. In 1990, at the beginning of
The Decade of Discovery in Astronomy and Astrophysics (Bahcall [2]), the UN/ESA
Workshops on Basic Space Science initiated the establishment of small
astronomical telescope facilities, among them many particularly supported by
Japan, in developing countries in Asia and the Pacific (Sri Lanka,
Philippines), Latin America and the Caribbean (Colombia, Costa
  Rica, Honduras, Paraguay), and Western Asia (Egypt, Jordan, Morocco). The
annual UN/ESA Workshops continue to pursue an agenda to network these small
observatory facilities through similar research and education programmes and at
the same time encourage the incorporation of cultural elements predominant in
the respective cultures. Cross-cultural integration and multi-lingual
scientific cooperation may well be a dominant theme in the new millennium
(Pyenson [20]). This trend is supported by the notion that astronomy has deep
roots in virtually every human culture, that it helps to understand humanity's
place in the vast scale of the Universe, and that it increases the knowledge of
humanity about its origins and evolution=2E Two of these Workshops have been
organized in Europe (Germany 1996 and France 2000) to strengthen cooperation
between developing and industrialized countries.

<id>
physics/9911032v1
<category>
physics.space-ph
<abstract>
We have constructed an asteroid model with the intent of tracking the radial
and temporal dependence of temperature and composition throughout a 100-km
diameter CM-type parent body, with emphasis on constraining the temperature and
duration of a liquid water phase. We produce a non-uniform distribution where
liquid water persists longest and is hottest in the deepest zones and the
regolith never sees conditions appropriate to aqueous alteration. We apply the
model predictions of the liquid water characteristics to the evolution of amino
acids. IN some regions of the parent body, very little change occurs in the
amino acids, but for the majority of the asteroid, complete racemization or
even destruction occurs. We attempt to match our thermal model results with CM
meteorite observations, but thus far, our model does not produce scenarios that
are fully consistent with these observations.

<id>
physics/9911075v2
<category>
physics.space-ph
<abstract>
The chaotic interaction between electrons and whistler mode waves has been
shown to provide a mechanism for enhanced diffusion in phase space. Pitch angle
diffusion is relevant for the scattering of electrons into the loss cones, thus
providing a source for auroral precipitating electrons. A single whistler mode
wave propagating parallel to the background magnetic field has resonance with
the electrons but the process is not stochastic. The presence of a second,
oppositely directed whistler wave has been shown previously to introduce
stochasticity into the system, thus enhancing phase space diffusion. Here we
generalise previous work to include relativistic effects. The full relativistic
Lorentz equations are solved numerically to permit application to a more
extensive parameter space. We consider parameters scaled to intrinsic planetary
magnetospheres, for electron populations with 'pancake' velocity distributions
i.e. large anisotropies in velocity space. We show that the diffusion is rapid,
occuring on timescales of the order of tens of electron gyroperiods, and is
strongly sensitive to the wave amplitude, the wave frequency and the
perpendicular velocity. Using Voyager 1 data we give an estimate of the
whistler wave amplitude in the Io torus at Jupiter and show that the two
whistler mechanism produces pitch angle diffusion of up to 10 degrees from an
initial pancake distribution, on millisecond timescales.

<id>
physics/0001043v2
<category>
physics.space-ph
<abstract>
The EISCAT radar system has been used for the first time in a four-beam
meridional mode. The FAST satellite and ALIS imaging system is used in
conjunction to support the radar data, which was used to identify a main
ionospheric trough. With this large latitude coverage the trough was passed in
2.5 hours period. Its 3-dimensional structure is investigated and discussed. It
is found that the shape is curved along the auroral oval, and that the trough
is wider closer to the midnight sector. The position of the trough coincide
rather well with various statistical models and this trough is found to be a
typical one.

<id>
physics/0003019v2
<category>
physics.space-ph
<abstract>
The aim of the Swedish-Japanese EISCAT campaign in February 1999 was to
measure the ionospheric parameters inside and outside the auroral arcs. The ion
line radar experiment was optimised to probe the E-region and lower F-region
with as high a speed as possible. Two extra channels were used for the plasma
line measurements covering the same altitudes, giving a total of 3 upshifted
and 3 downshifted frequency bands of 25 kHz each. For most of the time the
shifted channels were tuned to 3 (both), 4 (up), 5.5 (down) and 6.5 (both) MHz.
Weak signals are seen whenever the radar is probing the diffuse aurora,
corresponding to the relatively low plasma frequencies. At times when auroral
arcs pass the radar beam, significant increases in return power are observed.
Many cases with simultaneously up and down shifted plasma lines are recorded.
In spite of the rather active environment, the highly optimised measurements
enable investigation of the properties of the plasma lines. A modified
theoretical incoherent scatter spectrum is used to explain the measurements.
The general trend is an upgoing field-aligned current in the diffuse aurora,
confirmed with a full fit of the combined ion and plasma line spectra. There
are also cases with strong suprathermal currents indicated by large differences
in signal strength between up- and downshifted plasma lines.

<id>
physics/0006068v1
<category>
physics.space-ph
<abstract>
During periods of increased magnetospheric activity, whistler wave emissions
have been observed with increased wave amplitudes. We consider a pitch angle
diffusion mechanism that is shown to scale with whistler wave amplitude and
hence is 'switched on' during these periods of intense activity.
  We consider the interaction between relativistic electrons and two oppositely
directed, parallel propagating whistler mode waves. We show that for intense
whistlers this process is stochastic and results in strong electron pitch angle
diffusion.
  We show that the interaction is rapid, occur on timescales of the order of
tens of electron gyroperiods and that the interaction is sensitive to wave
amplitude, wave frequency and electron energy.

<id>
physics/0009012v1
<category>
physics.space-ph
<abstract>
To test the cosmic spatial isotropy, we use a rotatable torsion balance
carrying a transversely spin-polarized ferrimagnetic Dy_{6}Fe_{23} mass. With a
rotation period of one hour, the period of anisotropy signal is reduced from
one sidereal day by about 24 times, and hence the 1/f noise is greatly reduced.
Our present experimental results constrain the cosmic anisotropy Hamiltonian H
= C_{1} sigma_{1} + C_{2} sigma_{2} + C_{3} sigma_{3} (sigma_{3} is in the axis
of earth rotation) to (C_{1}^{2} +C_{2}^{2})^{1/2} = (1.8 +- 5.3) X 10^{-21} eV
and | C_{3} | = (1.2 +- 3.5) X 10^{-19} eV. This improves the previous limits
on (C_{1},C_{2}) by 120 times and C_{3} by a factor of 800.

<id>
physics/0009013v1
<category>
physics.space-ph
<abstract>
We use a rotatable torsion balance to perform an equivalence principle test
on a magnetically shielded spin-polarized body of HoFe_{3}. With a rotation
period of one hour, the period of possible signal is reduced from one solar day
by 24 times, and hence the 1/f noise is greatly reduced. Our present
experimental results gives a limit (0.25 +- 1.26) X 10^{-9} on the Eotvos
parameter \eta of equivalence of the polarized body compared with unpolarized
aluminium-brass cylinders in the solar gravitional field, and a limit (0.34 +-
0.52) X 10^{-9} in the earth gravitional fields. This improves the previous
limit on polarized bodies by a factor of 45 for solar field and by a factor of
11 for earth field.

<id>
physics/0009046v1
<category>
physics.space-ph
<abstract>
We present a statistical analysis of 132 dayside (LT 0700-1700) bow shock
crossings of the AMPTE/IRM spacecraft. We perform a superposed epoch analysis
of plasma and magnetic field parameters as well as of low frequency magnetic
power spectra some minutes upstream and downstream of the bow shock by dividing
the events into categories depending on the angle between bow shock normal and
interplanetary magnetic field and on the plasma-beta, i.e., the ratio of plasma
to magnetic pressure.
  Downstream of the quasi-perpendicular low-beta (beta < 0.5) bow shock we find
a dominance of the left-hand polarized component at frequencies just below the
ion cyclotron frequency with amplitudes of about 3 nT. These waves are
identified as ion cyclotron waves which grow in a low-beta regime due to the
proton temperature anisotropy. We find a strong correlation of this anisotropy
with the intensity of the left-hand polarized component. Downstream of some
nearly perpendicular high-beta (beta > 1.0) crossings mirror waves are
identified. However, there are also cases where the conditions for mirror modes
are met downstream of the nearly perpendicular shock, but no mirror waves are
observed.

<id>
physics/0011064v1
<category>
physics.space-ph
<abstract>
The first high solar latitude pass of the Ulysses spacecraft revealed the
presence of MeV particle increases up to latitudes above 60 degrees, well
outside the CIR belt but associated in time with the regular passage of these
plasma interfaces at more equatorial latitudes. The particle increases have
been explained variously as due to diffusion from a field line connection with
a CIR at a greater distance, perpendicular diffusion in latitude, propagation
from the inner Corona or an acceleration process on a connecting field line.
Numerical solutions to the 1-D propagation equation upstream of a CIR shock for
reasonable diffusion mean free paths are shown here to limit the source of the
increases to within about 2 AU of the CIR's, asssuming the CIR's either trap or
accelerate energetic particles and they diffuse from the nearby CIR interface.
The problem of allowing propagation from the CIR's is eased if the CIRs are
located according to the predictions for the current sheet with a solar source
surface at $2.5 R_{s}$, rather than at $3.25 R_{s}$. Energetic electron
observations showing delays with respect to the CIR-associated ions suggest
possibly an acceleration in the inner corona, rather than in the CIR-associated
shock. A more likely explanation, however, is the ability of electrons to take
a longer a longer route, from beyond the point of observation, than for that of
the ions and yet arrive within a few days of CIR closest approach time.

<id>
physics/0101035v1
<category>
physics.space-ph
<abstract>
An analysis of the Alfven wave generation associated with the barium vapor
release at altitudes ~ 5.2 Earth's radii (ER) in the magnetosphere is
presented. Such injections were executed in G-8 and G-10 experiments of the
Combined Radiation and Radiation Effects Satellite (CRRES) mission. It is shown
that the generation of Alfven waves is possible during the total time of plasma
expansion. The maximum intensity of these waves corresponds to the time of
complete retardation of the diamagnetic cavity created by the expansion of
plasma cloud. The Alfven wave exhibits a form of an impulse with an effective
frequency ~ 0.03-0.05 Hz. Due to the background conditions and wave frequency,
the wave mainly oscillates along the geomagnetic field between the mirror
reflection points situated at ~ 0.7 ER. The wave amplitude is sufficient to the
generation of plasma instabilities and longitudinal electric field, and to an
increase in the longitudinal energy of electrons to ~ 1 keV. These processes
are the most probable for altitudes ~ 1 ER. The auroral kilometric radiation
(AKR) at frequencies ~ 100 kHz is associated with these accelerated electrons.
The acceleration of electrons and AKR can be observed almost continuously
during the first minute and then from time to time with pauses about 35-40 s
till 6-8 min after the release. The betatron acceleration of electrons at the
recovery of the geomagnetic field is also discussed. This mechanism could be
responsible for the acceleration of electrons resulting in the aurorae and
ultra short radio wave storm at frequencies 50-300 MHz observed at the 8-10th
min after the release.

<id>
physics/0106098v1
<category>
physics.space-ph
<abstract>
Observations indicate that the magnetotail convection is turbulent and
bi-modal, consisting of fast bursty bulk flows (BBF) and a nearly stagnant
background. We demonstrate that this observed phenomenon may be understood in
terms of the intermittent interactions, dynamic mergings and preferential
accelerations of coherent magnetic structures under the influence of a
background magnetic field geometry that is consistent with the development of
an X-point mean-field structure.

<id>
physics/0110042v1
<category>
physics.space-ph
<abstract>
Gravitational radiation is an elusive form of radiation predicted by general
relativity, it is the subject of intense theoretical and experimental research
at the limit of the sensitivity of today's instrumentation. In spite of the
fact that no direct evidence of this radiation now exist, observed
astrophysical phenomena have given convincing proofs of its existence. Theories
predict that gravitational radiation may also be employed for propulsion,
moreover the nonlinear behaviour of spacetime may permit the generation of
spacetime singularities with colliding beams of gravitational radiation, this
phenomenon could become a form of propellantless propulsion. Both applications
would require gravitational wave generators with high power and appropriate
optical properties. Among the proposed techniques that could be applicable to
the production of gravitational waves, a promising one is the possible emission
of gravitons by quantum systems. A hypothesis describing the production of
gravitons in s-wave/d-wave superconductor junction is presented.

<id>
physics/0111114v1
<category>
physics.space-ph
<abstract>
Method of the laboratory modeling of the Earth radiation belt is presented.
Method can be used for the estimation of consequences of global energetic and
communication projects realizations.

<id>
physics/0111145v1
<category>
physics.space-ph
<abstract>
In the past thirty years the Near-Earth Neutral Line model for substorms has
been proven to be correct in its basic assumption, i.e., that a new neutral
line is created near the Earth during substorm onset. Yet there have also been
numerous changes and additions to this model, which were necessary to give a
more complete and deeper understanding of the substorm process. This process is
still continuing.

<id>
physics/0212073v1
<category>
physics.space-ph
<abstract>
In this communication, we examine in situ observations of the magnetic field
in the vicinity of Phobos on the "Phobos-2" mission and give some analysis of
the data during a unique experiment ``Celestial Mechanics,'' which leads to the
support of evidence of the Phobos magnetic field. In particular, it is
suggested that the peculiarity of the solar wind interaction with Phobos and
rotating direction of the magnetic field obtained on the circular orbits around
Mars are the evidence for the existence of an intrinsic planetary field of
Phobos.

<id>
physics/0303123v1
<category>
physics.space-ph
<abstract>
Saturn's dynamic F-Ring still presents a challenge for understanding and
explaining the kinematic processes that lead to the changing structure visible
in our observations of this ring. This study examines the effect of Saturn's
magnetic field on the dynamics of micron-sized grains that may become
electrically charged due to interaction with plasma in Saturn's rigidly
corotating magnetosphere. The numerical model calculates the dynamics of
charged dust grains and includes forces due to Saturn's gravitational field,
the plasma polarization electric field, a third order harmonic expansion of
Saturn's magnetic field, and the F Ring's Shepherding moons, Prometheus and
Pandora.

<id>
physics/0303124v3
<category>
physics.space-ph
<abstract>
The dynamics of Saturn's F Ring have been a matter of curiosity ever since
Voyagers 1 and 2 sent back pictures of the ring's unusual features. Some of
these images showed three distinct ringlets with the outer two displaying a
kinked and braided appearance. Many models have been proposed to explain the
braiding seen in these images; most of these invoke perturbations caused by the
shepherding moons or km sized moonlets imbedded in the ring and are purely
gravitational in nature. These models also assume that the plasma densities and
charges on the grains are small enough that electromagnetic forces can be
ignored. However, Saturn's magnetic field exerts a significant perturbative
force on even weakly charged micron and sub-micron sized grains causing the
grains to travel in epicyclic orbits about a guiding center. This study
examines the effect of Saturn's magnetic field on the dynamics of micron-sized
grains along with gravitational interactions between the F Ring's shepherding
moons, Prometheus and Pandora. Due to differences in the charge-to-mass ratios
of the various sized grains, a phase difference between different size
populations is observed in the wavy orbits imposed by passage of the
shepherding moons.

<id>
physics/0308073v1
<category>
physics.space-ph
<abstract>
Dust particles immersed within a plasma environment, such as those in
protostellar clouds, planetary rings or cometary environments, will acquire an
electric charge. If the ratio of the inter-particle potential energy to the
average kinetic energy is high enough the particles will form either a "liquid"
structure with short-range ordering or a crystalline structure with long range
ordering. Many experiments have been conducted over the past several years on
such colloidal plasmas to discover the nature of the crystals formed, but more
work is needed to fully understand these complex colloidal systems. Most
previous experiments have employed monodisperse spheres to form Coulomb
crystals. However, in nature (as well as in most plasma processing
environments) the distribution of particle sizes is more randomized and
disperse. This paper reports experiments which were carried out in a GEC rf
reference cell modified for use as a dusty plasma system, using varying sizes
of particles to determine the manner in which the correlation function depends
upon the overall dust grain size distribution. (The correlation function
determines the overall crystalline structure of the lattice.) Two dimensional
plasma crystals were formed of assorted glass spheres with specific size
distributions in an argon plasma. Using various optical techniques, the pair
correlation function was determined and compared to those calculated
numerically.

<id>
physics/0308074v1
<category>
physics.space-ph
<abstract>
Dust particles immersed within a plasma environment, such as those found in
planetary rings or comets, will acquire an electric charge. If the ratio of the
inter-particle potential energy to average kinetic energy is large enough the
particles will form either a "liquid" structure with short-range ordering or a
crystalline structure with long-range ordering. Since their discovery in
laboratory environments in 1994, such crystals have been the subject of a
variety of experimental, theoretical and numerical investigations. Most
numerical and theoretical investigations have examined infinite systems
assuming periodic boundary conditions. Since experimentally observed crystals
can be comprised of a few hundred particles, this often leads to discrepancies
between predicted theoretical results and experimental data. In addition,
recent studies have concentrated on the importance of random charge variations
between individual dust particles, but very little on the importance of size
variations between the grains. Such size variations naturally lead to
inter-grain charge variations which can easily become more important than those
due to random charge fluctuations (which are typically less than one percent).
Although such size variations can be largely eliminated experimentally by
introducing mono-dispersive particles, many laboratory systems and all
astrophysical environments contain significant size distributions. This study
utilizes a program to find the equilibrium positions of a dusty plasma system
as well as a modified Barnes-Hut code to model the dynamic behavior of such
systems. It is shown that in terms of inter-particle spacing and ordering,
finite systems are significantly different than infinite ones, particularly for
the most-highly ordered states.

<id>
physics/0308075v1
<category>
physics.space-ph
<abstract>
Many asteroids show indications they have undergone impacts with meteoroid
particles having radii between 0.01 m and 1 m. During such impacts, small dust
grains will be ejected at the impact site. The possibility of these dust grains
(with radii greater than 2.2x10-6 m) forming a halo around a spherical asteroid
(such as Ceres) is investigated using standard numerical integration
techniques. The orbital elements, positions, and velocities are determined for
particles with varying radii taking into account both the influence of gravity,
radiation pressure, and the interplanetary magnetic field (for charged
particles). Under the influence of these forces it is found that dust grains
(under the appropriate conditions) can be injected into orbits with lifetimes
in excess of one year. The lifetime of the orbits is shown to be highly
dependent on the location of the ejection point as well as the angle between
the surface normal and the ejection path. It is also shown that only particles
ejected within 10 degrees relative to the surface tangential survive more than
a few hours and that the longest-lived particles originate along a line
perpendicular to the Ceres-Sun line.

<id>
physics/0401150v1
<category>
physics.space-ph
<abstract>
Among the problems yet to be solved by current theories of solar system
formation is the origin of the Jupiter-family comets. Several possibilities are
examined using a fifth-order Runge-Kutta algorithm.

<id>
physics/0401152v1
<category>
physics.space-ph
<abstract>
The question of whether asteroid ejecta can enter stable orbits about the
parent body is examined.

<id>
physics/0401160v1
<category>
physics.space-ph
<abstract>
Numerical simulations show that planetesimal systems with unevenly
distributed masses may actually be more stable than those where all the bodies
are of roughly equal size.

<id>
physics/0402122v1
<category>
physics.space-ph
<abstract>
Isolated electrostatic structures are observed throughout much of the 4 Re by
19.6 Re Cluster orbit. These structures are observed in the Wideband plasma
wave instrument's waveform data as bipolar and tripolar pulses. These
structures are observed at all of the boundary layers, in the solar wind and
magnetosheath, and along auroral field lines at 4.5-6.5 Re. Using the Wideband
waveform data from the various Cluster spacecraft we have carried out a survey
of the amplitudes and time durations of these structures and how these
quantities vary with the local magnetic field strength. Such a survey has not
been carried out before, and it reveals certain characteristics of solitary
structures in a finite magnetic field, a topic still inadequately addressed by
theories. We find that there is a broad range of electric field amplitudes at
any specific magnetic field strength, and there is a general trend for the
electric field amplitudes to increase as the strength of the magnetic field
increases over a range of 5 to 500 nT. We provide a possible explanation for
this trend that releates to the structures being Bernstein-Greene-Kruskal mode
solitary waves. There is no corresponding dependence of the duration of the
structures on the magnetic field strength, although a plot of these two
quantities reveals the unexpected result that with the exception of the
magnetosheath, all of the time durations for all of the other regions are
comparable, wheras the magnetosheath time durations clearly are in a different
category of much smaller time duration. We speculate that this implies the
structures are much smaller in size.

<id>
physics/0403105v2
<category>
physics.space-ph
<abstract>
A relativistic bounce-averaged quasilinear diffusion equation is derived to
describe stochastic particle transport associated with arbitrary-frequency
electromagnetic fluctuations in a nonuniform magnetized plasma. Expressions for
the elements of a relativistic quasilinear diffusion tensor are calculated
explicitly for magnetically-trapped particle distributions in axisymmetric
magnetic geometry in terms of gyro-drift-bounce wave-particle resonances. The
resonances can destroy any one of the three invariants of the unperturbed
guiding-center Hamiltonian dynamics.

<id>
physics/0407113v1
<category>
physics.space-ph
<abstract>
Shematovich et al. (2003) recently showed plasma induced sputtering in
Titan's atmosphere is a source of neutral nitrogen in Saturn's magnetosphere
comparable to the photo-dissociation source. These sources form a toroidal
nitrogen cloud roughly centered at Titan's orbital radius but gravitationally
bound to Saturn. Once ionized, these particles contribute to Saturn's plasma.
When Titan is inside Saturn's magnetopause, newly formed ions can diffuse
inward becoming inner magnetospheric energetic nitrogen where they can sputter
and be implanted into icy satellite surfaces. Our 3-D simulation produces the
first consistent Titan generated N and N2 neutral clouds; solar UV radiation
and magnetospheric plasma subject these particles to dissociation and
ionization. The cloud morphologies and associated nitrogen plasma source rates
are predicted in anticipation of Cassini data. Since the amount of molecular
nitrogen ejected from Titan by photo-dissociation is small, molecular nitrogen
ions detection by Cassini will be an indicator of atmospheric sputtering.

<id>
quant-ph/9412001v1
<category>
quant-ph
<abstract>
We investigate the general case of the photon distribution of a two-mode
squeezed vacuum and show that the distribution of photons among the two modes
depends on four parameters: two squeezing parameters, the relative phase
between the two oscillators and their spatial orientation. The distribution of
the total number of photons depends only on the two squeezing parameters. We
derive analytical expressions and present pictures for both distributions.

<id>
quant-ph/9412002v1
<category>
quant-ph
<abstract>
Maximally predictive states, as defined in recent work by Zurek, Habib and
Paz, are studied for more elaborate environment models than a linear coupling.
An environment model which includes spatial correlations in the noise is
considered in the non-dissipative regime. The Caldeira-Leggett model is also
reconsidered in the context of an averaging procedure which produces a
completely positive form for the quantum master equation. In both cases, the
maximally predictive states for the harmonic oscillator are the coherent
states, which is the same result found by Zurek,Habib and Paz for the
Caldeira-Legget environment.

<id>
quant-ph/9412003v1
<category>
quant-ph
<abstract>
This is a writeup of a talk given at the Oskar Klein Centenery Symposium,
Stockholm, September 19-21, 1994. It is an essay on the black hole information
paradox and its connection with thermodynamics and the foundations of quantum
mechanics.

<id>
quant-ph/9412005v1
<category>
quant-ph
<abstract>
Recently a stronger statement of Levinson's theorem for the Dirac equation
was presented, where the limits of the phase shifts at $E=\pm M$ are related to
the numbers of nodes of radial functions at the same energies, respectively.
However, in this letter we show that this statement has to be modified because
the limits of the phase shifts may be negative for the Dirac equation.

<id>
quant-ph/9412006v1
<category>
quant-ph
<abstract>
A Kochen-Specker contradiction is produced with 36 vectors in a real
8-dimensional Hilbert space. These vectors can be combined into 30 distinct
projection operators (14 of rank 2, and 16 of rank 1). A state-specific variant
of this contradiction requires only 13 vectors, a remarkably low number for 8
dimensions.

<id>
quant-ph/9412007v2
<category>
quant-ph
<abstract>
The momentum operator for a particle in a box is represented by an infinite
order Hermitian matrix $P$. Its square $P^2$ is well defined (and diagonal),
but its cube $P^3$ is ill defined, because $P P^2\neq P^2 P$. Truncating these
matrices to a finite order restores the associative law, but leads to other
curious results.

<id>
quant-ph/9412008v1
<category>
quant-ph
<abstract>
A general formlulation for discrete-time quantum mechanics, based on
Feynman's method in ordinary quantum mechanics, is presented. It is shown that
the ambiguities present in ordinary quantum mechanics (due to noncommutativity
of the operators), are no longer present here. Then the criteria for the
unitarity of the evolution operator is examined. It is shown that the unitarity
of the evolution operator puts restrictions on the form of the action, and also
implies the existence of a solution for the classical initial-value problem.

<id>
quant-ph/9412009v1
<category>
quant-ph
<abstract>
An analogue of Kolmogorov's superconvergent perturbation theory in classical
mechanics is constructed for self adjoint operators. It is different from the
usual Rayleigh--Schr\"odinger perturbation theory and yields expansions for
eigenvalues and eigenvectors in terms of functions of the perturbation
parameter.

<id>
quant-ph/9412010v1
<category>
quant-ph
<abstract>
An exact analogue of the method of averaging in classical mechanics is
constructed for self--adjoint operators. It is shown to be completely
equivalent to the usual Rayleigh--Schr\"odinger perturbation theory but gives
the sums over intermediate states in closed form expressions. The anharmonic
oscillator and the Henon--Heiles system are treated as examples to illustrate
the quantum averaging method.

<id>
quant-ph/9412011v1
<category>
quant-ph
<abstract>
A general analysis of squeezing transformations for two mode systems is given
based on the four dimensional real symplectic group $Sp(4,\Re)\/$. Within the
framework of the unitary metaplectic representation of this group, a
distinction between compact photon number conserving and noncompact photon
number nonconserving squeezing transformations is made. We exploit the
$Sp(4,\Re)-SO(3,2)\/$ local isomorphism and the $U(2)\/$ invariant squeezing
criterion to divide the set of all squeezing transformations into a two
parameter family of distinct equivalence classes with representative elements
chosen for each class. Familiar two mode squeezing transformations in the
literature are recognized in our framework and seen to form a set of measure
zero. Examples of squeezed coherent and thermal states are worked out. The need
to extend the heterodyne detection scheme to encompass all of $U(2)\/$ is
emphasized, and known experimental situations where all $U(2)\/$ elements can
be reproduced are briefly described.

<id>
quant-ph/9501001v3
<category>
quant-ph
<abstract>
Based on a relation between inertial time intervals and the Riemannian
curvature, we show that space--time uncertainty derived by Ng and van Dam
implies absurd uncertainties of the Riemannian curvature.

<id>
quant-ph/9501002v2
<category>
quant-ph
<abstract>
It is shown that the ``retrodiction paradox'' recently introduced by Peres
arises not because of the fallacy of the time-symmetric approach as he claimed,
but due to an inappropriate usage of retrodiction.

<id>
quant-ph/9501003v1
<category>
quant-ph
<abstract>
Comment on L. Hardy, Phys. Rev. Lett. {\bf 73}, 2279 (1994). It is argued
that the experiment proposed by Hardy should not be considered as a single
photon experiment.

<id>
quant-ph/9501005v2
<category>
quant-ph
<abstract>
In the standard physical interpretation of quantum theory, prediction and
retrodiction are not symmetric. The opposite assertion by some contributors results
from their use of non-standard interpretations of the theory.

<id>
quant-ph/9501006v1
<category>
quant-ph
<abstract>
Comment on [R.L. Ingraham, Phys. Rev. A 50, 4502 (1994)]. Ingraham suggested
``a delayed-choice experiment with partial, controllable memory erasing''. It
is shown that he cannot be right since his predictions contradict relativistic
causality. A subtle quantum effect which was overlooked by Ingraham is
explained.

<id>
quant-ph/9501007v1
<category>
quant-ph
<abstract>
General features of nonlinear quantum mechanics are discussed in the context
of applications to two-level atoms.

<id>
quant-ph/9501008v1
<category>
quant-ph
<abstract>
An extension of the Liouville-von Neumann dynamics to a Nambu-type dynamics
is proposed. The resulting theory is the first version of nonlinear QM which is
free from internal inconsistencies.

<id>
quant-ph/9501009v1
<category>
quant-ph
<abstract>
We discuss both the restricted path integral (RPI) and the wave equation (WE)
techniques in the theory of continuous quantum measurements. We intend to make
Mensky's fresh review complete by transforming his "effective" WE with complex
Hamiltonian into Ito-differential equations.

<id>
quant-ph/9501013v1
<category>
quant-ph
<abstract>
Using a two-photon interference technique, we measure the delay for
single-photon wavepackets to be transmitted through a multilayer dielectric
mirror, which functions as a ``photonic bandgap'' medium. By varying the angle
of incidence, we are able to confirm the behavior predicted by the group delay
(stationary phase approximation), including a variation of the delay time from
superluminal to subluminal as the band edge is tuned towards to the wavelength
of our photons. The agreement with theory is better than 0.5 femtoseconds (less
than one quarter of an optical period) except at large angles of incidence. The
source of the remaining discrepancy is not yet fully understood.

<id>
quant-ph/9501014v5
<category>
quant-ph
<abstract>
This MSc dissertation surveys nine interpretations of non-relativistic
quantum mechanics. Extensive references are given. The interpretations covered
are: the orthodox interpretation, Bohr's interpretation, the idea that the mind
causes collapse, hidden variables, the many-worlds interpretation, the
many-minds interpretation, Bohm's interpretation and two interpretations based
on decoherent histories.

<id>
quant-ph/9501015v1
<category>
quant-ph
<abstract>
The question in the title may be answered by considering the outcome of a
``weak measurement'' in the sense of Aharonov et al. Various properties of the
resulting time are discussed, including its close relation to the Larmor times.
It is a universal description of a broad class of measurement interactions, and
its physical implications are unambiguous.

<id>
quant-ph/9501016v1
<category>
quant-ph
<abstract>
We review some of our experiments performed over the past few years on
two-photon interference. These include a test of Bell's inequalities, a study
of the complementarity principle, an application of EPR correlations for
dispersion-free time-measurements, and an experiment to demonstrate the
superluminal nature of the tunneling process. The nonlocal character of the
quantum world is brought out clearly by these experiments. As we explain,
however, quantum nonlocality is not inconsistent with Einstein causality.

<id>
quant-ph/9501017v1
<category>
quant-ph
<abstract>
It is shown that the spin operator can be described by an algebra which is in
between so(3) and e(2). Relativistic version of the singlet state for two Dirac
electrons is discussed. It is shown that a measure of massless particle's
extension can be naturally constructed and that this measure corresponds at the
classical level to the radius of the Robinson congruence.

<id>
quant-ph/9501018v1
<category>
quant-ph
<abstract>
Thought experiments about the physical nature of set theoretical
counterexamples to the axiom of choice motivate the investigation of peculiar
constructions, e.g. an infinite dimensional Hilbert space with a modular
quantum logic. Applying a concept due to BENIOFF, we identify the intrinsically
effective Hamiltonians with those observables of quantum theory which may
coexist with a failure of the axiom of choice. Here a self adjoint operator is
intrinsically effective, iff the Schroedinger equation of its generated
semigroup is soluble by means of eigenfunction series expansions.

<id>
quant-ph/9501019v2
<category>
quant-ph
<abstract>
If a physical system contains a single particle, and if two distant detectors
test the presence of linear superpositions of one-particle and vacuum states, a
violation of classical locality can occur. It is due to the creation of a
two-particle component by the detecting process itself.

<id>
quant-ph/9501020v2
<category>
quant-ph
<abstract>
A teleportation method using standard present day optical technology is
presented.

<id>
quant-ph/9501021v1
<category>
quant-ph
<abstract>
According to Bell's theorem, the degree of correlation between spatially
separated measurements on a quantum system is limited by certain inequalities
if one assumes the condition of locality. Quantum mechanics predicts that this
limit can be exceeded, making it nonlocal. We analyse the effect of an
environment modelled by a fluctuating magnetic field on the quantum
correlations in an EPR singlet as seen in the Bell inequality. We show that in
an EPR setup, the system goes from the usual 'violation' of Bell inequality to
a 'non-violation' for times larger than a characteristic time scale which is
related to the parameters of the fluctuating field. We also look at these
inequalities as a function of the spatial separation between the EPR pair.

<id>
quant-ph/9501022v1
<category>
quant-ph
<abstract>
A dynamical model for the collapse of the wave function in a quantum
measurement process is proposed by considering the interaction of a quantum
system (spin-1/2) with a macroscopic quantum apparatus interacting with an
environment in a dissipative manner. The dissipative interaction leads to
decoherence in the superposition states of the apparatus, making its behaviour
classical in the sense that the density matrix becomes diagonal with time.
Since the apparatus is also interacting with the system, the probabilities of
the diagonal density matrix are determined by the state vector of the system.
We consider a Stern-Gerlach type model, where a spin- 1/2 particle is in an
inhomogeneous magnetic field, the whole set up being in contact with a large
environment. Here we find that the density matrix of the combined system and
apparatus becomes diagonal and the momentum of the particle becomes correlated
with a spin operator, selected by the choice of the system-apparatus
interaction. This allows for a measurement of spin via a momentum measurement
on the particle with associated probabilities in accordance with quantum
principles.

<id>
quant-ph/9502001v1
<category>
quant-ph
<abstract>
A model of repeated quantum measurements of magnetic flux in superconducting
circuits manifesting tunneling is discussed. The perturbation due to the
previous measurements of magnetic flux is always present unless quantum
nondemolition measurements are performed. By replacing the classical notion of
noninvasivity with this condition, temporal Bell-like inequalities allows one
to test the observability at the macroscopic level of the conflict between
realism and quantum theory.

<id>
quant-ph/9502002v1
<category>
quant-ph
<abstract>
We model measuring processes of a single spin-1/2 object and of a pair of
spin-1/2 objects in the EPR-Bohm state by systems of differential equations.
Our model is a local model with hidden-variables of the EPR-Bohm
Gedankenexperiment. Although there is no dynamical interaction between a pair
of spin-1/2 objects, the model can reproduce approximately the
quantum-mechanical correlations by the coincidence counting. Hence the Bell
inequality is violated. This result supports the idea that the coincidence
counting is the source of the non-locality in the EPR-Bohm Gedankenexperiment.

