<id>
0704.1711v2
<category>
stat.AP
<abstract>
The paper deals with the study of labor market dynamics, and aims to
characterize its equilibriums and possible trajectories. The theoretical
background is the theory of the segmented labor market. The main idea is that
this theory is well adapted to interpret the observed trajectories, due to the
heterogeneity of the work situations.

<id>
0704.3474v1
<category>
stat.AP
<abstract>
The estimation of missing input vector elements in real time processing
applications requires a system that possesses the knowledge of certain
characteristics such as correlations between variables, which are inherent in
the input space. Computational intelligence techniques and maximum likelihood
techniques do possess such characteristics and as a result are important for
imputation of missing data. This paper compares two approaches to the problem
of missing data estimation. The first technique is based on the current state
of the art approach to this problem, that being the use of Maximum Likelihood
(ML) and Expectation Maximisation (EM. The second approach is the use of a
system based on auto-associative neural networks and the Genetic Algorithm as
discussed by Adbella and Marwala3. The estimation ability of both of these
techniques is compared, based on three datasets and conclusions are made.

<id>
0704.3862v1
<category>
stat.AP
<abstract>
In this paper we develop a scientific approach to control inter-country
conflict. This system makes use of a neural network and a feedback control
approach. It was found that by controlling the four controllable inputs:
Democracy, Dependency, Allies and Capability simultaneously, all the predicted
dispute outcomes could be avoided. Furthermore, it was observed that
controlling a single input Dependency or Capability also avoids all the
predicted conflicts. When the influence of each input variable on conflict is
assessed, Dependency, Capability, and Democracy emerge as key variables that
influence conflict.

<id>
0705.0569v1
<category>
stat.AP
<abstract>
Longitudinal studies could be complicated by left-censored repeated measures.
For example, in Human Immunodeficiency Virus infection, there is a detection
limit of the assay used to quantify the plasma viral load. Simple imputation of
the limit of the detection or of half of this limit for left-censored measures
biases estimations and their standard errors. In this paper, we review two
likelihood-based methods proposed to handle left-censoring of the outcome in
linear mixed model. We show how to fit these models using SAS Proc NLMIXED and
we compare this tool with other programs. Indications and limitations of the
programs are discussed and an example in the field of HIV infection is shown.

<id>
0705.2515v1
<category>
stat.AP
<abstract>
This paper compares the Maximum-likelihood method and Bayesian method for
finite element model updating. The Maximum-likelihood method was implemented
using genetic algorithm while the Bayesian method was implemented using the
Markov Chain Monte Carlo. These methods were tested on a simple beam and an
unsymmetrical H-shaped structure. The results show that the Bayesian method
gave updated finite element models that predicted more accurate modal
properties than the updated finite element models obtained through the use of
the Maximum-likelihood method. Furthermore, both these methods were found to
require the same levels of computational loads.

<id>
0705.3257v2
<category>
stat.AP
<abstract>
We present a quantitative analysis of throwing ability for major league
outfielders and catchers. We use detailed game event data to tabulate success
and failure events in outfielder and catcher throwing opportunities. We
attribute a run contribution to each success or failure which are tabulated for
each player in each season. We use four seasons of data to estimate the overall
throwing ability of each player using a Bayesian hierarchical model. This model
allows us to shrink individual player estimates towards an overall population
mean depending on the number of opportunities for each player. We use the
posterior distribution of player abilities from this model to identify players
with significant positive and negative throwing contributions.

<id>
0706.0073v1
<category>
stat.AP
<abstract>
This paper presents a dynamic linear model for modeling hourly ozone
concentrations over the eastern United States. That model, which is developed
within an Bayesian hierarchical framework, inherits the important feature of
such models that its coefficients, treated as states of the process, can change
with time. Thus the model includes a time--varying site invariant mean field as
well as time varying coefficients for 24 and 12 diurnal cycle components. This
cost of this model's great flexibility comes at the cost of computational
complexity, forcing us to use an MCMC approach and to restrict application of
our model domain to a small number of monitoring sites. We critically assess
this model and discover some of its weaknesses in this type of application.

<id>
0706.1401v1
<category>
stat.AP
<abstract>
Longitudinal data tracking repeated measurements on individuals are highly
valued for research because they offer controls for unmeasured individual
heterogeneity that might otherwise bias results. Random effects or mixed models
approaches, which treat individual heterogeneity as part of the model error
term and use generalized least squares to estimate model parameters, are often
criticized because correlation between unobserved individual effects and other
model variables can lead to biased and inconsistent parameter estimates.
Starting with an examination of the relationship between random effects and
fixed effects estimators in the standard unobserved effects model, this article
demonstrates through analysis and simulation that the mixed model approach has
a ``bias compression'' property under a general model for individual
heterogeneity that can mitigate bias due to uncontrolled differences among
individuals. The general model is motivated by the complexities of longitudinal
student achievement measures, but the results have broad applicability to
longitudinal modeling.

<id>
0707.0462v1
<category>
stat.AP
<abstract>
Probability models are proposed for passage time data collected in
experiments with a device designed to measure particle flow during aerial
application of fertilizer. Maximum likelihood estimation of flow intensity is
reviewed for the simple linear Boolean model, which arises with the assumption
that each particle requires the same known passage time. M-estimation is
developed for a generalization of the model in which passage times behave as a
random sample from a distribution with a known mean. The generalized model
improves fit in these experiments. An estimator of total particle flow is
constructed by conditioning on lengths of multi-particle clumps.

<id>
0707.0600v1
<category>
stat.AP
<abstract>
A two-sex Basic Reproduction Number (BRN) is used to investigate the
conditions under which the Human Immunodeficiency Virus (HIV) may spread
through heterosexual contacts in Sub-Saharan Africa. (The BRN is the expected
number of new infections generated by one infected individual; the disease
spreads if the BRN is larger than 1). A simple analytical expression for the
BRN is derived on the basis of recent data on survival rates, transmission
probabilities, and levels of sexual activity. Baseline results show that in the
population at large (characterized by equal numbers of men and women) the BRN
is larger than 1 if every year each person has 82 sexual contacts with
different partners. the BRN is also larger than 1 for commercial sex workers
(CSWs) and their clients (two populations of different sizes) if each CSW has
about 256 clients per year and each client visits one CSW every two weeks. A
sensitivity analysis explores the effect on the BRN of a doubling (or a
halving) of the transmission probabilities. Implications and extensions are
discussed.

<id>
0707.3013v1
<category>
stat.AP
<abstract>
This paper defines and implements a non-Bayesian fusion rule for combining
densities of probabilities estimated by local (non-linear) filters for tracking
a moving target by passive sensors. This rule is the restriction to a strict
probabilistic paradigm of the recent and efficient Proportional Conflict
Redistribution rule no 5 (PCR5) developed in the DSmT framework for fusing
basic belief assignments. A sampling method for probabilistic PCR5 (p-PCR5) is
defined. It is shown that p-PCR5 is more robust to an erroneous modeling and
allows to keep the modes of local densities and preserve as much as possible
the whole information inherent to each densities to combine. In particular,
p-PCR5 is able of maintaining multiple hypotheses/modes after fusion, when the
hypotheses are too distant in regards to their deviations. This new p-PCR5 rule
has been tested on a simple example of distributed non-linear filtering
application to show the interest of such approach for future developments. The
non-linear distributed filter is implemented through a basic particles
filtering technique. The results obtained in our simulations show the ability
of this p-PCR5-based filter to track the target even when the models are not
well consistent in regards to the initialization and real cinematic.

<id>
0708.0945v1
<category>
stat.AP
<abstract>
This paper introduces an iterative tomogravity algorithm for the estimation
of a network traffic matrix based on one snapshot observation of the link loads
in the network. The proposed method does not require complete observation of
the total load on individual edge links or proper tuning of a penalty parameter
as existing methods do. Numerical results are presented to demonstrate that the
iterative tomogravity method controls the estimation error well when the link
data is fully observed and produces robust results with moderate amount of
missing link data.

<id>
0708.0974v1
<category>
stat.AP
<abstract>
A stratified sampling plan to audit health insurance claims is offered. The
stratification is by dollar amount of the claim. The plan is representative in
the sense that with high probability for each stratum, the difference in the
average dollar amount of the claim in the sample and the average dollar amount
in the population, is ``small.'' Several notions of ``small'' are presented.
The plan then yields a relatively small total sample size with the property
that the overall average dollar amount in the sample is close to the average
dollar amount in the population. Three different estimators and corresponding
lower confidence bounds for over (under) payments are studied.

<id>
0708.1401v1
<category>
stat.AP
<abstract>
The nurse Lucia de Berk was convicted by the Dutch courts as a serial killer
with 7 murders and 3 attempts at murder in three hospitals where she worked.
The nurse however always professed her innocence and indeed was never observed
in such an act of murder. The courts based their decision on circumstantial
evidence and upon the use of statistics. In the appeal court, the use of
statistical calculations was repealed but the use of "data" and "statistical
insights" were not excluded. The trial hinged importantly on the role of
statistics and data gathering. It appears that data selection and confounding
feature strongly in this case. The notion of "nominal correlation" can be used
to highlight those two features. This suggests a mistrial with the conviction
of an innocent person.

<id>
0708.3545v1
<category>
stat.AP
<abstract>
The main phases of applied statistical work are discussed in general terms.
The account starts with the clarification of objectives and proceeds through
study design, measurement and analysis to interpretation. An attempt is made to
extract some general notions.

<id>
0708.3601v2
<category>
stat.AP
<abstract>
Topic models, such as latent Dirichlet allocation (LDA), can be useful tools
for the statistical analysis of document collections and other discrete data.
The LDA model assumes that the words of each document arise from a mixture of
topics, each of which is a distribution over the vocabulary. A limitation of
LDA is the inability to model topic correlation even though, for example, a
document about genetics is more likely to also be about disease than X-ray
astronomy. This limitation stems from the use of the Dirichlet distribution to
model the variability among the topic proportions. In this paper we develop the
correlated topic model (CTM), where the topic proportions exhibit correlation
via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982)
139--177]. We derive a fast variational inference algorithm for approximate
posterior inference in this model, which is complicated by the fact that the
logistic normal is not conjugate to the multinomial. We apply the CTM to the
articles from Science published from 1990--1999, a data set that comprises 57M
words. The CTM gives a better fit of the data than LDA, and we demonstrate its
use as an exploratory tool of large document collections.

<id>
0708.4318v1
<category>
stat.AP
<abstract>
Cis-regulatory modules (CRMs) composed of multiple transcription factor
binding sites (TFBSs) control gene expression in eukaryotic genomes.
Comparative genomic studies have shown that these regulatory elements are more
conserved across species due to evolutionary constraints. We propose a
statistical method to combine module structure and cross-species orthology in
de novo motif discovery. We use a hidden Markov model (HMM) to capture the
module structure in each species and couple these HMMs through multiple-species
alignment. Evolutionary models are incorporated to consider correlated
structures among aligned sequence positions across different species. Based on
our model, we develop a Markov chain Monte Carlo approach, MultiModule, to
discover CRMs and their component motifs simultaneously in groups of
orthologous sequences from multiple species. Our method is tested on both
simulated and biological data sets in mammals and Drosophila, where significant
improvement over other motif and module discovery methods is observed.

<id>
0708.4337v1
<category>
stat.AP
<abstract>
Mobile robots require basic information to navigate through an environment:
they need to know where they are (localization) and they need to know where
they are going. For the latter, robots need a map of the environment. Using
sensors of a variety of forms, robots gather information as they move through
an environment in order to build a map. In this paper we present a novel
sampling algorithm to solving the simultaneous mapping and localization (SLAM)
problem in indoor environments. We approach the problem from a Bayesian
statistics perspective. The data correspond to a set of range finder and
odometer measurements, obtained at discrete time instants. We focus on the
estimation of the posterior distribution over the space of possible maps given
the data. By exploiting different factorizations of this distribution, we
derive three sampling algorithms based on importance sampling. We illustrate
the results of our approach by testing the algorithms with two real data sets
obtained through robot navigation inside office buildings at Carnegie Mellon
University and the Pontificia Universidad Catolica de Chile.

<id>
0708.4350v1
<category>
stat.AP
<abstract>
A prespecified set of genes may be enriched, to varying degrees, for genes
that have altered expression levels relative to two or more states of a cell.
Knowing the enrichment of gene sets defined by functional categories, such as
gene ontology (GO) annotations, is valuable for analyzing the biological
signals in microarray expression data. A common approach to measuring
enrichment is by cross-classifying genes according to membership in a
functional category and membership on a selected list of significantly altered
genes. A small Fisher's exact test $p$-value, for example, in this $2\times2$
table is indicative of enrichment. Other category analysis methods retain the
quantitative gene-level scores and measure significance by referring a
category-level statistic to a permutation distribution associated with the
original differential expression problem. We describe a class of random-set
scoring methods that measure distinct components of the enrichment signal. The
class includes Fisher's test based on selected genes and also tests that
average gene-level evidence across the category. Averaging and selection
methods are compared empirically using Affymetrix data on expression in
nasopharyngeal cancer tissue, and theoretically using a location model of
differential expression. We find that each method has a domain of superiority
in the state space of enrichment problems, and that both methods have benefits
in practice. Our analysis also addresses two problems related to
multiple-category inference, namely, that equally enriched categories are not
detected with equal probability if they are of different sizes, and also that
there is dependence among category statistics owing to shared genes. Random-set
enrichment calculations do not require Monte Carlo for implementation. They are
made available in the R package allez.

<id>
0708.4358v1
<category>
stat.AP
<abstract>
While it is widely accepted that lead-based paint and leaded gasoline are
primary sources of elevated concentrations of lead in residential soils,
conclusions regarding their relative contributions are mixed and generally
study specific. We develop a novel nonlinear regression for soil lead
concentrations over time. It is argued that this methodology provides useful
insights into the partitioning of the average soil lead concentration by source
and time over large residential areas. The methodology is used to investigate
soil lead concentrations from the 1987 Minnesota Lead Study and the 1990
National Lead Survey. Potential litigation issues are discussed briefly.

<id>
0709.0165v1
<category>
stat.AP
<abstract>
In high-throughput genomics, large-scale designed experiments are becoming
common, and analysis approaches based on highly multivariate regression and
anova concepts are key tools. Shrinkage models of one form or another can
provide comprehensive approaches to the problems of simultaneous inference that
involve implicit multiple comparisons over the many, many parameters
representing effects of design factors and covariates. We use such approaches
here in a study of cardiovascular genomics. The primary experimental context
concerns a carefully designed, and rich, gene expression study focused on
gene-environment interactions, with the goals of identifying genes implicated
in connection with disease states and known risk factors, and in generating
expression signatures as proxies for such risk factors. A coupled exploratory
analysis investigates cross-species extrapolation of gene expression
signatures--how these mouse-model signatures translate to humans. The latter
involves exploration of sparse latent factor analysis of human observational
data and of how it relates to projected risk signatures derived in the animal
models. The study also highlights a range of applied statistical and genomic
data analysis issues, including model specification, computational questions
and model-based correction of experimental artifacts in DNA microarray data.

<id>
0709.0366v1
<category>
stat.AP
<abstract>
The Bonferroni multiple testing procedure is commonly perceived as being
overly conservative in large-scale simultaneous testing situations such as
those that arise in microarray data analysis. The objective of the present
study is to show that this popular belief is due to overly stringent
requirements that are typically imposed on the procedure rather than to its
conservative nature. To get over its notorious conservatism, we advocate using
the Bonferroni selection rule as a procedure that controls the per family error
rate (PFER). The present paper reports the first study of stability properties
of the Bonferroni and Benjamini--Hochberg procedures. The Bonferroni procedure
shows a superior stability in terms of the variance of both the number of true
discoveries and the total number of discoveries, a property that is especially
important in the presence of correlations between individual $p$-values. Its
stability and the ability to provide strong control of the PFER make the
Bonferroni procedure an attractive choice in microarray studies.

<id>
0709.0394v1
<category>
stat.AP
<abstract>
The spatial dependence of total column ozone varies strongly with latitude,
so that homogeneous models (invariant to all rotations) are clearly unsuitable.
However, an assumption of axial symmetry, which means that the process model is
invariant to rotations about the Earth's axis, is much more plausible and
considerably simplifies the modeling. Using TOMS (Total Ozone Mapping
Spectrometer) measurements of total column ozone over a six-day period, this
work investigates the modeling of axially symmetric processes on the sphere
using expansions in spherical harmonics. It turns out that one can capture many
of the large scale features of the spatial covariance structure using a
relatively small number of terms in such an expansion, but the resulting fitted
model provides a horrible fit to the data when evaluated via its likelihood
because of its inability to describe accurately the process's local behavior.
Thus, there remains the challenge of developing computationally tractable
models that capture both the large and small scale structure of these data.

<id>
0709.0406v1
<category>
stat.AP
<abstract>
Early detection of person-to-person transmission of emerging infectious
diseases such as avian influenza is crucial for containing pandemics. We
developed a simple permutation test and its refined version for this purpose. A
simulation study shows that the refined permutation test is as powerful as or
outcompetes the conventional test built on asymptotic theory, especially when
the sample size is small. In addition, our resampling methods can be applied to
a broad range of problems where an asymptotic test is not available or fails.
We also found that decent statistical power could be attained with just a small
number of cases, if the disease is moderately transmissible between humans.

<id>
0709.0421v1
<category>
stat.AP
<abstract>
The Joint United Nations Programme on HIV/AIDS (UNAIDS) has developed the
Estimation and Projection Package (EPP) for making national estimates and
short-term projections of HIV prevalence based on observed prevalence trends at
antenatal clinics. Assessing the uncertainty about its estimates and
projections is important for informed policy decision making, and we propose
the use of Bayesian melding for this purpose. Prevalence data and other
information about the EPP model's input parameters are used to derive a
probabilistic HIV prevalence projection, namely a probability distribution over
a set of future prevalence trajectories. We relate antenatal clinic prevalence
to population prevalence and account for variability between clinics using a
random effects model. Predictive intervals for clinic prevalence are derived
for checking the model. We discuss predictions given by the EPP model and the
results of the Bayesian melding procedure for Uganda, where prevalence peaked
at around 28% in 1990; the 95% prediction interval for 2010 ranges from 2% to
7%.

<id>
0709.0427v1
<category>
stat.AP
<abstract>
Storm surge, the onshore rush of sea water caused by the high winds and low
pressure associated with a hurricane, can compound the effects of inland
flooding caused by rainfall, leading to loss of property and loss of life for
residents of coastal areas. Numerical ocean models are essential for creating
storm surge forecasts for coastal areas. These models are driven primarily by
the surface wind forcings. Currently, the gridded wind fields used by ocean
models are specified by deterministic formulas that are based on the central
pressure and location of the storm center. While these equations incorporate
important physical knowledge about the structure of hurricane surface wind
fields, they cannot always capture the asymmetric and dynamic nature of a
hurricane. A new Bayesian multivariate spatial statistical modeling framework
is introduced combining data with physical knowledge about the wind fields to
improve the estimation of the wind vectors. Many spatial models assume the data
follow a Gaussian distribution. However, this may be overly-restrictive for
wind fields data which often display erratic behavior, such as sudden changes
in time or space. In this paper we develop a semiparametric multivariate
spatial model for these data. Our model builds on the stick-breaking prior,
which is frequently used in Bayesian modeling to capture uncertainty in the
parametric form of an outcome. The stick-breaking prior is extended to the
spatial setting by assigning each location a different, unknown distribution,
and smoothing the distributions in space with a series of kernel functions.
This semiparametric spatial model is shown to improve prediction compared to
usual Bayesian Kriging methods for the wind field of Hurricane Ivan.

<id>
0709.1307v1
<category>
stat.AP
<abstract>
We propose a new statistics for the detection of differentially expressed
genes, when the genes are activated only in a subset of the samples. Statistics
designed for this unconventional circumstance has proved to be valuable for
most cancer studies, where oncogenes are activated for a small number of
disease samples. Previous efforts made in this direction include COPA, OS and
ORT. We propose a new statistics called maximum ordered subset t-statistics
(MOST) which seems to be natural when the number of activated samples is
unknown. We compare MOST to other statistics and find the proposed method often
has more power then its competitors.

<id>
0709.1640v1
<category>
stat.AP
<abstract>
This paper introduces a novel paradigm to impute missing data that combines a
decision tree with an auto-associative neural network (AANN) based model and a
principal component analysis-neural network (PCA-NN) based model. For each
model, the decision tree is used to predict search bounds for a genetic
algorithm that minimize an error function derived from the respective model.
The models' ability to impute missing data is tested and compared using HIV
sero-prevalance data. Results indicate an average increase in accuracy of 13%
with the AANN based model's average accuracy increasing from 75.8% to 86.3%
while that of the PCA-NN based model increasing from 66.1% to 81.6%.

<id>
0709.4166v2
<category>
stat.AP
<abstract>
A wealth of epidemiological data suggests an association between
mortality/morbidity from pulmonary and cardiovascular adverse events and air
pollution, but uncertainty remains as to the extent implied by those
associations although the abundance of the data. In this paper we describe an
SSA (Singular Spectrum Analysis) based approach in order to decompose the
time-series of particulate matter concentration into a set of exposure
variables, each one representing a different timescale. We implement our
methodology to investigate both acute and long-term effects of $PM_{10}$
exposure on morbidity from respiratory causes within the urban area of Bari,
Italy.

<id>
0710.0559v1
<category>
stat.AP
<abstract>
The problem addressed in this article is the bias to income and expenditure
elasticities estimated on pseudo-panel data caused by measurement error and
unobserved heterogeneity. We gauge empirically these biases by comparing
cross-sectional, pseudo-panel and true panel data from both Polish and American
expenditure surveys. Our results suggest that unobserved heterogeneity imparts
a downward bias to cross-section estimates of income elasticities of at-home
food expenditures and an upward bias to estimates of income elasticities of
away-from-home food expenditures. "Within" and first-difference estimators
suffer less bias, but only if the effects of measurement error are accounted
for with instrumental variables.

<id>
0706.0787v2
<category>
stat.CO
<abstract>
The problem of the definition and the estimation of generative models based
on deformable templates from raw data is of particular importance for modelling
non aligned data affected by various types of geometrical variability. This is
especially true in shape modelling in the computer vision community or in
probabilistic atlas building for Computational Anatomy (CA). A first coherent
statistical framework modelling the geometrical variability as hidden variables
has been given by Allassonni\`ere, Amit and Trouv\'e (JRSS 2006). Setting the
problem in a Bayesian context they proved the consistency of the MAP estimator
and provided a simple iterative deterministic algorithm with an EM flavour
leading to some reasonable approximations of the MAP estimator under low noise
conditions. In this paper we present a stochastic algorithm for approximating
the MAP estimator in the spirit of the SAEM algorithm. We prove its convergence
to a critical point of the observed likelihood with an illustration on images
of handwritten digits.

<id>
0707.0167v1
<category>
stat.CO
<abstract>
The computation of the Tukey depth, also called halfspace depth, is very
demanding, even in low dimensional spaces, because it requires the
consideration of all possible one-dimensional projections. In this paper we
propose a random depth which approximates the Tukey depth. It only takes into
account a finite number of one-dimensional projections which are chosen at
random. Thus, this random depth requires a very small computation time even in
high dimensional spaces. Moreover, it is easily extended to cover the
functional framework.
  We present some simulations indicating how many projections should be
considered depending on the sample size and on the dimension of the sample
space. We also compare this depth with some others proposed in the literature.
It is noteworthy that the random depth, based on a very low number of
projections, obtains results very similar to those obtained with other depths.

<id>
0708.1051v1
<category>
stat.CO
<abstract>
Given samples (x_1,...,x_m) and (z_1,...,z_n) which we believe are
independent realizations of random variables X and Z respectively, where we
further believe that Z=X+Y with Y independent of X, the problem is to estimate
the distribution of Y. We present a new method for doing this, involving
simulation. Experiments suggest that the method provides useful estimates.

<id>
0709.1721v1
<category>
stat.CO
<abstract>
Monte Carlo sampling methods often suffer from long correlation times.
Consequently, these methods must be run for many steps to generate an
independent sample. In this paper a method is proposed to overcome this
difficulty. The method utilizes information from rapidly equilibrating coarse
Markov chains that sample marginal distributions of the full system. This is
accomplished through exchanges between the full chain and the auxiliary coarse
chains. Results of numerical tests on the bridge sampling and
filtering/smoothing problems for a stochastic differential equation are
presented.

<id>
0709.3560v7
<category>
stat.CO
<abstract>
The super-parametric density estimators and its related algorism were
suggested by Y. -S. Tsai et al [7]. The number of parameters is unlimited in
the super- parametric estimators and it is a general theory in sense of
unifying or connecting nonparametric and parametric estimators. Before applying
to numerical examples, we can not give any comment of the estimators. In this
paper, we will focus on the implementation, the computer programming, of the
algorism and strategies of choosing window functions. B-splines, Bezier splines
and covering windows are studied as well. According to the criterion of the
convergence conditions for Parzen window, the number of the window functions
shall be, roughly, proportional to the number of samples and so is the number
of the variables. Since the algorism is designed for solving the optimization
of likelihood function, there will be a set of nonlinear equations with a large
number of variables. The results show that algorism suggested by Y. -S. Tsai is
very powerful and effective in the sense of mathematics, that is, the iteration
procedures converge and the rates of convergence are very fast. Also, the
numerical results of different window functions show that the approach of
super-parametric density estimators has ushered a new era of statistics.

<id>
0710.4242v4
<category>
stat.CO
<abstract>
In this paper, we propose an adaptive algorithm that iteratively updates both
the weights and component parameters of a mixture importance sampling density
so as to optimise the importance sampling performances, as measured by an
entropy criterion. The method is shown to be applicable to a wide class of
importance sampling densities, which includes in particular mixtures of
multivariate Student t distributions. The performances of the proposed scheme
are studied on both artificial and real examples, highlighting in particular
the benefit of a novel Rao-Blackwellisation device which can be easily
incorporated in the updating scheme.

<id>
0710.5098v1
<category>
stat.CO
<abstract>
We consider multiscale stochastic systems that are partially observed at
discrete points of the slow time scale. We introduce a particle filter that
takes advantage of the multiscale structure of the system to efficiently
approximate the optimal filter.

<id>
0710.5670v2
<category>
stat.CO
<abstract>
Generating multivariate Poisson data is essential in many applications.
Current simulation methods suffer from limitations ranging from computational
complexity to restrictions on the structure of the correlation matrix. We
propose a computationally efficient and conceptually appealing method for
generating multivariate Poisson data. The method is based on simulating
multivariate Normal data and converting them to achieve a specific correlation
matrix and Poisson rate vector. This allows for generating data that have
positive or negative correlations as well as different rates.

<id>
0711.0186v1
<category>
stat.CO
<abstract>
In this paper we present an extension of population-based Markov chain Monte
Carlo (MCMC) to the trans-dimensional case. One of the main challenges in
MCMC-based inference is that of simulating from high and trans-dimensional
target measures. In such cases, MCMC methods may not adequately traverse the
support of the target; the simulation results will be unreliable. We develop
population methods to deal with such problems, and give a result proving the
uniform ergodicity of these population algorithms, under mild assumptions. This
result is used to demonstrate the superiority, in terms of convergence rate, of
a population transition kernel over a reversible jump sampler for a Bayesian
variable selection problem. We also give an example of a population algorithm
for a Bayesian multivariate mixture model with an unknown number of components.
This is applied to gene expression data of 1000 data points in six dimensions
and it is demonstrated that our algorithm out performs some competing Markov
chain samplers.

<id>
0801.3552v3
<category>
stat.CO
<abstract>
This paper considers the problem of cardinality estimation in data stream
applications. We present a statistical analysis of probabilistic counting
algorithms, focusing on two techniques that use pseudo-random variates to form
low-dimensional data sketches. We apply conventional statistical methods to
compare probabilistic algorithms based on storing either selected order
statistics, or random projections. We derive estimators of the cardinality in
both cases, and show that the maximal-term estimator is recursively computable
and has exponentially decreasing error bounds. Furthermore, we show that the
estimators have comparable asymptotic efficiency, and explain this result by
demonstrating an unexpected connection between the two approaches.

<id>
0801.3559v1
<category>
stat.CO
<abstract>
In recent years, large high-dimensional data sets have become commonplace in
a wide range of applications in science and commerce. Techniques for dimension
reduction are of primary concern in statistical analysis. Projection methods
play an important role. We investigate the use of projection algorithms that
exploit properties of the alpha-stable distributions. We show that l_{alpha}
distances and quasi-distances can be recovered from random projections with
full statistical efficiency by L-estimation. The computational requirements of
our algorithm are modest; after a once-and-for-all calculation to determine an
array of length k, the algorithm runs in O(k) time for each distance, where k
is the reduced dimension of the projection.

<id>
0802.3690v1
<category>
stat.CO
<abstract>
Population Monte Carlo has been introduced as a sequential importance
sampling technique to overcome poor fit of the importance function. In this
paper, we compare the performances of the original Population Monte Carlo
algorithm with a modified version that eliminates the influence of the
transition particle via a double Rao-Blackwellisation. This modification is
shown to improve the exploration of the modes through an large simulation
experiment on posterior distributions of mean mixtures of distributions.

<id>
0803.2394v1
<category>
stat.CO
<abstract>
The EM procedure is a principal tool for parameter estimation in the hidden
Markov models. However, applications replace EM by Viterbi extraction, or
training (VT). VT is computationally less intensive, more stable and has more
of an intuitive appeal, but VT estimation is biased and does not satisfy the
following fixed point property. Hypothetically, given an infinitely large
sample and initialized to the true parameters, VT will generally move away from
the initial values. We propose adjusted Viterbi training (VA), a new method to
restore the fixed point property and thus alleviate the overall imprecision of
the VT estimators, while preserving the computational advantages of the
baseline VT algorithm. Simulations elsewhere have shown that VA appreciably
improves the precision of estimation in both the special case of mixture models
and more general HMMs. However, being entirely analytic, the VA correction
relies on infinite Viterbi alignments and associated limiting probability
distributions. While explicit in the mixture case, the existence of these
limiting measures is not obvious for more general HMMs. This paper proves that
under certain mild conditions, the required limiting distributions for general
HMMs do exist.

<id>
0804.0390v1
<category>
stat.CO
<abstract>
In the manuscript, we present a practical way to find the matching priors
proposed by Welch & Peers (1963) and Peers (1965). We investigate the use of
saddlepoint approximations combined with matching priors and obtain p-values of
the test of an interest parameter in the presence of nuisance parameter. The
advantage of our procedure is the flexibility of choosing different initial
conditions so that one can adjust the performance of the test. Two examples
have been studied, with coverage verified via Monte Carlo simulation. One
relates to the ratio of two exponential means, and the other relates the
logistic regression model. Particularly, we are interested in small sample
settings.

<id>
0805.1971v2
<category>
stat.CO
<abstract>
Consider the observation of n iid realizations of an experiment with d>1
possible outcomes, which corresponds to a single observation of a multinomial
distribution M(n,p) where p is an unknown discrete distribution on {1,...,d}.
In many applications, the construction of a confidence region for p when n is
small is crucial. This concrete challenging problem has a long history. It is
well known that the confidence regions built from asymptotic statistics do not
have good coverage when n is small. On the other hand, most available methods
providing non-asymptotic regions with controlled coverage are limited to the
binomial case d=2. In the present work, we propose a new method valid for any
d>1. This method provides confidence regions with controlled coverage and small
volume, and consists of the inversion of the "covering collection"' associated
with level-sets of the likelihood. The behavior when d/n tends to infinity
remains an interesting open problem beyond the scope of this work.

<id>
0805.2256v9
<category>
stat.CO
<abstract>
Sequential techniques can enhance the efficiency of the approximate Bayesian
computation algorithm, as in Sisson et al.'s (2007) partial rejection control
version. While this method is based upon the theoretical works of Del Moral et
al. (2006), the application to approximate Bayesian computation results in a
bias in the approximation to the posterior. An alternative version based on
genuine importance sampling arguments bypasses this difficulty, in connection
with the population Monte Carlo method of Cappe et al. (2004), and it includes
an automatic scaling of the forward kernel. When applied to a population
genetics example, it compares favourably with two other versions of the
approximate algorithm.

<id>
0805.3079v1
<category>
stat.CO
<abstract>
This note describes the results of some tests of the ABC-PRC algorithm of
Sissons et al. (PNAS, 2007), and demonstrates with a toy example that the
method does not converge on the true posterior distribution.

<id>
0805.3602v2
<category>
stat.CO
<abstract>
Inference in Bayesian statistics involves the evaluation of marginal
likelihood integrals. We present algebraic algorithms for computing such
integrals exactly for discrete data of small sample size. Our methods apply to
both uniform priors and Dirichlet priors. The underlying statistical models are
mixtures of independent distributions, or, in geometric language, secant
varieties of Segre-Veronese varieties.

<id>
0807.0725v2
<category>
stat.CO
<abstract>
Case-deleted analysis is a popular method for evaluating the influence of a
subset of cases on inference. The use of Monte Carlo estimation strategies in
complicated Bayesian settings leads naturally to the use of importance sampling
techniques to assess the divergence between full-data and case-deleted
posteriors and to provide estimates under the case-deleted posteriors. However,
the dependability of the importance sampling estimators depends critically on
the variability of the case-deleted weights. We provide theoretical results
concerning the assessment of the dependability of case-deleted importance
sampling estimators in several Bayesian models. In particular, these results
allow us to establish whether or not the estimators satisfy a central limit
theorem. Because the conditions we derive are of a simple analytical nature,
the assessment of the dependability of the estimators can be verified routinely
before estimation is performed. We illustrate the use of the results in several
examples.

<id>
0809.2274v4
<category>
stat.CO
<abstract>
Principal component analysis (PCA) requires the computation of a low-rank
approximation to a matrix containing the data being analyzed. In many
applications of PCA, the best possible accuracy of any rank-deficient
approximation is at most a few digits (measured in the spectral norm, relative
to the spectral norm of the matrix being approximated). In such circumstances,
efficient algorithms have not come with guarantees of good accuracy, unless one
or both dimensions of the matrix being approximated are small. We describe an
efficient algorithm for the low-rank approximation of matrices that produces
accuracy very close to the best possible, for matrices of arbitrary sizes. We
illustrate our theoretical results via several numerical examples.

<id>
0809.4047v1
<category>
stat.CO
<abstract>
This paper presents an improved result on the negative-binomial Monte Carlo
technique analyzed in a previous paper for the estimation of an unknown
probability p. Specifically, the confidence level associated to a relative
interval [p/\mu_2, p\mu_1], with \mu_1, \mu_2 > 1, is proved to exceed its
asymptotic value for a broader range of intervals than that given in the
referred paper, and for any value of p. This extends the applicability of the
estimator, relaxing the conditions that guarantee a given confidence level.

<id>
0809.4654v1
<category>
stat.CO
<abstract>
In areas such as kernel smoothing and non-parametric regression there is
emphasis on smooth interpolation and smooth statistical models. Splines are
known to have optimal smoothness properties in one and higher dimensions. It is
shown, with special attention to polynomial models, that smooth interpolators
can be constructed by first extending the monomial basis and then minimising a
measure of smoothness with respect to the free parameters in the extended
basis. Algebraic methods are a help in choosing the extended basis which can
also be found as a saturated basis for an extended experimental design with
dummy design points. One can get arbitrarily close to optimal smoothing for any
dimension and over any region, giving a simple alternative models of spline
type. The relationship to splines is shown in one and two dimensions. A case
study is given which includes benchmarking against kriging methods.

<id>
0810.1163v1
<category>
stat.CO
<abstract>
We present a sequential Monte Carlo sampler algorithm for the Bayesian
analysis of generalised linear mixed models (GLMMs). These models support a
variety of interesting regression-type analyses, but performing inference is
often extremely difficult, even when using the Bayesian approach combined with
Markov chain Monte Carlo (MCMC). The Sequential Monte Carlo sampler (SMC) is a
new and general method for producing samples from posterior distributions. In
this article we demonstrate use of the SMC method for performing inference for
GLMMs. We demonstrate the effectiveness of the method on both simulated and
real data, and find that sequential Monte Carlo is a competitive alternative to
the available MCMC techniques.

<id>
0811.2843v1
<category>
stat.CO
<abstract>
A descent algorithm, "Quasi-Quadratic Minimization with Memory" (QQMM), is
proposed for unconstrained minimization of the sum, $F$, of a non-negative
convex function, $V$, and a quadratic form. Such problems come up in
regularized estimation in machine learning and statistics. In addition to
values of $F$, QQMM requires the (sub)gradient of $V$. Two features of QQMM
help keep low the number of evaluations of the objective function it needs.
First, QQMM provides good control over stopping the iterative search. This
feature makes QQMM well adapted to statistical problems because in such
problems the objective function is based on random data and therefore stopping
early is sensible. Secondly, QQMM uses a complex method for determining trial
minimizers of $F$. After a description of the problem and algorithm a
simulation study comparing QQMM to the popular BFGS optimization algorithm is
described. The simulation study and other experiments suggest that QQMM is
generally substantially faster than BFGS in the problem domain for which it was
designed. A QQMM-BFGS hybrid is also generally substantially faster than BFGS
but does better than QQMM when QQMM is very slow.

<id>
0811.4095v3
<category>
stat.CO
<abstract>
Recently developed adaptive Markov chain Monte Carlo (MCMC) methods have been
applied successfully to many problems in Bayesian statistics. Grapham is a new
open source implementation covering several such methods, with emphasis on
graphical models for directed acyclic graphs. The implemented algorithms
include the seminal Adaptive Metropolis algorithm adjusting the proposal
covariance according to the history of the chain and a Metropolis algorithm
adjusting the proposal scale based on the observed acceptance probability.
Different variants of the algorithms allow one, for example, to use these two
algorithms together, employ delayed rejection and adjust several parameters of
the algorithms. The implemented Metropolis-within-Gibbs update allows arbitrary
sampling blocks. The software is written in C and uses a simple extension
language Lua in configuration.

<id>
0901.0876v1
<category>
stat.CO
<abstract>
The presence of groups containing high leverage outliers makes linear
regression a difficult problem due to the masking effect. The available high
breakdown estimators based on Least Trimmed Squares often do not succeed in
detecting masked high leverage outliers in finite samples.
  An alternative to the LTS estimator, called Penalised Trimmed Squares (PTS)
estimator, was introduced by the contributors in \cite{ZiouAv:05,ZiAvPi:07} and it
appears to be less sensitive to the masking problem. This estimator is defined
by a Quadratic Mixed Integer Programming (QMIP) problem, where in the objective
function a penalty cost for each observation is included which serves as an
upper bound on the residual error for any feasible regression line. Since the
PTS does not require presetting the number of outliers to delete from the data
set, it has better efficiency with respect to other estimators. However, due to
the high computational complexity of the resulting QMIP problem, exact
solutions for moderately large regression problems is infeasible.
  In this paper we further establish the theoretical properties of the PTS
estimator, such as high breakdown and efficiency, and propose an approximate
algorithm called Fast-PTS to compute the PTS estimator for large data sets
efficiently. Extensive computational experiments on sets of benchmark instances
with varying degrees of outlier contamination, indicate that the proposed
algorithm performs well in identifying groups of high leverage outliers in
reasonable computational time.

<id>
0902.0442v1
<category>
stat.CO
<abstract>
One of the most popular class of tests for independence between two random
variables is the general class of rank statistics which are invariant under
permutations. This class contains Spearman's coefficient of rank correlation
statistic, Fisher-Yates statistic, weighted Mann statistic and others. Under
the null hypothesis of independence these test statistics have a permutation
distribution that usually the normal asymptotic theory used to approximate the
p-values for these tests. In this note we suggest using a saddlepoint approach
that almost exact and need no extensive simulation calculations to calculate
the p-value of such class of tests.

<id>
0902.4117v1
<category>
stat.CO
<abstract>
This note presents a simple and elegant sampler which could be used as an
alternative to the reversible jump MCMC methodology.

<id>
0903.5292v2
<category>
stat.CO
<abstract>
Recent advances in adaptive Markov chain Monte Carlo (AMCMC) include the need
for regional adaptation in situations when the optimal transition kernel is
different across different regions of the sample space. Motivated by these
findings, we propose a mixture-based approach to determine the partition needed
for regional AMCMC. The mixture model is fitted using an online EM algorithm
(see Andrieu and Moulines, 2006) which allows us to bypass simultaneously the
heavy computational load and to implement the regional adaptive algorithm with
online recursion (RAPTOR). The method is tried on simulated as well as real
data examples.

<id>
0904.0779v1
<category>
stat.CO
<abstract>
The approximate joint diagonalization (AJD) is an important analytic tool at
the base of numerous independent component analysis (ICA) and other blind
source separation (BSS) methods, thus finding more and more applications in
medical imaging analysis. In this work we present a new AJD algorithm named
SDIAG (Spheric Diagonalization). It imposes no constraint either on the input
matrices or on the joint diagonalizer to be estimated, thus it is very general.
Whereas it is well grounded on the classical leastsquares criterion, a new
normalization reveals a very simple form of the solution matrix. Numerical
simulations shown that the algorithm, named SDIAG (spheric diagonalization),
behaves well as compared to state-of-the art AJD algorithms.

<id>
0705.2363v1
<category>
stat.ML
<abstract>
We consider the problem of binary classification where one can, for a
particular cost, choose not to classify an observation. We present a simple
proof for the oracle inequality for the excess risk of structural risk
minimizers using a lasso type penalty.

<id>
0706.3499v1
<category>
stat.ML
<abstract>
The distance metric plays an important role in nearest neighbor (NN)
classification. Usually the Euclidean distance metric is assumed or a
Mahalanobis distance metric is optimized to improve the NN performance. In this
paper, we study the problem of embedding arbitrary metric spaces into a
Euclidean space with the goal to improve the accuracy of the NN classifier. We
propose a solution by appealing to the framework of regularization in a
reproducing kernel Hilbert space and prove a representer-like theorem for NN
classification. The embedding function is then determined by solving a
semidefinite program which has an interesting connection to the soft-margin
linear binary support vector machine classifier. Although the main focus of
this paper is to present a general, theoretical framework for metric embedding
in a NN setting, we demonstrate the performance of the proposed method on some
benchmark datasets and show that it performs better than the Mahalanobis metric
learning algorithm in terms of leave-one-out and generalization errors.

<id>
0707.3536v1
<category>
stat.ML
<abstract>
Dendrograms used in data analysis are ultrametric spaces, hence objects of
nonarchimedean geometry. It is known that there exist $p$-adic representation
of dendrograms. Completed by a point at infinity, they can be viewed as
subtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.
The implications are that certain moduli spaces known in algebraic geometry are
$p$-adic parameter spaces of (families of) dendrograms, and stochastic
classification can also be handled within this framework. At the end, we
calculate the topology of the hidden part of a dendrogram.

<id>
0707.4072v1
<category>
stat.ML
<abstract>
A conceptual framework for cluster analysis from the viewpoint of p-adic
geometry is introduced by describing the space of all dendrograms for n
datapoints and relating it to the moduli space of p-adic Riemannian spheres
with punctures using a method recently applied by Murtagh (2004b). This method
embeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the
p-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.
After explaining the definitions, the concept of classifiers is discussed in
the context of moduli spaces, and upper bounds for the number of hidden
vertices in dendrograms are given.

<id>
0708.2377v1
<category>
stat.ML
<abstract>
We present and analyse three online algorithms for learning in discrete
Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.
Using the Kullback-Leibler divergence as a measure of generalisation error we
draw learning curves in simplified situations. The performance for learning
drifting concepts of one of the presented algorithms is analysed and compared
with the Baldi-Chauvin algorithm in the same situations. A brief discussion
about learning and symmetry breaking based on our results is also presented.

<id>
0709.2760v3
<category>
stat.ML
<abstract>
In recent years, kernel density estimation has been exploited by computer
scientists to model machine learning problems. The kernel density estimation
based approaches are of interest due to the low time complexity of either O(n)
or O(n*log(n)) for constructing a classifier, where n is the number of sampling
instances. Concerning design of kernel density estimators, one essential issue
is how fast the pointwise mean square error (MSE) and/or the integrated mean
square error (IMSE) diminish as the number of sampling instances increases. In
this article, it is shown that with the proposed kernel function it is feasible
to make the pointwise MSE of the density estimator converge at O(n^-2/3)
regardless of the dimension of the vector space, provided that the probability
density function at the point of interest meets certain conditions.

<id>
0709.2936v1
<category>
stat.ML
<abstract>
This thesis responds to the challenges of using a large number, such as
thousands, of features in regression and classification problems.
  There are two situations where such high dimensional features arise. One is
when high dimensional measurements are available, for example, gene expression
data produced by microarray techniques. For computational or other reasons,
people may select only a small subset of features when modelling such data, by
looking at how relevant the features are to predicting the response, based on
some measure such as correlation with the response in the training data.
Although it is used very commonly, this procedure will make the response appear
more predictable than it actually is. In Chapter 2, we propose a Bayesian
method to avoid this selection bias, with application to naive Bayes models and
mixture models.
  High dimensional features also arise when we consider high-order
interactions. The number of parameters will increase exponentially with the
order considered. In Chapter 3, we propose a method for compressing a group of
parameters into a single one, by exploiting the fact that many predictor
variables derived from high-order interactions have the same values for all the
training cases. The number of compressed parameters may have converged before
considering the highest possible order. We apply this compression method to
logistic sequence prediction models and logistic classification models.
  We use both simulated data and real data to test our methods in both
chapters.

<id>
0709.2989v1
<category>
stat.ML
<abstract>
Simulated annealing is a popular method for approaching the solution of a
global optimization problem. Existing results on its performance apply to
discrete combinatorial optimization where the optimization variables can assume
only a finite set of possible values. We introduce a new general formulation of
simulated annealing which allows one to guarantee finite-time performance in
the optimization of functions of continuous variables. The results hold
universally for any optimization problem on a bounded domain and establish a
connection between simulated annealing and up-to-date theory of convergence of
Markov chain Monte Carlo methods on continuous domains. This work is inspired
by the concept of finite-time learning with known accuracy and confidence
developed in statistical learning theory.

<id>
0710.0845v3
<category>
stat.ML
<abstract>
We present the nested Chinese restaurant process (nCRP), a stochastic process
which assigns probability distributions to infinitely-deep,
infinitely-branching trees. We show how this stochastic process can be used as
a prior distribution in a Bayesian nonparametric model of document collections.
Specifically, we present an application to information retrieval in which
documents are modeled as paths down a random tree, and the preferential
attachment dynamics of the nCRP leads to clustering of documents according to
sharing of topics at multiple levels of abstraction. Given a corpus of
documents, a posterior inference algorithm finds an approximation to a
posterior distribution over trees, topics and allocations of words to levels of
the tree. We demonstrate this algorithm on collections of scientific abstracts
from several journals. This model exemplifies a recent trend in statistical
machine learning--the use of Bayesian nonparametric methods to infer
distributions on flexible data structures.

<id>
0710.3183v1
<category>
stat.ML
<abstract>
We provide self-contained proof of a theorem relating probabilistic coherence
of forecasts to their non-domination by rival forecasts with respect to any
proper scoring rule. The theorem appears to be new but is closely related to
results achieved by other investigators.

<id>
0710.3742v1
<category>
stat.ML
<abstract>
Changepoints are abrupt variations in the generative parameters of a data
sequence. Online detection of changepoints is useful in modelling and
prediction of time series in application areas such as finance, biometrics, and
robotics. While frequentist methods have yielded online filtering and
prediction techniques, most Bayesian papers have focused on the retrospective
segmentation problem. Here we examine the case where the model parameters
before and after the changepoint are independent and we derive an online
algorithm for exact inference of the most recent changepoint. We compute the
probability distribution of the length of the current ``run,'' or time since
the last changepoint, using a simple message-passing algorithm. Our
implementation is highly modular so that the algorithm may be applied to a
variety of types of data. We illustrate this modularity by demonstrating the
algorithm on three different real-world data sets.

<id>
0711.2434v1
<category>
stat.ML
<abstract>
We characterize and study variable importance (VIMP) and pairwise variable
associations in binary regression trees. A key component involves the node mean
squared error for a quantity we refer to as a maximal subtree. The theory
naturally extends from single trees to ensembles of trees and applies to
methods like random forests. This is useful because while importance values
from random forests are used to screen variables, for example they are used to
filter high throughput genomic data in Bioinformatics, very little theory
exists about their properties.

<id>
0712.0248v1
<category>
stat.ML
<abstract>
This monograph deals with adaptive supervised classification, using tools
borrowed from statistical mechanics and information theory, stemming from the
PACBayesian approach pioneered by David McAllester and applied to a conception
of statistical learning theory forged by Vladimir Vapnik. Using convex analysis
on the set of posterior probability measures, we show how to get local measures
of the complexity of the classification model involving the relative entropy of
posterior distributions with respect to Gibbs posterior measures. We then
discuss relative bounds, comparing the generalization error of two
classification rules, showing how the margin assumption of Mammen and Tsybakov
can be replaced with some empirical measure of the covariance structure of the
classification model.We show how to associate to any posterior distribution an
effective temperature relating it to the Gibbs prior distribution with the same
level of expected error rate, and how to estimate this effective temperature
from data, resulting in an estimator whose expected error rate converges
according to the best possible power of the sample size adaptively under any
margin and parametric complexity assumptions. We describe and study an
alternative selection scheme based on relative bounds between estimators, and
present a two step localization technique which can handle the selection of a
parametric model from a family of those. We show how to extend systematically
all the results obtained in the inductive setting to transductive learning, and
use this to improve Vapnik's generalization bounds, extending them to the case
when the sample is made of independent non-identically distributed pairs of
patterns and labels. Finally we review briefly the construction of Support
Vector Machines and show how to derive generalization bounds for them,
measuring the complexity either through the number of support vectors or
through the value of the transductive or inductive margin.

<id>
0802.2906v2
<category>
stat.ML
<abstract>
Dimensionality reduction is a topic of recent interest. In this paper, we
present the classification constrained dimensionality reduction (CCDR)
algorithm to account for label information. The algorithm can account for
multiple classes as well as the semi-supervised setting. We present an
out-of-sample expressions for both labeled and unlabeled data. For unlabeled
data, we introduce a method of embedding a new point as preprocessing to a
classifier. For labeled data, we introduce a method that improves the embedding
during the training phase using the out-of-sample extension. We investigate
classification performance using the CCDR algorithm on hyper-spectral satellite
imagery data. We demonstrate the performance gain for both local and global
classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors
algorithm performance. We present a connection between intrinsic dimension
estimation and the optimal embedding dimension obtained using the CCDR
algorithm.

<id>
0803.1628v1
<category>
stat.ML
<abstract>
Being among the easiest ways to find meaningful structure from discrete data,
Latent Dirichlet Allocation (LDA) and related component models have been
applied widely. They are simple, computationally fast and scalable,
interpretable, and admit nonparametric priors. In the currently popular field
of network modeling, relatively little work has taken uncertainty of data
seriously in the Bayesian sense, and component models have been introduced to
the field only recently, by treating each node as a bag of out-going links. We
introduce an alternative, interaction component model for communities (ICMc),
where the whole network is a bag of links, stemming from different components.
The former finds both disassortative and assortative structure, while the
alternative assumes assortativity and finds community-like structures like the
earlier methods motivated by physics. With Dirichlet Process priors and an
efficient implementation the models are highly scalable, as demonstrated with a
social network from the Last.fm web site, with 670,000 nodes and 1.89 million
links.

<id>
0804.1026v1
<category>
stat.ML
<abstract>
We propose to investigate test statistics for testing homogeneity in
reproducing kernel Hilbert spaces. Asymptotic null distributions under null
hypothesis are derived, and consistency against fixed and local alternatives is
assessed. Finally, experimental evidence of the performance of the proposed
approach on both artificial data and a speaker verification task is provided.

<id>
0804.1325v1
<category>
stat.ML
<abstract>
When using the K-nearest neighbors method, one often ignores uncertainty in
the choice of K. To account for such uncertainty, Holmes and Adams (2002)
proposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN
(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain
Monte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams
(2002) focused on the performance of BKNN in terms of misclassification error
but did not assess its ability to quantify uncertainty. We present some
evidence to show that BKNN still significantly underestimates model
uncertainty.

<id>
0804.2848v1
<category>
stat.ML
<abstract>
Flow cytometry is often used to characterize the malignant cells in leukemia
and lymphoma patients, traced to the level of the individual cell. Typically,
flow cytometric data analysis is performed through a series of 2-dimensional
projections onto the axes of the data set. Through the years, clinicians have
determined combinations of different fluorescent markers which generate
relatively known expression patterns for specific subtypes of leukemia and
lymphoma -- cancers of the hematopoietic system. By only viewing a series of
2-dimensional projections, the high-dimensional nature of the data is rarely
exploited. In this paper we present a means of determining a low-dimensional
projection which maintains the high-dimensional relationships (i.e.
information) between differing oncological data sets. By using machine learning
techniques, we allow clinicians to visualize data in a low dimension defined by
a linear combination of all of the available markers, rather than just 2 at a
time. This provides an aid in diagnosing similar forms of cancer, as well as a
means for variable selection in exploratory flow cytometric research. We refer
to our method as Information Preserving Component Analysis (IPCA).

<id>
0805.1390v1
<category>
stat.ML
<abstract>
A simple and computationally efficient scheme for tree-structured vector
quantization is presented. Unlike previous methods, its quantization error
depends only on the intrinsic dimension of the data distribution, rather than
the apparent dimension of the space in which the data happen to lie.

<id>
0806.2646v1
<category>
stat.ML
<abstract>
We analyze the performance of a class of manifold-learning algorithms that
find their output by minimizing a quadratic form under some normalization
constraints. This class consists of Locally Linear Embedding (LLE), Laplacian
Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and
Diffusion maps. We present and prove conditions on the manifold that are
necessary for the success of the algorithms. Both the finite sample case and
the limit case are analyzed. We show that there are simple manifolds in which
the necessary conditions are violated, and hence the algorithms cannot recover
the underlying manifolds. Finally, we present numerical results that
demonstrate our claims.

<id>
0806.2669v1
<category>
stat.ML
<abstract>
We present the Procrustes measure, a novel measure based on Procrustes
rotation that enables quantitative comparison of the output of manifold-based
embedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum
et al, 2000)). The measure also serves as a natural tool when choosing
dimension-reduction parameters. We also present two novel dimension-reduction
techniques that attempt to minimize the suggested measure, and compare the
results of these techniques to the results of existing algorithms. Finally, we
suggest a simple iterative method that can be used to improve the output of
existing algorithms.

<id>
0806.2831v1
<category>
stat.ML
<abstract>
The problem of supervised classification (or discrimination) with functional
data is considered, with a special interest on the popular k-nearest neighbors
(k-NN) classifier. First, relying on a recent result by Cerou and Guyader
(2006), we prove the consistency of the k-NN classifier for functional data
whose distribution belongs to a broad family of Gaussian processes with
triangular covariance functions. Second, on a more practical side, we check the
behavior of the k-NN method when compared with a few other functional
classifiers. This is carried out through a small simulation study and the
analysis of several real functional data sets. While no global "uniform" winner
emerges from such comparisons, the overall performance of the k-NN method,
together with its sound intuitive motivation and relative simplicity, suggests
that it could represent a reasonable benchmark for the classification problem
with functional data.

<id>
0806.4115v4
<category>
stat.ML
<abstract>
We propose a new sparsity-smoothness penalty for high-dimensional generalized
additive models. The combination of sparsity and smoothness is crucial for
mathematical theory as well as performance for finite-sample data. We present a
computationally efficient algorithm, with provable numerical convergence
properties, for optimizing the penalized likelihood. Furthermore, we provide
oracle results which yield asymptotic optimality of our estimator for high
dimensional but sparse additive models. Finally, an adaptive version of our
sparsity-smoothness penalized approach yields large additional performance
gains.

<id>
0808.0780v1
<category>
stat.ML
<abstract>
The local linear embedding algorithm (LLE) is a non-linear dimension-reducing
technique, widely used due to its computational simplicity and intuitive
approach. LLE first linearly reconstructs each input point from its nearest
neighbors and then preserves these neighborhood relations in the
low-dimensional embedding. We show that the reconstruction weights computed by
LLE capture the high-dimensional structure of the neighborhoods, and not the
low-dimensional manifold structure. Consequently, the weight vectors are highly
sensitive to noise. Moreover, this causes LLE to converge to a linear
projection of the input, as opposed to its non-linear embedding goal. To
overcome both of these problems, we propose to compute the weight vectors using
a low-dimensional neighborhood representation. We prove theoretically that this
straightforward and computationally simple modification of LLE reduces LLE's
sensitivity to noise. This modification also removes the need for
regularization when the number of neighbors is larger than the dimension of the
input. We present numerical examples demonstrating both the perturbation and
linear projection problems, and the improved outputs using the low-dimensional
neighborhood representation.

<id>
0808.2241v1
<category>
stat.ML
<abstract>
We construct a framework for studying clustering algorithms, which includes
two key ideas: persistence and functoriality. The first encodes the idea that
the output of a clustering scheme should carry a multiresolution structure, the
second the idea that one should be able to compare the results of clustering
algorithms as one varies the data set, for example by adding points or by
applying functions to it. We show that within this framework, one can prove a
theorem analogous to one of J. Kleinberg, in which one obtains an existence and
uniqueness theorem instead of a non-existence result. We explore further
properties of this unique scheme, stability and convergence are established.

<id>
0808.2337v1
<category>
stat.ML
<abstract>
We consider principal component analysis (PCA) in decomposable Gaussian
graphical models. We exploit the prior information in these models in order to
distribute its computation. For this purpose, we reformulate the problem in the
sparse inverse covariance (concentration) domain and solve the global
eigenvalue problem using a sequence of local eigenvalue problems in each of the
cliques of the decomposable graph. We demonstrate the application of our
methodology in the context of decentralized anomaly detection in the Abilene
backbone network. Based on the topology of the network, we propose an
approximate statistical graphical model and distribute the computation of PCA.

<id>
0810.0901v2
<category>
stat.ML
<abstract>
Many problems of low-level computer vision and image processing, such as
denoising, deconvolution, tomographic reconstruction or super-resolution, can
be addressed by maximizing the posterior distribution of a sparse linear model
(SLM). We show how higher-order Bayesian decision-making problems, such as
optimizing image acquisition in magnetic resonance scanners, can be addressed
by querying the SLM posterior covariance, unrelated to the density's mode. We
propose a scalable algorithmic framework, with which SLM posteriors over full,
high-resolution images can be approximated for the first time, solving a
variational optimization problem which is convex iff posterior mode finding is
convex. These methods successfully drive the optimization of sampling
trajectories for real-world magnetic resonance imaging through Bayesian
experimental design, which has not been attempted before. Our methodology
provides new insight into similarities and differences between sparse
reconstruction and approximate Bayesian inference, and has important
implications for compressive sensing of real-world images.

<id>
0810.4553v1
<category>
stat.ML
<abstract>
We present a new online boosting algorithm for adapting the weights of a
boosted classifier, which yields a closer approximation to Freund and
Schapire's AdaBoost algorithm than previous online boosting algorithms. We also
contribute a new way of deriving the online algorithm that ties together
previous online boosting work. We assume that the weak hypotheses were selected
beforehand, and only their weights are updated during online boosting. The
update rule is derived by minimizing AdaBoost's loss when viewed in an
incremental form. The equations show that optimization is computationally
expensive. However, a fast online approximation is possible. We compare
approximation error to batch AdaBoost on synthetic datasets and generalization
error on face datasets and the MNIST dataset.

<id>
0810.5117v1
<category>
stat.ML
<abstract>
In this report, we derive a non-negative series expansion for the
Jensen-Shannon divergence (JSD) between two probability distributions. This
series expansion is shown to be useful for numerical calculations of the JSD,
when the probability distributions are nearly equal, and for which,
consequently, small numerical errors dominate evaluation.

<id>
0811.1239v1
<category>
stat.ML
<abstract>
We consider the problem of jointly estimating the parameters as well as the
structure of binary valued Markov Random Fields, in contrast to earlier work
that focus on one of the two problems. We formulate the problem as a
maximization of $\ell_1$-regularized surrogate likelihood that allows us to
find a sparse solution. Our optimization technique efficiently incorporates the
cutting-plane algorithm in order to obtain a tighter outer bound on the
marginal polytope, which results in improvement of both parameter estimates and
approximation to marginals. On synthetic data, we compare our algorithm on the
two estimation tasks to the other existing methods. We analyze the method in
the high-dimensional setting, where the number of dimensions $p$ is allowed to
grow with the number of observations $n$. The rate of convergence of the
estimate is demonstrated to depend explicitly on the sparsity of the underlying
graph.

<id>
0705.0700v3
<category>
stat.ME
<abstract>
This paper considers the issue of modeling fractional data observed in the
interval [0,1), (0,1] or [0,1]. Mixed continuous-discrete distributions are
proposed. The beta distribution is used to describe the continuous component of
the model since its density can have quite diferent shapes depending on the
values of the two parameters that index the distribution. Properties of the
proposed distributions are examined. Also, maximum likelihood and method of
moments estimation is discussed. Finally, practical applications that employ
real data are presented.

<id>
0705.2938v1
<category>
stat.ME
<abstract>
Using predictive adaptive arithmetic coding and the Minimum Description
Length principle, we derive an efficient tool for model selection problems :
the RIC information criterion. We then present an extension of these coding
techniques to non-parametrical estimation of a distribution and illustrate it
on the gray scales histogram of an image.
  Key-words : Information criteria, MDL, model selection, non-parametrical
estimation, histograms.

<id>
0705.4588v1
<category>
stat.ME
<abstract>
We propose the variable selection procedure incorporating prior constraint
information into lasso. The proposed procedure combines the sample and prior
information, and selects significant variables for responses in a narrower
region where the true parameters lie. It increases the efficiency to choose the
true model correctly. The proposed procedure can be executed by many
constrained quadratic programming methods and the initial estimator can be
found by least square or Monte Carlo method. The proposed procedure also enjoys
good theoretical properties. Moreover, the proposed procedure is not only used
for linear models but also can be used for generalized linear models({\sl
GLM}), Cox models, quantile regression models and many others with the help of
Wang and Leng (2007)'s LSA, which changes these models as the approximation of
linear models. The idea of combining sample and prior constraint information
can be also used for other modified lasso procedures. Some examples are used
for illustration of the idea of incorporating prior constraint information in
variable selection procedures.

<id>
0706.1287v1
<category>
stat.ME
<abstract>
A Bayesian approach is used to estimate the covariance matrix of Gaussian
data. Ideas from Gaussian graphical models and model selection are used to
construct a prior for the covariance matrix that is a mixture over all
decomposable graphs. For this prior the probability of each graph size is
specified by the user and graphs of equal size are assigned equal probability.
Most previous approaches assume that all graphs are equally probable. We show
empirically that the prior that assigns equal probability over graph sizes
outperforms the prior that assigns equal probability over all graphs, both in
identifying the correct decomposable graph and in more efficiently estimating
the covariance matrix.

<id>
0706.1408v1
<category>
stat.ME
<abstract>
We provide sensitivity comparisons for two competing versions of the
dimension reduction method principal Hessian directions (pHd). These
comparisons consider the effects of small perturbations on the estimation of
the dimension reduction subspace via the influence function. We show that the
two versions of pHd can behave completely differently in the presence of
certain observational types. Our results also provide evidence that outliers in
the traditional sense may or may not be highly influential in practice. Since
influential observations may lurk within otherwise typical data, we consider
the influence function in the empirical setting for the efficient detection of
influential observations in practice.

<id>
0706.2912v1
<category>
stat.ME
<abstract>
Suppose several two-valued input-output systems are designed by setting the
levels of several controllable factors. For this situation, Taguchi method has
proposed to assign the controllable factors to the orthogonal array and use
ANOVA model for the standardized SN ratio, which is a natural measure for
evaluating the performance of each input-output system. Though this procedure
is simple and useful in application indeed, the result can be unreliable when
the estimated standard errors of the standardized SN ratios are unbalanced. In
this paper, we treat the data arising from the full factorial or fractional
factorial designs of several controllable factors as the frequencies of
high-dimensional contingency tables, and propose a general testing procedure
for the main effects or the interaction effects of the controllable factors.

<id>
0706.3435v1
<category>
stat.ME
<abstract>
We present a novel solution technique for the blind subspace deconvolution
(BSSD) problem, where temporal convolution of multidimensional hidden
independent components is observed and the task is to uncover the hidden
components using the observation only. We carry out this task for the
undercomplete case (uBSSD): we reduce the original uBSSD task via linear
prediction to independent subspace analysis (ISA), which we can solve. As it
has been shown recently, applying temporal concatenation can also reduce uBSSD
to ISA, but the associated ISA problem can easily become `high dimensional'
[1]. The new reduction method circumvents this dimensionality problem. We
perform detailed studies on the efficiency of the proposed technique by means
of numerical simulations. We have found several advantages: our method can
achieve high quality estimations for smaller number of samples and it can cope
with deeper temporal convolutions.

<id>
0706.4190v1
<category>
stat.ME
<abstract>
Smoothing methods and SiZer are a useful statistical tool for discovering
statistically significant structure in data. Based on scale space ideas
originally developed in the computer vision literature, SiZer (SIgnificant ZERo
crossing of the derivatives) is a graphical device to assess which observed
features are `really there' and which are just spurious sampling artifacts. In
this paper, we develop SiZer like ideas in time series analysis to address the
important issue of significance of trends. This is not a straightforward
extension, since one data set does not contain the information needed to
distinguish `trend' from `dependence'. A new visualization is proposed, which
shows the statistician the range of trade-offs that are available. Simulation
and real data results illustrate the effectiveness of the method.

<id>
0707.0143v1
<category>
stat.ME
<abstract>
This is an expos\'e on the use of O'Sullivan penalised splines in
contemporary semiparametric regression, including mixed model and Bayesian
formulations. O'Sullivan penalised splines are similar to P-splines, but have
an advantage of being a direct generalisation of smoothing splines. Exact
expressions for the O'Sullivan penalty matrix are obtained. Comparisons between
the two reveals that O'Sullivan penalised splines more closely mimic the
natural boundary behaviour of smoothing splines. Implementation in modern
computing environments such as Matlab, R and BUGS is discussed.

<id>
0707.0246v1
<category>
stat.ME
<abstract>
We present in this paper a new tool for outliers detection in the context of
multiple regression models. This graphical tool is based on recursive
estimation of the parameters. Simulations were carried out to illustrate the
performance of this graphical procedure. As a conclusion, this tool is applied
to real data containing outliers according to the classical available tools.

<id>
0707.0481v3
<category>
stat.ME
<abstract>
In many modern applications, including analysis of gene expression and text
documents, the data are noisy, high-dimensional, and unordered--with no
particular meaning to the given order of the variables. Yet, successful
learning is often possible due to sparsity: the fact that the data are
typically redundant with underlying structures that can be represented by only
a few features. In this paper we present treelets--a novel construction of
multi-scale bases that extends wavelets to nonsmooth signals. The method is
fully adaptive, as it returns a hierarchical tree and an orthonormal basis
which both reflect the internal structure of the data. Treelets are especially
well-suited as a dimensionality reduction and feature selection tool prior to
regression and classification, in situations where sample sizes are small and
the data are sparse with unknown groupings of correlated or collinear
variables. The method is also simple to implement and analyze theoretically.
Here we describe a variety of situations where treelets perform better than
principal component analysis, as well as some common variable selection and
cluster averaging schemes. We illustrate treelets on a blocked covariance model
and on several data sets (hyperspectral image data, DNA microarray data, and
internet advertisements) with highly complex dependencies between variables.

<id>
0707.2158v1
<category>
stat.ME
<abstract>
We express the mean and variance terms in a double exponential regression
model as additive functions of the predictors and use Bayesian variable
selection to determine which predictors enter the model, and whether they enter
linearly or flexibly. When the variance term is null we obtain a generalized
additive model, which becomes a generalized linear model if the predictors
enter the mean linearly. The model is estimated using Markov chain Monte Carlo
simulation and the methodology is illustrated using real and simulated data
sets.

<id>
0707.2257v1
<category>
stat.ME
<abstract>
A class of semi-parametric hazard/failure rates with a bathtub shape is of
interest. It does not only provide a great deal of flexibility over existing
parametric methods in the modeling aspect but also results in a closed and
tractable Bayes estimator for the bathtub-shaped failure rate (BFR). Such an
estimator is derived to be a finite sum over two $\mathbf{S}$-paths due to an
explicit posterior analysis in terms of two (conditionally independent)
$\mathbf{S}$-paths. These, newly discovered, explicit results can be proved to
be a Rao-Blackwellization of counterpart results in terms of partitions that
are readily available by a specialization of James (2005)'s work. We develop
both iterative and non-iterative computational procedures based on existing
efficient Monte Carlo methods for sampling one single $\mathbf{S}$-path.
Nmerical simulations are given to demonstrate the practicality and the
effectiveness of our methodology. Last but not least, two applications of the
proposed method are discussed, of which one is about a Bayesian test for
failure rates and the other is related to modeling with covariates.

<id>
0708.0165v1
<category>
stat.ME
<abstract>
In this paper, we introduce a family of robust estimates for the parametric
and nonparametric components under a generalized partially linear model, where
the data are modeled by $y_i|(\mathbf{x}_i,t_i)\sim F(\cdot,\mu_i)$ with
$\mu_i=H(\eta(t_i)+\mathbf{x}_i^{$\mathrm{T}$}\beta)$, for some known
distribution function F and link function H. It is shown that the estimates of
$\beta$ are root-n consistent and asymptotically normal. Through a Monte Carlo
study, the performance of these estimators is compared with that of the
classical ones.

<id>
0708.0279v1
<category>
stat.ME
<abstract>
This paper reviews the role of expert judgement to support reliability
assessments within the systems engineering design process. Generic design
processes are described to give the context and a discussion is given about the
nature of the reliability assessments required in the different systems
engineering phases. It is argued that, as far as meeting reliability
requirements is concerned, the whole design process is more akin to a
statistical control process than to a straightforward statistical problem of
assessing an unknown distribution. This leads to features of the expert
judgement problem in the design context which are substantially different from
those seen, for example, in risk assessment. In particular, the role of experts
in problem structuring and in developing failure mitigation options is much
more prominent, and there is a need to take into account the reliability
potential for future mitigation measures downstream in the system life cycle.
An overview is given of the stakeholders typically involved in large scale
systems engineering design projects, and this is used to argue the need for
methods that expose potential judgemental biases in order to generate analyses
that can be said to provide rational consensus about uncertainties. Finally, a
number of key points are developed with the aim of moving toward a framework
that provides a holistic method for tracking reliability assessment through the
design process.

<id>
0708.0285v1
<category>
stat.ME
<abstract>
Comment: Expert Elicitation for Reliable System Design [arXiv:0708.0279]

<id>
0708.0287v1
<category>
stat.ME
<abstract>
Comment: Expert Elicitation for Reliable System Design [arXiv:0708.0279]

<id>
0708.0288v1
<category>
stat.ME
<abstract>
Comment: Expert Elicitation for Reliable System Design [arXiv:0708.0279]

<id>
0708.0293v1
<category>
stat.ME
<abstract>
Rejoinder: Expert Elicitation for Reliable System Design [arXiv:0708.0279]

<id>
0708.0295v1
<category>
stat.ME
<abstract>
This special volume of Statistical Sciences presents some innovative, if not
provocative, ideas in the area of reliability, or perhaps more appropriately
named, integrated system assessment. In this age of exponential growth in
science, engineering and technology, the capability to evaluate the
performance, reliability and safety of complex systems presents new challenges.
Today's methodology must respond to the ever-increasing demands for such
evaluations to provide key information for decision and policy makers at all
levels of government and industry--problems ranging from international security
to space exploration. We, the co-editors of this volume and the contributors,
believe that scientific progress in reliability assessment requires the
development of processes, methods and tools that combine diverse information
types (e.g., experiments, computer simulations, expert knowledge) from diverse
sources (e.g., scientists, engineers, business developers, technology
integrators, decision makers) to assess quantitative performance metrics that
can aid decision making under uncertainty. These are highly interdisciplinary
problems. The principal role of statistical sciences is to bring statistical
rigor, thinking and methodology to these problems.

<id>
0708.0302v1
<category>
stat.ME
<abstract>
Networked applications have software components that reside on different
computers. Email, for example, has database, processing, and user interface
components that can be distributed across a network and shared by users in
different locations or work groups. End-to-end performance and reliability
metrics describe the software quality experienced by these groups of users,
taking into account all the software components in the pipeline. Each user
produces only some of the data needed to understand the quality of the
application for the group, so group performance metrics are obtained by
combining summary statistics that each end computer periodically (and
automatically) sends to a central server. The group quality metrics usually
focus on medians and tail quantiles rather than on averages. Distributed
quantile estimation is challenging, though, especially when passing large
amounts of data around the network solely to compute quality metrics is
undesirable. This paper describes an Incremental Quantile (IQ) estimation
method that is designed for performance monitoring at arbitrary levels of
network aggregation and time resolution when only a limited amount of data can
be transferred. Applications to both real and simulated data are provided.

<id>
0708.0317v1
<category>
stat.ME
<abstract>
Comment: Monitoring Networked Applications With Incremental Quantile
Estimation [arXiv:0708.0302]

<id>
0708.0336v1
<category>
stat.ME
<abstract>
Our comments are in two parts. First, we make some observations regarding the
methodology in Chambers et al. [arXiv:0708.0302]. Second, we briefly describe
another interesting network monitoring problem that arises in the context of
assessing quality of service, such as loss rates and delay distributions, in
packet-switched networks.

<id>
0708.0338v1
<category>
stat.ME
<abstract>
Comment: Monitoring Networked Applications With Incremental Quantile
Estimation [arXiv:0708.0302]

<id>
0708.0339v1
<category>
stat.ME
<abstract>
Rejoinder: Monitoring Networked Applications With Incremental Quantile
Estimation [arXiv:0708.0302]

<id>
0708.0343v1
<category>
stat.ME
<abstract>
This review article provides an overview of recent work in the modeling and
analysis of recurrent events arising in engineering, reliability, public
health, biomedicine and other areas. Recurrent event modeling possesses unique
facets making it different and more difficult to handle than single event
settings. For instance, the impact of an increasing number of event occurrences
needs to be taken into account, the effects of covariates should be considered,
potential association among the interevent times within a unit cannot be
ignored, and the effects of performed interventions after each event occurrence
need to be factored in. A recent general class of models for recurrent events
which simultaneously accommodates these aspects is described. Statistical
inference methods for this class of models are presented and illustrated
through applications to real data sets. Some existing open research problems
are described.

<id>
0708.0346v1
<category>
stat.ME
<abstract>
Many researchers have investigated first hitting times as models for survival
data. First hitting times arise naturally in many types of stochastic
processes, ranging from Wiener processes to Markov chains. In a survival
context, the state of the underlying process represents the strength of an item
or the health of an individual. The item fails or the individual experiences a
clinical endpoint when the process reaches an adverse threshold state for the
first time. The time scale can be calendar time or some other operational
measure of degradation or disease progression. In many applications, the
process is latent (i.e., unobservable). Threshold regression refers to
first-hitting-time models with regression structures that accommodate covariate
data. The parameters of the process, threshold state and time scale may depend
on the covariates. This paper reviews aspects of this topic and discusses
fruitful avenues for future research.

<id>
0708.0355v1
<category>
stat.ME
<abstract>
The systems that statisticians are asked to assess, such as nuclear weapons,
infrastructure networks, supercomputer codes and munitions, have become
increasingly complex. It is often costly to conduct full system tests. As such,
we present a review of methodology that has been proposed for addressing system
reliability with limited full system testing. The first approaches presented in
this paper are concerned with the combination of multiple sources of
information to assess the reliability of a single component. The second general
set of methodology addresses the combination of multiple levels of data to
determine system reliability. We then present developments for complex systems
beyond traditional series/parallel representations through the use of Bayesian
networks and flowgraph models. We also include methodological contributions to
resource allocation considerations for system relability assessment. We
illustrate each method with applications primarily encountered at Los Alamos
National Laboratory.

<id>
0708.0362v1
<category>
stat.ME
<abstract>
We review basic modeling approaches for failure and maintenance data from
repairable systems. In particular we consider imperfect repair models, defined
in terms of virtual age processes, and the trend-renewal process which extends
the nonhomogeneous Poisson process and the renewal process. In the case where
several systems of the same kind are observed, we show how observed covariates
and unobserved heterogeneity can be included in the models. We also consider
various approaches to trend testing. Modern reliability data bases usually
contain information on the type of failure, the type of maintenance and so
forth in addition to the failure times themselves. Basing our work on recent
literature we present a framework where the observed events are modeled as
marked point processes, with marks labeling the types of events. Throughout the
paper the emphasis is more on modeling than on statistical inference.

<id>
0708.0369v1
<category>
stat.ME
<abstract>
Engineers in the manufacturing industries have used accelerated test (AT)
experiments for many decades. The purpose of AT experiments is to acquire
reliability information quickly. Test units of a material, component, subsystem
or entire systems are subjected to higher-than-usual levels of one or more
accelerating variables such as temperature or stress. Then the AT results are
used to predict life of the units at use conditions. The extrapolation is
typically justified (correctly or incorrectly) on the basis of physically
motivated models or a combination of empirical model fitting with a sufficient
amount of previous experience in testing similar units. The need to extrapolate
in both time and the accelerating variables generally necessitates the use of
fully parametric models. Statisticians have made important contributions in the
development of appropriate stochastic models for AT data [typically a
distribution for the response and regression relationships between the
parameters of this distribution and the accelerating variable(s)], statistical
methods for AT planning (choice of accelerating variable levels and allocation
of available test units to those levels) and methods of estimation of suitable
reliability metrics. This paper provides a review of many of the AT models that
have been used successfully in this area.

<id>
math/0107135v2
<category>
math.ST
<abstract>
We consider two kinds of stochastic volatility models. Both kinds of models
contain a stationary volatility process, the density of which, at a fixed
instant in time, we aim to estimate.
  We discuss discrete time models where for instance a log price process is
modeled as the product of a volatility process and i.i.d. noise. We also
consider samples of certain continuous time diffusion processes. The sampled
time instants will be be equidistant with vanishing distance.
  A Fourier type deconvolution kernel density estimator based on the logarithm
of the squared processes is proposed to estimate the volatility density.
Expansions of the bias and bounds on the variances are derived.

<id>
math/0109002v1
<category>
math.ST
<abstract>
We show that that the jackknife variance estimator $v_{jack}$ and the the
infinitesimal jackknife variance estimator are asymptotically equivalent if the
functional of interest is a smooth function of the mean or a smooth trimmed
L-statistic. We calculate the asymptotic variance of $v_{jack}$ for these
functionals.

<id>
math/0111152v1
<category>
math.ST
<abstract>
In this paper an iterated function system on the space of distribution
functions is built. The inverse problem is introduced and studied by convex
optimization problems. Some applications of this method to approximation of
distribution functions and to estimation theory are given.

<id>
math/0111153v1
<category>
math.ST
<abstract>
A subthreshold signal is transmitted through a channel and may be detected
when some noise -- with known structure and proportional to some level -- is
added to the data. There is an optimal noise level, called stochastic
resonance, that corresponds to the highest Fisher information in the problem of
estimation of the signal. As noise we consider an ergodic diffusion process and
the asymptotic is considered as time goes to infinity. We propose consistent
estimators of the subthreshold signal and we solve further a problem of
hypotheses testing. We also discuss evidence of stochastic resonance for both
estimation and hypotheses testing problems via examples.

<id>
math/0112032v1
<category>
math.ST
<abstract>
We derive asymptotic normality of kernel type deconvolution estimators of the
density, the distribution function at a fixed point, and of the probability of
an interval. We consider the so called super smooth case where the
characteristic function of the known distribution decreases exponentially.
  It turns out that the limit behavior of the pointwise estimators of the
density and distribution function is relatively straightforward while the
asymptotics of the estimator of the probability of an interval depends in a
complicated way on the sequence of bandwidths.

<id>
math/0112298v1
<category>
math.ST
<abstract>
In the article we consider accumulated values of annuities-certain with
yearly payments with independent random interest rates. We focus on annuities
with payments varying in arithmetic and geometric progression which are
important basic varying annuities (see Kellison, 1991). They appear to be a
generalization of the types studied recently by Zaks (2001). We derive, via
recursive relationships, mean and variance formulae of the final values of the
annuities. As a consequence, we obtain moments related to the already discussed
cases, which leads to a correction of main results from Zaks (2001).

<id>
math/0202274v1
<category>
math.ST
<abstract>
This paper is speculated to propose a class of shrinkage estimators for shape
parameter beta in failure censored samples from two-parameter Weibull
distribution when some 'apriori' or guessed interval containing the parameter
beta is available in addition to sample information and analyses their
properties. Some estimators are generated from the proposed class and compared
with the minimum mean squared error (MMSE) estimator. Numerical computations in
terms of percent relative efficiency and absolute relative bias indicate that
certain of these estimators substantially improve the MMSE estimator in some
guessed interval of the parameter space of beta, especially for censored
samples with small sizes. Subsequently, a modified class of shrinkage
estimators is proposed with its properties.

<id>
math/0203080v2
<category>
math.ST
<abstract>
By the method of Poissonization we confirm some existing results concerning
consistent estimation of the structural distribution function in the situation
of a large number of rare events. Inconsistency of the so called natural
estimator is proved. The method of grouping in cells of equal size is
investigated and its consistency derived. A bound on the mean squared error is
derived.

<id>
math/0206006v2
<category>
math.ST
<abstract>
We give a visually appealing counterexample to the proposition that unbiased
estimators are better than biased estimators.

<id>
math/0206142v1
<category>
math.ST
<abstract>
We consider discrete time models for asset prices with a stationary
volatility process. We aim at estimating the multivariate density of this
process at a set of consecutive time instants. A Fourier type deconvolution
kernel density estimator based on the logarithm of the squared process is
proposed to estimate the volatility density. Expansions of the bias and bounds
on the variance are derived.

<id>
math/0207044v1
<category>
math.ST
<abstract>
We construct an on-line estimator with equidistant design for tracking a
smooth function from Stone-Ibragimov-Khasminskii class. This estimator has the
optimal convergence rate of risk to zero in sample size. The procedure for
setting coefficients of the estimator is controlled by a single parameter and
has a simple numerical solution. The off-line version of this estimator allows
to eliminate a boundary layer. Simulation results are given.

<id>
math/0210425v1
<category>
math.ST
<abstract>
We consider estimation of the structural distribution function of the cell
probabilities of a multinomial sample in situations where the number of cells
is large. We review the performance of the natural estimator, an estimator
based on grouping the cells and a kernel type estimator. Inconsistency of the
natural estimator and weak consistency of the other two estimators is derived
by Poissonization and other, new, technical devices.

<id>
math/0211079v3
<category>
math.ST
<abstract>
We construct a density estimator and an estimator of the distribution
function in the uniform deconvolution model. The estimators are based on
inversion formulas and kernel estimators of the density of the observations and
its derivative. Asymptotic normality and the asymptotic biases are derived.

<id>
math/0212007v2
<category>
math.ST
<abstract>
We derive asymptotic normality of kernel type deconvolution density
estimators. In particular we consider deconvolution problems where the known
component of the convolution has a symmetric lambda-stable distribution,
0<lambda<= 2. It turns out that the limit behavior changes if the exponent
parameter lambda passes the value one, the case of Cauchy deconvolution.

<id>
math/0212350v1
<category>
math.ST
<abstract>
In this paper we will discuss a procedure to improve the usual estimator of a
linear functional of the unknown regression function in inverse nonparametric
regression models. In Klaassen, Lee, and Ruymgaart (2001) it has been proved
that this traditional estimator is not asymptotically efficient (in the sense
of the H\'{a}jek - Le Cam convolution theorem) except, possibly, when the error
distribution is normal. Since this estimator, however, is still root-n
consistent a procedure in Bickel, Klaassen, Ritov, and Wellner (1993) applies
to construct a modification which is asymptotically efficient. A self-contained
proof of the asymptotic efficiency is included.

<id>
math/0212395v1
<category>
math.ST
<abstract>
Classical multiscale analysis based on wavelets has a number of successful
applications, e.g. in data compression, fast algorithms, and noise removal.
Wavelets, however, are adapted to point singularities, and many phenomena in
several variables exhibit intermediate-dimensional singularities, such as
edges, filaments, and sheets. This suggests that in higher dimensions, wavelets
ought to be replaced in certain applications by multiscale analysis adapted to
intermediate-dimensional singularities.
  My lecture described various initial attempts in this direction. In
particular, I discussed two approaches to geometric multiscale analysis
originally arising in the work of Harmonic Analysts Hart Smith and Peter Jones
(and others): (a) a directional wavelet transform based on parabolic dilations;
and (b) analysis via anistropic strips. Perhaps surprisingly, these tools have
potential applications in data compression, inverse problems, noise removal,
and signal detection; applied mathematicians, statisticians, and engineers are
eagerly pursuing these leads.

<id>
math/0212410v1
<category>
math.ST
<abstract>
State space models have long played an important role in signal processing.
The Gaussian case can be treated algorithmically using the famous Kalman
filter. Similarly since the 1970s there has been extensive application of
Hidden Markov models in speech recognition with prediction being the most
important goal. The basic theoretical work here, in the case $X$ and $Y$ finite
(small) providing both algorithms and asymptotic analysis for inference is that
of Baum and colleagues. During the last 30-40 years these general models have
proved of great value in applications ranging from genomics to finance.
  Unless the $X,Y$ are jointly Gaussian or $X$ is finite and small the problem
of calculating the distributions discussed and the likelihood exactly are
numerically intractable and if $Y$ is not finite asymptotic analysis becomes
much more difficult. Some new developments have been the construction of
so-called ``particle filters'' (Monte Carlo type) methods for approximate
calculation of these distributions (see Doucet et al. [4]) for instance and
general asymptotic methods for analysis of statistical methods in HMM [2] and
other contributors.
  We will discuss these methods and results in the light of exponential mixing
properties of the conditional (posterior) distribution of $(X_1,X_2,...)$ given
$(Y_1,Y_2,...)$ already noted by Baum and Petrie and recent work of the contributors
Bickel, Ritov and Ryden, Del Moral and Jacod, Douc and Matias.

<id>
math/0212411v1
<category>
math.ST
<abstract>
A classical limit theorem of stochastic process theory concerns the sample
cumulative distribution function (CDF) from independent random variables. If
the variables are uniformly distributed then these centered CDFs converge in a
suitable sense to the sample paths of a Brownian Bridge. The so-called
Hungarian construction of Komlos, Major and Tusnady provides a strong form of
this result. In this construction the CDFs and the Brownian Bridge sample paths
are coupled through an appropriate representation of each on the same
measurable space, and the convergence is uniform at a suitable rate.
  Within the last decade several asymptotic statistical-equivalence theorems
for nonparametric problems have been proven, beginning with Brown and Low
(1996) and Nussbaum (1996). The approach here to statistical-equivalence is
firmly rooted within the asymptotic statistical theory created by L. Le Cam but
in some respects goes beyond earlier results.
  This talk demonstrates the analogy between these results and those from the
coupling method for proving stochastic process limit theorems. These two
classes of theorems possess a strong inter-relationship, and technical methods
from each domain can profitably be employed in the other. Results in a recent
paper by Carter, Low, Zhang and myself will be described from this perspective.

<id>
math/0301363v1
<category>
math.ST
<abstract>
The jackknife variance estimator and the the infinitesimal jackknife variance
estimator are shown to be asymptotically equivalent if the functional of
interest is a smooth function of the mean or a trimmed L-statistic with Hoelder
continuous weight function.

<id>
math/0302079v1
<category>
math.ST
<abstract>
Log-linear models are a well-established method for describing statistical
dependencies among a set of n random variables. The observed frequencies of the
n-tuples are explained by a joint probability such that its logarithm is a sum
of functions, where each function depends on as few variables as possible. We
obtain for this class a new model selection criterion using nonasymptotic
concepts of statistical learning theory. We calculate the VC dimension for the
class of k-factor log-linear models. In this way we are not only able to select
the model with the appropriate complexity, but obtain also statements on the
reliability of the estimated probability distribution. Furthermore we show that
the selection of the best model among a set of models with the same complexity
can be written as a convex optimization problem.

<id>
math/0305234v1
<category>
math.ST
<abstract>
Consider estimation of the regression parameter in the accelerated failure
time model, when data are obtained by cross sectional sampling. It is shown
that it is possible under regularity of the model to construct an efficient
estimator of the unknown Euclidean regression parameter if the distribution of
the covariate vector is known and also if it is unknown with vanishing mean.

<id>
math/0305273v2
<category>
math.ST
<abstract>
This paper introduces a family of recursively defined estimators of the
parameters of a diffusion process. We use ideas of stochastic algorithms for
the construction of the estimators. Asymptotic consistency of these estimators
and asymptotic normality of an appropriate normalization are proved. The
results are applied to two examples from the financial literature; viz.,
Cox-Ingersoll-Ross' model and the constant elasticity of variance (CEV) process
illustrate the use of the technique proposed herein.

<id>
math/0306237v1
<category>
math.ST
<abstract>
Let $X$ and $Y$ be two independent identically distributed random variables
with density $p(x)$ and $Z=\alpha X+\beta Y$ for some constants $\alpha>0$ and
$\beta>0$. We consider the problem of estimating $p(x)$ by means of the samples
from the distribution of $Z$. Non-parametric estimator based on the sync kernel
is constructed and asymptotic behaviour of the corresponding mean integrated
square error is investigated.

<id>
math/0309355v1
<category>
math.ST
<abstract>
Let X be a n*p matrix and l_1 the largest eigenvalue of the covariance matrix
X^{*}*X. The "null case" where X_{i,j} are independent Normal(0,1) is of
particular interest for principal component analysis. For this model, when n, p
tend to infinity and n/p tends to gamma in (0,\infty), it was shown in
Johnstone (2001) that l_1, properly centered and scaled, converges to the
Tracy-Widom law. We show that with the same centering and scaling, the result
is true even when p/n or n/p tends to infinity. The derivation uses ideas and
techniques quite similar to the ones presented in Johnstone (2001). Following
Soshnikov (2002), we also show that the same is true for the joint distribution
of the k largest eigenvalues, where k is a fixed integer. Numerical experiments
illustrate the fact that the Tracy-Widom approximation is reasonable even when
one of the dimension is "small".

<id>
math/0310006v3
<category>
math.ST
<abstract>
The marginalization paradox involves a disagreement between two Bayesians who
use two different procedures for calculating a posterior in the presence of an
improper prior. We show that the argument used to justify the procedure of one
of the Bayesians is inapplicable. There is therefore no reason to expect
agreement, no paradox, and no evidence that improper priors are inherently
inconsistent. We show further that the procedure in question can be interpreted
as the cancellation of infinities in the formal posterior. We suggest that the
implicit use of this formal procedure is the source of the observed
disagreement.

<id>
math/0312056v1
<category>
math.ST
<abstract>
The subject of robust estimation in time series is widely discussed in
literature. One of the approaches is to use GM-estimation. This method
incorporates a broad class of nonparametric estimators which under suitable
conditions includes estimators robust to outliers in data. For the linear
models the sensitivity of GM-estimators to outliers have been studied in the
work by Martin and Yohai [5], and influence functionals for this estimator were
derived. In this paper we follow this direction and examine the asymptotical
properties of the class of M-estimators, which is narrower than the class of
GM-estimators, but gives more insight into asymptotical properties of such
estimators. This paper gives an asymptotic expansion of the residual weighted
empirical process, which allows to prove asymptotic normality of these
estimators in case of non-smooth objective functions. For simplicity MA(1)
model is considered, but it will be shown that even in this case mathematical
techniques used to derive these asymptotic properties appear to be rather
complicated.However, the approach used in this paper could be applied to
GM-estimators and to more realistic models.

<id>
math/0403373v1
<category>
math.ST
<abstract>
Grade of membership (GoM) analysis was introduced in 1974 as a means of
analyzing multivariate categorical data. Since then, it has been successfully
applied to many problems. The primary goal of GoM analysis is to derive
properties of individuals based on results of multivariate measurements; such
properties are given in the form of the expectations of a hidden random
variable (state of an individual) conditional on the result of observations.
  In this article, we present a new perspective for the GoM model, based on
considering distribution laws of observed random variables as realizations of
another random variable. It happens that some moments of this new random
variable are directly estimable from observations. Our approach allows us to
establish a number of important relations between estimable moments and values
of interest, which, in turn, provides a basis for a new numerical procedure.

<id>
math/0405511v1
<category>
math.ST
<abstract>
Vertex direction algorithms have been around for a few decades in the
experimental design and mixture models literature. We briefly review this type
of algorithm and describe a new member of the family: the support reduction
algorithm. The support reduction algorithm is applied to the problem of
computing nonparametric estimates in two inverse problems: convex density
estimation and the Gaussian deconvolution problem. Usually, VD algorithms solve
a finite dimensional (version of the) optimization problem of interest. We
introduce a method to solve the true infinite dimensional optimization problem.

<id>
math/0406424v1
<category>
math.ST
<abstract>
We describe here a framework for a certain class of multiscale likelihood
factorizations wherein, in analogy to a wavelet decomposition of an L^2
function, a given likelihood function has an alternative representation as a
product of conditional densities reflecting information in both the data and
the parameter vector localized in position and scale. The framework is
developed as a set of sufficient conditions for the existence of such
factorizations, formulated in analogy to those underlying a standard
multiresolution analysis for wavelets, and hence can be viewed as a
multiresolution analysis for likelihoods. We then consider the use of these
factorizations in the task of nonparametric, complexity penalized likelihood
estimation. We study the risk properties of certain thresholding and
partitioning estimators, and demonstrate their adaptivity and near-optimality,
in a minimax sense over a broad range of function spaces, based on squared
Hellinger distance as a loss function. In particular, our results provide an
illustration of how properties of classical wavelet-based estimators can be
obtained in a single, unified framework that includes models for continuous,
count and categorical data types.

<id>
math/0406425v1
<category>
math.ST
<abstract>
Starting from the observation of an R^n-Gaussian vector of mean f and
covariance matrix \sigma^2 I_n (I_n is the identity matrix), we propose a
method for building a Euclidean confidence ball around f, with prescribed
probability of coverage. For each n, we describe its nonasymptotic property and
show its optimality with respect to some criteria.

<id>
q-bio/0309007v1
<category>
q-bio.BM
<abstract>
We consider the regime in which the bands of the torsional acoustic (TA) and
the hydrogen-bond-stretch (HBS) modes of the DNA interpenetrate each other.
Within the framework of a model that accommodates the structure of the double
helix, we find the three-wave interaction between the TA- and the HBS-modes,
and show that microwave radiation could bring about torsional vibrations that
could serve as a pump mode for maintaining the HBS-one. Rayleigh's threshold
condition for the parametric resonance provides an estimate for the power
density of the mw-field necessary for generating the HBS-mode.

<id>
q-bio/0309017v1
<category>
q-bio.BM
<abstract>
Identifying the driving forces and the mechanism of association of
huntingtin-exon1, a close marker for the progress of Huntington's disease, is
an important prerequisite towards finding potential drug targets, and
ultimately a cure. We introduce here a modelling framework based on a key
analogy of the physico-chemical properties of the exon1 fragment to block
copolymers. We use a systematic mesoscale methodology, based on Dissipative
Particle Dynamics, which is capable of overcoming kinetic barriers, thus
capturing the dynamics of significantly larger systems over longer times than
considered before. Our results reveal that the relative hydrophobicity of the
poly-glutamine block as compared to the rest of the (proline-based) exon1
fragment, ignored to date, constitutes a major factor in the initiation of the
self-assembly process. We find that the assembly is governed by both the
concentration of exon1 and the length of the poly-glutamine stretch, with a low
length threshold for association even at the lowest volume fractions we
considered. Moreover, this self-association occurs irrespective of whether the
glutamine stretch is in random coil or hairpin configuration, leading to
spherical or cylindrical assemblies, respectively. We discuss the implications
of these results for reinterpretation of existing research within this context,
including that the routes towards aggregation of exon1 may be distinct to those
of the widely studied homopolymeric poly-glutamine peptides.

<id>
q-bio/0310013v1
<category>
q-bio.BM
<abstract>
The molecular mechanism of the solvent motion that is required to instigate
the protein structural relaxation above a critical hydration level or
transition temperature has yet to be determined. In this work we use
quasi-elastic neutron scattering (QENS) and molecular dynamics simulation to
investigate hydration water dynamics near a greatly simplified protein surface.
We consider the hydration water dynamics near the completely deuterated
N-acetyl-leucine-methylamide (NALMA) solute, a hydrophobic amino acid side
chain attached to a polar blocked polypeptide backbone, as a function of
concentration between 0.5M-2.0M, under ambient conditions. In this
Communication, we focus our results of hydration dynamics near a model protein
surface on the issue of how enzymatic activity is restored once a critical
hydration level is reached, and provide a hypothesis for the molecular
mechanism of the solvent motion that is required to trigger protein structural
relaxation when above the hydration transition.

<id>
q-bio/0310020v1
<category>
q-bio.BM
<abstract>
We analyze the dependence of thermal denaturation transition and folding
rates of globular proteins on the number of amino acid residues, N. Using
lattice Go models we show that DeltaT/T_F ~ N^-1, where T_F is the folding
transition temperature and DeltaT is the folding transition width. This finding
is consistent with finite size effects expected for the systems undergoing a
phase transition from a disordered to an ordered phase. The dependence of the
folding rates k_F on N for lattice models and the dataset of 57 proteins and
peptides shows that k_F = k_F^0 exp(-CN^beta) provides a good fit, if 0 < beta
<= 2/3 and C is a constant. We find that k_F = k_F^0 exp(-1.1N^0.5) with k_F^0
=(0.4x10^-6 s)^-1 can estimate optimal protein folding rates to within an order
of magnitude in most cases. By using this fit for a set of proteins with
beta-sheet topology we find that k_F^0 is approximately equal to k_U^0, the
prefactor for unfolding rates. The maximum ratio of k_U^0/k_F^0 is 10 for this
class of proteins.

<id>
q-bio/0310023v1
<category>
q-bio.BM
<abstract>
The asymmetry in the shapes of folded and unfolded states are probed using
two parameters, one being a measure of the sphericity and the other that
describes the shape. For the folded states, whose interiors are densely packed,
the radii of gyration (Rg) and these two parameters are calculated using the
coordinates of the experimentally determined structures. Although Rg scales as
expected for maximally compact structures, the distributions of the shape
parameters show that there is considerable asymmetry in the shapes of folded
structures. The degree of asymmetry is greater for proteins that form
oligomers. Analysis of the two- and three-body contacts in the native
structures shows that the presence of near equal number of contacts between
backbone and side-chains and between side-chains gives rise to dense packing.
We suggest that proteins with relatively large values of shape parameters can
tolerate volume mutations without greatly affecting the network of contacts or
their stability. To probe shape characteristics of denatured states we have
developed a model of a WW-like domain. The shape parameters, which are
calculated using Langevin simulations, change dramatically in the course of
coil to globule transition. Comparison of the values of shape parameters
between the globular state and the folded state of WW domain shows that both
energetic (especially dispersion in the hydrophobic interactions) and steric
effects are important in determining packing in proteins.

<id>
q-bio/0310026v1
<category>
q-bio.BM
<abstract>
Instead of conformation states of single residues, refined conformation
states of quintuplets are proposed to reflect conformation correlation. Simple
hidden Markov models combining with sliding window scores are used for
predicting secondary structure of a protein from its amino acid sequence. Since
the length of protein conformation segments varies in a narrow range, we ignore
the duration effect of the length distribution. The window scores for residues
are a window version of the Chou-Fasman propensities estimated under an
approximation of conditional independency. Different window widths are
examined, and the optimal width is found to be 17. A high accuracy about 70% is
achieved.

<id>
q-bio/0310034v1
<category>
q-bio.BM
<abstract>
Is protein secondary structure primarily determined by local interactions
between residues closely spaced along the amino acid backbone, or by non-local
tertiary interactions? To answer this question we have measured the entropy
densities of primary structure and secondary structure sequences, and the local
inter-sequence mutual information density. We find that the important
inter-sequence interactions are short ranged, that correlations between
neighboring amino acids are essentially uninformative, and that only 1/4 of the
total information needed to determine the secondary structure is available from
local inter-sequence correlations. Since the remaining information must come
from non-local interactions, this observation supports the view that the
majority of most proteins fold via a cooperative process where secondary and
tertiary structure form concurrently. To provide a more direct comparison to
existing secondary structure prediction methods, we construct a simple hidden
Markov model (HMM) of the sequences. This HMM achieves a prediction accuracy
comparable to other single sequence secondary structure prediction algorithms,
and can extract almost all of the inter-sequence mutual information. This
suggests that these algorithms are almost optimal, and that we should not
expect a dramatic improvement in prediction accuracy. However, local
correlations between secondary and primary structure are probably of
under-appreciated importance in many tertiary structure prediction methods,
such as threading.

<id>
q-bio/0311008v1
<category>
q-bio.BM
<abstract>
The determination of the folding mechanisms of proteins is critical to
understand the topological change that can propagate Alzheimer and
Creutzfeld-Jakobs diseases, among others. The computational community has paid
considerable attention to this problem; however, the associated time scale,
typically on the order of milliseconds or more, represents a formidable
challenge. Ab initio protein folding from long molecular dynamics (MD)
simulations or ensemble dynamics is not feasible with ordinary computing
facilities and new techniques must be introduced. Here we present a detailed
study of the folding of a 16-residue beta-hairpin, described by a generic
energy model and using the activation-relaxation technique. From a total of 90
trajectories at 300 K, three folding pathways emerge. All involve a
simultaneous optimization of the complete hydrophobic and hydrogen bonding
interactions. The first two follow closely those observed by previous
theoretical studies. The third pathway, never observed by previous all-atom
folding, unfolding and equilibrium simulations, can be described as a reptation
move of one strand of the beta-sheet with respect to the other. This reptation
move indicates that non-native interactions can play a dominant role in the
folding of secondary structures. These results point to a more complex folding
picture than expected for a simple beta-hairpin.

<id>
q-bio/0311010v1
<category>
q-bio.BM
<abstract>
Analytic estimates for the forces and free energy generated by bilayer
deformation reveal a compelling and intuitive model for MscL channel gating
analogous to the nucleation of a second phase. We argue that the competition
between hydrophobic mismatch and tension results in a surprisingly rich story
which can provide both a quantitative comparison to measurements of opening
tension for MscL when reconstituted in bilayers of different thickness and
qualitative insights into the function of the MscL channel and other
transmembrane proteins.

<id>
q-bio/0311011v1
<category>
q-bio.BM
<abstract>
It is important to understand how protein folding and evolution influences
each other. Several studies based on entropy calculation correlating
experimental measurement of residue participation in folding nucleus and
sequence conservation have reached different conclusions. Here we report
analysis of conservation of folding nucleus using an evolutionary model
alternative to entropy based approaches. We employ a continuous time Markov
model of codon substitution to distinguish mutation fixed by evolution and
mutation fixed by chance. This model takes into account bias in codon
frequency, bias favoring transition over transversion, as well as explicit
phylogenetic information. We measure selection pressure using the ratio
$\omega$ of synonymous vs. non-synonymous substitution at individual residue
site. The $\omega$-values are estimated using the {\sc Paml} method, a
maximum-likelihood estimator. Our results show that there is little correlation
between the extent of kinetic participation in protein folding nucleus as
measured by experimental $\phi$-value and selection pressure as measured by
$\omega$-value. In addition, two randomization tests failed to show that
folding nucleus residues are significantly more conserved than the whole
protein. These results suggest that at the level of codon substitution, there
is no indication that folding nucleus residues are significantly more conserved
than other residues. We further reconstruct candidate ancestral residues of the
folding nucleus and suggest possible test tube mutation studies of ancient
folding nucleus.

<id>
q-bio/0311023v2
<category>
q-bio.BM
<abstract>
Many signalling functions in molecular biology require proteins bind to
substrates such as DNA in response to environmental signals such as the
simultaneous binding to a small molecule. Examples are repressor proteins which
may transmit information via a conformational change in response to the ligand
binding. An alternative entropic mechanism of ``allostery'' suggests that the
inducer ligand changes the intramolecular vibrational entropy not just the
static structure. We present a quantitative, coarse-grained model of entropic
allostery that suggests design rules for internal cohesive potentials in
proteins employing this effect. It also addresses the issue of how the signal
information to bind or unbind is transmitted through the protein. The model may
be applicable to a wide range of repressors and also to signalling in
transmembrane proteins.

<id>
q-bio/0311033v1
<category>
q-bio.BM
<abstract>
How DNA repair enzymes find the relatively rare sites of damage is not known
in great detail. Recent experiments and molecular data suggest that the
individual repair enzymes do not work independently of each other, but rather
interact with each other through currents exchanged along DNA. A damaged site
in DNA hinders this exchange and this makes it possible to quickly free up
resources from error free stretches of DNA. Here the size of the speedup gained
from this current exchange mechanism is calculated and the characteristic
length and time scales are identified. In particular for Escherichia coli we
estimate the speedup to be 50000/N, where N is the number of repair enzymes
participating in the current exchange mechanism. Even though N is not exactly
known a speedup of order 10 is not entirely unreasonable. Furthermore upon over
expression of repair enzymes the detection time only varies as one over the
squareroot of N and not as 1/N. This behavior is of interest in assessing the
impact of stress full and radioactive environments on individual cell mutation
rates.

<id>
q-bio/0311038v1
<category>
q-bio.BM
<abstract>
We study DNA adsorption and renaturation in a water-phenol two-phase system,
with or without shaking. In very dilute solutions, single-stranded DNA is
adsorbed at the interface in a salt-dependent manner. At high salt
concentrations the adsorption is irreversible. The adsorption of the
single-stranded DNA is specific to phenol and relies on stacking and hydrogen
bonding. We establish the interfacial nature of a DNA renaturation at a high
salt concentration. In the absence of shaking, this reaction involves an
efficient surface diffusion of the single-stranded DNA chains. In the presence
of a vigorous shaking, the bimolecular rate of the reaction exceeds the
Smoluchowski limit for a three-dimensional diffusion-controlled reaction. DNA
renaturation in these conditions is known as the Phenol Emulsion Reassociation
Technique or PERT. Our results establish the interfacial nature of PERT. A
comparison of this interfacial reaction with other approaches shows that PERT
is the most efficient technique and reveals similarities between PERT and the
renaturation performed by single-stranded nucleic acid binding proteins. Our
results lead to a better understanding of the partitioning of nucleic acids in
two-phase systems, and should help design improved extraction procedures for
damaged nucleic acids. We present arguments in favor of a role of phenol and
water-phenol interface in prebiotic chemistry. The most efficient renaturation
reactions (in the presence of condensing agents or with PERT) occur in
heterogeneous systems. This reveals the limitations of homogeneous approaches
to the biochemistry of nucleic acids. We propose a heterogeneous approach to
overcome the limitations of the homogeneous viewpoint.

<id>
q-bio/0312010v1
<category>
q-bio.BM
<abstract>
Hydrophobicity is thought to be one of the primary forces driving the folding
of proteins. On average, hydrophobic residues occur preferentially in the core,
whereas polar residues tends to occur at the surface of a folded protein. By
analyzing the known protein structures, we quantify the degree to which the
hydrophobicity sequence of a protein correlates with its pattern of surface
exposure. We have assessed the statistical significance of this correlation for
several hydrophobicity scales in the literature, and find that the computed
correlations are significant but far from optimal. We show that this less than
optimal correlation arises primarily from the large degree of mutations that
naturally occurring proteins can tolerate. Lesser effects are due in part to
forces other than hydrophobicity and we quantify this by analyzing the surface
exposure distributions of all amino acids. Lastly we show that our database
findings are consistent with those found from an off-lattice hydrophobic-polar
model of protein folding.

<id>
q-bio/0312034v1
<category>
q-bio.BM
<abstract>
The approach for the description of the DNA conformational transformations on
the mesoscopic scales in the frame of the double helix is presented. Due to
consideration of the joint motions of DNA structural elements along the
conformational pathways the models for different transformations may be
constructed in the unifying two-component form. One component of the model is
the degree of freedom of the elastic rod and another component -- the effective
coordinate of the conformational transformation. The internal and external
model components are interrelated, as it is characteristic for the DNA
structure organization. It is shown that the kinetic energy of the
conformational transformation of heterogeneous DNA may be put in homogeneous
form. In the frame of the developed approach the static excitations of the DNA
structure under the transitions between the stable states are found for
internal and external components. The comparison of the data obtained with the
experiment on intrinsic DNA deformability shows good qualitative agreement. The
conclusion is made that the found excitations in the DNA structure may be
classificated as the static conformational solitons.

<id>
q-bio/0312035v1
<category>
q-bio.BM
<abstract>
Molecular combing is a powerful and simple method for aligning DNA molecules
onto a surface. Using this technique combined with fluorescence microscopy, we
observed that the length of lambda-DNA molecules was extended to about 1.6
times their contour length (unextended length, 16.2 micrometers) by the combing
method on hydrophobic polymethylmetacrylate (PMMA) coated surfaces. The effects
of sodium and magnesium ions and pH of the DNA solution were investigated.
Interestingly, we observed force-induced melting of single DNA molecules.

<id>
q-bio/0312036v2
<category>
q-bio.BM
<abstract>
Using a Brownian dynamics simulation, we numerically studied the interaction
of DNA with histone and proposed an octamer-rotation model to describe the
process of nucleosome formation. Nucleosome disruption under stretching was
also simulated. The theoretical curves of extension versus time as well as of
force versus extension are consistent with previous experimental results.

<id>
q-bio/0312040v1
<category>
q-bio.BM
<abstract>
We propose a two-dimensional model for a complete description of the dynamics
of molecular motors, including both the processive movement along track
filaments and the dissociation from the filaments. The theoretical results on
the distributions of the run length and dwell time at a given ATP
concentration, the dependences of mean run length, mean dwell time and mean
velocity on ATP concentration and load are in good agreement with the previous
experimental results.

<id>
q-bio/0312043v1
<category>
q-bio.BM
<abstract>
Kinesin motors have been studied extensively both experimentally and
theoretically. However, the microscopic mechanism of the processive movement of
kinesin is still an open question. In this paper, we propose a hand-over-hand
model for the processivity of kinesin, which is based on chemical, mechanical,
and electrical couplings. In the model the processive movement does not need to
rely on the two heads' coordination in their ATP hydrolysis and mechanical
cycles. Rather, the ATP hydrolyses at the two heads are independent. The much
higher ATPase rate at the trailing head than the leading head makes the motor
walk processively in a natural way, with one ATP being hydrolyzed per step. The
model is consistent with the structural study of kinesin and the measured
pathway of the kinesin ATPase. Using the model the estimated driving force of ~
5.8 pN is in agreements with the experimental results (5~7.5 pN). The
prediction of the moving time in one step (~10 microseconds) is also consistent
with the measured values of 0~50 microseconds. The previous observation of
substeps within the 8-nm step is explained. The shapes of velocity-load (both
positive and negative) curves show resemblance to previous experimental
results.

<id>
q-bio/0312044v1
<category>
q-bio.BM
<abstract>
Myosin V and myosin VI are two classes of two-headed molecular motors of the
myosin superfamily that move processively along helical actin filaments in
opposite directions. Here we present a hand-over-hand model for their
processive movements. In the model, the moving direction of a dimeric molecular
motor is automatically determined by the relative orientation between its two
heads at free state and its head's binding orientation on track filament. This
determines that myosin V moves toward the barbed end and myosin VI moves toward
the pointed end of actin. During the moving period in one step, one head
remains bound to actin for myosin V whereas two heads are detached for myosin
VI: The moving manner is determined by the length of neck domain. This
naturally explains the similar dynamic behaviors but opposite moving directions
of myosin VI and mutant myosin V (the neck of which is truncated to only
one-sixth of the native length). Because of different moving manners, myosin VI
and mutant myosin V exhibit significantly broader step-size distribution than
native myosin V. However, all three motors give the same mean step size of 36
nm (the pseudo-repeat of actin helix). Using the model we study the dynamics of
myosin V quantitatively, with theoretical results in agreement with previous
experimental ones.

<id>
q-bio/0401011v1
<category>
q-bio.BM
<abstract>
We describe a faster and more accurate algorithm for computing the
statistical mechanics of DNA denaturation according to the Poland-Scheraga
type. Nearest neighbor thermodynamics is included in a complete and general
way. The algorithm represents an optimization with respect to algorithmic
complexity of the partition function algorithm of Yeramian et al.: We reduce
the computation time for a base-pairing probability profile from O(N2) to O(N).
This speed-up comes in addition to the speed-up due to a multiexponential
approximation of the loop entropy factor as introduced by Fixman and Freire.
The speed-up, however, is independent of the multiexponential approximation and
reduces time from O(N3) to O(N2) in the exact case. In addition to calculating
the standard base-pairing probability profiles, we propose to use the algorithm
to calculate various other probabilities (loops, helices, tails) for a more
direct view of the melting regions and their positions and sizes.

<id>
q-bio/0401012v1
<category>
q-bio.BM
<abstract>
A joint experimental / theoretical investigation of the elastin-like
octapeptide GVG(VPGVG) was carried out. In this paper a comprehensive molecular
dynamics study of the temperature dependent folding and unfolding of the
octapeptide is presented. The current study, as well as its experimental
counterpart find that this peptide undergoes an "inverse temperature
transition", ITT, leading to a folding at about 310-330 K. In addition, an
unfolding transition is identified at unusually high temperatures approaching
the boiling point of water. Due to the small size of the system two broad
temperature regimes are found: the "ITT regime" (at about 280-320 K) and the
"unfolding regime" at about T > 330 K, where the peptide has a maximum
probability of being folded at approximately 330 K. A detailed molecular
picture involving a thermodynamic order parameter, or reaction coordinate, for
this process is presented along with a time-correlation function analysis of
the hydrogen bond dynamics within the peptide as well as between the peptide
and solvating water molecules. Correlation with experimental evidence and
ramifications on the properties of elastin are discussed.

<id>
q-bio/0401021v1
<category>
q-bio.BM
<abstract>
A simplified model for the closed circular DNA (ccDNA) is proposed to
describe some specific features of the helix-coil transition in such molecule.
The Hamiltonian of ccDNA is related to the one introduced earlier for the
linear DNA. The basic assumption is that the reduced energy of the hydrogen
bond is not constant through the transition process but depends effectively on
the fraction of already broken bonds. A transformation formula is obtained
which relates the temperature of ccDNA at a given degree of helicity during the
transition to the temperature of the corresponding linear chain at the same
degree of helicity. The formula provides a simple method to calculate the
melting curve for the ccDNA from the experimental melting curve of the linear
DNA with the same nucleotide sequence.

<id>
q-bio/0401034v2
<category>
q-bio.BM
<abstract>
We develop a simple but rigorous model of protein-protein association
kinetics based on diffusional association on free energy landscapes obtained by
sampling configurations within and surrounding the native complex binding
funnels. Guided by results obtained on exactly solvable model problems, we
transform the problem of diffusion in a potential into free diffusion in the
presence of an absorbing zone spanning the entrance to the binding funnel. The
free diffusion problem is solved using a recently derived analytic expression
for the rate of association of asymmetrically oriented molecules. Despite the
required high steric specificity and the absence of long-range attractive
interactions, the computed rates are typically on the order of 10^4-10^6 M-1
s-1, several orders of magnitude higher than rates obtained using a purely
probabilistic model in which the association rate for free diffusion of
uniformly reactive molecules is multiplied by the probability of a correct
alignment of the two partners in a random collision. As the association rates
of many protein-protein complexes are also in the 10^5-10^6 M-1 s-1, our
results suggest that free energy barriers arising from desolvation and/or
side-chain freezing during complex formation or increased ruggedness within the
binding funnel, which are completely neglected in our simple diffusional model,
do not contribute significantly to the dynamics of protein-protein association.
The transparent physical interpretation of our approach that computes
association rates directly from the size and geometry of protein-protein
binding funnels makes it a useful complement to Brownian dynamics simulations.

<id>
q-bio/0401038v1
<category>
q-bio.BM
<abstract>
Functional proteins must fold with some minimal stability to a structure that
can perform a biochemical task. Here we use a simple model to investigate the
relationship between the stability requirement and the capacity of a protein to
evolve the function of binding to a ligand. Although our model contains no
built-in tradeoff between stability and function, proteins evolved function
more efficiently when the stability requirement was relaxed. Proteins with both
high stability and high function evolved more efficiently when the stability
requirement was gradually increased than when there was constant selection for
high stability. These results show that in our model, the evolution of function
is enhanced by allowing proteins to explore sequences corresponding to
marginally stable structures, and that it is easier to improve stability while
maintaining high function than to improve function while maintaining high
stability. Our model also demonstrates that even in the absence of a
fundamental biophysical tradeoff between stability and function, the speed with
which function can evolve is limited by the stability requirement imposed on
the protein.

<id>
q-bio/0402018v1
<category>
q-bio.BM
<abstract>
Using the model for the processive movement of a dimeric kinesin we proposed
before, we study the dynamics of a number of mutant homodimeric and
heterodimeric kinesins that were constructed by Kaseda et al. (Kaseda, K.,
Higuchi, H. and Hirose, K. PNAS 99, 16058 (2002)). The theoretical results of
ATPase rate per head, moving velocity, and stall force of the motors show good
agreement with the experimental results by Kaseda et al.: The puzzling dynamic
behaviors of heterodimeric kinesin that consists of two distinct heads compared
with its parent homodimers can be easily explained by using independent ATPase
rates of the two heads in our model. We also study the collective kinetic
behaviors of kinesins in MT-gliding motility. The results explains well that
the average MT-gliding velocity is independent of the number of bound motors
and is equal to the moving velocity of a single kinesin relative to MT.

<id>
q-bio/0402039v1
<category>
q-bio.BM
<abstract>
The simplest approximation of interaction potential between amino-acids in
proteins is the contact potential, which defines the effective free energy of a
protein conformation by a set of amino acid contacts formed in this
conformation. Finding a contact potential capable of predicting free energies
of protein states across a variety of protein families will aid protein folding
and engineering in silico on a computationally tractable time-scale. We test
the ability of contact potentials to accurately and transferably (across
various protein families) predict stability changes of proteins upon mutations.
We develop a new methodology to determine the contact potentials in proteins
from experimental measurements of changes in protein thermodynamic stabilities
(ddG) upon mutations. We apply our methodology to derive sets of contact
interaction parameters for a hierarchy of interaction models including
solvation and multi-body contact parameters. We test how well our models
reproduce experimental measurements by statistical tests. We evaluate the
maximum accuracy of predictions obtained by using contact potentials and the
correlation between parameters derived from different data-sets of experimental
ddG values. We argue that it is impossible to reach experimental accuracy and
derive fully transferable contact parameters using the contact models of
potentials. However, contact parameters can yield reliable predictions of ddG
for datasets of mutations confined to specific amino-acid positions in the
sequence of a single protein.

<id>
q-bio/0403019v3
<category>
q-bio.BM
<abstract>
We first review how to determine the rate of vibrational energy relaxation
(VER) using perturbation theory. We then apply those theoretical results to the
problem of VER of a CD stretching mode in the protein cytochrome c. We model
cytochrome c in vacuum as a normal mode system with the lowest-order anharmonic
coupling elements. We find that, for the ``lifetime'' width parameter $\gamma=3
\sim 30$ cm$^{-1}$, the VER time is $0.2 \sim 0.3$ ps, which agrees rather well
with the previous classical calculation using the quantum correction factor
method, and is consistent with spectroscopic experiments by Romesberg's group.
We decompose the VER rate into separate contributions from two modes, and find
that the most significant contribution, which depends on the ``lifetime'' width
parameter, comes from those modes most resonant with the CD vibrational mode.

<id>
q-bio/0404005v1
<category>
q-bio.BM
<abstract>
The three-dimensional structures of two common repeat motifs
Val$^1$-Pro$^2$-Gly$^3$-Val$^4$-Gly$^5$ and
Val$^1$-Gly$^2$-Val$^3$-Pro$^4$-Gly$^5$-Val$^6$-Gly$^7$-Val$^8$-Pro$^9$ of
tropoelastin are investigated by using the multicanonical simulation procedure.
By minimizing the energy structures along the trajectory the thermodynamically
most stable low-energy microstates of the molecule are determined. The
structural predictions are in good agreement with X-ray diffraction
experiments.

<id>
q-bio/0404011v1
<category>
q-bio.BM
<abstract>
We address the controversial hot question concerning the validity of the
loose-coupling versus the lever-arm models in the actomyosin dynamics by
re-interpreting and extending the washboard potential model proposed by some of
us in a previous paper. In the new theory, a loose-coupling mechanism co-exists
with the deterministic lever-arm model. The synergetic action of a random
component, originating from the harnessed thermal energy, and of the
power-stroke generated by the lever-arm classical mechanism is seen to yield an
excellent fit of the set of data obtained in T. Yanagida's laboratory on the
sliding of Myosin II heads on actin filaments under various load conditions.
Our theoretical arguments are complemented by accurate numerical simulations,
and the robustness of theory is tested via different combination of parameters
and potential profiles.

<id>
q-bio/0309016v1
<category>
q-bio.CB
<abstract>
We report the cell biological applications of a recently developed
multiphoton fluorescence lifetime imaging microscopy system using a streak
camera (StreakFLIM). The system was calibrated with standard fluorophore
specimens and was shown to have high accuracy and reproducibility. We
demonstrate the applicability of this instrument in living cells for measuring
the effects of protein targeting and point mutations in the protein sequence
which are not obtainable in conventional intensity based fluorescence
microscopy methods. We discuss the relevance of such time resolved information
in quantitative energy transfer microscopy and in measurement of the parameters
characterizing intracellular physiology.

<id>
q-bio/0310014v3
<category>
q-bio.CB
<abstract>
A novel assay based on micropatterning and time-lapse microscopy has been
developed for the study of nuclear migration dynamics in cultured mammalian
cells. When cultured on 10-20 um wide adhesive stripes, the motility of C6
glioma and primary mouse fibroblast cells is diminished. Nevertheless, nuclei
perform an unexpected auto-reverse motion: when a migrating nucleus approaches
the leading edge, it decelerates, changes the direction of motion and
accelerates to move toward the other end of the elongated cell. During this
process cells show signs of polarization closely following the direction of
nuclear movement. The observed nuclear movement requires a functioning
microtubular system, as revealed by experiments disrupting the main
cytoskeletal components with specific drugs. On the basis of our results we
argue that auto-reverse nuclear migration is due to forces determined by the
interplay of microtubule dynamics and the changing position of the microtubule
organizing center as the nucleus reaches the leading edge. Our assay
recapitulates specific features of nuclear migration (cell polarization,
oscillatory nuclear movement), while allows the systematic study of a large
number of individual cells. In particular, our experiments yielded the first
direct evidence of reversive nuclear motion in mammalian cells, induced by
attachment constraints.

<id>
q-bio/0310016v1
<category>
q-bio.CB
<abstract>
We investigate the final stage of cytokinesis in two types of amoeba,
pointing out the existence of biphasic furrow contraction. The first phase is
characterized by a constant contraction rate, is better studied, and seems
universal to a large extent. The second phase is more diverse. In Dictyostelium
discoideum the transition involves a change in the rate of contraction, and
occurs when the width of the cleavage furrow is comparable to the height of the
cell. In Entamoeba invadens the contractile ring carries the cell through the
first phase, but cannot complete the second stage of cytokinesis. As a result,
a cooperative mechanism has evolved in that organism, where a neighboring
amoeba performs directed motion towards the dividing cell, and physically
causes separation by means of extending a pseudopod. We expand here on a
previous report of this novel chemotactic signaling mechanism.

<id>
q-bio/0310030v1
<category>
q-bio.CB
<abstract>
The immune system protects the body against health-threatening entities,
known as antigens, through very complex interactions involving the antigens and
the system's own entities. One remarkable feature resulting from such
interactions is the immune system's ability to improve its capability to fight
antigens commonly found in the individual's environment. This adaptation
process is called the evolution of specificity. In this paper, we introduce a
new mathematical model for the evolution of specificity in humoral immunity,
based on Jerne's functional, or idiotypic, network. The evolution of
specificity is modeled as the dynamic updating of connection weights in a graph
whose nodes are related to the network's idiotypes. At the core of this
weight-updating mechanism are the increase in specificity caused by clonal
selection and the decrease in specificity due to the insertion of uncorrelated
idiotypes by the bone marrow. As we demonstrate through numerous computer
experiments, for appropriate choices of parameters the new model correctly
reproduces, in qualitative terms, several immune functions.

<id>
q-bio/0401032v1
<category>
q-bio.CB
<abstract>
The problem of the onset and growth of solid tumour in homogeneous tissue is
regarded using an approach based on local interaction between the tumoral and
the sane tissue cells. The characteristic sizes and growth rates of spherical
tumours, the points of the beginning and the end of spherical growth, and the
further development of complex structures (elongated outgrowths, dendritic
structures, and metastases) are derived from the assumption that the
reproduction rate of a population of cancer cells is a non-monotone function of
their local concentration. The predicted statistical distribution of the
characteristic tumour sizes, when compared to the clinical data, will make a
basis for checking the validity of the theory.

<id>
q-bio/0406028v1
<category>
q-bio.CB
<abstract>
In this paper we consider a continuous-time anisotropic swarm model with an
attraction/repulsion function and study its aggregation properties. It is shown
that the swarm members will aggregate and eventually form a cohesive cluster of
finite size around the swarm center. We also study the swarm cohesiveness when
the motion of each agent is a combination of the inter-individual interactions
and the interaction of the agent with external environment. Moreover, we extend
our results to more general attraction/repulsion functions. The model in this
paper is more general than isotropic swarms and our results provide further
insight into the effect of the interaction pattern on individual motion in a
swarm system.

<id>
q-bio/0408001v1
<category>
q-bio.CB
<abstract>
Experimental evidence indicates that human brain cancer cells proliferate or
migrate, yet do not display both phenotypes at the same time. Here, we present
a novel computational model simulating this cellular decision-process leading
up to either phenotype based on a molecular interaction network of genes and
proteins. The model's regulatory network consists of the epidermal growth
factor receptor (EGFR), its ligand transforming growth factor-a (TGFa), the
downstream enzyme phospholipaseC-gamma (PLCg) and a mitosis-associated response
pathway. This network is activated by autocrine TGFa secretion, and the
EGFR-dependent downstream signaling this step triggers, as well as modulated by
an extrinsic nutritive glucose gradient. Employing a framework of mass action
kinetics within a multiscale agent-based environment, we analyze both the
emergent multicellular behavior of tumor growth and the single-cell molecular
profiles that change over time and space. Our results show that one can indeed
simulate the dichotomy between cell migration and proliferation based solely on
an EGFR decision network. It turns out that these behavioral decisions on the
single cell level impact the spatial dynamics of the entire cancerous system.
Furthermore, the simulation results yield intriguing experimentally testable
hypotheses also on the sub-cellular level such as spatial cytosolic
polarization of PLCg towards an extrinsic chemotactic gradient. Implications of
these results for future works, both on the modeling and experimental side are
discussed.

<id>
q-bio/0408003v1
<category>
q-bio.CB
<abstract>
Biological systems excel at building spatial structures on scales ranging
from nanometers to kilometers and exhibit temporal patterning from milliseconds
to years. One approach that nature has taken to accomplish this relies on the
harnessing of pattern-forming processes of non-equilibrium physics and
chemistry. For these systems, the study of biological pattern formation starts
with placing a biological phenomenon of interest within the context of the
proper pattern-formation schema and then focusing on the ways in which control
is exerted to adapt the pattern to the needs of the organism. This approach is
illustrated by several examples, notably bacterial colonies (diffusive-growth
schema) and intracellular calcium waves (excitable-media schema).

<id>
q-bio/0411018v1
<category>
q-bio.CB
<abstract>
During embryonic development, a spatial pattern is formed in which
proportions are established precisely. As an early pattern formation step in
Drosophila embryos, an anterior-posterior gradient of Bicoid (Bcd) induces
hunchback (hb) expression (Driever et al. 1989; Tautz et al. 1988). In contrast
to the Bcd gradient, the Hb profile includes information about the scale of the
embryo. Furthermore, the resulting hb expression pattern shows a much lower
embryo-to-embryo variability than the Bcd gradient (Houchmandzadeh et al.
2002). An additional graded posterior repressing activity could theoretically
account for the observed scaling. However, we show that such a model cannot
produce the observed precision in the Hb boundary, such that a fundamentally
different mechanism must be at work. We describe and simulate a model that can
account for the observed precise generation of the scaled Hb profile in a
highly robust manner. The proposed mechanism includes Staufen (Stau), an RNA
binding protein that appears essential to precision scaling (Houchmandzadeh et
al. 2002). In the model, Stau is released from both ends of the embryo and
relocalises hb RNA by increasing its mobility. This leads to an effective
transport of hb away from the respective Stau sources. The balance between
these opposing effects then gives rise to scaling and precision. Considering
the biological importance of robust precision scaling and the simplicity of the
model, the same principle may be employed more often during development.

<id>
q-bio/0501007v2
<category>
q-bio.CB
<abstract>
We investigate noise-induced pattern formation in a model of cancer growth
based on Michaelis-Menten kinetics, subject to additive and multiplicative
noises. We analyse stability properties of the system and discuss the role of
diffusion and noises in the system's dynamics. We find that random dichotomous
fluctuations in the immune response intensity along with Gaussian environmental
noise lead to emergence of a spatial pattern of two phases, in which cancer
cells, or, respectively, immune cells predominate.

<id>
q-bio/0502041v1
<category>
q-bio.CB
<abstract>
Morphogenesis is the ensemble of processes that determines form, shape and
patterns in organisms. Based on a reaction-diffusion theoretical setting and
some prototype reaction schemes, we make a review of the models and experiments
that support possible mechanisms of morphogenesis. We present specific case
studies from chemistry (Belousov-Zhabotinsky reaction) and biology (formation
of wing eyespots patterns in butterflies). We show the importance of
conservation laws in the establishment of patterning in biological systems, and
their relevance to explain phenotypic plasticity in living organisms. Mass
conservation introduces a memory effect in biological development and
phenotypic plasticity in patterns of living organisms can be explained by
differences on the initial conditions occurring during development.

<id>
q-bio/0503016v1
<category>
q-bio.CB
<abstract>
Monte Carlo simulations of the number of stem cells in human colon crypts
allow for fluctuations which kill the population after sufficiently long times.

<id>
q-bio/0506036v1
<category>
q-bio.CB
<abstract>
Although Turing pattern is one of the most universal mechanisms for pattern
formation, in its standard model the number of stripes changes with the system
size, since the wavelength of the pattern is invariant: It fails to preserve
the proportionality of the pattern, i.e., the ratio of the wavelength to the
size, that is often required in biological morphogeneis. To get over this
problem, we show that the Turing pattern can preserve proportionality by
introducing a catalytic chemical whose concentration depends on the system
size. Several plausible mechanisms for such size dependence of the
concentration are discussed. Following this general discussion, two models are
studied in which arising Turing patterns indeed preserve the proportionality.
Relevance of the present mechanism to biological morphogenesis is discussed
from the viewpoint of its generality, robustness, and evolutionary
accessibility.

<id>
q-bio/0507011v1
<category>
q-bio.CB
<abstract>
Many complex systems obey allometric, or power, laws y=Yx^{a}. Here y is the
measured value of some system attribute a, Y is a constant, and x is a
stochastic variable. Remarkably, for many living systems the exponent a is
limited to values +or- n/4, n=0,1,2... Here x is the mass of a randomly
selected creature in the population. These quarter-power laws hold for many
attributes, such as pulse rate (n=-1). Allometry has, in the past, been
theoretically justified on a case-by-case basis. An ultimate goal is to find a
common cause for allometry of all types and for both living and nonliving
systems. The principle I - J = extrem. of Extreme physical information (EPI) is
found to provide such a cause. It describes the flow of Fisher information J =>
I from an attribute value a on the cell level to its exterior observation y.
Data y are formed via a system channel function y = f(x,a), with f(x,a) to be
found. Extremizing the difference I - J through variation of f(x,a) results in
a general allometric law f(x,a)= y = Yx^{a}. Darwinian evolution is presumed to
cause a second extremization of I - J, now with respect to the choice of a. The
solution is a=+or-n/4, n=0,1,2..., defining the particular powers of biological
allometry. Under special circumstances, the model predicts that such biological
systems are controlled by but two distinct intracellular information sources.
These sources are conjectured to be cellular DNA and cellular transmembrane ion
gradients

<id>
q-bio/0507021v1
<category>
q-bio.CB
<abstract>
A key problem of eukaryotic cell motility is the signaling mechanism of
chemoattractant gradient sensing. Recent experiments have revealed the
molecular correlate of gradient sensing: Frontness molecules, such as PI3P and
Rac, localize at the front end of the cell, and backness molecules, such as Rho
and myosin II, accumulate at the back of the cell. Importantly, this
frontness-backness polarization occurs "spontaneously" even if the cells are
exposed to uniform chemoattractant profiles. The spontaneous polarization
suggests that the gradient sensing machinery undergoes a Turing bifurcation.
This has led to several classical activator-inhibitor and activator-substrate
models which identify the frontness molecules with the activator. Conspicuously
absent from these models is any accounting of the backness molecules. This
stands in sharp contrast to experiments which show that the backness pathways
inhibit the frontness pathways. Here, we formulate a model based on the
mutually inhibitory interaction between the frontness and backness pathways.
The model builds upon the mutual inhibition model proposed by Bourne and
coworkers (Xu et al, Cell, 114, 201--214, 2003). We show that mutual inhibition
alone, without the help of any positive feedback, can trigger spontaneous
polarization of the frontness and backness pathways. The spatial distribution
of the frontness and backness molecules in response to inhbition and activation
of the frontness and backness pathways are consistent with those observed in
experiments. Furthermore, depending on the parameter values, the model yields
spatial distributions corresponding to chemoattraction (frontness pathways
in-phase with the external gradient) and chemorepulsion (frontness pathways
out-of-phase with the external gradient).

<id>
q-bio/0507034v1
<category>
q-bio.CB
<abstract>
In this work, we study the in-vitro dynamics of the most malignant form of
the primary brain tumor: Glioblastoma Multiforme. Typically, the growing tumor
consists of the inner dense proliferating zone and the outer less dense
invasive region. Experiments with different types of cells show qualitatively
different behavior. Wild-type cells invade a spherically symmetric manner, but
mutant cells are organized in tenuous branches. We formulate a model for this
sort of growth using two coupled reaction-diffusion equations for the cell and
nutrient concentrations. When the ratio of the nutrient and cell diffusion
coefficients exceeds some critical value, the plane propagating front becomes
unstable with respect to transversal perturbations. The instability threshold
and the full phase-plane diagram in the parameter space are determined. The
results are in a good agreement with experimental findings for the two types of
cells.

<id>
q-bio/0507035v1
<category>
q-bio.CB
<abstract>
We present a discrete stochastic model which represents many of the salient
features of the biological process of wound healing. The model describes fronts
of cells invading a wound. We have numerical results in one and two dimensions.
In one dimension we can give analytic results for the front speed as a power
series expansion in a parameter, p, that gives the relative size of
proliferation and diffusion processes for the invading cells. In two dimensions
the model becomes the Eden model for p near 1. In both one and two dimensions
for small p, front propagation for this model should approach that of the
Fisher-Kolmogorov equation. However, as in other cases, this discrete model
approaches Fisher-Kolmogorov behavior slowly.

<id>
q-bio/0508041v1
<category>
q-bio.CB
<abstract>
Membrane potential in a mathematical model of a cardiac myocyte can be
formulated in different ways. Assuming a spatially homogeneous myocyte that is
strictly charge-conservative and electroneutral as a whole, two methods will be
compared: (1) the differential formulation dV/dt=-I/C_m of membrane potential
used traditionally; and (2) the capacitor formulation, where membrane potential
is defined algebraically by the capacitor equation V=Q/C_m. We examine the
relationship between the formulations, assumptions under which each formulation
is consistent, and show that the capacitor formulation provides a transparent,
physically realistic formulation of membrane potential, whereas use of the
differential formulation may introduce unintended and undesirable behavior,
such as monotonic drift of concentrations. We prove that the drift of
concentrations in the differential formulation arises as a compensation for
failure to assign all currents in concentrations. As an example of these
considerations, we present an electroneutral, explicitly charge-conservative
formulation of Winslow et al. model (1999), and extend it to describe membrane
potentials between intracellular compartments.

<id>
q-bio/0511015v3
<category>
q-bio.CB
<abstract>
Artificial biological pacemakers were developed and tested in canine
ventricles. Next steps will require obtaining oscillations sensitive to
external regulations, and robust with respect to long term drifts of expression
levels of pacemaker currents and gap junctions. We introduce mathematical
models intended to be used in parallel with the experiments.
  The models describe human mesenchymal stem cells ({\it hMSC}) transfected
with HCN2 genes and connected to myocytes. They are intended to mimic
experiments with oscillation induction in a cell pair, in cell culture and in
the cardiac tissue. We give examples of oscillations in a cell pair, in a 1 dim
cell culture, and oscillation dependence on number of pacemaker channels per
cell and number of gap junctions. The models permit to mimic experiments with
levels of gene expressions not achieved yet, and to predict if the work to
achieve this levels will significantly increase the quality of oscillations.
This give arguments for selecting the directions of the experimental work.

<id>
q-bio/0512017v1
<category>
q-bio.CB
<abstract>
In this article we highlight chemotaxis (cellular movement) as a rich source
of potential engineering applications and computational models, highlighting
current research and possible future work. We first give a brief description of
the biological mechanism, before describing recent work on modelling it in
silico. We then propose a methodology for extending existing models and their
possible application as a fundamental tool in engineering cellular pattern
formation. We discuss possible engineering applications of human-defined cell
patterns, as well as the potential for using abstract models of chemotaxis for
generalised computation, before concluding with a brief discussion of future
challenges and opportunities in this field.

<id>
q-bio/0603021v1
<category>
q-bio.CB
<abstract>
We build a model for the hepatic fatty acid metabolism and its metabolic and
genetic regulations. The model has two functioning modes: synthesis and
oxidation of fatty acids. We provide a sufficient condition (the strong
lipolytic condition) for the uniqueness of its equilibrium. Under this
condition, modifications of the glucose input produce equilibrium shifts, which
are gradual changes from one functioning mode to the other. We also discuss the
concentration variations of various metabolites during equilibrium shifts. The
model can explain a certain amount of experimental observations, assess the
role of poly-unsaturated fatty acids in genetic regulation, and predict the
behavior of mutants. The analysis of the model is based on block elimination of
variables and uses a modular decomposition of the system dictated by
mathematical global univalence conditions.

<id>
q-bio/0605046v3
<category>
q-bio.CB
<abstract>
We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.

<id>
q-bio/0607035v1
<category>
q-bio.CB
<abstract>
The reproduction of a living cell requires a repeatable set of chemical
events to be properly coordinated. Such events define a replication cycle,
coupling the growth and shape change of the cell membrane with internal
metabolic reactions. Although the logic of such process is determined by
potentially simple physico-chemical laws, the modeling of a full,
self-maintained cell cycle is not trivial. Here we present a novel approach to
the problem which makes use of so called symmetry breaking instabilities as the
engine of cell growth and division. It is shown that the process occurs as a
consequence of the breaking of spatial symmetry and provides a reliable
mechanism of vesicle growth and reproduction. Our model opens the possibility
of a synthetic protocell lacking information but displaying self-reproduction
under a very simple set of chemical reactions.

<id>
q-bio/0607051v1
<category>
q-bio.CB
<abstract>
Many cell types exhibit oscillatory activity, such as repetitive action
potential firing due to the Hodgkin-Huxley dynamics of ion channels in the cell
membrane or reveal intracellular inositol triphosphate (IP$_3$) mediated
calcium oscillations (CaOs) by calcium-induced calcium release channels
(IP$_3$-receptor) in the membrane of the endoplasmic reticulum (ER). The
dynamics of the excitable membrane and that of the IP$_3$-mediated CaOs have
been the subject of many studies. However, the interaction between the
excitable cell membrane and IP$_3$-mediated CaOs, which are coupled by
cytosolic calcium which affects the dynamics of both, has not been studied.
This study for the first time applied stability analysis to investigate the
dynamic behavior of a model, which includes both an excitable membrane and an
intracellular IP$_3$-mediated calcium oscillator. Taking the IP$_3$
concentration as a control parameter, the model exhibits a novel rich spectrum
of stable and unstable states with hysteresis. The four stable states of the
model correspond in detail to previously reported growth-state dependent states
of the membrane potential of normal rat kidney fibroblasts in cell culture. The
hysteresis is most pronounced for experimentally observed parameter values of
the model, suggesting a functional importance of hysteresis. This study shows
that the four growth-dependent cell states may not reflect the behavior of
cells that have differentiated into different cell types with different
properties, but simply reflect four different states of a single cell type,
that is characterized by a single model.

<id>
q-bio/0608025v1
<category>
q-bio.CB
<abstract>
Various molecules exclusively accumulate at the front or back of migrating
eukaryotic cells in response to a shallow gradient of extracellular signals.
Directional sensing and signal amplification highlight the essential properties
in the migrating cells, known as cell polarity. In addition to these, such
properties of cell polarity involve unique determination of migrating direction
(uniqueness of axis) and localized gradient sensing at the front edge
(localization of sensitivity), both of which may be required for smooth
migration. Here we provide the mass conservation system based on the
reaction-diffusion system with two components, where the mass of the two
components is always conserved. Using two models belonging to this mass
conservation system, we demonstrate through both numerical simulation and
analytical approximations that the spatial pattern with a single peak
(uniqueness of axis) can be generally observed and that the existent peak
senses a gradient of parameters at the peak position, which guides the movement
of the peak. We extended this system with multiple components, and we developed
a multiple-component model in which cross-talk between members of the Rho
family of small GTPases is involved. This model also exhibits the essential
properties of the two models with two components. Thus, the mass conservation
system shows properties similar to those of cell polarity, such as uniqueness
of axis and localization of sensitivity, in addition to directional sensing and
signal amplification.

<id>
q-bio/0608039v1
<category>
q-bio.CB
<abstract>
During the aging of the red-blood cell, or under conditions of extreme
echinocytosis, membrane is shed from the cell plasma membrane in the form of
nano-vesicles. We propose that this process is the result of the
self-adaptation of the membrane surface area to the elastic stress imposed by
the spectrin cytoskeleton, via the local buckling of membrane under increasing
cytoskeleton stiffness. This model introduces the concept of force balance as a
regulatory process at the cell membrane, and quantitatively reproduces the rate
of area loss in aging red-blood cells.

<id>
q-bio/0609019v1
<category>
q-bio.CB
<abstract>
We derive a physiologically structured multiscale model for biofilm
development. The model has components on two spatial scales, which induce
different time scales into the problem. The macroscopic behavior of the system
is modeled using growth-induced flow in a domain with a moving boundary.
Cell-level processes are incorporated into the model using a so-called
physiologically structured variable to represent cell senescence, which in turn
affects cell division and mortality. We present computational results for our
models which shed light on modeling the combined role senescence and the
biofilm state play in the defense strategy of bacteria.

<id>
q-bio/0609043v1
<category>
q-bio.CB
<abstract>
All materials enter or exit the cell nucleus through nuclear pore complexes
(NPCs), efficient transport devices that combine high selectivity and
throughput. A central feature of this transport is the binding of
cargo-carrying soluble transport factors to flexible, unstructured
proteinaceous filaments called FG-nups that line the NPC. We have modeled the
dynamics of transport factors and their interaction with the flexible FG-nups
as diffusion in an effective potential, using both analytical theory and
computer simulations. We show that specific binding of transport factors to the
FG-nups facilitates transport and provides the mechanism of selectivity. We
show that the high selectivity of transport can be accounted for by competition
for both binding sites and space inside the NPC, which selects for transport
factors over other macromolecules that interact only non-specifically with the
NPC. We also show that transport is relatively insensitive to changes in the
number and distribution of FG-nups in the NPC, due mainly to their flexibility;
this accounts for recent experiments where up to half of the total mass of the
NPC has been deleted, without abolishing the transport. Notably, we demonstrate
that previously established physical and structural properties of the NPC can
account for observed features of nucleocytoplasmic transport. Finally, our
results suggest strategies for creation of artificial nano-molecular sorting
devices.

<id>
q-bio/0610005v1
<category>
q-bio.CB
<abstract>
Morphologies of moving amoebae are categorized into two types. One is the
``neutrophil'' type in which the long axis of cell roughly coincides with its
moving direction. This type of cell extends a leading edge at the front and
retracts a narrow tail at the rear, whose shape has been often drawn as a
typical amoeba in textbooks. The other one is the ``keratocyte'' type with
widespread lamellipodia along the front side arc. Short axis of cell in this
type roughly coincides with its moving direction. In order to understand what
kind of molecular feature causes conversion between two types of morphologies,
and how two typical morphologies are maintained, a mathematical model of
amoebic cells is developed. This model describes movement of cell and
intracellular reactions of activator, inhibitor and actin filaments in a
unified way. It is found that the producing rate of activator is a key factor
of conversion between two types. This model also explains the observed data
that the keratocye type cells tend to rapidly move along a straight line. The
neutrophil type cells move along a straight line when the moving velocity is
small, but they show fluctuated motions deviating from a line when they move as
fast as the keratocye type cells. Efficient energy consumption in the
neutrophil type cells is predicted.

<id>
q-bio/0610015v1
<category>
q-bio.CB
<abstract>
We present a stochastic model which describes fronts of cells invading a
wound. In the model cells can move, proliferate, and experience cell-cell
adhesion. We find several qualitatively different regimes of front motion and
analyze the transitions between them. Above a critical value of adhesion and
for small proliferation large isolated clusters are formed ahead of the front.
This is mapped onto the well-known ferromagnetic phase transition in the Ising
model. For large adhesion, and larger proliferation the clusters become
connected (at some fixed time). For adhesion below the critical value the
results are similar to our previous work which neglected adhesion. The results
are compared with experiments, and possible directions of future work are
proposed.

<id>
q-bio/0309020v1
<category>
q-bio.GN
<abstract>
We identify a set of 575 human genes that are expressed in all conditions
tested in a publicly available database of microarray results. Based on this
common occurrence, the set is expected to be rich in "housekeeping" genes,
showing constitutive expression in all tissues. We compare selected aspects of
their genomic structure with a set of background genes. We find that the
introns, untranslated regions and coding sequences of the housekeeping genes
are shorter, indicating a selection for compactness in these genes.

<id>
q-bio/0310022v1
<category>
q-bio.GN
<abstract>
We propose a new kernel for biological sequences which borrows ideas and
techniques from information theory and data compression. This kernel can be
used in combination with any kernel method, in particular Support Vector
Machines for protein classification. By incorporating prior biological
assumptions on the properties of amino-acid sequences and using a Bayesian
averaging framework, we compute the value of this kernel in linear time and
space, benefiting from previous achievements proposed in the field of universal
coding. Encouraging classification results are reported on a standard protein
homology detection experiment.

<id>
q-bio/0310027v1
<category>
q-bio.GN
<abstract>
We introduce a random bit-string model of post-transcriptional genetic
regulation based on sequence matching. The model spontaneously yields a scale
free network with power law scaling with $ \gamma=-1$ and also exhibits
log-periodic behaviour. The in-degree distribution is much narrower, and
exhibits a pronounced peak followed by a Gaussian distribution. The network is
of the smallest world type, with the average minimum path length independent of
the size of the network, as long as the network consists of one giant cluster.
The percolation threshold depends on the system size.

<id>
q-bio/0310040v2
<category>
q-bio.GN
<abstract>
BACKGROUND: Transcriptional regulation is a key mechanism in the functioning
of the cell, and is mostly effected through transcription factors binding to
specific recognition motifs located upstream of the coding region of the
regulated gene. The computational identification of such motifs is made easier
by the fact that they often appear several times in the upstream region of the
regulated genes, so that the number of occurrences of relevant motifs is often
significantly larger than expected by pure chance. RESULTS: To exploit this
fact, we construct sets of genes characterized by the statistical
overrepresentation of a certain motif in their upstream regions. Then we study
the functional characterization of these sets by analyzing their annotation to
Gene Ontology terms. For the sets showing a statistically significant specific
functional characterization, we conjecture that the upstream motif
characterizing the set is a binding site for a transcription factor involved in
the regulation of the genes in the set. CONCLUSIONS: The method we propose is
able to identify many known binding sites in S. cerevisiae and new candidate
targets of regulation by known transcription factors. Its application to less
well studied organisms is likely to be valuable in the exploration of their
regulatory interaction network.

<id>
q-bio/0311018v1
<category>
q-bio.GN
<abstract>
We describe a new global multiple alignment program capable of aligning a
large number of genomic regions. Our progressive alignment approach
incorporates the following ideas: maximum-likelihood inference of ancestral
sequences, automatic guide-tree construction, protein based anchoring of
ab-initio gene predictions, and constraints derived from a global homology map
of the sequences. We have implemented these ideas in the MAVID program, which
is able to accurately align multiple genomic regions up to megabases long.
MAVID is able to effectively align divergent sequences, as well as incomplete
unfinished sequences. We demonstrate the capabilities of the program on the
benchmark CFTR region which consists of 1.8Mb of human sequence and 20
orthologous regions in marsupials, birds, fish, and mammals. Finally, we
describe two large MAVID alignments: an alignment of all the available HIV
genomes and a multiple alignment of the entire human, mouse and rat genomes.

<id>
q-bio/0312006v1
<category>
q-bio.GN
<abstract>
The Relevance Vector Machine (RVM) is a recently developed machine learning
framework capable of building simple models from large sets of candidate
features. Here, we describe a protocol for using the RVM to explore very large
numbers of candidate features, and a family of models which apply the power of
the RVM to classifying and detecting interesting points and regions in
biological sequence data. The models described here have been used successfully
for predicting transcription start sites and other features in genome
sequences.

<id>
q-bio/0312007v1
<category>
q-bio.GN
<abstract>
Background: In addition to known protein-coding genes, large amount of
apparently non-coding sequence are conserved between the human and mouse
genomes. It seems reasonable to assume that these conserved regions are more
likely to contain functional elements than less-conserved portions of the
genome. Here we used a motif-oriented machine learning method to extract the
strongest signal from a set of non-coding conserved sequences.
  Results: We successfully fitted models to reflect the non-coding sequences,
and showed that the results were quite consistent for repeated training runs.
Using the learned model to scan genomic sequence, we found that it often made
predictions close to the start of annotated genes. We compared this method with
other published promoter-prediction systems, and show that the set of promoters
which are detected by this method seems to be substantially similar to that
detected by existing methods.
  Conclusions: The results presented here indicate that the promoter signal is
the strongest single motif-based signal in the non-coding functional fraction
of the genome. They also lend support to the belief that there exists a
substantial subset of promoter regions which share common features and are
detectable by a variety of computational methods.

<id>
q-bio/0312017v1
<category>
q-bio.GN
<abstract>
This paper studies sequencing and mapping methods that rely solely on pooling
and shotgun sequencing of clones. First, we scrutinize and improve the recently
proposed Clone-Array Pooled Shotgun Sequencing (CAPSS) method, which delivers a
BAC-linked assembly of a whole genome sequence. Secondly, we introduce a novel
physical mapping method, called Clone-Array Pooled Shotgun Mapping (CAPS-MAP),
which computes the physical ordering of BACs in a random library. Both CAPSS
and CAPS-MAP construct subclone libraries from pooled genomic BAC clones.
  We propose algorithmic and experimental improvements that make CAPSS a viable
option for sequencing a set of BACs. We provide the first probabilistic model
of CAPSS sequencing progress. The model leads to theoretical results supporting
previous, less formal arguments on the practicality of CAPSS. We demonstrate
the usefulness of CAPS-MAP for clone overlap detection with a probabilistic
analysis, and a simulated assembly of the Drosophila melanogaster genome. Our
analysis indicates that CAPS-MAP is well-suited for detecting BAC overlaps in a
highly redundant library, relying on a low amount of shotgun sequence
information. Consequently, it is a practical method for computing the physical
ordering of clones in a random library, without requiring additional clone
fingerprinting. Since CAPS-MAP requires only shotgun sequence reads, it can be
seamlessly incorporated into a sequencing project with almost no experimental
overhead.

<id>
q-bio/0312027v1
<category>
q-bio.GN
<abstract>
Using Monte Carlo methods, we simulated the effects of bias in generation and
elimination of paralogs on the size distribution of paralog groups. It was
found that the function describing the decay of the number of paralog groups
with their size depends on the ratio between the probability of duplications of
genes and their deletions, which corresponds to different selection pressures
on the genome size. Slightly different slopes of curves describing the decay of
the number of paralog groups with their size were also observed when the
threshold of homology between paralogous sequences was changed.

<id>
q-bio/0312039v1
<category>
q-bio.GN
<abstract>
To test whether X-chromosome has unique genomic characteristics, X-chromosome
and 22 autosomes were compared for RNA binding density. Nucleotide sequences on
the chromosomes were divided into 50kb per segment that was recoded as a set of
frequency values of 7-nucleotide (7nt) strings using all possible 7nt strings
(47=16384). 120 genes highly expressed in tonsil germinal center B cells were
selected for calculating 7nt string frequency values of all introns (RNAs). The
binding density of DNA segments and RNAs was determined by the amount of
complement sequences. It was shown for the first time that gene-poor and low
gene expression X-chromosome had the lowest percentage of the DNA segments that
can highly bind RNAs, whereas gene-rich and high gene expression chromosome 19
had the highest percentage of the segments. On the basis of these results, it
is proposed that the nonrandom properties of distribution of RNA highly binding
DNA segments on the chromosomes provide strong evidence that lack of RNA highly
binding segments may be a cause of X-chromosome inactivation

<id>
q-bio/0402044v1
<category>
q-bio.GN
<abstract>
In this paper, we study the equilibrium behavior of Eigen's quasispecies
equations for an arbitrary gene network. We consider a genome consisting of $ N
$ genes, so that each gene sequence $ \sigma $ may be written as $ \sigma =
\sigma_1 \sigma_2 ... \sigma_N $. We assume a single fitness peak (SFP) model
for each gene, so that gene $ i $ has some ``master'' sequence $ \sigma_{i, 0}
$ for which it is functioning. The fitness landscape is then determined by
which genes in the genome are functioning, and which are not. The equilibrium
behavior of this model may be solved in the limit of infinite sequence length.
The central result is that, instead of a single error catastrophe, the model
exhibits a series of localization to delocalization transitions, which we term
an ``error cascade.'' As the mutation rate is increased, the selective
advantage for maintaining functional copies of certain genes in the network
disappears, and the population distribution delocalizes over the corresponding
sequence spaces. The network goes through a series of such transitions, as more
and more genes become inactivated, until eventually delocalization occurs over
the entire genome space, resulting in a final error catastrophe. This model
provides a criterion for determining the conditions under which certain genes
in a genome will lose functionality due to genetic drift. It also provides
insight into the response of gene networks to mutagens. In particular, it
suggests an approach for determining the relative importance of various genes
to the fitness of an organism, in a more accurate manner than the standard
``deletion set'' method. The results in this paper also have implications for
mutational robustness and what C.O. Wilke termed ``survival of the flattest.''

<id>
q-bio/0403024v1
<category>
q-bio.GN
<abstract>
Background: Exonic splice enhancers are sequences embedded within exons which
promote and regulate the splicing of the transcript in which they are located.
A class of exonic splice enhancers are the SR proteins, which are thought to
mediate interactions between splicing factors bound to the 5' and 3' splice
sites. Method and results: We present a novel strategy for analysing
protein-coding sequence by first randomizing the codons used at each position
within the coding sequence, then applying a motif-based machine learning
algorithm to compare the true and randomized sequences. This strategy
identified a collection of motifs which can successfully discriminate between
real and randomized coding sequence, including -- but not restricted to --
several previously reported splice enhancer elements. As well as successfully
distinguishing coding exons from randomized sequences, we show that our model
is able to recognize non-coding exons. Conclusions: Our strategy succeeded in
detecting signals in coding exons which seem to be orthogonal to the sequences'
primary function of coding for proteins. We believe that many of the motifs
detected here may represent binding sites for previously unrecognized proteins
which influence RNA splicing. We hope that this development will lead to
improved knowledge of exonic splice enhancers, and new developments in the
field of computational gene prediction.

<id>
q-bio/0403032v1
<category>
q-bio.GN
<abstract>
The genetic code is considered to be universal. In order to test if some
statistical properties of the coding bacterial genome were due to inherent
properties of the genetic code, we compared the autocorrelation function, the
scaling properties and the maximum entropy of the distribution of distances of
amino acids in sequences obtained by translating protein-coding regions from
the genome of Borrelia burgdorferi, under different genetic codes. Overall our
results indicate that these properties are very stable to perturbations made by
altering the genetic code. We also discuss the evolutionary likely implications
of the present results.

<id>
q-bio/0404024v1
<category>
q-bio.GN
<abstract>
Much of the on-going statistical analysis of DNA sequences is focused on the
estimation of characteristics of coding and non-coding regions that would
possibly allow discrimination of these regions. In the current approach, we
concentrate specifically on genes and intergenic regions. To estimate the level
and type of correlation in these regions we apply various statistical methods
inspired from nonlinear time series analysis, namely the probability
distribution of tuplets, the Mutual Information and the Identical Neighbour
Fit. The methods are suitably modified to work on symbolic sequences and they
are first tested for validity on sequences obtained from well--known simple
deterministic and stochastic models. Then they are applied to the DNA sequence
of chromosome 1 of {\em arabidopsis thaliana}. The results suggest that
correlations do exist in the DNA sequence but they are weak and that intergenic
sequences tend to be more correlated than gene sequences. The use of
statistical tests with surrogate data establish these findings in a rigorous
statistical manner.

<id>
q-bio/0408014v1
<category>
q-bio.GN
<abstract>
Herein it is shown that in order to study the statistical properties of DNA
sequences in bacterial chromosomes it suffices to consider only one half of the
chromosome because they are similar to its corresponding complementary sequence
in the other half. This is due to the inverse bilateral symmetry of bacterial
chromosomes. Contrary to the classical result that DNA coding regions of
bacterial genomes are purely uncorrelated random sequences, here it is shown,
via a renormalization group approach, that DNA random fluctuations of single
bases are modulated by log-periodic variations. Distance series of triplets
display long-range correlations in each half of the intact chromosome and in
intronless protein-coding sequences, or both long-range correlations and
log-periodic modulations along the whole chromosome. Hence scaling analyses of
distance series of DNA sequences have to consider the functional units of
bacterial chromosomes.

<id>
q-bio/0408017v1
<category>
q-bio.GN
<abstract>
The rules that specify how the information contained in DNA codes amino
acids, is called "the genetic code". Using a simplified version of the Penna
nodel, we are using computer simulations to investigate the importance of the
genetic code and the number of amino acids in Nature on population dynamics. We
find that the genetic code is not a random pairing of codons to amino acids and
the number of amino acids in Nature is an optimum under mutations.

<id>
q-bio/0409011v1
<category>
q-bio.GN
<abstract>
Small Ubiquitin-related modifier (SUMO) proteins are widely expressed in
eukaryotic cells, which are reversibly coupled to their substrates by motif
recognition, called sumoylation. Two interesting questions are 1) how many
potential SUMO substrates may be included in mammalian proteomes, such as human
and mouse, 2) and given a SUMO substrate, can we recognize its sumoylation
sites? To answer these two questions, previous prediction systems of SUMO
substrates mainly adopted the pattern recognition methods, which could get high
sensitivity with relatively too many potential false positives. So we use
phylogenetic conservation between mouse and human to reduce the number of
potential false positives.

<id>
q-bio/0410008v1
<category>
q-bio.GN
<abstract>
With the sponsorship of ``Fundacio La Caixa'' we met in Barcelona, November
21st and 22nd, to analyze the reasons why, after the completion of the human
genome sequence, the identification all protein coding genes and their variants
remains a distant goal. Here we report on our discussions and summarize some of
the major challenges that need to be overcome in order to complete the human
gene catalog.

<id>
q-bio/0411024v1
<category>
q-bio.GN
<abstract>
Gene sequences in the vicinity of splice sites are found to possess
dinucleotide periodicities, especially RR and YY, with the period close to the
pitch of nucleosome DNA. This confirms previously reported finding about
preferential positioning of splice junctions within the nucleosomes. The RR and
YY dinucleotides oscillate counterphase, i.e., their respective preferred
positions are shifted about half-period one from another, as it was observed
earlier for AA and TT dinucleotides. Species specificity of nucleosome
positioning DNA pattern is indicated by predominant use of the periodical
GG(CC) dinucleotides in human and mouse genes, as opposed to predominant AA(TT)
dinucleotides in Arabidopsis and C.elegans.
  Keywords: chromatin; gene splicing; intron; exon; dinucleotide; periodical
pattern

<id>
q-bio/0411045v1
<category>
q-bio.GN
<abstract>
RNA editing by members of the double-stranded RNA-specific ADAR family leads
to site-specific conversion of adenosine to inosine (A-to-I) in precursor
messenger RNAs. Editing by ADARs is believed to occur in all metazoa, and is
essential for mammalian development. Currently, only a limited number of human
ADAR substrates are known, while indirect evidence suggests a substantial
fraction of all pre-mRNAs being affected. Here we describe a computational
search for ADAR editing sites in the human transcriptome, using millions of
available expressed sequences. 12,723 A-to-I editing sites were mapped in 1,637
different genes, with an estimated accuracy of 95%, raising the number of known
editing sites by two orders of magnitude. We experimentally validated our
method by verifying the occurrence of editing in 26 novel substrates. A-to-I
editing in humans primarily occurs in non-coding regions of the RNA, typically
in Alu repeats. Analysis of the large set of editing sites indicates the role
of editing in controlling dsRNA stability.

<id>
q-bio/0412037v1
<category>
q-bio.GN
<abstract>
Shannon information (SI) and its special case, divergence, are defined for a
DNA sequence in terms of probabilities of chemical words in the sequence and
are computed for a set of complete genomes highly diverse in length and
composition. We find the following: SI (but not divergence) is inversely
proportional to sequence length for a random sequence but is length-independent
for genomes; the genomic SI is always greater and, for shorter words and longer
sequences, hundreds to thousands times greater than the SI in a random sequence
whose length and composition match those of the genome; genomic SIs appear to
have word-length dependent universal values. The universality is inferred to be
an evolution footprint of a universal mode for genome growth.

<id>
q-bio/0412043v1
<category>
q-bio.GN
<abstract>
A-To-I RNA editing is common to all eukaryotes, associated with various
neurological functions. Recently, A-to-I editing was found to occur abundantly
in the human transcriptome. Here we show that the frequency of A-to-I editing
in humans is at least an order of magnitude higher as that of mouse, rat,
chicken or fly. The extraordinary frequency of RNA editing in human is
explained by the dominance of the primate-specific Alu element in the human
transcriptome, which increases the number of double-stranded RNA substrates.

<id>
q-bio/0501012v1
<category>
q-bio.GN
<abstract>
We proposed a probabilistic algorithm to solve the Multiple Sequence
Alignment problem. The algorithm is a Simulated Annealing (SA) that exploits
the representation of the Multiple Alignment between $D$ sequences as a
directed polymer in $D$ dimensions. Within this representation we can easily
track the evolution in the configuration space of the alignment through local
moves of low computational cost. At variance with other probabilistic
algorithms proposed to solve this problem, our approach allows for the creation
and deletion of gaps without extra computational cost. The algorithm was tested
aligning proteins from the kinases family. When D=3 the results are consistent
with those obtained using a complete algorithm. For $D>3$ where the complete
algorithm fails, we show that our algorithm still converges to reasonable
alignments. Moreover, we study the space of solutions obtained and show that
depending on the number of sequences aligned the solutions are organized in
different ways, suggesting a possible source of errors for progressive
algorithms.

<id>
q-bio/0501018v1
<category>
q-bio.GN
<abstract>
The presence of neighbor dependencies generated a specific pattern of
dinucleotide frequencies in all organisms. Especially, the
CpG-methylation-deamination process is the predominant substitution process in
vertebrates and needs to be incorporated into a more realistic model for
nucleotide substitutions. Based on a general framework of nucleotide
substitutions we develop a method that is able to identify the most relevant
neighbor dependent substitution processes, measure their strength, and judge
their importance to be included into the modeling. Starting from a model for
neighbor independent nucleotide substitution we successively add neighbor
dependent substitution processes in the order of their ability to increase the
likelihood of the model describing given data. The analysis of neighbor
dependent nucleotide substitutions in human, zebrafish and fruit fly is
presented. A web server to perform the presented analysis is publicly
available.

<id>
q-bio/0501020v1
<category>
q-bio.GN
<abstract>
This study presents the first global, 1 Mbp level analysis of patterns of
nucleotide substitutions along the human lineage. The study is based on the
analysis of a large amount of repetitive elements deposited into the human
genome since the mammalian radiation, yielding a number of results that would
have been difficult to obtain using the more conventional comparative method of
analysis. This analysis revealed substantial and consistent variability of
rates of substitution, with the variability ranging up to 2-fold among
different regions. The rates of substitutions of C or G nucleotides with A or T
nucleotides vary much more sharply than the reverse rates suggesting that much
of that variation is due to differences in mutation rates rather than in the
probabilities of fixation of C/G vs. A/T nucleotides across the genome. For all
types of substitution we observe substantially more hotspots than coldspots,
with hotspots showing substantial clustering over tens of Mbp's. Our analysis
revealed that GC-content of surrounding sequences is the best predictor of the
rates of substitution. The pattern of substitution appears very different near
telomeres compared to the rest of the genome and cannot be explained by the
genome-wide correlations of the substitution rates with GC content or exon
density. The telomere pattern of substitution is consistent with natural
selection or biased gene conversion acting to increase the GC-content of the
sequences that are within 10-15 Mbp away from the telomere.

<id>
q-bio/0501030v2
<category>
q-bio.GN
<abstract>
In molecular phylogeny, relationships among organisms are reconstructed using
DNA or protein sequences and are displayed as trees. A linear increase in the
number of sequences results in an exponential increase of possible trees. Thus,
inferring trees from molecular data was shown to be NP-hard. This causes
problems, if large data sets are used. This review gives an introduction to
molecular phylogenetic methods and to the problems biologists are facing in
molecular phylogenetic analyses.

<id>
q-bio/0501033v3
<category>
q-bio.GN
<abstract>
We model the competition between recombination and point mutation in
microbial genomes, and present evidence for two distinct phases, one uniform,
the other genetically diverse. Depending on the specifics of homologous
recombination, we find that global sequence divergence can be mediated by
fronts propagating along the genome, whose characteristic signature on genome
structure is elucidated, and apparently observed in closely-related {\it
Bacillus} strains. Front propagation provides an emergent, generic mechanism
for microbial "speciation", and suggests a classification of microorganisms on
the basis of their propensity to support propagating fronts.

<id>
q-bio/0502009v1
<category>
q-bio.GN
<abstract>
The human genome contains repetitive DNA at different level of sequence
length, number and dispersion. Highly repetitive DNA is particularly rich in
homo-- and di--nucleotide repeats, while middle repetitive DNA is rich of
families of interspersed, mobile elements hundreds of base pairs (bp) long,
among which the Alu families. A link between homo- and di-polymeric tracts and
mobile elements has been recently highlighted. In particular, the mobility of
Alu repeats, which form 10% of the human genome, has been correlated with the
length of poly(A) tracts located at one end of the Alu. These tracts have a
rigid and non-bendable structure and have an inhibitory effect on nucleosomes,
which normally compact the DNA. We performed a statistical analysis of the
genome-wide distribution of lengths and inter--tract separations of poly(X) and
poly(XY) tracts in the human genome. Our study shows that in humans the length
distributions of these sequences reflect the dynamics of their expansion and
DNA replication. By means of general tools from linguistics, we show that the
latter play the role of highly-significant content-bearing terms in the DNA
text. Furthermore, we find that such tracts are positioned in a non-random
fashion, with an apparent periodicity of 150 bases. This allows us to extend
the link between repetitive, highly mobile elements such as Alus and
low-complexity words in human DNA. More precisely, we show that Alus are
sources of poly(X) tracts, which in turn affect in a subtle way the combination
and diversification of gene expression and the fixation of multigene families.

<id>
q-bio/0502045v1
<category>
q-bio.GN
<abstract>
A-to-I RNA editing by ADARs is a post-transcriptional mechanism for expanding
the proteomic repertoire. Genetic recoding by editing was so far observed for
only a few mammalian RNAs that are predominantly expressed in nervous tissues.
However, as these editing targets fail to explain the broad and severe
phenotypes of ADAR1 knockout mice, additional targets for editing by ADARs were
always expected. Using comparative genomics and expressed sequence analysis, we
identified and experimentally verified four additional candidate human
substrates for ADAR-mediated editing: FLNA, BLCAP, CYFIP2 and IGFBP7.
Additionally, editing of three of these substrates was verified in the mouse
while two of them were validated in chicken. Interestingly, none of these
substrates encodes a receptor protein but two of them are strongly expressed in
the CNS and seem important for proper nervous system function. The editing
pattern observed suggests that some of the affected proteins might have altered
physiological properties leaving the possibility that they can be related to
the phenotypes of ADAR1 knockout mice.

<id>
q-bio/0504012v2
<category>
q-bio.GN
<abstract>
The initial analysis of the recently sequenced genome of Acanthamoeba
polyphaga Mimivirus, the largest known double-stranded DNA virus, predicted a
proteome of size and complexity more akin to small parasitic bacteria than to
other nucleo-cytoplasmic large DNA viruses, and identified numerous functions
never before described in a virus. It has been proposed that the Mimivirus
lineage could have emerged before the individualization of cellular organisms
from the 3 domains of life. An exhaustive in silico analysis of the non-coding
moiety of all known viral genomes, now uncovers the unprecedented perfect
conservation of a AAAATTGA motif in close to 50% of the Mimivirus genes. This
motif preferentially occurs in genes transcribed from the predicted leading
strand and is associated with functions required early in the viral infectious
cycle, such as transcription and protein translation. A comparison with the
known promoter of unicellular eukaryotes, in particular amoebal protists,
strongly suggests that the AAAATTGA motif is the structural equivalent of the
TATA box core promoter element. This element is specific to the Mimivirus
lineage, and may correspond to an ancestral promoter structure predating the
radiation of the eukaryotic kingdoms. This unprecedented conservation of core
promoter regions is another exceptional features of Mimivirus, that again
raises the question of its evolutionary origin.

<id>
q-bio/0310018v1
<category>
q-bio.MN
<abstract>
We study a recent model for calcium signal transduction. This model displays
spiking, bursting and chaotic oscillations in accordance with experimental
results. We calculate bifurcation diagrams and study the bursting behaviour in
detail. This behaviour is classified according to the dynamics of separated
slow and fast subsystems. It is shown to be of the Fold-Hopf type, a type which
was previously only described in the context of neuronal systems, but not in
the context of signal transduction in the cell.

<id>
q-bio/0310021v1
<category>
q-bio.MN
<abstract>
Former work on an application of order-disorder theory is recalled as a
vehicle to add further development and significance to the recent paper on
motifs in protein interactions.

<id>
q-bio/0310041v1
<category>
q-bio.MN
<abstract>
We study genetic switches formed from pairs of mutually repressing operons.
The switch stability is characterised by a well defined lifetime which grows
sub-exponentially with the number of copies of the most-expressed transcription
factor, in the regime accessible by our numerical simulations. The stability
can be markedly enhanced by a suitable choice of overlap between the upstream
regulatory domains. Our results suggest that robustness against biochemical
noise can provide a selection pressure that drives operons, that regulate each
other, together in the course of evolution.

<id>
q-bio/0311019v1
<category>
q-bio.MN
<abstract>
Expression of the Drosophila segment polarity genes is initiated by a
prepattern of pair-rule gene products and maintained by a network of regulatory
interactions throughout several stages of embryonic development. Analysis of a
model of gene interactions based on differential equations showed that
wild-type expression patterns of these genes can be obtained for a wide range
of kinetic parameters, which suggests that the steady states are determined by
the topology of the network and the type of regulatory interactions between
components, not the detailed form of the rate laws. To investigate this, we
propose and analyze a Boolean model of this network which is based on a binary
ON/OFF representation of transcription and protein levels, and in which the
interactions are formulated as logical functions. In this model the spatial and
temporal patterns of gene expression are determined by the topology of the
network and whether components are present or absent, rather than the absolute
levels of the mRNAs and proteins and the functional details of their
interactions. The model is able to reproduce the wild type gene expression
patterns, as well as the ectopic expression patterns observed in
over-expression experiments and various mutants. Furthermore, we compute
explicitly all steady states of the network and identify the basin of
attraction of each steady state. The model gives important insights into the
functioning of the segment polarity gene network, such as the crucial role of
the wingless and sloppy paired genes, and the network's ability to correct
errors in the prepattern.

<id>
q-bio/0311031v1
<category>
q-bio.MN
<abstract>
We consider a model of large regulatory gene expression networks where the
thresholds activating the sigmoidal interactions between genes and the signs of
these interactions are shuffled randomly. Such an approach allows for a
qualitative understanding of network dynamics in a lack of empirical data
concerning the large genomes of living organisms. Local dynamics of network
nodes exhibits the multistationarity and oscillations and depends crucially
upon the global topology of a "maximal" graph (comprising of all possible
interactions between genes in the network). The long time behavior observed in
the network defined on the homogeneous "maximal" graphs is featured by the
fraction of positive interactions ($0\leq \eta\leq 1$) allowed between genes.
There exists a critical value $\eta_c<1$ such that if $\eta<\eta_c$, the
oscillations persist in the system, otherwise, when $\eta>\eta_c,$ it tends to
a fixed point (which position in the phase space is determined by the initial
conditions and the certain layout of switching parameters). In networks defined
on the inhomogeneous directed graphs depleted in cycles, no oscillations arise
in the system even if the negative interactions in between genes present
therein in abundance ($\eta_c=0$). For such networks, the bidirectional edges
(if occur) influence on the dynamics essentially. In particular, if a number of
edges in the "maximal" graph is bidirectional, oscillations can arise and
persist in the system at any low rate of negative interactions between genes
($\eta_c=1$). Local dynamics observed in the inhomogeneous scalable regulatory
networks is less sensitive to the choice of initial conditions. The scale free
networks demonstrate their high error tolerance.

<id>
q-bio/0401029v1
<category>
q-bio.MN
<abstract>
Theoretical physics is used for a toy model of molecular biology to assess
conditions that lead to the edge of chaos (EOC) in a network of biomolecules.
Results can enhance our ability to understand complex diseases and their
treatment or cure.

<id>
q-bio/0402017v1
<category>
q-bio.MN
<abstract>
Recent genomic and bioinformatic advances have motivated the development of
numerous random network models purporting to describe graphs of biological,
technological, and sociological origin. The success of a model has been
evaluated by how well it reproduces a few key features of the real-world data,
such as degree distributions, mean geodesic lengths, and clustering
coefficients. Often pairs of models can reproduce these features with
indistinguishable fidelity despite being generated by vastly different
mechanisms. In such cases, these few target features are insufficient to
distinguish which of the different models best describes real world networks of
interest; moreover, it is not clear a priori that any of the presently-existing
algorithms for network generation offers a predictive description of the
networks inspiring them. To derive discriminative classifiers, we construct a
mapping from the set of all graphs to a high-dimensional (in principle
infinite-dimensional) ``word space.'' This map defines an input space for
classification schemes which allow us for the first time to state unambiguously
which models are most descriptive of the networks they purport to describe. Our
training sets include networks generated from 17 models either drawn from the
literature or introduced in this work, source code for which is freely
available. We anticipate that this new approach to network analysis will be of
broad impact to a number of communities.

<id>
q-bio/0402027v2
<category>
q-bio.MN
<abstract>
We employed the random graph theory approach to analyze the protein-protein
interaction database DIP (Feb. 2004), for seven species (S. cerevisiae, H.
pylori, E. coli, C. elegans, H. sapiens, M. musculus and D. melanogaster).
Several global topological parameters (such as node connectivity, average
diameter, node connectivity correlation) were used to characterize these
protein-protein interaction networks (PINs). The logarithm of the connectivity
distribution vs. the logarithm of connectivity study indicated that PINs follow
a power law (P(k) ~ k-\gamma) behavior. Using the regression analysis method we
determined that \gamma lies between 1.5 and 2.4, for the seven species.
Correlation analysis provides good evidence supporting the fact that the seven
PINs form a scale-free network. The average diameters of the networks and their
randomized version are found to have large difference. We also demonstrated
that the interaction networks are quite robust when subject to random
perturbation. Average node connectivity correlation study supports the earlier
results that nodes of low connectivity are correlated, whereas nodes of high
connectivity are not directly linked. These results provided some evidence
suggesting such correlation relations might be a general feature of the PINs
across different species.

<id>
q-bio/0403012v1
<category>
q-bio.MN
<abstract>
We perform a reverse engineering from the ``extended Spellman data'',
consisting of 6178 mRNA levels measured by microarrays at 73 instances in four
time series during the cell cycle of the yeast Saccharomyces cerevisae. By
assuming a linear model of the genetic regulatory network, and imposing an
extra constraint (the Lasso), we obtain a unique inference of coupling
parameters. These parameters are transfered into an adjacent matrix, which is
analyzed with respect to topological properties and biological relevance. We
find a very broad distribution of outdegrees in the network, compatible with
earlier findings for biological systems and totally incompatible with a random
graph, and also indications of modules in the network. Finally, we show there
is an excess of genes coding for transcription factors among the genes of
highest outdegrees, a fact which indicates that our approach has biological
relevance.

<id>
q-bio/0403027v1
<category>
q-bio.MN
<abstract>
We prove the global asymptotic stability of a well-known delayed
negative-feedback model of testosterone dynamics, which has been proposed as a
model of oscillatory behavior. We establish stability (and hence the
impossibility of oscillations) even in the presence of delays of arbitrary
length.

<id>
q-bio/0403033v1
<category>
q-bio.MN
<abstract>
We discuss properties which must be satisfied by a genetic network in order
for it to allow differentiation.
  These conditions are expressed as follows in mathematical terms. Let $F$ be a
differentiable mapping from a finite dimensional real vector space to itself.
The signs of the entries of the Jacobian matrix of $F$ at a given point $a$
define an interaction graph, i.e. a finite oriented finite graph $G(a)$ where
each edge is equipped with a sign. Ren\'e Thomas conjectured twenty years ago
that, if $F$ has at least two non degenerate zeroes, there exists $a$ such that
$G(a)$ contains a positive circuit. Different contributors proved this in special
cases, and we give here a general proof of the conjecture. In particular, we
get this way a necessary condition for genetic networks to lead to
multistationarity, and therefore to differentiation.
  We use for our proof the mathematical literature on global univalence, and we
show how to derive from it several variants of Thomas' rule, some of which had
been anticipated by Kaufman and Thomas.

<id>
q-bio/0403045v2
<category>
q-bio.MN
<abstract>
Many real networks can be understood as two complementary networks with two
kind of nodes. This is the case of metabolic networks where the first network
has chemical compounds as nodes and the second one has nodes as reactions. The
second network can be related to the first one by a technique called line graph
transformation (i.e., edges in an initial network are transformed into nodes).
Recently, the main topological properties of the metabolic networks have been
properly described by means of a hierarchical model. In our work, we apply the
line graph transformation to a hierarchical network and the clustering
coefficient $C(k)$ is calculated for the transformed network, where $k$ is the
node degree. While $C(k)$ follows the scaling law $C(k)\sim k^{-1.1}$ for the
initial hierarchical network, $C(k)$ scales weakly as $k^{0.08}$ for the
transformed network. These results indicate that the reaction network can be
identified as a degree-independent clustering network.

<id>
q-bio/0404002v2
<category>
q-bio.MN
<abstract>
Biochemical networks are the analog computers of life. They allow living
cells to control a large number of biological processes, such as gene
expression and cell signalling. In biochemical networks, the concentrations of
the components are often low. This means that the discrete nature of the
reactants and the stochastic character of their interactions have to be taken
into account. Moreover, the spatial distribution of the components can be of
crucial importance. However, the current numerical techniques for simulating
biochemical networks either ignore the particulate nature of matter or treat
the spatial fluctuations in a mean-field manner. We have developed a new
technique, called Green's Function Reaction Dynamics (GFRD), that makes it
possible to simulate biochemical networks at the particle level and in both
time and space. In this scheme, a maximum time step is chosen such that only
single particles or pairs of particles have to be considered. For these
particles, the Smoluchowski equation can be solved analytically using Green's
functions. The main idea of GFRD is to exploit the exact solution of the
Smoluchoswki equation to set up an event-driven algorithm. This allows GFRD to
make large jumps in time when the particles are far apart from each other.
Here, we apply the technique to a simple model of gene expression. The
simulations reveal that the scheme is highly efficient. Under biologically
relevant conditions, GFRD is up to six orders of magnitude faster than
conventional particle-based techniques for simulating biochemical networks in
time and space.

<id>
q-bio/0404006v1
<category>
q-bio.MN
<abstract>
We study the bifurcations of a set of nine nonlinear ordinary differential
equations that describe the regulation of the cyclin-dependent kinase that
triggers DNA synthesis and mitosis in the budding yeast, Saccharomyces
cerevisiae. We show that Clb2-dependent kinase exhibits bistability (stable
steady states of high or low kinase activity). The transition from low to high
Clb2-dependent kinase activity is driven by transient activation of
Cln2-dependent kinase, and the reverse transition is driven by transient
activation of the Clb2 degradation machinery. We show that a four-variable
model retains the main features of the nine-variable model. In a three-variable
model exhibiting birhythmicity (two stable oscillatory states), we explore
possible effects of extrinsic fluctuations on cell cycle progression.

<id>
q-bio/0404017v1
<category>
q-bio.MN
<abstract>
Networks have been used to model many real-world phenomena to better
understand the phenomena and to guide experiments in order to predict their
behavior. Since incorrect models lead to incorrect predictions, it is vital to
have a correct model. As a result, new techniques and models for analyzing and
modeling real-world networks have recently been introduced. One example of
large and complex networks involves protein-protein interaction (PPI) networks.
We demonstrate that the currently popular scale-free model of PPI networks
fails to fit the data in several respects. We show that a random geometric
model provides a much more accurate model of the PPI data.

<id>
q-bio/0405020v1
<category>
q-bio.MN
<abstract>
Signal processing (SP) techniques convert DNA and protein sequences into
information that lead to successful drug discovery. One must, however, be aware
about the difference between information and entropy1. Eight other physical
properties of DNA and protein segments are suggested for SP analysis other than
ones already used in the literature. QSAR formulations of these properties are
suggested for ranking the amino acids that maximize efficiency of the amino
acids in proteins. Multiobjective programs are suggested for constraining or
searching the components of such sequences. Geometric maps of the networks of
proteins are preferable to scale-free descriptions in most cases. The genetic
code is presented as graphlets which show interesting correspondence to each
other, leading to possible new revelations.

<id>
q-bio/0406006v1
<category>
q-bio.MN
<abstract>
Bistable biochemical switches are ubiquitous in gene regulatory networks and
signal transduction pathways. Their switching dynamics, however, are difficult
to study directly in experiments or conventional computer simulations, because
switching events are rapid, yet infrequent. We present a simulation technique
that makes it possible to predict the rate and mechanism of flipping of
biochemical switches. The method uses a series of interfaces in phase space
between the two stable steady states of the switch to generate transition
trajectories in a ratchet-like manner. We demonstrate its use by calculating
the spontaneous flipping rate of a symmetric model of a genetic switch
consisting of two mutually repressing genes. The rate constant can be obtained
orders of magnitude more efficiently than using brute-force simulations. For
this model switch, we show that the switching mechanism, and consequently the
switching rate, depends crucially on whether the binding of one regulatory
protein to the DNA excludes the binding of the other one. Our technique could
also be used to study rare events and non-equilibrium processes in soft
condensed matter systems.

<id>
q-bio/0406043v1
<category>
q-bio.MN
<abstract>
It is becoming increasingly appreciated that the signal transduction systems
used by eukaryotic cells to achieve a variety of essential responses represent
highly complex networks rather than simple linear pathways. While significant
effort is being made to experimentally measure the rate constants for
individual steps in these signaling networks, many of the parameters required
to describe the behavior of these systems remain unknown, or at best,
estimates. With these goals and caveats in mind, we use methods of statistical
mechanics to extract useful predictions for complex cellular signaling
networks. To establish the usefulness of our approach, we have applied our
methods towards modeling the nerve growth factor (NGF)-induced differentiation
of neuronal cells. Using our approach, we are able to extract predictions that
are highly specific and accurate, thereby enabling us to predict the influence
of specific signaling modules in determining the integrated cellular response
to the two growth factors. We show that extracting biologically relevant
predictions from complex signaling models appears to be possible even in the
absence of measurements of all the individual rate constants. Our methods also
raise some interesting insights into the design and possible evolution of
cellular systems, highlighting an inherent property of these systems wherein
particular ''soft'' combinations of parameters can be varied over wide ranges
without impacting the final output and demonstrating that a few ''stiff''
parameter combinations center around the paramount regulatory steps of the
network. We refer to this property -- which is distinct from robustness -- as
''sloppiness.''

<id>
q-bio/0406046v1
<category>
q-bio.MN
<abstract>
A new approach to the modular, complex systems analysis of nonlinear dynamics
in cell cycling network transformations involved in carcinogenesis is proposed.
Carcinogenesis is a complex process that involves dynamically inter-connected
biomolecules in the intercellular, membrane, cytosolic, nuclear and nucleolar
compartments that form numerous inter-related pathways. One such family of
pathways contains the cell cyclins. Cyclins are proteins that link several
critical pro-apoptotic and other cell cycling/division components, including
the tumor suppressor gene TP53 and its product, the Thomsen-Friedenreich
antigen (T antigen), Rb, mdm2, c-Myc, p21, p27, Bax, Bad and Bcl-2, which all
play major roles in neoplastic transformation of many tissues. This novel
theoretical analysis based on recently published studies of cyclin signaling,
with special emphasis placed on the roles of cyclins D1 and E, suggests novel
clinical trials and rational therapies of cancer through reestablishment of
cell cycling inhibition in metastatic cancer cells.

<id>
q-bio/0407031v1
<category>
q-bio.MN
<abstract>
A four-node network consisting of a negative loop controlling a positive one
is studied. It models some of the features of the p53 gene network. Using
piecewise linear dynamics with thresholds, the allowed dynamical classes are
fully characterized and coded. The biologically relevant situations are
identified and conclusions drawn concerning the effectiveness of the p53
network as a tumour inhibitor mechanism.

<id>
q-bio/0409020v1
<category>
q-bio.MN
<abstract>
Immune cells coordinate their efforts for the correct and efficient
functioning of the immune system (IS). Each cell type plays a distinct role and
communicates with other cell types through mediators such as cytokines,
chemokines and hormones, among others, that are crucial for the functioning of
the IS and its fine tuning. Nevertheless, a quantitative analysis of the
topological properties of an immunological network involving this complex
interchange of mediators among immune cells is still lacking. Here we present a
method for quantifying the relevance of different mediators in the immune
network, which exploits a definition of centrality based on the concept of
efficient communication. The analysis, applied to the human immune system,
indicates that its mediators significantly differ in their network relevance.
We found that cytokines involved in innate immunity and inflammation and some
hormones rank highest in the network, revealing that the most prominent
mediators of the IS are molecules involved in these ancestral types of defence
mechanisms highly integrated with the adaptive immune response, and at the
interplay among the nervous, the endocrine and the immune systems.

<id>
q-bio/0410003v1
<category>
q-bio.MN
<abstract>
We study by mean-field analysis and stochastic simulations chemical models
for genetic toggle switches formed from pairs of genes that mutually repress
each other. In order to determine the stability of the genetic switches, we
make a connection with reactive flux theory and transition state theory. The
switch stability is characterised by a well defined lifetime $\tau$. We find
that $\tau$ grows exponentially with the mean number $\Nmean$ of transcription
factor molecules involved in the switching. In the regime accessible to direct
numerical simulations, the growth law is well characterised by
$\tau\sim\Nmean{}^{\alpha}\exp(b\Nmean)$, where $\alpha$ and $b$ are
parameters. The switch stability is decreased by phenomena that increase the
noise in gene expression, such as the production of multiple copies of a
protein from a single mRNA transcript (shot noise), and fluctuations in the
number of proteins produced per transcript. However, robustness against
biochemical noise can be drastically enhanced by arranging the transcription
factor binding domains on the DNA such that competing transcription factors
mutually exclude each other on the DNA. We also elucidate the origin of the
enhanced stability of the exclusive switch with respect to that of the general
switch: while the kinetic prefactor is roughly the same for both switches, the
`barrier' for flipping the switch is significantly higher for the exclusive
switch than for the general switch.

<id>
q-bio/0410009v1
<category>
q-bio.MN
<abstract>
Recent progress has clarified many features of the global architecture of
biological metabolic networks, which have highly organized and optimized
tolerances and tradeoffs (HOT) for functional requirements of flexibility,
efficiency, robustness, and evolvability, with constraints on conservation of
energy, redox, and many small moieties. One consequence of this architecture is
a highly structured modularity that is self-dissimilar and scale-rich, with
extremes in low and high variability, including power laws, in both metabolite
and reaction degree distributions. This paper illustrates these features using
the well-understood stoichiometry of metabolic networks in bacteria, and a
simple model of an abstract metabolism.

<id>
q-bio/0410017v1
<category>
q-bio.MN
<abstract>
Many genes have been identified as driving cellular differentiation, but
because of their complex interactions, the understanding of their collective
behaviour requires mathematical modelling. Intriguingly, it has been observed
in numerous developmental contexts, and particularly hematopoiesis, that genes
regulating differentiation are initially co-expressed in progenitors despite
their antagonism, before one is upregulated and others downregulated. We
characterise conditions under which 3 classes of generic "master regulatory
networks", modelled at the molecular level after experimentally-observed
interactions (including bHLH protein dimerisation), and including an arbitrary
number of antagonistic components, can behave as a "multi-switch", directing
differentiation in an all-or-none fashion to a specific cell-type chosen among
more than 2 possible outcomes. bHLH dimerisation networks can readily display
coexistence of many antagonistic factors when competition is low (a simple
characterisation is derived). Decision-making can be forced by a transient
increase in competition, which could correspond to some unexplained
experimental observations related to Id proteins; the speed of response varies
with the initial conditions the network is subjected to, which could explain
some aspects of cell behaviour upon reprogramming. The coexistence of
antagonistic factors at low levels, early in the differentiation process or in
pluripotent stem cells, could be an intrinsic property of the interaction
between those factors, not requiring a specific regulatory system.

<id>
q-bio/0411035v1
<category>
q-bio.MN
<abstract>
The evolutionary reason for the increase in gene length from archaea to
prokaryotes to eukaryotes observed in large scale genome sequencing efforts has
been unclear. We propose here that the increasing complexity of protein-protein
interactions has driven the selection of longer proteins, as longer proteins
are more able to distinguish among a larger number of distinct interactions due
to their greater average surface area. Annotated protein sequences available
from the SWISS-PROT database were analyzed for thirteen eukaryotes, eight
bacteria, and two archaea species. The number of subcellular locations to which
each protein is associated is used as a measure of the number of interactions
to which a protein participates. Two databases of yeast protein-protein
interactions were used as another measure of the number of interactions to
which each \emph{S. cerevisiae} protein participates. Protein length is shown
to correlate with both number of subcellular locations to which a protein is
associated and number of interactions as measured by yeast two-hybrid
experiments. Protein length is also shown to correlate with the probability
that the protein is encoded by an essential gene. Interestingly, average
protein length and number of subcellular locations are not significantly
different between all human proteins and protein targets of known, marketed
drugs. Increased protein length appears to be a significant mechanism by which
the increasing complexity of protein-protein interaction networks is
accommodated within the natural evolution of species. Consideration of protein
length may be a valuable tool in drug design, one that predicts different
strategies for inhibiting interactions in aberrant and normal pathways.

<id>
q-bio/0412045v2
<category>
q-bio.MN
<abstract>
Despite considerable progress in genome- and proteome-based high-throughput
screening methods and rational drug design, the number of successful single
target drugs did not increase appreciably during the past decade. Network
models suggest that partial inhibition of a surprisingly small number of
targets can be more efficient than the complete inhibition of a single target.
This and the success stories of multi-target drugs and combinatorial therapies
led us to suggest that systematic drug design strategies should be directed
against multiple targets. We propose that the final effect of partial, but
multiple drug actions might often surpass that of complete drug action at a
single target. The future success of this novel drug design paradigm will
depend not only on a new generation of computer models to identify the correct
multiple hits and their multi-fitting, low-affinity drug candidates but also on
more efficient in vivo testing.

<id>
q-bio/0501016v1
<category>
q-bio.MN
<abstract>
Complex dynamical networks consisting of many components that interact and
produce each other are difficult to understand, especially, when new components
may appear. In this paper we outline a theory to deal with such systems. The
theory consists of two parts. The first part introduces the concept of a
chemical organization as a closed and mass-maintaining set of components. This
concept allows to map a complex (reaction) network to the set of organizations,
providing a new view on the system's structure. The second part connects
dynamics with the set of organizations, which allows to map a movement of the
system in state space to a movement in the set of organizations.

<id>
q-bio/0501037v1
<category>
q-bio.MN
<abstract>
Interactions between genes and gene products give rise to complex circuits
that enable cells to process information and respond to external signals.
Theoretical studies often describe these interactions using continuous,
stochastic, or logical approaches. We propose a new modeling framework for gene
regulatory networks, that combines the intuitive appeal of a qualitative
description of gene states with a high flexibility in incorporating
stochasticity in the duration of cellular processes. We apply our methods to
the regulatory network of the segment polarity genes, thus gaining novel
insights into the development of gene expression patterns. For example, we show
that very short synthesis and decay times can perturb the wild type pattern. On
the other hand, separation of timescales between pre- and posttranslational
processes and a minimal prepattern ensure convergence to the wild type
expression pattern regardless of fluctuations.

<id>
q-bio/0502018v1
<category>
q-bio.MN
<abstract>
Thanks to recent progress in high-throughput experimental techniques, the
datasets of large-scale protein interactions of prototypical multicellular
species, the nematode worm Caenorhabditis elegans and the fruit fly Drosophila
melanogaster, have been assayed. The datasets are obtained mainly by using the
yeast hybrid method, which contains false-positive and false-negative
simultaneously. Accordingly, while it is desirable to test such datasets
through further wet experiments, here we invoke recent developed network theory
to test such high throughput datasets in a simple way. Based on the fact that
the key biological processes indispensable to maintaining life are universal
across eukaryotic species, and the comparison of structural properties of the
protein interaction networks (PINs) of the two species with those of the yeast
PIN, we find that while the worm and the yeast PIN datasets exhibit similar
structural properties, the current fly dataset, though most comprehensively
screened ever, does not reflect generic structural properties correctly as it
is. The modularity is suppressed and the connectivity correlation is lacking.
Addition of interlogs to the current fly dataset increases the modularity and
enhances the occurrence of triangular motifs as well. The connectivity
correlation function of the fly, however, remains distinct under such interlogs
addition, for which we present a possible scenario through an in silico
modeling.

<id>
q-bio/0502033v1
<category>
q-bio.MN
<abstract>
The structure of a genetic network is uncovered by studying its response to
external stimuli (input signals). We present a theory of propagation of an
input signal through a linear stochastic genetic network. It is found that
there are important advantages in using oscillatory signals over step or
impulse signals, and that the system may enter into a pure fluctuation
resonance for a specific input frequency.

<id>
q-bio/0309024v1
<category>
q-bio.NC
<abstract>
We study some mechanisms responsible for synchronous oscillations and loss of
synchrony at physiologically relevant frequencies (10-200 Hz) in a network of
heterogeneous inhibitory neurons. We focus on the factors that determine the
level of synchrony and frequency of the network response, as well as the
effects of mild heterogeneity on network dynamics. With mild heterogeneity,
synchrony is never perfect and is relatively fragile. In addition, the effects
of inhibition are more complex in mildly heterogeneous networks than in
homogeneous ones. In the former, synchrony is broken in two distinct ways,
depending on the ratio of the synaptic decay time to the period of repetitive
action potentials ($\tau_s/T$), where $T$ can be determined either from the
network or from a single, self-inhibiting neuron. With $\tau_s/T > 2$,
corresponding to large applied current, small synaptic strength or large
synaptic decay time, the effects of inhibition are largely tonic and
heterogeneous neurons spike relatively independently. With $\tau_s/T < 1$,
synchrony breaks when faster cells begin to suppress their less excitable
neighbors; cells that fire remain nearly synchronous. We show numerically that
the behavior of mildly heterogeneous networks can be related to the behavior of
single, self-inhibiting cells, which can be studied analytically.

<id>
q-bio/0309025v1
<category>
q-bio.NC
<abstract>
We analyze the control of frequency for a synchronized inhibitory neuronal
network. The analysis is done for a reduced membrane model with a
biophysically-based synaptic influence. We argue that such a reduced model can
quantitatively capture the frequency behavior of a larger class of neuronal
models. We show that in different parameter regimes, the network frequency
depends in different ways on the intrinsic and synaptic time constants. Only in
one portion of the parameter space, called `phasic', is the network period
proportional to the synaptic decay time. These results are discussed in
connection with previous work of the contributors, which showed that for mildly
heterogeneous networks, the synchrony breaks down, but coherence is preserved
much more for systems in the phasic regime than in the other regimes. These
results imply that for mildly heterogeneous networks, the existence of a
coherent rhythm implies a linear dependence of the network period on synaptic
decay time, and a much weaker dependence on the drive to the cells. We give
experimental evidence for this conclusion.

<id>
q-bio/0309026v1
<category>
q-bio.NC
<abstract>
We would like to know whether the statistics of neuronal responses vary
across cortical areas. We examined stimulus-elicited spike count response
distributions in V1 and IT cortices of awake monkeys. In both areas the
distribution of spike counts for each stimulus was well-described by a
Gaussian, with the log of the variance in the spike count linearly related to
the log of the mean spike count. Two significant differences in response
characteristics were found: both the range of spike counts and the slope of the
log(variance) vs. log(mean) regression were larger in V1 than in IT. However,
neurons in the two areas transmitted approximately the same amount of
information about the stimuli, and had about the same channel capacity (the
maximum possible transmitted information given noise in the responses). These
results suggest that neurons in V1 use more variable signals over a larger
dynamic range than neurons in IT, which use less variable signals over a
smaller dynamic range. The two coding strategies are approximately as effective
in transmitting information.

<id>
q-bio/0309027v1
<category>
q-bio.NC
<abstract>
We review recent developments in the measurement of the dynamics of the
response properties of auditory cortical neurons to broadband sounds, which is
closely related to the perception of timbre. The emphasis is on a method that
characterizes the spectro-temporal properties of single neurons to dynamic,
broadband sounds, akin to the drifting gratings used in vision. The method
treats the spectral and temporal aspects of the response on an equal footing.

<id>
q-bio/0309030v1
<category>
q-bio.NC
<abstract>
Here we analyze synaptic transmission from an information-theoretic
perspective. We derive closed-form expressions for the lower-bounds on the
capacity of a simple model of a cortical synapse under two explicit coding
paradigms. Under the ``signal estimation'' paradigm, we assume the signal to be
encoded in the mean firing rate of a Poisson neuron. The performance of an
optimal linear estimator of the signal then provides a lower bound on the
capacity for signal estimation. Under the ``signal detection'' paradigm, the
presence or absence of the signal has to be detected. Performance of the
optimal spike detector allows us to compute a lower bound on the capacity for
signal detection. We find that single synapses (for empirically measured
parameter values) transmit information poorly but significant improvement can
be achieved with a small amount of redundancy.

<id>
q-bio/0309033v2
<category>
q-bio.NC
<abstract>
Studies of insect olfactory processing indicate that odors are represented by
rich spatio-temporal patterns of neural activity. These patterns are very
difficult to predict a priori, yet they are stimulus specific and reliable upon
repeated stimulation with the same input. We formulate here a theoretical
framework in which we can interpret these experimental results. We propose a
paradigm of ``dynamic competition'' in which inputs (odors) are represented by
internally competing neural assemblies. Each pattern is the result of dynamical
motion within the network and does not involve a ``winner'' among competing
possibilities. The model produces spatio-temporal patterns with strong
resemblance to those observed experimentally and possesses many of the general
features one desires for pattern classifiers: large information capacity,
reliability, specific responses to specific inputs, and reduced sensitivity to
initial conditions or influence of noise. This form of neural processing may
thus describe the organizational principles of neural information processing in
sensory systems and go well beyond the observations on insect olfactory
processing which motivated its development.

<id>
q-bio/0309034v1
<category>
q-bio.NC
<abstract>
A number of cortical structures are reported to have elevated single unit
firing rates sustained throughout the memory period of a working memory task.
How the nervous system forms and maintains these memories is unknown but
reverberating neuronal network activity is thought to be important. We studied
the temporal structure of single unit (SU) activity and simultaneously recorded
local field potential (LFP) activity from area LIP in the inferior parietal
lobe of two awake macaques during a memory-saccade task. Using multitaper
techniques for spectral analysis, which play an important role in obtaining the
present results, we find elevations in spectral power in a 50--90 Hz (gamma)
frequency band during the memory period in both SU and LFP activity. The
activity is tuned to the direction of the saccade providing evidence for
temporal structure that codes for movement plans during working memory. We also
find SU and LFP activity are coherent during the memory period in the 50--90 Hz
gamma band and no consistent relation is present during simple fixation.
Finally, we find organized LFP activity in a 15--25 Hz frequency band that may
be related to movement execution and preparatory aspects of the task. Neuronal
activity could be used to control a neural prosthesis but SU activity can be
hard to isolate with cortical implants. As the LFP is easier to acquire than SU
activity, our finding of rich temporal structure in LFP activity related to
movement planning and execution may accelerate the development of this medical
application.

<id>
q-bio/0310005v2
<category>
q-bio.NC
<abstract>
Generalized language-of-thought arguments appropriate to interacting
cognitive modules permit exploration of how disease states interact with
medical treatment. The interpenetrating feedback between treatment and response
to it creates a kind of idiotypic hall-of-mirrors generating a synergistic
pattern of efficacy, treatment failure, adverse reactions, and patient
noncompliance which, from a Rate Distortion perspective, embodies a distorted
image of externally-imposed structured psychosocial stress. For the US,
accelerating spatial and social diffusion of such stress enmeshes both dominant
and subordinate populations in a linked system which will express itself, not
only in an increasingly unhealthy society, but in the diffusion of therapeutic
failure, including, but not limited to, drug-based treatments.

<id>
q-bio/0310039v4
<category>
q-bio.NC
<abstract>
In animal experiments, the observed orientation preference (OP) and ocular
dominance (OD) columns in the visual cortex of the brain show various pattern
types. Here, we show that the different visual map formations in various
species are due to the crossover behavior in anisotropic systems composed of
orientational and scalar components such as easy-plane Heisenberg models. We
predict the transition boundary between different pattern types with the
anisotropy as a main bifurcation parameter, which is consistent with
experimental observations.

<id>
q-bio/0311006v1
<category>
q-bio.NC
<abstract>
This paper proposes an approach to framing and answering fundamental
questions about consciousness. It argues that many of the more theoretical
debates about consciousness, such as debates about "when does it begin?", are
misplaced and meaningless, in part because "consciousness" as a word has many
valid and interesting definitions, and in part because consciousness qua mind
or intelligence (the main focus here)is a matter of degree or level, not a
binary variable. It proposes that new mathematical work related to functional
neural network designs -- designs so functional that they can be used in
engineering -- is essential to a functional understanding of intelligence as
such, and outlines some key mathematics as of 1999, citing earlier work for
more details. Quantum theory is relevant, but not in the simple ways proposed
in more popular philosophies.

<id>
q-bio/0311026v1
<category>
q-bio.NC
<abstract>
The question: whether quantum coherent states can sustain decoherence,
heating and dissipation over time scales comparable to the dynamical timescales
of the brain neurons, is actively discussed in the last years. Positive answer
on this question is crucial, in particular, for consideration of brain neurons
as quantum computers. This discussion was mainly based on theoretical
arguments. In present paper nonlinear statistical properties of the Ventral
Tegmental Area (VTA) of genetically depressive limbic brain are studied {\it in
vivo} on the Flinders Sensitive Line of rats (FSL). VTA plays a key role in
generation of pleasure and in development of psychological drug addiction. We
found that the FSL VTA (dopaminergic) neuron signals exhibit multifractal
properties for interspike frequencies on the scales where healthy VTA
dopaminergic neurons exhibit bursting activity. For high moments the observed
multifractal (generalized dimensions) spectrum coincides with the generalized
dimensions spectrum calculated for a spectral measure of a {\it quantum} system
(so-called kicked Harper model, actively used as a model of quantum chaos).
This observation can be considered as a first experimental ({\it in vivo})
indication in the favour of the quantum (at least partially) nature of the
brain neurons activity.

<id>
q-bio/0311027v1
<category>
q-bio.NC
<abstract>
We study a mathematical model for ocular dominance patterns (ODPs) in primary
visual cortex. This model is based on the premise that ODP is an adaptation to
minimize the length of intra-cortical wiring. Thus we attempt to understand the
existing ODPs by solving a wire length minimization problem. We divide all the
neurons into two classes: left- and right-eye dominated. We find that
segregation of neurons into monocular regions reduces wire length if the number
of connections to the neurons of the same class (intraocular) differs from the
number of interocular connections. The shape of the regions depends on the
relative fraction of neurons in the two classes. We find that if both classes
are almost equally represented, the optimal ODP consists of interdigitating
stripes. If one class is less numerous than the other, the optimal ODP consists
of patches of the less abundant class surrounded by the neurons of the other
class. We predict that the transition from stripes to patches occurs when the
fraction of neurons dominated by the underrepresented eye is about 40%. This
prediction agrees with the data in macaque and Cebus monkeys. We also study the
dependence of the periodicity of ODP on the parameters of our model.

<id>
q-bio/0311030v1
<category>
q-bio.NC
<abstract>
Two rate code models -- a reconstruction network model and a control model --
of the hippocampal-entorhinal loop are merged. The hippocampal-entorhinal loop
plays a double role in the unified model, it is part of a reconstruction
network and a controller, too. This double role turns the bottom-up information
flow into top-down control like signals. The role of bottom-up filtering is
information maximization, noise filtering, temporal integration and prediction,
whereas the role of top-down filtering is emphasizing, i.e., highlighting or
`paving of the way' as well as context based pattern completion. In the joined
model, the control task is performed by cortical areas, whereas reconstruction
networks can be found between cortical areas. While the controller is highly
non-linear, the reconstruction network is an almost linear architecture, which
is optimized for noise estimation and noise filtering. A conjecture of the
reconstruction network model -- that the long-term memory of the visual stream
is the linear feedback connections between neocortical areas -- is reinforced
by the joined model. Falsifying predictions are presented; some of them have
recent experimental support. Connections to attention and to awareness are
made.

<id>
q-bio/0312025v1
<category>
q-bio.NC
<abstract>
Artificial spike-based computation, inspired by models of computations in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. In this paper,
we study new models for two common instances of such computation,
winner-take-all and coincidence detection. In both cases, very fast convergence
is achieved independent of initial conditions, and network complexity is linear
in the number of inputs.

<id>
q-bio/0312038v1
<category>
q-bio.NC
<abstract>
Random walk methods are used to calculate the moments of negative image
equilibrium distributions in synaptic weight dynamics governed by spike-timing
dependent plasticity (STDP). The neural architecture of the model is based on
the electrosensory lateral line lobe (ELL) of mormyrid electric fish, which
forms a negative image of the reafferent signal from the fish's own electric
discharge to optimize detection of sensory electric fields. Of particular
behavioral importance to the fish is the variance of the equilibrium
postsynaptic potential in the presence of noise, which is determined by the
variance of the equilibrium weight distribution. Recurrence relations are
derived for the moments of the equilibrium weight distribution, for arbitrary
postsynaptic potential functions and arbitrary learning rules. For the case of
homogeneous network parameters, explicit closed form solutions are developed
for the covariances of the synaptic weight and postsynaptic potential
distributions.

<id>
q-bio/0401001v1
<category>
q-bio.NC
<abstract>
Artificial spike-based computation, inspired by models of computation in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. This paper
describes very simple network architectures for k-winners-take-all and
soft-winner-take-all computation using neural oscillators. Fast convergence is
achieved from arbitrary initial conditions, which makes the networks
particularly suitable to track time-varying inputs.

<id>
q-bio/0401009v4
<category>
q-bio.NC
<abstract>
Effects of distractions such as noises and parameter heterogeneity have been
studied on the firing activity of ensemble neurons, each of which is described
by the extended Morris-Lecar model showing the graded persisting firings with
the aid of an included ${\rm Ca}^{2+}$-dependent cation current. Although the
sustained activity of {\it single} neurons is rather robust in a sense that the
activity is realized even in the presence of the distractions, the graded
frequency of sustained firings is vulnerable to them. It has been shown,
however, that the graded persisting activity of {\it ensemble} neurons becomes
much robust to the distractions by the pooling (ensemble) effect. When the
coupling is introduced, the synchronization of firings in ensemble neurons is
enhanced, which is beneficial to firings of target neurons.

<id>
q-bio/0401013v2
<category>
q-bio.NC
<abstract>
The response of a neural cell to an external stimulus can follow one of the
two patterns: Nonresonant neurons monotonously relax to the resting state after
excitation while resonant ones show subthreshold oscillations. We investigate
how do these subthreshold properties of neurons affect their suprathreshold
response. Vice versa we ask: Can we distinguish between both types of neuronal
dynamics using suprathreshold spike trains? The dynamics of neurons is given by
stochastic FitzHugh-Nagumo and Morris-Lecar models with either having a focus
or a node as the stable fixpoint. We determine numerically the spectral power
density as well as the interspike interval density in response to a random
(noise-like) signals. We show that the information about the type of dynamics
obtained from power spectra is of limited validity. In contrast, the interspike
interval density gives a very sensitive instrument for the diagnostics of
whether the dynamics has resonant or nonresonant properties. For the latter
value we formulate a fit formula and use it to reconstruct theoretically the
spectral power density, which coincides with the numerically obtained spectra.
We underline that the renewal theory is applicable to analysis of
suprathreshold responses even of resonant neurons.

<id>
q-bio/0401019v1
<category>
q-bio.NC
<abstract>
The cognitive frame in which most neuropsychological research on the neural
basis of behavior is conducted contains the assumption that brain mechanisms
per se fully suffice to explain all psychologically described phenomena. This
assumption stems from the idea that the brain is made up entirely of material
particles and fields, and that all causal mechanisms must therefore be
formulated solely in terms of properties of these elements. One consequence of
this stance is that psychological terms having intrinsic mentalistic and/or
experiential content (terms such as "feeling," "knowing," and "effort") have
not been included as primary causal factors in neuropsychological research:
insofar as properties are not described in material terms they are deemed
irrelevant to the causal mechanisms underlying brain function. However, the
origin of this demand that experiential realities be excluded from the causal
base is a theory of nature that has been known for more that three quarters of
a century to be fundamentally incorrect. It is explained here why it is
consequently scientifically unwarranted to assume that material factors alone
can in principle explain all causal mechanisms relevant to neuroscience. More
importantly, it is explained how a key quantum effect can be introduced into
brain dynamics in a simple and practical way that provides a rationally
coherent, causally formulated, physics-based way of understanding and using the
psychological and physical data derived from the growing set of studies of the
capacity of directed attention and mental effort to systematically alter brain
function.

<id>
q-bio/0402022v1
<category>
q-bio.NC
<abstract>
We study the spike statistics of neurons in a network with dynamically
balanced excitation and inhibition. Our model, intended to represent a generic
cortical column, comprises randomly connected excitatory and inhibitory leaky
integrate-and-fire neurons, driven by excitatory input from an external
population. The high connectivity permits a mean-field description in which
synaptic currents can be treated as Gaussian noise, the mean and
autocorrelation function of which are calculated self-consistently from the
firing statistics of single model neurons. Within this description, we find
that the irregularity of spike trains is controlled mainly by the strength of
the synapses relative to the difference between the firing threshold and the
post-firing reset level of the membrane potential. For moderately strong
synapses we find spike statistics very similar to those observed in primary
visual cortex.

<id>
q-bio/0402023v1
<category>
q-bio.NC
<abstract>
We review the use of mean field theory for describing the dynamics of dense,
randomly connected cortical circuits. For a simple network of excitatory and
inhibitory leaky integrate-and-fire neurons, we can show how the firing
irregularity, as measured by the Fano factor, increases with the strength of
the synapses in the network and with the value to which the membrane potential
is reset after a spike. Generalizing the model to include conductance-based
synapses gives insight into the connection between the firing statistics and
the high-conductance state observed experimentally in visual cortex. Finally,
an extension of the model to describe an orientation hypercolumn provides
understanding of how cortical interactions sharpen orientation tuning, in a way
that is consistent with observed firing statistics.

<id>
q-bio/0402026v1
<category>
q-bio.NC
<abstract>
Measured responses from visual cortical neurons show that spike times tend to
be correlated rather than exactly Poisson distributed. Fano factors vary and
are usually greater than 1 due to the tendency of spikes being clustered into
bursts. We show that this behavior emerges naturally in a balanced cortical
network model with random connectivity and conductance-based synapses. We
employ mean field theory with correctly colored noise to describe temporal
correlations in the neuronal activity. Our results illuminate the connection
between two independent experimental findings: high conductance states of
cortical neurons in their natural environment, and variable non-Poissonian
spike statistics with Fano factors greater than 1.

<id>
q-bio/0402035v1
<category>
q-bio.NC
<abstract>
Recent experiments revealed that a certain class of inhibitory neurons in the
cerebral cortex make synapses not onto cell bodies but at distal parts of
dendrites of the target neurons, mediating highly nonlinear dendritic
inhibition. We propose a novel form of competitive neural network model that
realizes such dendritic inhibition. Contrary to the conventional lateral
inhibition in neural networks, our dendritic inhibition models don't always
show winner-take-all behaviors; instead, they converge to "I don't know" states
when unknown input patterns are presented. We derive reduced two-dimensional
dynamics for the network, showing that a drastic shift of the fixed point from
a winner-take-all state to an "I don't know" state occurs in accordance with
the increase in noise added to the stored patterns. By preventing
misrecognition in such a way, dendritic inhibition networks achieve fine
pattern discrimination, which could be one of the basic computations by
inhibitory connected recurrent neural networks in the brain.

<id>
q-bio/0402045v1
<category>
q-bio.NC
<abstract>
The performance of the Hopfield neural network model is numerically studied
on various complex networks, such as the Watts-Strogatz network, the
Barab{\'a}si-Albert network, and the neuronal network of the C. elegans.
Through the use of a systematic way of controlling the clustering coefficient,
with the degree of each neuron kept unchanged, we find that the networks with
the lower clustering exhibit much better performance. The results are discussed
in the practical viewpoint of application, and the biological implications are
also suggested.

<id>
q-bio/0403031v1
<category>
q-bio.NC
<abstract>
The melodic consonance of a sequence of tones is explained using the overtone
series: the overtones form "flow lines" that link the tones melodically; the
strength of these flow lines determines the melodic consonance. This hypothesis
admits of psychoacoustical and neurophysiological interpretations that fit well
with the place theory of pitch perception. The hypothesis is used to create a
model for how the auditory system judges melodic consonance, which is used to
algorithmically construct melodic sequences of tones.

<id>
q-bio/0403037v1
<category>
q-bio.NC
<abstract>
We present a complete mean field theory for a balanced state of a simple
model of an orientation hypercolumn. The theory is complemented by a
description of a numerical procedure for solving the mean-field equations
quantitatively. With our treatment, we can determine self-consistently both the
firing rates and the firing correlations, without being restricted to specific
neuron models. Here, we solve the analytically derived mean-field equations
numerically for integrate-and-fire neurons. Several known key properties of
orientation selective cortical neurons emerge naturally from the description:
Irregular firing with statistics close to -- but not restricted to -- Poisson
statistics; an almost linear gain function (firing frequency as a function of
stimulus contrast) of the neurons within the network; and a contrast-invariant
tuning width of the neuronal firing. We find that the irregularity in firing
depends sensitively on synaptic strengths. If Fano factors are bigger than 1,
then they are so for all stimulus orientations that elicit firing. We also find
that the tuning of the noise in the input current is the same as the tuning of
the external input, while that for the mean input current depends on both the
external input and the intracortical connectivity.

<id>
q-bio/0404021v1
<category>
q-bio.NC
<abstract>
We analyze two pulse-coupled resonate-and-fire neurons. Numerical simulation
reveals that an anti-phase state is an attractor of this model. We can
analytically explain the stability of anti-phase states by means of a return
map of firing times, which we propose in this paper. The resultant stability
condition turns out to be quite simple. The phase diagram based on our theory
shows that there are two types of anti-phase states. One of these cannot be
seen in coupled integrate-and-fire models and is peculiar to resonate-and-fire
models. The results of our theory coincide with those of numerical simulations.

<id>
q-bio/0405002v1
<category>
q-bio.NC
<abstract>
We have designed a toy brain and have written computer code that simulates
it. This toy brain is flexible, modular, has hierarchical learning and
recognition, has short and long term memory, is distributed (i.e. has no
central control), is asynchronous, and includes parallel and series processing.
We have simulated the neurons calculating their internal voltages as a function
of time. We include in the simulation the ion pumps of the neurons, the
synapses with glutamate or GABA neurotransmitters, and the delays of the action
pulses in axons and dendrites. We have used known or plausible circuits of real
brains. The toy brain reads books and learns languages using the Hebb
mechanism. Finally, we have related the toy brain with what might be occurring
in a real brain.

<id>
q-bio/0405027v3
<category>
q-bio.NC
<abstract>
We exhibit a mathematical framework to represent the neural dynamics at
cortical level. Our description of neural dynamics with columnar and functional
modularity, named fibre bundle representation (FBM) method, is based both on
neuroscience and informatics, whereas they correspond with the conventional
formulas in statistical physics. In spite of complex interactions in neural
circuitry and various cortical modification rules per models, some significant
factors determine the typical phenomena in cortical dynamics. The FBM
representation method reveals them plainly and gives profit in building or
analyzing the cortical dynamic models. Not only the similarity in formulas, the
cortical dynamics can share the statistical properties with other physical
systems, which validated in primary visual maps. We apply our method to
proposed models in visual map formations, in addition our suggestion using the
lateral interaction scheme. In this paper, we will show that the neural dynamic
procedures can be treated through conventional physics expressions and
theories.

<id>
q-bio/0406050v1
<category>
q-bio.NC
<abstract>
In a recent paper on Cortical Dynamics, Francis and Grossberg raise the
question how visual forms and motion information are integrated to generate a
coherent percept of moving forms? In their investigation of illusory contours
(which are, like Kanizsa squares, mental constructs rather than stimuli on the
retina) they quantify the subjective impression of apparent motion between
illusory contours that are formed by two subsequent stimuli with delay times of
about 0.2 second (called the interstimulus interval ISI). The impression of
apparent motion is due to a back referral of a later experience to an earlier
time in the conscious representation. A model is developed which describes the
state of awareness in the observer in terms of a time dependent Schroedinger
equation to which a second order time derivative is added. This addition
requires as boundary conditions the values of the solution both at the
beginning and after the process. Satisfactory quantitative agreement is found
between the results of the model and the experimental results. We recall that
in the von Neumann interpretation of the collapse of the quantum mechanical
wave-function, the collapse was associated with an observer's awareness. Some
questions of causality and determinism that arise from later-time boundary
conditions are touched upon.

<id>
q-bio/0312024v1
<category>
q-bio.OT
<abstract>
It has been suggested by several contributors that nonlinear excitations, in
particular solitary waves, could play a fundamental functional role in the
process of DNA transcription, effecting the opening of the double chain needed
for RNA Polymerase to be able to copy the genetic code. Some models have been
proposed to model the relevant DNA dynamics in terms of a reduced number of
effective degrees of freedom. Here I discuss advantages and disadvantages of
such an approach, and discuss in more detail one of the models, i.e. the one
proposed by Yakushevich.

<id>
q-bio/0312031v2
<category>
q-bio.OT
<abstract>
Numerous problems connected with an assumption of the life origin on the
Earth do not arise on Galilean satellites. Here, in presence of a practically
non-salt water and of a great deal (~5-10%) of abiogenic organics, a great
diversity of conditions, which are unthinkable for the Earth, were realized
more than once. They were caused by global electrochemical processes in the
magnetic field presence what could entail an absolute enantiomeric synthesis.
The subsequent explosions of the satellites' icy envelopes saturated by the
electrolysis products resulted in appearance of hot massive atmospheres and
warm deep oceans and ejection of the dirty ice fragments (=comet nuclei), what
led to the material exchange with other bodies, etc.

<id>
q-bio/0406026v1
<category>
q-bio.OT
<abstract>
A healthy human brain is perfused with blood flowing laminarly through
cerebral vessels, providing brain tissue with substrates such as oxygen and
glucose. Under normal conditions, cerebral blood flow is controlled by
autoregulation as well as metabolic, chemical and neurogenic regulation.
Physiological complexity of these mechanisms invariably leads to a question as
to what are the relations between the statistical properties of arterial and
intracranial pressure fluctuations. To shed new light on cerebral hemodynamics,
we employ a complex continuous wavelet transform to determine the instantaneous
phase difference between the arterial blood pressure (ABP) and intracranial
pressure (ICP) in patients with traumatic brain injuries or spontaneous
cerebral hemorrhage. For patients with mild to moderate injury, the phase
difference slowly evolves in time. However, severe neurological injury with
elevated ICP are herein associated with synchronization of arterial and
intracranial pressure. We use Shannon entropy to quantify the stability of
ABP-ICP phase difference and discuss the clinical applicability of such measure
to assessment of cerebrovascular reactivity and autoregulation integrity.

<id>
q-bio/0406045v2
<category>
q-bio.OT
<abstract>
Carcinogenesis is a complex process that involves dynamically inter-connected
modular sub-networks that evolve under the influence of micro-environmentally
induced perturbations, in non-random, pseudo-Markov chain processes. An
appropriate n-stage model of carcinogenesis involves therefore n-valued Logic
treatments of nonlinear dynamic transformations of complex functional genomes
and cell interactomes. Lukasiewicz Algebraic Logic models of genetic networks
and signaling pathways in cells are formulated in terms of nonlinear dynamic
systems with n-state components that allow for the generalization of previous,
Boolean or "fuzzy", logic models of genetic activities in vivo. Such models are
then applied to cell transformations during carcinogenesis based on very
extensive genomic transcription and translation data from the CGAP databases
supported by NCI. Such models are represented in a Lukasiewicz-Topos with an
n-valued Lukasiewicz Algebraic Logics subobject classifier description that
represents non-random and nonlinear network activities as well as their
transformations in carcinogeness. Specific models for different types of cancer
are then derived from representations of the dynamic state-space of LT
non-random, pseudo-Markov chain process, network models in terms of cDNA and
proteomic, high throughput analyses by ultra-sensitive techniques. This novel
theoretical analysis is based on extensive CGAP genomic data for human tumors,
as well as recently published studies of cyclin signaling. Several such
specific models suggest novel clinical trials and rational therapies of cancer
through re-establishment of cell cycling inhibition in stage III cancers.

<id>
q-bio/0406047v1
<category>
q-bio.OT
<abstract>
Selected applications of novel techniques in Agricultural Biotechnology,
Health Food formulations and Medical Biotechnology are being reviewed with the
aim of unraveling future developments and policy changes that are likely to
open new niches for Biotechnology and prevent the shrinking or closing the
existing ones. Amongst the selected novel techniques with applications to both
Agricultural and Medical Biotechnology are: immobilized bacterial cells and
enzymes, microencapsulation and liposome production, genetic manipulation of
microorganisms, development of novel vaccines from plants, epigenomics of
mammalian cells and organisms, as well as biocomputational tools for molecular
modeling related to disease and Bioinformatics. Both fundamental and applied
aspects of the emerging new techniques are being discussed in relation to their
anticipated impact on future biotechnology applications together with policy
changes that are needed for continued success in both Agricultural and Medical
Biotechnology. Several novel techniques are illustrated in an attempt to convey
the most representative and powerful tools that are currently being developed
for both immediate and long term applications in Agriculture, Health Food
formulation and production, pharmaceuticals and Medicine. The research aspects
are naturally emphasized in our review as they are key to further developments
in Medical and Agricultural Biotechnology.

<id>
q-bio/0407005v1
<category>
q-bio.OT
<abstract>
The response of the immune system to different vaccination patterns is
studied with a simple model. It is argued that the history and characteristics
of the pattern defines very different secondary immune responses in the case of
infection. The memory function of the immune response can be set to work in
very different modes depending on the pattern followed during immunizations. It
is argued that the history and pattern of immunizations can be a decisive (and
experimentally accessible) factor to tailor the effectiveness of a specific
vaccine.

<id>
q-bio/0409001v1
<category>
q-bio.OT
<abstract>
This paper traces the seminal roles that physicists and mathematicians have
played in the conceptual development of the biological sciences in the past,
and especially in the 19th and 20th centuries.

<id>
q-bio/0410001v1
<category>
q-bio.OT
<abstract>
We present the first systematic evidence for the origins of 1/f-type temporal
scaling in human heart rate. The heart rate is regulated by the activity of two
branches of the autonomic nervous system: the parasympathetic (PNS) and the
sympathetic (SNS) nervous systems. We examine alterations in the scaling
property when the balance between PNS and SNS activity is modified, and find
that the relative PNS suppression by congestive heart failure results in a
substantial increase in the Hurst exponent H towards random walk scaling
$1/f^{2}$ and a similar breakdown is observed with relative SNS suppression by
primary autonomic failure. These results suggest that 1/f scaling in heart rate
requires the intricate balance between the antagonistic activity of PNS and
SNS.

<id>
q-bio/0503002v1
<category>
q-bio.OT
<abstract>
In this article we use global and regional data from the SARS epidemic in
conjunction with a model of susceptible, exposed, infective, diagnosed, and
recovered classes of people (``SEIJR'') to extract average properties and rate
constants for those populations. The model is fitted to data from the Ontario
(Toronto) in Canada, Hong Kong in China and Singapore outbreaks and predictions
are made based on various assumptions and observations, including the current
effect of isolating individuals diagnosed with SARS. The epidemic dynamics for
Hong Kong and Singapore appear to be different from the dynamics in Toronto,
Ontario. Toronto shows a very rapid increase in the number of cases between
March 31st and April 6th, followed by a {\it significant} slowing in the number
of new cases. We explain this as the result of an increase in the diagnostic
rate and in the effectiveness of patient isolation after March 26th. Our best
estimates are consistent with SARS eventually being contained in Toronto,
although the time of containment is sensitive to the parameters in our model.
It is shown that despite the empirically modeled heterogeneity in transmission,
SARS' average reproductive number is 1.2, a value quite similar to that
computed for some strains of influenza \cite{CC2}. Although it would not be
surprising to see levels of SARS infection higher than ten per cent in some
regions of the world (if unchecked), lack of data and the observed
heterogeneity and sensitivity of parameters prevent us from predicting the
long-term impact of SARS.

<id>
q-bio/0503006v1
<category>
q-bio.OT
<abstract>
Despite improved control measures, Ebola remains a serious public health risk
in African regions where recurrent outbreaks have been observed since the
initial epidemic in 1976. Using epidemic modeling and data from two
well-documented Ebola outbreaks (Congo 1995 and Uganda 2000), we estimate the
number of secondary cases generated by an index case in the absence of control
interventions ($R_0$). Our estimate of $R_0$ is 1.83 (SD 0.06) for Congo (1995)
and 1.34 (SD 0.03) for Uganda (2000). We model the course of the outbreaks via
an SEIR (susceptible-exposed-infectious-removed) epidemic model that includes a
smooth transition in the transmission rate after control interventions are put
in place. We perform an uncertainty analysis of the basic reproductive number
$R_0$ to quantify its sensitivity to other disease-related parameters. We also
analyze the sensitivity of the final epidemic size to the time interventions
begin and provide a distribution for the final epidemic size. The control
measures implemented during these two outbreaks (including education and
contact tracing followed by quarantine) reduce the final epidemic size by a
factor of 2 relative the final size with a two-week delay in their
implementation.

<id>
q-bio/0505028v2
<category>
q-bio.OT
<abstract>
Data discretization, also known as binning, is a frequently used technique in
computer science, statistics, and their applications to biological data
analysis. We present a new method for the discretization of real-valued data
into a finite number of discrete values. Novel aspects of the method are the
incorporation of an information-theoretic criterion and a criterion to
determine the optimal number of values. While the method can be used for data
clustering, the motivation for its development is the need for a discretization
algorithm for several multivariate time series of heterogeneous data, such as
transcript, protein, and metabolite concentration measurements. As several
modeling methods for biochemical networks employ discrete variable states, the
method needs to preserve correlations between variables as well as the dynamic
features of the time series. A C++ implementation of the algorithm is available
from the contributors at http://polymath.vbi.vt.edu/discretization .

<id>
q-bio/0506042v1
<category>
q-bio.OT
<abstract>
Sleep is one of the most noticeable and widespread phenomena occurring in
multicellular animals. Nevertheless, no consensus for a theory of its origins
has emerged. In particular, no explicit, quantitative theory exists that
elucidates or distinguishes between the myriad hypotheses proposed for sleep.
Here, we develop a general, quantitative theory for mammalian sleep that
relates many of its fundamental parameters to metabolic rate and body size.
Most mechanisms suggested for the function of sleep can be placed in this
framework, e.g., cellular repair of damage caused by metabolic processes and
cortical reorganization to process sensory input. Our theory leads to
predictions for sleep time, sleep cycle time, and REM (rapid-eye-movement) time
as functions of body and brain mass, and explains, for example, why mice sleep
\~14 hours per day relative to the 3.5 hours per day that elephants sleep. Data
for 96 species of mammals, spanning six orders of magnitude in body size, are
consistent with these predictions and provide strong evidence that time scales
for sleep are set by the brain's, not the whole-body, metabolic rate.

<id>
q-bio/0508024v2
<category>
q-bio.OT
<abstract>
Sensory systems pass information about an animal's environment to higher
nervous system units through sequences of action potentials. When these action
potentials have essentially equivalent waveforms, all information is contained
in the interspike intervals (ISIs) of the spike sequence. We address the
question: How do neural circuits recognize and read these ISI sequences?
  Our answer is given in terms of a biologically inspired neural circuit that
we construct using biologically realistic neurons. The essential ingredients of
the ISI Reading Unit (IRU) are (i) a tunable time delay circuit modelled after
one found in the anterior forebrain pathway of the birdsong system and (ii) a
recently observed rule for inhibitory synaptic plasticity. We present a circuit
that can both learn the ISIs of a training sequence using inhibitory synaptic
plasticity and then recognize the same ISI sequence when it is presented on
subsequent occasions. We investigate the ability of this IRU to learn in the
presence of two kinds of noise: jitter in the time of each spike and random
spikes occurring in the ideal spike sequence. We also discuss how the circuit
can be detuned by removing the selected ISI sequence and replacing it by an ISI
sequence with ISIs drawn from a probability distribution.
  We have investigated realizations of the time delay circuit using
Hodgkin-Huxley conductance based neurons connected by realistic excitatory and
inhibitory synapses. Our models for the time delay circuit are tunable from
about 10 ms to 100 ms allowing one to learn and recognize ISI sequences within
that range of ISIs. ISIs down to a few ms and longer than 100 ms are possible
with other intrinsic and synaptic currents in the component neurons.

<id>
q-bio/0511001v1
<category>
q-bio.OT
<abstract>
Global assessments have shown that future climate change is likely to
significantly impact forest ecosystems. The present study makes an assessment
of the impact of projected climate change on forest ecosystems in India. This
assessment is based on climate projections of Regional Climate Model of the
Hadley Centre (HadRM3) using the A2 (740 ppm CO2) and B2 (575 ppm CO2)
scenarios of Special Report on Emissions Scenarios and the BIOME4 vegetation
response model. The main conclusion is that under the climate projection for
the year 2085, 77% and 68% of the forested grids in India are likely to
experience shift in forest types under A2 and B2 scenario, respectively.
Indications are a shift towards wetter forest types in the northeastern region
and drier forest types in the northwestern region in the absence of human
influence. Increasing atmospheric CO2 concentration and climate warming could
also result in a doubling of net primary productivity under the A2 scenario and
nearly 70% increase under the B2 scenario. The trends of impacts could be
considered as robust but the magnitudes should be viewed with caution, due to
the uncertainty in climate projections. Given the projected trends of likely
impacts of climate change on forest ecosystems, it is important to incorporate
climate change consideration in forest sector long-term planning process.

<id>
q-bio/0512016v1
<category>
q-bio.OT
<abstract>
Intrinsic transcriptional noise induced by operator fluctuations is
investigated with a simple spin like stochastic model. The effects of
transcriptional fluctuations in protein synthesis is probed by coupling
transcription and translation by an amplificative interaction. In the presence
of repression a new term contributes to the noise which depends on the rate of
mRNA production. If the switching time is small compared with the mRNA life
time the noise is also small. In general the dumping of protein production by a
repressive agent occurs linearly but the fluctuations can show a maxima at
intermediate repression. The discrepancy between the switching time, the mRNA
degradation and protein degradation is crucial for the repressive control in
translation without large fluctuations. The noise profiles obtained here are in
quantitative agreement with recent experiments.

<id>
q-bio/0601016v1
<category>
q-bio.OT
<abstract>
The effects of pure multiplicative noise on stochastic resonance in an
anti-tumor system modulated by a seasonal external field are investigated by
using theoretical analyses of the generalized potential and numerical
simulations. For optimally selected values of the multiplicative noise
intensity quasi-symmetry of two potential minima and stochastic resonance are
observed. Theoretical results and numerical simulations are in good
quantitative agreement.

<id>
q-bio/0601040v1
<category>
q-bio.OT
<abstract>
We have proposed the neck linker swing model to investigate the mechanism of
mechanochemical coupling of kinesin. The Michaelis-Menten-like curve for
velocity vs ATP concentration at different loads has been obtained, which is in
agreement with experiments. We have predicted that Michaelis constant doesn't
increase monotonically and an elastic instability will happen with increasing
of applied force.

<id>
q-bio/0602017v1
<category>
q-bio.OT
<abstract>
It has been observed that male mice who are consistently winning fights with
conspecifics can raise their tail, which is very similar to a morphine-induced
Straub tail response. Since this response is a typical index of opiate
activation, it has been proposed that the opioidergic systems of such mice are
chronically activated. This activation appeared to be a potent factor, which
leads to addiction to aggression. To check this hypothesis, we exposed the mice
who had won 20 fights in succession with conspecifics to a behavioral
sensitization procedure. The effects of the mu-opioid receptor agonist morphine
(10 mg/kg, i.p.) on the behavior of winners were examined in an open-field test
before and after 5- and 14-day deprivation of aggression. Morphine had a much
stronger stimulating effect on the open-field behavior of 60 % of the winners
deprived of aggression for 14 days than on that of the control mice. Morphine
did not stimulate behavioral activity in the winners before or after
deprivation for 5 days. The aggression level in the winners was higher after
than before deprivation. It has been concluded that, in the winners, the
mu-opioid receptors became tolerant to the effects of morphine and became
sensitized after long aggression deprivation. The development of addiction to
aggression due to repeated victories is discussed in the light of the theory of
addiction (Robinson,Berridge, 2003).

<id>
q-bio/0605029v1
<category>
q-bio.OT
<abstract>
Ammonoids are a group of extinct mollusks belonging to the same class of the
living genus Nautilus (Cephalopoda). In both Nautili and ammonoids, the
(usually planospiral) shell is divided into chambers separated by septa that
during the lifetime were filled with gas at atmospheric pressure. The
intersection of septa with the external shell generates a curve called the
suture line, which in living and most fossil Nautili is fairly uncomplicated.
In contrast, suture lines of ancient ammonoid were gently curved and during the
evolution of the group became highly complex, in some cases so extensively
frilled to be considerable as fractal curves. Numerous theories have been put
forward to explain the complexity of suture ammonoid lines. Calculations
presented here lend support to the hypothesis that complex suture lines aided
in counteracting the effect of the external water pressure. Additionally, it is
found that complex suture lines diminished shell shrinkage caused by water
pressure, and thus aided improve buoyancy. Understanding the reason for complex
sutures in ammonoids does not only represent an important issue in
paleobiology, but is also a challenging problem in the resistance of complex
mechanical structures subjected to high pressure.

<id>
q-bio/0606024v4
<category>
q-bio.OT
<abstract>
In the last century mercury levels in the global environment have tripled as
a result of increased pollution from industrial, occupational, medicinal and
domestic uses \cite{BaMe03}. Glutathione is known to be the main agent
responsible for the excretion of mercury (we refer to \cite{Thim05},
\cite{ZalBar99} and \cite{Lyn02}). It has also been shown that mercury inhibits
glutathione synthetase (an enzyme acting in the synthesization of Glutathione),
therefore leading to decreased glutathione levels
  (we refer to \cite{Thim05}, \cite{GeGe05}, \cite{GeGe06} and \cite{RDeth04}).
Mercury also interferes with the production of heme in the porphyrin pathway
\cite{WoMaEc93}. Heme is needed for biological energy production and ability to
detox organic toxins via the P450 enzymes \cite{Boy06}. The purpose of this
paper is to show that body's response to mercury exposure is hysteretic, i.e.
when this feedback of mercury on its main detoxifying agents is strong enough
then mercury body burden has two points of equilibrium: one with normal
abilities to detoxify and low levels of mercury and one with inhibited
abilities to detoxify and high levels of mercury. Furthermore, a small increase
of body's mercury burden may not be sufficient to trigger observable neurotoxic
effects but it may be sufficient to act as a switch leading to an accumulation
of mercury in the body through environmental exposure until its toxicity
becomes manifest.

<id>
q-bio/0606038v2
<category>
q-bio.OT
<abstract>
The Metropolis implementation of the Monte Carlo algorithm has been developed
to study the equilibrium thermodynamics of many-body systems. Choosing small
trial moves, the trajectories obtained applying this algorithm agree with those
obtained by Langevin's dynamics. Applying this procedure to a simplified
protein model, it is possible to show that setting a threshold of 1 degree on
the movement of the dihedrals of the protein backbone in a single Monte Carlo
step, the mean quantities associated with the off-equilibrium dynamics (e.g.,
energy, RMSD, etc.) are well reproduced, while the good description of higher
moments requires smaller moves. An important result is that the time duration
of a Monte Carlo step depends linearly on the temperature, something which
should be accounted for when doing simulations at different temperatures.

<id>
q-bio/0608011v1
<category>
q-bio.OT
<abstract>
Cicadas (Homoptera:Cicadidae) are insects able to produce loudly songs and it
is known that the mechanism to produce sound of tymballing cicadas works as a
Helmholtz resonator. In this work we offer evidence on the participation of the
wings in a high quality resonating process which defines the details of the
acoustic properties of the calling song. The study is carry on \textit{Quesada
gigas} species and it is dividied in three stages: (i) the acoustical
characterization of the abdominal cavity, (ii) the record and calculation of
frequency spectrum of the calling song, and (iii) the measurement of the
vibration modes of the wings. The comparison between all the results
unequivocally show the dramatic influence of the wings in the moment in which
the insect emits its calling song.

<id>
q-bio/0610044v1
<category>
q-bio.OT
<abstract>
In a certain way, this paper presents the continuation of the previous one
which discussed the harmonic structure of the genetic code (Rakocevic, 2004).
Several new harmonic structures presented in this paper, through specific unity
and coherence, together with the previously presented (Rakocevic, 2004), show
that it makes sense to understand genetic code as a set of several different
harmonic structures. Thereby, the harmonicity itself represents a specific
unity and coherence of physico-chemical properties of amino acid molecules and
the number of atoms and/or nucleons in the molecules themselves (in the form of
typical balances). A specific Gauss' arithmetical algorithm has the central
position among all these structures and it corresponds to the patterns of the
number of atoms within the side chains of amino acid molecules in the following
sense: G+V = 11; P+I = 21; S+T+L+A+G = 31; D+E+M+C+P = 41; K+R+Q+N+V = 61;
F+Y+W+H+I = 71; (L+M+Q+W) + (A+C+N+H) = 81; (S+D+K+F) + (T+E+R+Y) = 91;
(F+L+M+S+P) = (T+A+Y+H+I) = (Q+N+K+D+V) = (E+C+W+R+G) = 51. Bearing in mind all
these regularities it make sense to talk about genetic code as a harmonic
system. On the other hand, such an order provides new evidence supporting the
hypothesis established in the previous paper (Rakocevic, 2004) that genetic
code has been complete from the very beginning and as such was the condition
for the origin and evolution of life.

<id>
q-bio/0611009v1
<category>
q-bio.OT
<abstract>
The possible role of quantum effects in transfer of genetic information is
studied. It's argued that the nucleotides selection during DNA replication is
performed by means of proton tunneling between nucleotide and DNA-polimerase
bound by hydrogen bonds. Such mechanism is sensitive to the structure of
nucleotide hydrogen bonds, consequently only one nucleotide sort is captured by
DNA-polimerase in each event.The algorithm of this multistep selection
mechanism is also analysed from the point of its optimality. It's shown that
it's equivalent to Grover algorithm of data base search.

<id>
q-bio/0611034v1
<category>
q-bio.OT
<abstract>
In order to face environmental constraints, trees are able to re-orient their
axes by controlling the stress level in the newly formed wood layers.
Angiosperms and gymnosperms evolved into two distinct mechanisms: the former
produce a wood with large tension pre-stress on the upper side of the tilted
axis, while the latter produce a wood with large compression pre-stress on the
lower side. In both cases, the difference between this stress level and that of
the opposite side, in light tension, generates the bending of the axis.
However, light values of compression were sometimes measured in the opposite
side of angiosperms. By analysing old data on chestnut and mani and new data on
poplar, this study shows that these values were not measurement artefacts. This
reveals that generating light compression stress in opposite wood contributes
to improve the performance of the re-orientation mechanism.

<id>
q-bio/0611041v1
<category>
q-bio.OT
<abstract>
This work presents a mathematical model that establishes an interesting
connection between nucleotide frequencies in human single-stranded DNA and the
famous Fibonacci's numbers. The model relies on two assumptions. First,
Chargaff's second parity rule should be valid, and, second, the nucleotide
frequencies should approach limit values when the number of bases is
sufficiently large. Under these two hypotheses, it is possible to predict the
human nucleotide frequencies with accuracy. It is noteworthy, that the
predicted values are solutions of an optimization problem, which is commonplace
in many nature's phenomena.

<id>
q-bio/0611068v2
<category>
q-bio.OT
<abstract>
We perform geometrization of genetics by representing genetic information by
points of the 4-adic {\it information space.} By well known theorem of number
theory this space can also be represented as the 2-adic space. The process of
DNA-reproduction is described by the action of a 4-adic (or equivalently
2-adic) dynamical system. As we know, the genes contain information for
production of proteins. The genetic code is a degenerate map of codons to
proteins. We model this map as functioning of a polynomial dynamical system.
The purely mathematical problem under consideration is to find a dynamical
system reproducing the degenerate structure of the genetic code. We present one
of possible solutions of this problem.

<id>
q-bio/0612022v2
<category>
q-bio.OT
<abstract>
We discuss the similarity of the degeneration structure of the genetic code
with a pure number theoretic -- ``divisors code.'' The most interesting thing
about our observation is not that there is a connection between number theory
and the genetic code, but the simplicity of the rule. We hope that the
observation and the naive model presented in this paper will serve for ideas to
other models of the degeneracy of the genetic code. Maybe, the ideas of this
article can also be used in the area of artificial life to syntesize artificial
genetic codes.

<id>
q-bio/0701016v1
<category>
q-bio.OT
<abstract>
The contributors study the short-time dynamics of helix-forming polypeptide chains
using an all-atom representation of the molecules and an implicit solvation
model to approximate the interaction with the surrounding solvent. The results
confirm earlier observations that the helix-coil transition in proteins can be
described by a set of critical exponents. The high statistics of the
simulations allows the contributors to determine the exponents values with increased
precision and support universality of the helix-coil transition in homopolymers
and (helical) proteins.

<id>
q-bio/0701033v1
<category>
q-bio.OT
<abstract>
Understanding the cause of the synchronization of population evolution is an
important issue for ecological improvement. Here we present a
Lotka-Volterra-type model driven by two correlated environmental noises and
show, via theoretical analysis and direct simulation, that noise correlation
can induce a synchronization of the mutualists. The time series of mutual
species exhibit a chaotic-like fluctuation, which is independent to the noise
correlation, however, the chaotic fluctuation of mutual species ratio decreases
with the noise correlation. A quantitative parameter defined for characterizing
chaotic fluctuation provides a good approach to measure when the complete
synchronization happens.

<id>
q-bio/0310003v2
<category>
q-bio.PE
<abstract>
The defense response in plants challenged with pathogens is characterized by
the activation of a diverse set of genes. Many of the same genes are induced in
the defense responses of a wide range of plant species. How plant defense gene
families evolve may therefore provide an important clue to our understanding of
how disease resistance evolves. Because studies usually focus on a single host
species, little data are available regarding changes in defense gene expression
patterns as species diverge. The expression of defense-induced genes PR10,
chitinase and chalcone synthase was assayed in four pea species (Pisum sativum,
P. humile, P. elatius and P. fulvum) and two Lathyrus species (L. sativus and
L. tingitanus) which exhibited a range of infection phenotypes with Fusarium
solani . In P. sativum, resistance was accompanied by a strong induction of
defense genes at 8 hr. post-inoculation. Weaker induction was seen in
susceptible interactions in wild species. Divergence in the timing of PR10
expression was most striking between P. sativum and its closest realtive, P.
humile. Two members of this multigene family, designated PR10.1 and PR10.2, are
strongly-expressed in response to Fusarium, while the PR10.3 gene is more
weakly expressed, among Pisum species. The rapidity with which PR10 expression
evolves raises the question, is divergence of defense gene expression a part of
the phenotypic diversity underlying plant/pathogen coevolution?

<id>
q-bio/0310004v1
<category>
q-bio.PE
<abstract>
Hierarchical structure is an essential part of complexity, important notion
relevant for a wide range of applications ranging from biological population
dynamics through robotics to social sciences. In this paper we propose a simple
cellular-automata tool for study of hierarchical population dynamics.

<id>
q-bio/0310006v1
<category>
q-bio.PE
<abstract>
We investigate the process of fixation of advantageous mutations in an
asexual population. We assume that the effect of each beneficial mutation is
exponentially distributed with mean value $\omega_{med}=1/\beta$. The model
also considers that the effect of each new deleterious mutation reduces the
fitness of the organism independent on the previous number of mutations. We use
the branching process formulation and also extensive simulations to study the
model. The agreement between the analytical predictions and the simulational
data is quite satisfactory. Surprisingly, we observe that the dependence of the
probability of fixation $P_{fix}$ on the parameter $\omega_{med}$ is precisely
described by a power-law relation, $P_{fix} \sim \omega_{med}^{\gamma}$. The
exponent $\gamma$ is an increase function of the rate of deleterious mutations
$U$, whereas the probability $P_{fix}$ is a decreasing function of $U$. The
mean value $\omega_{fix}$ of the beneficial mutations which reach ultimate
fixation depends on $U$ and $\omega_{med}$. The ratio
$\omega_{fix}/\omega_{med}$ increases as we consider higher values of mutation
value $U$ in the region of intermediate to large values of $\omega_{med}$,
whereas for low $\omega_{med}$ we observe the opposite behavior.

<id>
q-bio/0310017v1
<category>
q-bio.PE
<abstract>
We study a four species ecological system with cyclic dominance whose
individuals are distributed on a square lattice. Randomly chosen individuals
migrate to one of the neighboring sites if it is empty or invade this site if
occupied by their prey. The cyclic dominance maintains the coexistence of all
the four species if the concentration of vacant sites is lower than a threshold
value. Above the treshold, a symmetry breaking ordering occurs via growing
domains containing only two neutral species inside. These two neutral species
can protect each other from the external invaders (predators) and extend their
common territory. According to our Monte Carlo simulations the observed phase
transition is equivalent to those found in spreading models with two equivalent
absorbing states although the present model has continuous sets of absorbing
states with different portions of the two neutral species. The selection
mechanism yielding symmetric phases is related to the domain growth process
whith wide boundaries where the four species coexist.

<id>
q-bio/0310031v1
<category>
q-bio.PE
<abstract>
Animal mitochondrial genomes usually have two transfer RNAs for Leucine: one,
with anticodon UAG, translates the four-codon family CUN, whilst the other,
with anticodon UAA, translates the two-codon family UUR. These two genes must
differ at the third anticodon position, but in some species the genes differ at
many additional sites, indicating that these genes have been independent for a
long time. Duplication and deletion of genes in mitochondrial genomes occurs
frequently during the evolution of the Metazoa. If a tRNA-Leu gene were
duplicated and a substitution occurred in the anticodon, this would effectively
turn one type of tRNA into the other. The original copy of the second tRNA type
might then be lost by a deletion elsewhere in the genome. There are several
groups of species in which the two tRNA-Leu genes occur next to one another (or
very close) on the genome, which suggests that tandem duplication has occurred.
Here we use RNA-specific phylogenetic methods to determine evolutionary trees
for both genes. We present evidence that the process of duplication, anticodon
mutation and deletion of tRNA-Leu genes has occurred at least five times during
the evolution of the Metazoa - once in the common ancestor of all Protostomes,
once in the common ancestor of Echinoderms and Hemichordates, once in the
hermit crab, and twice independently in Molluscs.

<id>
q-bio/0311002v1
<category>
q-bio.PE
<abstract>
We propose a generic model of eco-systems, with a {\it hierarchical} food web
structure. In our computer simulations we let the eco-system evolve
continuously for so long that that we can monitor extinctions as well as
speciations over geological time scales. {\it Speciation} leads not only to
horizontal diversification of species at any given trophic level but also to
vertical bio-diversity that accounts for the emergence of complex species from
simpler forms of life. We find that five or six trophic levels appear as the
eco-system evolves for sufficiently long time, starting initially from just one
single level. Moreover, the time intervals between the successive collections
of ecological data is so short that we could also study ``micro''-evolution of
the eco-system, i.e., the birth, ageing and death of individual organisms.

<id>
q-bio/0311020v2
<category>
q-bio.PE
<abstract>
Recent work on mutation-selection models has revealed that, under specific
assumptions on the fitness function and the mutation rates, asymptotic
estimates for the leading eigenvalue of the mutation-reproduction matrix may be
obtained through a low-dimensional maximum principle in the limit N to infinity
(where N is the number of types). In order to extend this variational principle
to a larger class of models, we consider here a family of reversible N by N
matrices and identify conditions under which the high-dimensional Rayleigh-Ritz
variational problem may be reduced to a low-dimensional one that yields the
leading eigenvalue up to an error term of order 1/N. For a large class of
mutation-selection models, this implies estimates for the mean fitness, as well
as a concentration result for the ancestral distribution of types.

<id>
q-bio/0311035v1
<category>
q-bio.PE
<abstract>
A simulation approach to the stochastic growth of bacterial towers is
presented, in which a non-uniform and finite nutrient supply essentially
determines the emerging structure through elementary chemotaxis. The method is
based on cellular automata and we use simple, microscopic, local rules for
bacterial division in nutrient-rich surroundings. Stochastic nutrient
diffusion, while not crucial to the dynamics of the total population, is
influential in determining the porosity of the bacterial tower and the
roughness of its surface. As the bacteria run out of food, we observe an
exponentially rapid saturation to a carrying capacity distribution, similar in
many respects to that found in a recently proposed phenomenological
hierarchical population model, which uses heuristic parameters and macroscopic
rules. Complementary to that phenomenological model, the simulation aims at
giving more microscopic insight into the possible mechanisms for one of the
recently much studied bacterial morphotypes, known as "towering biofilm",
observed experimentally using confocal laser microscopy. A simulation
suggesting a mechanism for biofilm resistance to antibiotics is also shown.

<id>
q-bio/0312014v3
<category>
q-bio.PE
<abstract>
The evolutionary persistence of symbiotic associations is a puzzle.
Adaptation should eliminate cooperative traits if it is possible to enjoy the
advantages of cooperation without reciprocating - a facet of cooperation known
in game theory as the Prisoner's Dilemma. Despite this barrier, symbioses are
widespread, and may have been necessary for the evolution of complex life. The
discovery of strategies such as tit-for-tat has been presented as a general
solution to the problem of cooperation. However, this only holds for
within-species cooperation, where a single strategy will come to dominate the
population. In a symbiotic association each species may have a different
strategy, and the theoretical analysis of the single species problem is no
guide to the outcome. We present basic analysis of two-species cooperation and
show that a species with a fast adaptation rate is enslaved by a slowly
evolving one. Paradoxically, the rapidly evolving species becomes highly
cooperative, whereas the slowly evolving one gives little in return. This helps
understand the occurrence of endosymbioses where the host benefits, but the
symbionts appear to gain little from the association.

<id>
q-bio/0312016v1
<category>
q-bio.PE
<abstract>
A simplified susceptible-infected-recovered (SIR) epidemic model and a
small-world model are applied to analyse the spread and control of Severe Acute
Respiratory Syndrome (SARS) for Hong Kong in early 2003. From data available in
mid April 2003, we predict that SARS would be controlled by June and nearly
1700 persons would be infected based on the SIR model. This is consistent with
the known data. A simple way to evaluate the development and efficacy of
control is described and shown to provide a useful measure for the future
evolution of an epidemic. This may contribute to improve strategic response
from the government. The evaluation process here is universal and therefore
applicable to many similar homogeneous epidemic diseases within a fixed
population. A novel model consisting of map systems involving the Small-World
network principle is also described. We find that this model reproduces
qualitative features of the random disease propagation observed in the true
data. Unlike traditional deterministic models, scale-free phenomena are
observed in the epidemic network. The numerical simulations provide theoretical
support for current strategies and achieve more efficient control of some
epidemic diseases, including SARS.

<id>
q-bio/0312029v1
<category>
q-bio.PE
<abstract>
Using daily infection data for Hong Kong we explore the validity of a variety
of models of disease propagation when applied to the SARS epidemic. Surrogate
data methods show that simple random models are insufficient and that the
standard epidemic susceptible-infected-removed model does not fully account for
the underlying variability in the observed data. As an alternative, we consider
a more complex small world network model and show that such a structure can be
applied to reliably produce simulations quantitative similar to the true data.
The small world network model not only captures the apparently random
fluctuation in the reported data, but can also reproduces mini-outbreaks such
as those caused by so-called ``super-spreaders'' and in the Hong Kong housing
estate Amoy Gardens.

<id>
q-bio/0401023v1
<category>
q-bio.PE
<abstract>
We analyze the properties of model food webs and of fifteen community food
webs from a variety of environments. We first perform a theoretical analysis of
the niche model of Williams and Martinez. We derive analytical expressions for
the distributions of species' number of prey, number of predators, and total
number of trophic links and find that they follow universal functional forms.
We also derive expressions for a number of other biologically relevant
parameters, including the fraction of top, intermediate, basal, and cannibal
species, the standard deviations of generality and vulnerability, the
correlation coefficient between species' number of prey and number of
predators, and assortativity. We show that our findings are robust under rather
general conditions. We then use our analytical predictions as a guide to the
analysis of fifteen of the most complete empirical food webs available. We
uncover quantitative unifying patterns that describe the properties of the
model food webs and most of the trophic webs considered. Our results support a
strong new hypothesis that the empirical distributions of number of prey and
number of predators follow universal functional forms that, without free
parameters, match our analytical predictions. Further, we find that the
empirically observed correlation coefficient, assortativity, and fraction of
cannibal species are consistent with our analytical expressions and simulations
of the niche model. Finally, we show that the average distance between nodes
and the average clustering coefficient show a high degree of regularity for
both the empirical data and simulations of the niche model. Our findings
suggest that statistical physics concepts such as scaling and universality may
be useful in the description of natural ecosystems.

<id>
q-bio/0401035v1
<category>
q-bio.PE
<abstract>
A fundamental problem in evolutionary ecology research is to explain how
different species coexist in natural ecosystems. This question is directly
related with species trophic competition. However, competition theory, based on
the classical logistic Lotka-Volterra equations, leads to erroneous conclusions
about species coexistence. The reason for this is incorrectly interpreted
interspecific interactions, expressed in the form of the competition
coefficients. Here I use the logistic Lotka-Volterra type competition equations
derived from the so called resource competition models to obtain the necessary
conditions for species coexistence. These models show that only species with
identical competitive abilities may coexist. Due to such relations between
competing species ecosystems biodiversity decreases in the course of evolution.

<id>
q-bio/0402007v4
<category>
q-bio.PE
<abstract>
It is possible to consider stochastic models of sequence evolution in
phylogenetics in the context of a dynamical tensor description inspired from
physics. Approaching the problem in this framework allows for the well
developed methods of mathematical physics to be exploited in the biological
arena. We present the tensor description of the homogeneous continuous time
Markov chain model of phylogenetics with branching events generated by
dynamical operations. Standard results from phylogenetics are shown to be
derivable from the tensor framework. We summarize a powerful approach to
entanglement measures in quantum physics and present its relevance to
phylogenetic analysis. Entanglement measures are found to give distance
measures that are equivalent to, and expand upon, those already known in
phylogenetics. In particular we make the connection between the group invariant
functions of phylogenetic data and phylogenetic distance functions. We
introduce a new distance measure valid for three taxa based on the group
invariant function known in physics as the "tangle". All work is presented for
the homogeneous continuous time Markov chain model with arbitrary rate
matrices.

<id>
q-bio/0402009v3
<category>
q-bio.PE
<abstract>
In large asexual populations, beneficial mutations have to compete with each
other for fixation. Here, I derive explicit analytic expressions for the rate
of substitution and the mean beneficial effect of fixed mutations, under the
assumptions that the population size N is large, that the mean effect of new
beneficial mutations is smaller than the mean effect of new deleterious
mutations, and that new beneficial mutations are exponentially distributed. As
N increases, the rate of substitution approaches a constant, which is equal to
the mean effect of new beneficial mutations. The mean effect of fixed mutations
continues to grow logarithmically with N. The speed of adaptation, measured as
the change of log fitness over time, also grows logarithmically with N for
moderately large N, and it grows double-logarithmically for extremely large N.
Moreover, I derive a simple formula that determines whether at given N
beneficial mutations are expected to compete with each other or go to fixation
independently. Finally, I verify all results with numerical simulations.

<id>
q-bio/0402010v1
<category>
q-bio.PE
<abstract>
RNA viruses are a widely used tool to study evolution experimentally. Many
standard protocols of virus propagation and competition are done at nominally
low multiplicity of infection (m.o.i.), but lead during one passage to two or
more rounds of infection, of which the later ones are at high m.o.i. Here, we
develop a model of the competition between wild type (wt) and a mutant under a
regime of alternating m.o.i. We assume that the mutant is deleterious when it
infects cells on its own, but derives a selective advantage when rare and
coinfecting with wt, because it can profit from superior protein products
created by the wt. We find that, under these assumptions, replication at
alternating low and high m.o.i. may lead to the stable coexistence of wt and
mutant for a wide range of parameter settings. The predictions of our model are
consistent with earlier observations of frequency-dependent selection in VSV
and HIV-1. Our results suggest that frequency-dependent selection may be common
in typical evolution experiments with viruses.

<id>
q-bio/0402011v2
<category>
q-bio.PE
<abstract>
Trends in human longevity are puzzling, especially when considering the
limits of human longevity. Partially, the conflicting assertions are based upon
demographic evidence and the interpretation of survival and mortality curves
using the Gompertz model and the Weibull model; these models are sometimes
considered to be incomplete in describing the entire curves. In this paper a
new model is proposed to take the place of the traditional models. We directly
analysed the rectangularity (the parts of the curves being shaped like a
rectangle) of survival curves for 17 countries and for 1876-2001 in Switzerland
(it being one of the longest-lived countries) with a new model. This model is
derived from the Weibull survival function and is simply described by two
parameters, in which the shape parameter indicates 'rectangularity' and
characteristic life indicates the duration for survival to be 'exp(-1)'. The
shape parameter is essentially a function of age and it distinguishes humans
from technical devices. We find that although characteristic life has increased
up to the present time, the slope of the shape parameter for middle age has
been saturated in recent decades and that the rectangularity above
characteristic life has been suppressed, suggesting there are ultimate limits
to human longevity. The new model and subsequent findings will contribute
greatly to the interpretation and comprehension of our knowledge on the human
ageing processes.

<id>
q-bio/0402013v1
<category>
q-bio.PE
<abstract>
General functions for human survival and mortality may support a possibility
of general mechanisms in human ageing. We discovered that the survival and
mortality curves could be described very simply and accurately by the Weibull
survival function with age-dependent shape parameter. The age-dependence of
shape parameter determines the shape of the survival and mortality curves and
tells the nature of the ageing rate. Especially, the progression of shape
parameter with age may be explained by the increase of interaction among vital
processes or the evolution of susceptibility to faults with age. Age-related
diseases may be attributed to the evolution of susceptibility to faults with
age.

<id>
q-bio/0402016v1
<category>
q-bio.PE
<abstract>
In this paper the Penna model is reconsidered. With computer simulations we
check how the control parameters of the model influence the size of the stable
population.

<id>
q-bio/0402019v2
<category>
q-bio.PE
<abstract>
In this paper we study the household-structure SIS epidemic spreading on
general complex networks. The household structure gives us the way to
distinguish inner and the outer infection rate. Unlike household-structure
models on homogenous networks, such as regular and random networks, here we
consider heterogeneous networks with arbitrary degree distribution p(k). First
we introduce the epidemic model. Then rate equations under mean field
appropriation and computer simulations are used here to analyze our model. Some
unique phenomena only existing in divergent network with household structure is
found, while we also get some similar conclusions that some simple geometrical
quantities of networks have important impression on infection property of
infectous disease. It seems that in our model even when local cure rate is
greater than inner infection rate in every household, disease still can spread
on scale-free network. It implies that no disease is spreading in every single
household, but for the whole network, disease is spreading. Since our society
network seems like this structure, maybe this conclusion remind us that during
disease spreading we should pay more attention on network structure than local
cure condition.

<id>
q-bio/0402028v1
<category>
q-bio.PE
<abstract>
We study the effect of mutations in a simple model of colonization, based on
Montecarlo simulations. When the population colonizes the whole available
habitat, a maximum population density is reached, which depends on the mutation
rate. Depending on the values of other parameters, such as selection pressure,
fecundity and mobility, there is an optimal value for the mutation rate for
which the colonization reaches the highest density. We also investigate the
survival probabilities under different conditions and its relation to the
mutation rate.

<id>
q-bio/0402034v2
<category>
q-bio.PE
<abstract>
We wish to verify that the mortality deceleration (or decrease) is a
consequence of the bending of the shape parameter at old ages. This
investigation is based upon the Weon model (the Weibull model with an
age-dependent shape parameter) for human survival and mortality curves.
According to the Weon model, we are well able to describe the mortality
decrease after the mortality plateau, including the mortality deceleration.
Furthermore, we are able to simply define the mathematical limit of longevity
by the mortality decrease. From the demographic analysis of the historical
trends in Switzerland (1876-2001) and Sweden (1861-2001), and the most recent
trends in the other eleven developed countries (1996-2001), we confirm that the
bending of the shape parameter after characteristic life is correlated with the
mortality deceleration (or decrease). As a consequence, this bending of the
shape parameters and the mortality deceleration is associated with the
mathematical limit on longevity. These results suggest that the mathematical
limit of longevity can be induced by the mortality deceleration (or decrease)
in nature. These findings will give us a breakthrough for studying the
mortality dynamics at the highest ages.

<id>
q-bio/0402038v2
<category>
q-bio.PE
<abstract>
A microscopic model is developed, within the frame of the theory of
quantitative traits, to study both numerically and analytically the combined
effect of competition and assortativity on the sympatric speciation process,
i.e. speciation in the absence of geographical barriers. Two components of
fitness are considered: a static one that describes adaptation to environmental
factors not related to the population itself, and a dynamic one that accounts
for interactions between organisms, e.g. competition. The effects of finiteness
of population size on survival of coexisting species are also accounted for.
The simulations show that both in the case of flat and ripid static fitness
landscapes, competition and assortativity do exert a synergistic effect on
speciation. We also show that competition acts as a stabilizing force against
extinction due to random sampling in a finite population. Finally, evidence is
shown that speciation can be seen as a phase transition.

<id>
q-bio/0402042v3
<category>
q-bio.PE
<abstract>
In this paper, we introduce a modified epidemic model on regular and
scale-free networks respectively. We consider the birth rate $\delta$, cure
rate $\gamma$, infection rate $\lambda$, $\alpha$ from the infectious disease,
and death rate $\beta$ from other factors. Through mean-field analysis, we find
that on regular network there is an epidemic threshold $\lambda_{c}$ dependent
on the parameters $\delta$, $\gamma$, $\alpha$, and $\beta$; while for power
law degree distribution network epidemic threshold is absent in the
thermodynamic limit. The result is the same as that of the standard SIS model.
This reminds us the structure of the networks plays a very important role in
the spreading property of the infectious disease.

<id>
q-bio/0403004v2
<category>
q-bio.PE
<abstract>
Noise, through its interaction with the nonlinearity of the living systems,
can give rise to counter-intuitive phenomena such as stochastic resonance,
noise-delayed extinction, temporal oscillations, and spatial patterns. In this
paper we briefly review the noise-induced effects in three different
ecosystems: (i) two competing species; (ii) three interacting species, one
predator and two preys, and (iii) N-interacting species. The transient dynamics
of these ecosystems are analyzed through generalized Lotka-Volterra equations
in the presence of multiplicative noise, which models the interaction between
the species and the environment. The interaction parameter between the species
is random in cases (i) and (iii), and a periodical function, which accounts for
the environmental temperature, in case (ii). We find noise-induced phenomena
such as quasi-deterministic oscillations, stochastic resonance, noise-delayed
extinction, and noise-induced pattern formation with nonmonotonic behaviors of
patterns areas and of the density correlation as a function of the
multiplicative noise intensity. The asymptotic behavior of the time average of
the \emph{$i^{th}$} population when the ecosystem is composed of a great number
of interacting species is obtained and the effect of the noise on the
asymptotic probability distributions of the populations is discussed.

<id>
q-bio/0403010v2
<category>
q-bio.PE
<abstract>
In recent we introduced, developed and established a new concept, model,
methodology and principle for studying human longevity in terms of demographic
basis. We call the new model the "Weon model", which is a general model
modified from the Weibull model with an age-dependent shape parameter to
describe human survival and mortality curves. We demonstrate the application of
the Weon model to the mortality dynamics and the mathematical limit of
longevity (the mortality rate to be mathematically zero, implying a maximum
longevity) in the Section I. The mathematical limit of longevity can be induced
by the mortality dynamics in nature. As a result, we put forward the
complementarity principle, which explains the recent paradoxical trends that
the mathematical limit decreases as the longevity increases, in the Section II.
Our findings suggest that the human longevity can be limited by the
complementarity principle.

<id>
q-bio/0403014v1
<category>
q-bio.PE
<abstract>
We present a model for the growth of West Nile virus in mosquito and bird
populations based on observations of the initial epidemic in the U.S. Increase
of bird mortality as a result of infection, which is a feature of the epidemic,
is found to yield an effect which is observable in principle, viz., periodic
variations in the extent of infection. The vast difference between mosquito and
bird lifespans, another peculiarity of the system, is shown to lead to
interesting consequences regarding delay in the onset of the steady-state
infection. An outline of a framework is provided to treat mosquito diffusion
and bird migration.

<id>
q-bio/0403017v1
<category>
q-bio.PE
<abstract>
Accurate demographic functions help scientists define and understand
longevity. We summarize a new demographic model, the Weon model, and show the
application to the demographic data for Switzerland (1876-2002). Particularly,
the Weon model simply defines the maximum longevity, which is induced in nature
by the mortality dynamics. In this study, we reconsider the definition of the
maximum longevity and the effectiveness for longevity by the combined effect of
the survival and mortality functions. The results suggest that the mortality
function should be zero at the maximum longevity, since the density function is
zero but the survival function is not zero. Furthermore, the effectiveness for
longevity can be maximized at the characteristic life by the complementarity
between the survival and mortality functions, which suggests that there may be
two parts of rectangularization for longevity. The historical trends for
Switzerland (1876-2002) implies that there may be a fundamental limiting force
to restrict the increase of the effectiveness. As a result, it seems that the
density function is essential to define and understand the mortality dynamics,
the maximum longevity, the effectiveness for longevity, the paradigm of
rectangularization and the historical trends of the effectiveness by the
complementarity between the survival and mortality functions.

<id>
q-bio/0403030v1
<category>
q-bio.PE
<abstract>
In this paper, we investigate fitness landscapes (under point mutation and
recombination) from the standpoint of whether the induced evolutionary dynamics
have a "fast-slow" time scale associated with the differences in relaxation
time between local quasi-equilibria and the global equilibrium. This dynamical
behavior has been formally described in the econometrics literature in terms of
the spectral properties of the appropriate operator matrices by Simon and Ando
(1961), and we use the relations they derive to ask which fitness functions and
mutation/recombination operators satisfy these properties. It turns out that
quite a wide range of landscapes satisfy the condition (at least trivially)
under point mutation given a sufficiently low mutation rate, while the property
appears to be difficult to satisfy under genetic recombination. In spite of the
fact that Simon-Ando decomposability can be realized over fairly wide range of
parameters, it imposes a number of restrictions on which landscape
partitionings are possible. For these reasons, the Simon-Ando formalism doesn't
appear to be applicable to other forms of decomposition and aggregation of
variables that are important in evolutionary systems.

<id>
q-bio/0403035v1
<category>
q-bio.PE
<abstract>
A fundamental question in aging research concerns the demographic
trajectories at the highest ages, especially for supercentenarians (persons
aged 110 or more). We wish to demonstrate that the Weon model enables
scientists to describe the demographic trajectories for supercentenarians. We
evaluate the average survival data from the modern eight countries and the
valid and complete data for supercentenarians from the International Database
on Longevity (Robine and Vaupel, (2002) North American Actuarial Journal 6,
54-63). The results suggest that the Weon model predicts the maximum longevity
to exist around ages 120-130, which indicates that there is an intrinsic limit
to human longevity, and that the Weon model allows the best possible
description of the demographic trajectories for supercentenarians.

<id>
q-bio/0309006v1
<category>
q-bio.QM
<abstract>
Background: The availability of high throughput methods for measurement of
mRNA concentrations makes the reliability of conclusions drawn from the data
and global quality control of samples and hybridization important issues. We
address these issues by an information theoretic approach, applied to
discretized expression values in replicated gene expression data.
  Results: Our approach yields a quantitative measure of two important
parameter classes: First, the probability $P(\sigma | S)$ that a gene is in the
biological state $\sigma$ in a certain variety, given its observed expression
$S$ in the samples of that variety. Second, sample specific error probabilities
which serve as consistency indicators of the measured samples of each variety.
The method and its limitations are tested on gene expression data for
developing murine B-cells and a $t$-test is used as reference. On a set of
known genes it performs better than the $t$-test despite the crude
discretization into only two expression levels. The consistency indicators,
i.e. the error probabilities, correlate well with variations in the biological
material and thus prove efficient.
  Conclusions: The proposed method is effective in determining differential
gene expression and sample reliability in replicated microarray data. Already
at two discrete expression levels in each sample, it gives a good explanation
of the data and is comparable to standard techniques.

<id>
q-bio/0309021v1
<category>
q-bio.QM
<abstract>
We report the development and detailed calibration of a multiphoton
fluorescence lifetime imaging system (FLIM) using a streak camera. The present
system is versatile with high spatial (0.2 micron) and temporal (50 psec)
resolution and allows rapid data acquisition and reliable and reproducible
lifetime determinations. The system was calibrated with standard fluorescent
dyes and the lifetime values obtained were in very good agreement with values
reported in literature for these dyes. We also demonstrate the applicability of
the system to FLIM studies in cellular specimens including stained pollen
grains and fibroblast cells expressing green fluorescent protein. The lifetime
values obtained matched well with those reported earlier by other groups for
these same specimens. Potential applications of the present system include the
measurement of intracellular physiology and Fluorescence Resonance Energy
Transfer (FRET) imaging which are discussed in the context of live cell
imaging.

<id>
q-bio/0309023v1
<category>
q-bio.QM
<abstract>
We make available a library of documented IDL .pro files as well as a
shareable object library that allows IDL to call routines from LAPACK. The
routines are for use in the spectral analysis of time series data. The primary
focus of these routines are David Thomson's multitaper methods but a whole
range of functions will be made available in future revisions of the
submission. At present routines are provided to carry out the following
operations: calculate prolate spheroidal sequences and eigenvalues, project
time-series into frequency bands, calculate spectral estimates with or without
moving windows, and calculate the cross-coherence between two time series as a
function of frequency as well as the coherence between frequencies for a single
time series.

<id>
q-bio/0310007v1
<category>
q-bio.QM
<abstract>
We examined the changes in swimming behaviour of the bacterium Rhodobacter
sphaeroides in response to stepwise changes in a nutrient (propionate),
following the prestimulus motion, the initial response and the adaptation to
the sustained concentration of the chemical. This was carried out by tethering
motile cells by their flagella to glass slides and following the rotational
behaviour of their cell bodies in response to the nutrient change. Computerised
motion analysis was used to analyse the behaviour. Distributions of run and
stop times were obtained from rotation data for tethered cells. Exponential and
Weibull fits for these distributions, and variability in individual responses
are discussed. In terms of parameters derived from the run and stop time
distributions, we compare the responses to stepwise changes in the nutrient
concentration and the long-term behaviour of 84 cells under twelve propionate
concentration levels from 1 nM to 25 mM. We discuss traditional assumptions for
the random walk approximation to bacterial swimming and compare them with the
observed R. sphaeroides motile behaviour.

<id>
q-bio/0311022v1
<category>
q-bio.QM
<abstract>
An analysis of the protein content of several crystal forms of proteins has
been performed. We apply a new numerical technique, the Independent Component
Analysis (ICA), to determine the volume fraction of the asymmetric unit
occupied by the protein. This technique requires only the crystallographic data
of structure factors as input.

<id>
q-bio/0311024v1
<category>
q-bio.QM
<abstract>
We introduce, analyze, and implement a new method for parameter
identification for system of ordinary differential equations that are used to
model sets of biochemical reactions. Our method relies on the integral
formulation of the ODE system and a method of linear least squares applied to
the integral equations. Certain variants of this method are also introduced in
this paper.

<id>
q-bio/0402024v1
<category>
q-bio.QM
<abstract>
Microelectromagnet devices, a ring trap and a matrix, were developed for the
microscopic control of biological systems. The ring trap is a circular Au wire
with an insulator on top. The matrix has two arrays of straight Au wires, one
array perpendicular to the other, that are separated and topped by insulating
layers. Microelectromagnets can produce strong magnetic fields to stably
manipulate magnetically tagged biological systems in a fluid. Moreover, by
controlling the currents flowing through the wires, a microelectromagnet matrix
can move a peak in the magnetic field magnitude continuously over the surface
of the device, generate multiple peaks simultaneously and control them
independently. These capabilities of a matrix can be used to trap, continuously
transport, assemble, separate and sort biological samples on micrometer length
scales. Combining microelectromagnets with microfluidic systems, chip-based
experimental systems can be realized for novel applications in biological and
biomedical studies.

<id>
q-bio/0402030v1
<category>
q-bio.QM
<abstract>
A barrier penetration model has been proposed to explain the spontaneous
melting of the DNA oligomers into two separate single strands whereas the
partially melted intermediate states are shown to be the bound state solution
of the same effective potential that generates the barrier.

<id>
q-bio/0406002v1
<category>
q-bio.QM
<abstract>
Large numbers of MS/MS peptide spectra generated in proteomics experiments
require efficient, sensitive and specific algorithms for peptide
identification. In the Open Mass Spectrometry Search Algorithm [OMSSA],
specificity is calculated by a classic probability score using an explicit
model for matching experimental spectra to sequences. At default thresholds,
OMSSA matches more spectra from a standard protein cocktail than a comparable
algorithm. OMSSA is designed to be faster than published algorithms in
searching large MS/MS datasets.

<id>
q-bio/0406009v1
<category>
q-bio.QM
<abstract>
Sequence comparison is a widely used computational technique in modern
molecular biology. In spite of the frequent use of sequence comparisons the
important problem of assigning statistical significance to a given degree of
similarity is still outstanding. Analytical approaches to filling this gap
usually make use of an approximation that neglects certain correlations in the
disorder underlying the sequence comparison algorithm. Here, we use the longest
common subsequence problem, a prototype sequence comparison problem, to
analytically establish that this approximation does make a difference to
certain sequence comparison statistics. In the course of establishing this
difference we develop a method that can systematically deal with these disorder
correlations.

<id>
q-bio/0406016v2
<category>
q-bio.QM
<abstract>
We present a novel classification-based algorithm called GeneClass for
learning to predict gene regulatory response. Our approach is motivated by the
hypothesis that in simple organisms such as Saccharomyces cerevisiae, we can
learn a decision rule for predicting whether a gene is up- or down-regulated in
a particular experiment based on (1) the presence of binding site subsequences
(``motifs'') in the gene's regulatory region and (2) the expression levels of
regulators such as transcription factors in the experiment (``parents''). Thus
our learning task integrates two qualitatively different data sources:
genome-wide cDNA microarray data across multiple perturbation and mutant
experiments along with motif profile data from regulatory sequences. Rather
than focusing on the regression task of predicting real-valued gene expression
measurements, GeneClass performs the classification task of predicting +1 and
-1 labels, corresponding to up- and down-regulation beyond the levels of
biological and measurement noise in microarray measurements. GeneClass uses the
Adaboost learning algorithm with a margin-based generalization of decision
trees called alternating decision trees. In computational experiments based on
the Gasch S. cerevisiae dataset, we show that the GeneClass method predicts up-
and down-regulation on held-out experiments with high accuracy. We explore a
range of experimental setups related to environmental stress response, and we
retrieve important regulators, binding site motifs, and relationships between
regulators and binding sites that are known to be associated to specific stress
response pathways. Our method thus provides predictive hypotheses, suggests
biological experiments, and provides interpretable insight into the structure
of genetic regulatory networks.

<id>
q-bio/0407038v1
<category>
q-bio.QM
<abstract>
Action potential duration (APD) restitution, which relates APD to the
preceding diastolic interval (DI), is a useful tool for predicting the onset of
abnormal cardiac rhythms. However, it is known that different pacing protocols
lead to different APD restitution curves (RCs). This phenomenon, known as APD
rate-dependence, is a consequence of memory in the tissue. In addition to APD
restitution, conduction velocity restitution also plays an important role in
the spatiotemporal dynamics of cardiac tissue. We present new results
concerning rate-dependent restitution in the velocity of propagating action
potentials in a one-dimensional fiber. Our numerical simulations show that,
independent of the amount of memory in the tissue, waveback velocity exhibits
pronounced rate-dependence and the wavefront velocity does not. Moreover, the
discrepancy between waveback velocity RCs is most significant for small DI. We
provide an analytical explanation of these results, using a system of coupled
maps to relate the wavefront and waveback velocities. Our calculations show
that waveback velocity rate-dependence is due to APD restitution, not memory.

<id>
q-bio/0408012v1
<category>
q-bio.QM
<abstract>
A commonly employed measure of the signal amplification properties of an
input/output system is its induced L2 norm, sometimes also known as "H
infinity" gain. In general, however, it is extremely difficult to compute the
numerical value for this norm, or even to check that it is finite, unless the
system being studied is linear. This paper describes a class of systems for
which it is possible to reduce this computation to that of finding the norm of
an associated linear system. In contrast to linearization approaches, a precise
value, not an estimate, is obtained for the full nonlinear model. The class of
systems that we study arose from the modeling of certain biological
intracellular signaling cascades, but the results should be of wider
applicability.

<id>
q-bio/0409010v1
<category>
q-bio.QM
<abstract>
We analyze a microscopic RNA model, which includes two widely used models as
limiting cases, namely it contains terms for bond as well as for stacking
energies. We numerically investigate possible changes in the qualitative and
quantitative behaviour while going from one model to the other; in particular
we test, whether a transition occurs, when continuously moving from one model
to the other. For this we calculate various thermodynamic quantities, both at
zero temperature as well as at finite temperatures. All calculations can be
done efficiently in polynomial time by a dynamic programming algorithm. We do
not find a sign for transition between the models, but the critical exponent
$\nu$ of the correlation length, describing the phase transition in all models
to an ordered low-temperature phase, seems to depend continuously on the model.
Finally, we apply the epsilon-Coupling method, to study low excitations. The
exponent $\theta$ describing the energy-scaling of the excitations seems to
depend not much on the energy model.

<id>
q-bio/0410010v1
<category>
q-bio.QM
<abstract>
An analysis of the RR-interval time series, $t_i$, is presented for the case
in which the average time, $\bar{t}$, changes slowly. In particular, $\bar{t}$
and a short-time scale variability parameter, $V$, are simultaneously measured
while $\bar{t}$ decreases for subjects in the reclined position. The initial
decrease in $\bar{t}$ is usually linear with $V$ yielding parameters that can
be related to physiological quantities.

<id>
q-bio/0411028v1
<category>
q-bio.QM
<abstract>
We present a novel classification-based method for learning to predict gene
regulatory response. Our approach is motivated by the hypothesis that in simple
organisms such as Saccharomyces cerevisiae, we can learn a decision rule for
predicting whether a gene is up- or down-regulated in a particular experiment
based on (1) the presence of binding site subsequences (``motifs'') in the
gene's regulatory region and (2) the expression levels of regulators such as
transcription factors in the experiment (``parents''). Thus our learning task
integrates two qualitatively different data sources: genome-wide cDNA
microarray data across multiple perturbation and mutant experiments along with
motif profile data from regulatory sequences. We convert the regression task of
predicting real-valued gene expression measurement to a classification task of
predicting +1 and -1 labels, corresponding to up- and down-regulation beyond
the levels of biological and measurement noise in microarray measurements. The
learning algorithm employed is boosting with a margin-based generalization of
decision trees, alternating decision trees. This large-margin classifier is
sufficiently flexible to allow complex logical functions, yet sufficiently
simple to give insight into the combinatorial mechanisms of gene regulation. We
observe encouraging prediction accuracy on experiments based on the Gasch S.
cerevisiae dataset, and we show that we can accurately predict up- and
down-regulation on held-out experiments. Our method thus provides predictive
hypotheses, suggests biological experiments, and provides interpretable insight
into the structure of genetic regulatory networks.

<id>
q-bio/0412021v1
<category>
q-bio.QM
<abstract>
We introduce a new heuristic for the multiple alignment of a set of
sequences. The heuristic is based on a set cover of the residue alphabet of the
sequences, and also on the determination of a significant set of blocks
comprising subsequences of the sequences to be aligned. These blocks are
obtained with the aid of a new data structure, called a suffix-set tree, which
is constructed from the input sequences with the guidance of the
residue-alphabet set cover and generalizes the well-known suffix tree of the
sequence set. We provide performance results on selected BAliBASE amino-acid
sequences and compare them with those yielded by some prominent approaches.

<id>
q-bio/0503023v2
<category>
q-bio.QM
<abstract>
We present multiscale models of cancer tumor invasion with components at the
molecular, cellular, and tissue levels. We provide biological justifications
for the model components, present computational results from the model, and
discuss the scientific-computing methodology used to solve the model equations.
The models and methodology presented in this paper form the basis for
developing and treating increasingly complex, mechanistic models of tumor
invasion that will be more predictive and less phenomenological. Because many
of the features of the cancer models, such as taxis, aging and growth, are seen
in other biological systems, the models and methods discussed here also provide
a template for handling a broader range of biological problems.

<id>
q-bio/0503025v2
<category>
q-bio.QM
<abstract>
Random forest is a classification algorithm well suited for microarray data:
it shows excellent performance even when most predictive variables are noise,
can be used when the number of variables is much larger than the number of
observations, and returns measures of variable importance. Thus, it is
important to understand the performance of random forest with microarray data
and its use for gene selection.
  We first show the effects of changes in parameters of random forest on the
prediction error. Then we present an approach for gene selection that uses
measures of variable importance and error rate, and is targeted towards the
selection of small sets of genes. Using simulated and real microarray data, we
show that the gene selection procedure yields small sets of genes while
preserving predictive accuracy.
  Availability: All code is available as an R package, varSelRF, from CRAN,
http://cran.r-project.org/src/contrib/PACKAGES.html, or from the supplementary
material page.
  Supplementary information:
http://ligarto.org/rdiaz/Papers/rfVS/randomForestVarSel.html

<id>
q-bio/0504007v1
<category>
q-bio.QM
<abstract>
We introduce a new methodology for the determination of amino-acid
substitution matrices for use in the alignment of proteins. The new methodology
is based on a pre-existing set cover on the set of residues and on the
undirected graph that describes residue exchangeability given the set cover.
For fixed functional forms indicating how to obtain edge weights from the set
cover and, after that, substitution-matrix elements from weighted distances on
the graph, the resulting substitution matrix can be checked for performance
against some known set of reference alignments and for given gap costs. Finding
the appropriate functional forms and gap costs can then be formulated as an
optimization problem that seeks to maximize the performance of the substitution
matrix on the reference alignment set. We give computational results on the
BAliBASE suite using a genetic algorithm for optimization. Our results indicate
that it is possible to obtain substitution matrices whose performance is either
comparable to or surpasses that of several others, depending on the particular
scenario under consideration.

<id>
q-bio/0504028v1
<category>
q-bio.QM
<abstract>
We introduce a model for describing the dynamics of large numbers of
interacting cells. The fundamental dynamical variables in the model are
sub-cellular elements, which interact with each other through phenomenological
intra- and inter-cellular potentials. Advantages of the model include i)
adaptive cell-shape dynamics, ii) flexible accommodation of additional
intra-cellular biology, and iii) the absence of an underlying grid. We present
here a detailed description of the model, and use successive mean-field
approximations to connect it to more coarse-grained approaches, such as
discrete cell-based algorithms and coupled partial differential equations. We
also discuss efficient algorithms for encoding the model, and give an example
of a simulation of an epithelial sheet. Given the biological flexibility of the
model, we propose that it can be used effectively for modeling a range of
multi-cellular processes, such as tumor dynamics and embryogenesis.

<id>
q-bio/0505027v2
<category>
q-bio.QM
<abstract>
A model of growth of icosahedral viral capsids is proposed. It takes into
account the diversity of hexamers' compositions, leading to definite capsid
size. We show that the observed yield of capsid production implies a very high
level of self-organization of elementary building blocks. The exact number of
different protein dimers composing hexamers is related to the size of a given
capsid, labeled by its T-number. Simple rules determining these numbers for
each value of T are deduced and certain consequences are discussed.

<id>
q-bio/0507009v1
<category>
q-bio.QM
<abstract>
A quantitative measure of stability in stochastic dynamics starts to emerge
in recent experiments on bioswitches. This quantity, similar to the potential
function in mathematics, is deeply rooted in biology, dated back at the
beginning of quantitative description of biological processes: the adaptive
landscape of Wright (1932) and the development landscape of Waddington (1940).
Nevertheless, its quantitative implication has been frequently challenged by
biologists. Recent progresses in quantitative biology begin to meet those
outstanding challenges.

<id>
q-bio/0508038v2
<category>
q-bio.QM
<abstract>
The presented previously indirect optimization method (IOM) developed within
biochemical systems theory (BST) provides a versatile and mathematically
tractable optimization strategy for biochemical systems. However, due to the
local approximations nature of the BST formalism, the iterative version of this
technique possibly does not yield the true optimum solution. In this work, an
algorithm is proposed to obtain the correct and consistent optimum steady-state
operating point of biochemical systems. The existing linear optimization
problem of the direct IOM approach is modified by adding an equality constraint
of describing the consistency of solutions between the S-system and the
original model. Lagrangian analysis is employed to derive the first order
necessary optimality conditions for the above modified optimization problem.
This leads to a procedure that may be regarded as a modified iterative IOM
approach in which the optimization objective function includes an extra linear
term. The extra term contains a comparison of metabolite concentration
derivatives with respect to the enzyme activities between the S-system and the
original model and ensures that the new algorithm is still carried out within
linear programming techniques. The presented framework is applied to several
biochemical systems and shown to the tractability and effectiveness of the
method. The simulation is also studied to investigate the convergence
properties of the algorithm and to give a performance comparison of standard
and modified iterative IOM approach.

<id>
q-bio/0510031v1
<category>
q-bio.QM
<abstract>
Ecological systems are governed by complex interactions which are mainly
nonlinear. In order to capture this complexity and nonlinearity, statistical
models recently gained popularity. However, although these models are commonly
applied in ecology, there are no studies to date aiming to assess the
applicability and performance. We provide an overview for nature of the wide
range of the data sets and predictive variables, from both aquatic and
terrestrial ecosystems with different scales of time-dependent dynamics, and
the applicability and robustness of predictive modeling methods on such data
sets by comparing different statistical modeling approaches. The methods
considered k-NN, LDA, QDA, generalized linear models (GLM) feedforward
multilayer backpropagation networks and pseudo-supervised network ARTMAP. For
ecosystems involving time-dependent dynamics and periodicities whose frequency
are possibly less than the time scale of the data considered, GLM and
connectionist neural network models appear to be most suitable and robust,
provided that a predictive variable reflecting these time-dependent dynamics
included in the model either implicitly or explicitly. For spatial data, which
does not include any time-dependence comparable to the time scale covered by
the data, on the other hand, neighborhood based methods such as k-NN and ARTMAP
proved to be more robust than other methods considered in this study. In
addition, for predictive modeling purposes, first a suitable, computationally
inexpensive method should be applied to the problem at hand a good predictive
performance of which would render the computational cost and efforts associated
with complex variants unnecessary.

<id>
q-bio/0510032v1
<category>
q-bio.QM
<abstract>
Support vector machines and kernel methods are increasingly popular in
genomics and computational biology, due to their good performance in real-world
applications and strong modularity that makes them suitable to a wide range of
problems, from the classification of tumors to the automatic annotation of
proteins. Their ability to work in high dimension, to process non-vectorial
data, and the natural framework they provide to integrate heterogeneous data
are particularly relevant to various problems arising in computational biology.
In this chapter we survey some of the most prominent applications published so
far, highlighting the particular developments in kernel methods triggered by
problems in biology, and mention a few promising research directions likely to
expand in the future.

<id>
q-bio/0511034v1
<category>
q-bio.QM
<abstract>
A central task in the study of molecular sequence data from present-day
species is the reconstruction of the ancestral relationships. The most
established approach to tree reconstruction is the maximum likelihood (ML)
method. In this method, evolution is described in terms of a discrete-state
continuous-time Markov process on a phylogenetic tree. The substitution rate
matrix, that determines the Markov process, can be estimated using the
expectation maximization (EM) algorithm. Unfortunately, an exhaustive search
for the ML phylogenetic tree is computationally prohibitive for large data
sets. In such situations, the neighbor-joining (NJ) method is frequently used
because of its computational speed. The NJ method reconstructs trees by
clustering neighboring sequences recursively, based on pairwise comparisons
between the sequences. The NJ method can be generalized such that
reconstruction is based on comparisons of subtrees rather than pairwise
distances. In this paper, we present an algorithm for simultaneous substitution
rate estimation and phylogenetic tree reconstruction. The algorithm iterates
between the EM algorithm for estimating substitution rates and the generalized
NJ method for tree reconstruction. Preliminary results of the approach are
encouraging.

<id>
q-bio/0511042v2
<category>
q-bio.QM
<abstract>
This technical report provides the supplementary material for a paper
entitled "Information based clustering", to appear shortly in Proceedings of
the National Academy of Sciences (USA). In Section I we present in detail the
iterative clustering algorithm used in our experiments and in Section II we
describe the validation scheme used to determine the statistical significance
of our results. Then in subsequent sections we provide all the experimental
results for three very different applications: the response of gene expression
in yeast to different forms of environmental stress, the dynamics of stock
prices in the Standard and Poor's 500, and viewer ratings of popular movies. In
particular, we highlight some of the results that seem to deserve special
attention. All the experimental results and relevant code, including a freely
available web application, can be found at
http://www.genomics.princeton.edu/biophysics-theory .

<id>
q-bio/0511043v1
<category>
q-bio.QM
<abstract>
In an age of increasingly large data sets, investigators in many different
disciplines have turned to clustering as a tool for data analysis and
exploration. Existing clustering methods, however, typically depend on several
nontrivial assumptions about the structure of data. Here we reformulate the
clustering problem from an information theoretic perspective which avoids many
of these assumptions. In particular, our formulation obviates the need for
defining a cluster "prototype", does not require an a priori similarity metric,
is invariant to changes in the representation of the data, and naturally
captures non-linear relations. We apply this approach to different domains and
find that it consistently produces clusters that are more coherent than those
extracted by existing algorithms. Finally, our approach provides a way of
clustering based on collective notions of similarity rather than the
traditional pairwise measures.

<id>
q-bio/0512023v1
<category>
q-bio.QM
<abstract>
A salient feature of stationary patterns in tip-growing cells is the key role
played by the symports and antiports, membrane proteins that translocate two
ionic species at the same time. It is shown that these co-transporters
destabilize generically the membrane voltage if the two translocated ions
diffuse differently and carry a charge of opposite (same) sign for symports
(antiports). Orders of magnitude obtained for the time and lengthscale are in
agreement with experiments. A weakly nonlinear analysis characterizes the
bifurcation.

<id>
q-bio/0312028v2
<category>
q-bio.SC
<abstract>
The recent discovery of a lateral organization in cell membranes due to small
structures called 'rafts' has motivated a lot of biological and
physico-chemical studies. A new experiment on a model system has shown a
spectacular budding process with the expulsion of one or two rafts when one
introduces proteins on the membrane. In this paper, we give a physical
interpretation of the budding of the raft phase. An approach based on the
energy of the system including the presence of proteins is used to derive a
shape equation and to study possible instabilities. This model shows two
different situations which are strongly dependent on the nature of the
proteins: a regime of easy budding when the proteins are strongly coupled to
the membrane and a regime of difficult budding.

<id>
q-bio/0402040v1
<category>
q-bio.SC
<abstract>
We show that at the onset of a cyclic fold bifurcation, a birhythmic medium
composed of glycolytic oscillators displays turbulent dynamics. By computing
the largest Lyapunov exponent, the spatial correlation function, and the
average transient lifetime, we classify it as a weak turbulence with transient
nature. Virtual heterogeneities generating unstable fast oscillations are the
mechanism of the transient turbulence. In the presence of wavenumber
instability, unstable oscillations can be reinjected leading to stationary
turbulence. We also find similar turbulence in a cell cycle model. These
findings suggest that weak turbulence may be universal in biochemical
birhythmic media exhibiting cyclic fold bifurcations.

<id>
q-bio/0405024v2
<category>
q-bio.SC
<abstract>
We focused our attention on Ca$^{2+}$ release from the endoplasmic reticulum
through a cluster of inositol 1,4,5-trisphosphate (IP$_3$) receptor channels.
The random opening and closing of these receptors introduce stochastic effects
that have been observed experimentally. Here, we present a stochastic version
of Othmer-Tang model for IP$_3$ receptor clusters. We address the average
behavior of the channels in response to IP$_3$ stimuli. We found, by stochastic
simulation, that the shape of the receptor response to IP$_3$ (fraction of open
channels versus [IP$_3$]), is affected by the cytosolic Ca$^{2+}$ level. We
also study several aspects of the stochastic properties of Ca${2+}$ release and
we compare with experimental observations.

<id>
q-bio/0407011v3
<category>
q-bio.SC
<abstract>
By observing reconstituted chromatin by fluorescence microscopy (FM) and
atomic force microscopy (AFM), we found that the density of nucleosomes
exhibits a bimodal profile, i.e., there is a large transition between the dense
and dispersed states in reconstituted chromatin. Based on an analysis of the
spatial distribution of nucleosome cores, we deduced an effective thermodynamic
potential as a function of the nucleosome-nucleosome distance. This enabled us
to interpret the folding transition of chromatin in terms of a first-order
phase transition. This mechanism for the condensation of chromatin is discussed
in terms of its biological significance.

<id>
q-bio/0409006v1
<category>
q-bio.SC
<abstract>
In E. coli, accurate cell division depends upon the oscillation of Min
proteins from pole to pole. We provide a model for the polar localization of
MinD based only on diffusion, a delay for nucleotide exchange, and different
rates of attachment to the bare membrane and the occupied membrane. We derive
analytically the probability density, and correspondingly the length scale, for
MinD attachment zones. Our simple analytical model illustrates the processes
giving rise to the observed localization of cellular MinD zones.

<id>
q-bio/0409007v1
<category>
q-bio.SC
<abstract>
The charges in live cells interact with or produce electric fields, which
results in enormous dielectric responses, flexoelectricity, and related
phenomena. Here we report on a contraction of schizosacchraoymces pombe
(fission yeast) cells induced by magnetic fields, as observed using a phase
sensitive projection image technique. Unlike electric fields, magnetic fields
only act on moving charges. The observed behavior is quite remarkable, and may
result from a contractile Lorentz force acting on diamagnetic screening
currents. This would indicate extremely high intracellular charge mobilities.
Besides, we observed a large electro - optical response from fission yeast
cells.

<id>
q-bio/0411038v1
<category>
q-bio.SC
<abstract>
A significant part of the thin layers of counter-ions adjacent to the
exterior and interior surfaces of a cell membrane form quasi-two-dimensional
(2D) layers of mobile charge. Collective charge density oscillations, known as
plasmon modes, in these 2D charged systems of counter-ions are predicted in the
present paper. This is based on a calculation of the self-consistent response
of this system to a fast electric field fluctuation. The possibility that the
membrane channels might be using these excitations to carry out fast
communication is suggested and experiments are proposed to reveal the existence
of such excitations.

<id>
q-bio/0412023v1
<category>
q-bio.SC
<abstract>
In E. coli the determination of the middle of the cell and the proper
placement of the septum is essential to the division of the cell. This step
depends on the proteins MinC, MinD, and MinE. Exposure to a constant external
field e.g., an electric field or magnetic field may cause the bacteria cell
division mechanism to change resulting in an abnormal cytokinesis. To have
insight into the effects of an external field on this process, we model the
process using a set of the deterministic reaction diffusion equations, which
incorporate the influence of an external field, min protein reactions, and
diffusion of all species. Using the numerical method, we have found some
changes in the dynamics of the oscillations of the min proteins from pole to
pole when compared that of without the external field. The results show some
interesting effects, which are qualitatively in good agreement with some
experimental results.

<id>
q-bio/0501024v1
<category>
q-bio.SC
<abstract>
Based on experimental observations it is known that various biological cells
exhibit a persistent random walk during migration on flat substrates. The
persistent random walk is characterized by `stop-and-go' movements :
unidirectional motions over distances of the order of several cell diameter are
separated by localized short time erratic movements. Using computer simulations
the reasons for this phenomena had been unveiled and shown to be attributed to
two antagonistic nucleation processes during the polymerization of the cell's
actin cytoskeleton : the (ordinary) spontaneous nucleation and the dendritic
nucleation processes. Whereas spontaneous nucleations generate actin filaments
growing in different directions and hence create motions in random directions,
dendritic nucleations provide a unidirectional growth. Since dendritic growth
exhibits stochastic fluctuations, spontaneous nucleation may eventually compete
or even dominate, which results in a reorientation of filament growth and hence
a new direction of cell motion. The event of reorientation takes place at
instants of vanishing polarity of the actin skeleton.

<id>
q-bio/0501029v1
<category>
q-bio.SC
<abstract>
Trajectories of on-off events are the output of many single molecule
experiments. Usually, one describes the underlying mechanism that generates the
trajectory using a kinetic scheme, and by analyzing the trajectory aims at
deducing this scheme. In a previous work [O. Flomenbom, J. Klafter, and A.
Szabo, submitted (2004)], we showed that when successive events along a
trajectory are uncorrelated, all the information in the trajectory is contained
in two basic functions, which are the waiting time probability functions (PDFs)
of the on state and of the off state. The kinetic schemes that lead to such
uncorrelated trajectories were termed reducible. Here we discuss the reasons
that lead to reducible schemes. In particular, the topology of reducible
schemes is characterized and proven.

<id>
q-bio/0501032v1
<category>
q-bio.SC
<abstract>
Motor proteins that specifically interact with the ends of cytoskeletal
filaments can induce filament depolymerization. A phenomenological description
of this process is presented. We show that under certain conditions motors
dynamically accumulate at the filament ends. We compare simulations of two
microscopic models to the phenomenological description. The depolymerization
rate can exhibit maxima and dynamic instabilities as a function of the bulk
motor density for processive depolymerization. We discuss our results in
relation to experimental studies of Kin-13 family motor proteins.

<id>
q-bio/0504008v1
<category>
q-bio.SC
<abstract>
L-selectin mediated tethers result in leukocyte rolling only above a
threshold in shear. Here we present biophysical modeling based on recently
published data from flow chamber experiments (Dwir et al., J. Cell Biol. 163:
649-659, 2003) which supports the interpretation that L-selectin mediated
tethers below the shear threshold correspond to single L-selectin carbohydrate
bonds dissociating on the time scale of milliseconds, whereas L-selectin
mediated tethers above the shear threshold are stabilized by multiple bonds and
fast rebinding of broken bonds, resulting in tether lifetimes on the timescale
of $10^{-1}$ seconds. Our calculations for cluster dissociation suggest that
the single molecule rebinding rate is of the order of $10^4$ Hz. A similar
estimate results if increased tether dissociation for tail-truncated L-selectin
mutants above the shear threshold is modeled as diffusive escape of single
receptors from the rebinding region due to increased mobility. Using computer
simulations, we show that our model yields first order dissociation kinetics
and exponential dependence of tether dissociation rates on shear stress. Our
results suggest that multiple contacts, cytoskeletal anchorage of L-selectin
and local rebinding of ligand play important roles in L-selectin tether
stabilization and progression of tethers into persistent rolling on endothelial
surfaces.

<id>
q-bio/0504023v1
<category>
q-bio.SC
<abstract>
We address the controversial hot question concerning the validity of the
loose coupling versus the lever-arm theories in the actomyosin dynamics by
re-interpreting and extending the phenomenological washboard potential model
proposed by some of us in a previous paper. In this new model a Brownian motion
harnessing thermal energy is assumed to co-exist with the deterministic swing
of the lever-arm, to yield an excellent fit of the set of data obtained by some
of us on the sliding of Myosin II heads on immobilized actin filaments under
various load conditions. Our theoretical arguments are complemented by accurate
numerical simulations, and the robustness of the model is tested via different
choices of parameters and potential profiles.

<id>
q-bio/0510050v1
<category>
q-bio.SC
<abstract>
Motivated by the formation of ring-like filament structures in the cortex of
plant and animal cells, we study the dynamics of a two-dimensional layer of
cytoskeletal filaments and motor proteins near a surface by a general continuum
theory. As a result of active processes, dynamic patterns of filament
orientation and density emerge via instabilities. We show that
self-organization phenomena can lead to the formation of stationary and
oscillating rings. We present state diagrams which reveal a rich scenario of
asymptotic behaviors and discuss the role of boundary conditions.

<id>
q-bio/0511008v1
<category>
q-bio.SC
<abstract>
The conditions of the chromosomes inside the nucleus in the Rabl
configuration have been modelled as self-avoiding polymer chains under
restraining conditions. To ensure that the chromosomes remain stretched out and
lined up, we fixed their end points to two opposing walls. The numbers of
segments $N$, the distances $d_1$ and $d_2$ between the fixpoints, and the
wall-to-wall distance $z$ (as measured in segment lengths) determine an
approximate value for the Kuhn segment length $k_l$. We have simulated the
movement of the chromosomes using molecular dynamics to obtain the expected
distance distribution between the genetic loci in the absence of further
attractive or repulsive forces. A comparison to biological experiments on
\textit{Drosophila Melanogaster} yields information on the parameters for our
model. With the correct parameters it is possible to draw conclusions on the
strength and range of the attraction that leads to pairing.

<id>
q-bio/0512027v2
<category>
q-bio.SC
<abstract>
During the eukaryotic cell cycle, chromatin undergoes several conformational
changes, which are believed to play key roles in gene expression regulation
during interphase, and in genome replication and division during mitosis. In
this paper, we propose a scenario for chromatin structural reorganization
during mitosis, which bridges all the different scales involved in chromatin
architecture, from nucleosomes to chromatin loops. We build a model for
chromatin, based on available data, taking into account both physical and
topological constraints DNA has to deal with. Our results suggest that the
mitotic chromosome condensation/decondensation process is induced by a
structural change at the level of the nucleosome itself.

<id>
q-bio/0601013v1
<category>
q-bio.SC
<abstract>
The influence of intrinsic channel noise on the spontaneous spiking activity
of poisoned excitable membrane patches is studied by use of a stochastic
generalization of the Hodgkin-Huxley model. Internal noise stemming from the
stochastic dynamics of individual ion channels is known to affect the
collective properties of the whole ion channel cluster. For example, there
exists an optimal size of the membrane patch for which the internal noise alone
causes a regular spontaneous generation of action potentials. In addition to
varying the size of ion channel clusters, living organisms may adapt the
densities of ion channels in order to optimally regulate the spontaneous
spiking activity. The influence of channel block on the excitability of a
membrane patch of certain size is twofold: First, a variation of ion channel
densities primarily yields a change of the conductance level. Second, a
down-regulation of working ion channels always increases the channel noise.
While the former effect dominates in the case of sodium channel block resulting
in a reduced spiking activity, the latter enhances the generation of
spontaneous action potentials in the case of a tailored potassium channel
blocking. Moreover, by blocking some portion of either potassium or sodium ion
channels, it is possible to either increase or to decrease the regularity of
the spike train.

<id>
q-bio/0601022v1
<category>
q-bio.SC
<abstract>
Morphogens are proteins, often produced in a localised region, whose
concentrations spatially demarcate regions of differing gene expression in
developing embryos. The boundaries of expression must be set accurately and in
proportion to the size of the one-dimensional developing field; this cannot be
accomplished by a single gradient. Here, we show how a pair of morphogens
produced at opposite ends of a developing field can solve the pattern-scaling
problem. In the most promising scenario, the morphogens effectively interact
according to the annihilation reaction $A+B\to\emptyset$ and the switch occurs
according to the absolute concentration of $A$ or $B$. In this case embryonic
markers across the entire developing field scale approximately with system
size; this cannot be achieved with a pair of non-interacting gradients that
combinatorially regulate downstream genes. This scaling occurs in a window of
developing-field sizes centred at a few times the morphogen decay length.

<id>
q-bio/0603019v1
<category>
q-bio.SC
<abstract>
There is a correspondence between the circulation of blood in all higher
animals and the circulation of sap in all higher plants - up to heights h of
140 m - through the xylem and phloem vessels. Plants suck in water from the
soil, osmotically through the roothair zone, and subsequently lift it
osmotically again, and by capillary suction (via their buds, leaves, and
fruits) into their crowns. In between happens a reverse osmosis - the
endodermis jump - realized by two layers of subcellular mechanical pumps in the
endodermis walls which are powered by ATP, or in addition by two analogous
layers of such pumps in the exodermis. The thus established root pressure helps
forcing the absorbed ground water upward, through the whole plant, and often
out again, in the form of guttation, or exudation.

<id>
q-bio/0605017v3
<category>
q-bio.SC
<abstract>
Using a theoretical model for spontaneous partial DNA unwrapping from
histones, we study the transient exposure of protein-binding DNA sites within
nucleosomes. We focus on the functional dependence of the rates for site
exposure and reburial on the site position, which is measurable experimentally
and pertinent to gene regulation. We find the dependence to be roughly
described by a random walker model. Close inspection reveals a surprising
physical effect of flexibility-assisted barrier crossing, which we characterize
within a toy model, the "semiflexible Brownian rotor."

<id>
q-bio/0608030v3
<category>
q-bio.SC
<abstract>
In this paper we use a simple toy model to explore the function of the gene
Osteosarcoma-9. We are in particular interested in understanding the role of
this gene as a potent anti-apoptotic factor. The theoretical description is
constrained by experimental data from induction of apoptosis in cells where
OS-9 is overexpressed. The data available suggest that OS-9 promotes cell
viability and confers resistance to apoptosis, potentially implicating OS-9 in
the survival of cancer cells. Three different apoptosis inducing mechanisms
were tested and are here modelled. More complex and realistic models are also
discussed.

<id>
q-bio/0611085v1
<category>
q-bio.SC
<abstract>
The distribution of inclusion-rich domains in membranes with active two-state
inclusions is studied by simulations. Our study shows that typical size of
inclusion-rich domains ($L$) can be controlled by inclusion activities in
several ways. When there is effective attraction between state-1 inclusions, we
find: (i) Small domains with only several inclusions are observed for
inclusions with time scales ($\sim 10^{-3} {\rm s}$) and interaction energy
[$\sim \mathcal{O}({\rm k_BT})$] comparable to motor proteins. (ii) $L$ scales
as 1/3 power of the lifetime of state-1 for a wide range of parameters. (iii)
$L$ shows a switch-like dependence on state-2 lifetime $k_{12}^{-1}$. That is,
$L$ depends weakly on $k_{12}$ when $k_{12} < k_{12}^*$ but increases rapidly
with $k_{12}$ when $k_{12} > k_{12}^*$, the crossover $k_{12}^*$ occurs when
the diffusion length of a typical state-2 inclusion within its lifetime is
comparable to $L$. (iv) Inclusion-curvature coupling provides another length
scale that competes with the effects of transition rates.

<id>
q-bio/0702021v2
<category>
q-bio.SC
<abstract>
It is shown that the dual to the linear programming problem that arises in
constraint-based models of metabolism can be given a thermodynamic
interpretation in which the shadow prices are chemical potential analogues, and
the objective is to minimise free energy consumption given a free energy drain
corresponding to growth. The interpretation is distinct from conventional
non-equilibrium thermodynamics, although it does satisfy a minimum entropy
production principle. It can be used to motivate extensions of constraint-based
modelling, for example to microbial ecosystems.

<id>
q-bio/0703008v2
<category>
q-bio.SC
<abstract>
Viral infection requires the binding of receptors on the target cell membrane
to glycoproteins, or ``spikes,'' on the viral membrane. The initial entry is
usually classified as fusogenic or endocytotic. However, binding of viral
spikes to cell surface receptors not only initiates the viral adhesion and the
wrapping process necessary for internalization, but can simultaneously initiate
direct fusion with the cell membrane. Both fusion and internalization have been
observed to be viable pathways for many viruses. We develop a stochastic model
for viral entry that incorporates a competition between receptor mediated
fusion and endocytosis. The relative probabilities of fusion and endocytosis of
a virus particle initially nonspecifically adsorbed on the host cell membrane
are computed as functions of receptor concentration, binding strength, and
number of spikes. We find different parameter regimes where the entry pathway
probabilities can be analytically expressed. Experimental tests of our
mechanistic hypotheses are proposed and discussed.

<id>
q-bio/0703055v1
<category>
q-bio.SC
<abstract>
I point out the similarity between the microtubule experiment reported by
Priel et al [Biophys. J. 90, 4639 (2006)] and the ZnO nanowire experiment of
Wang et al [Nanolett. 6, 2768 (2006)]. It is quite possible that MTs are
similar to a piezoelectric field effect transistor for which the role of the
control gate electrode is played by the piezo-induced electric field across the
width of the MT walls and their elastic bending features

<id>
0706.0643v1
<category>
q-bio.SC
<abstract>
We present a model for diffusion in a molecularly crowded environment. The
model consists of random barriers in percolation network. Random walks in the
presence of slowly moving barriers show normal diffusion for long times, but
anomalous diffusion at intermediate times. The effective exponents for square
distance versus time usually are below one at these intermediate times, but can
be also larger than one for high barrier concentrations. Thus we observe sub-
as well as super-diffusion in a crowded environment.

<id>
0706.0683v1
<category>
q-bio.SC
<abstract>
We discuss various models of ion transport through cell membrane channels.
Recent experimental data shows that sizes of ion channels are compared to those
of ions and that only few ions may be simultaneously in any single channel.
Theoretical description of ion transport in such channels should therefore take
into account interactions between ions and between ions and channel proteins.
This is not satisfied by macroscopic continuum models based on
Poisson-Nernst-Planck equations. More realistic descriptions of ion transport
are offered by microscopic Brownian and molecular dynamics. One should also
take into account a dynamical character of the channel structure. This is not
yet addressed in the literature

<id>
0706.2353v2
<category>
q-bio.SC
<abstract>
Ongoing sub-cellular oscillation of Min proteins is required to block
minicelling in E. coli. Experimentally, Min oscillations are seen in newly
divided cells and no minicells are produced. In model Min systems many daughter
cells do not oscillate following septation because of unequal partitioning of
Min proteins between the daughter cells. Using the 3D model of Huang et al., we
investigate the septation process in detail to determine the cause of the
asymmetric partitioning of Min proteins between daughter cells. We find that
this partitioning problem arises at certain phases of the MinD and MinE
oscillations with respect to septal closure and it persists independently of
parameter variation. At most 85% of the daughter cells exhibit Min oscillation
following septation. Enhanced MinD binding at the static polar and dynamic
septal regions, consistent with cardiolipin domains, does not substantially
increase this fraction of oscillating daughters. We believe that this problem
will be shared among all existing Min models and discuss possible biological
mechanisms that may minimize partitioning errors of Min proteins following
septation.

<id>
0708.0528v1
<category>
q-bio.SC
<abstract>
Within individual bacteria, we combine force-dependent polymerization
dynamics of individual MreB protofilaments with an elastic model of
protofilament bundles buckled into helical configurations. We use variational
techniques and stochastic simulations to relate the pitch of the MreB helix,
the total abundance of MreB, and the number of protofilaments. By comparing our
simulations with mean-field calculations, we find that stochastic fluctuations
are significant. We examine the quasi-static evolution of the helical pitch
with cell growth, as well as timescales of helix turnover and denovo
establishment. We find that while the body of a polarized MreB helix treadmills
towards its slow-growing end, the fast-growing tips of laterally associated
protofilaments move towards the opposite fast-growing end of the MreB helix.
This offers a possible mechanism for targeted polar localization without
cytoplasmic motor proteins.

<id>
0801.1392v1
<category>
q-bio.SC
<abstract>
Mechanochemical coupling was studied for two different types of myosin motors
in cells: myosin V, which carries cargo over long distances by as a single
molecule; and myosin II, which generates a contracting force in cooperation
with other myosin II molecules. Both mean and variance of myosin V velocity at
various [ATP] obeyed Michaelis-Menten mechanics, consistent with tight
mechanochemical coupling. Myosin II, working in an ensemble, however, was
explained by a loose coupling mechanism, generating variable step sizes
depending on the ATP concentration and realizing a much larger step (200 nm)
per ATP hydrolysis than myosin V through its cooperative nature at zero load.
These different mechanics are ideal for the respective myosin's physiological
functions.

<id>
q-bio/0312041v1
<category>
q-bio.TO
<abstract>
We propose a travelling-wave perturbation method to control the
spatiotemporal dynamics in a cardiac model. It is numerically demonstrated that
the method can successfully suppress the wave instability (alternans in action
potential duration) in the one-dimensional case and convert spiral waves and
turbulent states to the normal travelling wave states in the two-dimensional
case. An experimental scheme is suggested which may provide a new design for a
cardiac defibrillator.

<id>
q-bio/0402025v1
<category>
q-bio.TO
<abstract>
The purpose of research was to check up the influence of decrease of
nonequality of ventilating (after bronchodilator (berotec) inhalation (BI)) on
the magnitude of dynamic compliance of lungs (Cdyn) at asthma patients with
ventilating infringements. Methods and materials: 20 patients (with 2 and 3
degrees of ventilating infringements (VC<73%, FEV1<51%, MVV<56%), without
restrictive disease of lungs, suffering from bronchial asthma were studied
before and after BI by plotting volume, rate flow, against the transpulmonare
pressure. About the change of nonequality of ventilating we consider by the
change after BI of Cdyn, Cdyn at once after flow interruption (Cdyn1), tissue
resistance at inhalation (Rti in) and exhalation (Rti ex), parameters of
ventilating and general parameters of respiratory mechanics. Results: the
parameters of ventilating were improved (P < 0,05). General parameters of
respiratory mechanics also improved. Rti in and Rti ex are made 0,48+0,16;
1,05+0,25 kPa/l/s before BI and decreased 0,09+0,04; 0,28+0,09 kPa/l/s after BI
(P < 0,05; P < 0,05). But Cdyn and Cdyn1 are not changed after BI. Conclusions:
1. The decrease of ventilation nonequality and tissue friction after BI do not
influence on the initially reduced dynamic compliance of lungs at asthma
patients without any restrictive diseases of lungs. 2. The cause of not
increasing of dynamic compliance after BI probably due by changes in elastic
component of parenchyma of lungs, insensitive to berotec.

<id>
q-bio/0404034v1
<category>
q-bio.TO
<abstract>
When the body is infected, it mounts an acute inflammatory response to rid
itself of the pathogens and restore health. Uncontrolled acute inflammation due
to infection is defined clinically as Sepsis and can culminate in organ failure
and death. We consider a three dimensional ordinary differential equation model
of inflammation consisting of a pathogen, and two inflammatory mediators. The
model reproduces the healthy outcome and diverse negative outcomes, depending
on initial conditions and parameters.when key parameters are changed and
suggest various therapeutic strategies. We suggest that the clinical condition
of sepsis can arise from several distinct physiological states, each of which
requires a different treatment approach. We analyze the various bifurcations
between the different outcomes

<id>
q-bio/0406001v1
<category>
q-bio.TO
<abstract>
A growth of malignant neoplasm is considered as a fractional transport
approach. We suggested that the main process of the tumor development through a
lymphatic net is fractional transport of cells. In the framework of this
fractional kinetics we were able to show that the mean size of main growth is
due to subdiffusion, while the appearance of metaphases is determined by
superdiffusion.

<id>
q-bio/0409002v1
<category>
q-bio.TO
<abstract>
We develop some techniques to prove analytically the existence and stability
of long period oscillations of stem cell populations in the case of periodic
chronic myelogenous leukemia. Such a periodic oscillation $p_\infty $ can be
analytically constructed when the hill coefficient involved in the nonlinear
feedback is infinite, and we show it is possible to obtain a contractive
returning map (for the semiflow defined by the modeling functional differential
equation) in a closed and convex cone containing $p_\infty $ when the hill
coefficient is large, and the fixed point of such a contractive map gives the
long period oscillation previously observed both numerically and
experimentally.

<id>
q-bio/0409003v1
<category>
q-bio.TO
<abstract>
In this paper we use a continuous model to describe the development of a
single cell lineage following the committal of stem cells. Three separate
controls are implemented in the model, namely the proliferative control of stem
cells, the proliferative control of developing blast cells, and the peripheral
control of stem cell committal by circulating blood cell density. We show that
variation of parameters in all three control systems can cause oscillations,
and that the characters of these oscillations are very different. This allows
us some potential insight into the mechanisms that may be operative in some of
the dynamic blood diseases like cyclical neutropenia and periodic chronic
myelogenous leukemia.

<id>
q-bio/0409039v1
<category>
q-bio.TO
<abstract>
We demonstrate the robust scale-invariance in the probability density
function (PDF) of detrended healthy human heart rate increments, which is
preserved not only in a quiescent condition, but also in a dynamic state where
the mean level of heart rate is dramatically changing. This scale-independent
and fractal structure is markedly different from the scale-dependent PDF
evolution observed in a turbulent-like, cascade heart rate model. These results
strongly support the view that healthy human heart rate is controlled to
converge continually to a critical state.

<id>
q-bio/0410034v1
<category>
q-bio.TO
<abstract>
This work introduces an in vivo marker in mice from the altitude of the well,
determined by this cancer.The duration of development and the altitude of the
cancer well are obtained from the synchronization of the mechanics and the
kinetics of the processes of normal and anomalous transport under tumor mass
growth.The mechanics and the kinetics of these two transports are found from
the cancer fugacity.The cancer fugacity is defined as a photograph of the rate
of the scaling exponent under tumor mass growth.

<id>
q-bio/0412040v1
<category>
q-bio.TO
<abstract>
A macroscopic model of the tumor Gompertzian growth is proposed. This
approach is based on the energetic balance among the different cell activities,
described by methods of statistical mechanics and related to the growth
inhibitor factors. The model is successfully applied to the multicellular tumor
spheroid data.

<id>
q-bio/0502004v1
<category>
q-bio.TO
<abstract>
Most human carcinomas exhibit telomere abnormalities early in the
carcinogenesis process suggesting that crisis caused by telomere shortening may
be a necessary event leading to human carcinomas. Epidemiological records of
the age at which each patient in a population develops carcinoma are known as
age-incidence data; these provide a quantitative measure of human tumor
initiation and dynamics. If crisis brought on by telomere shortening is
necessary for most human carcinomas, it may also be the rate limiting step. To
test this, we compared a mathematical model in which telomere loss is the rate
limiting step during carcinogenesis with age-incidence data compiled by the
Surveillance, Epidemiology and End Results (SEER) program. We found that this
model adequately explains the age-incidence data. The model also implies that
two distinct paths exist for carcinoma to develop in prostate, breast, and
ovary tissues. We conclude that a single step, crisis brought on by telomere
shortening, limits the rate of formation of human carcinomas.

<id>
q-bio/0502038v1
<category>
q-bio.TO
<abstract>
Eyespots are concentric motifs with contrasting colours on butterfly wings.
Eyespots have intra- and inter-specific visual signalling functions with
adaptive and selective roles. We propose a reaction-diffusion model that
accounts for eyespot development. The model considers two diffusive morphogens
and three non-diffusive pigment precursors. The first morphogen is produced in
the focus and determines the differentiation of the first eyespot ring. A
second morphogen is then produced, modifying the chromatic properties of the
wing background pigment precursor, inducing the differentiation of a second
ring. The model simulates the general structural organisation of eyespots,
their phenotypic plasticity and seasonal variability, and predicts effects from
microsurgical manipulations on pupal wings as reported in the literature.

<id>
q-bio/0503021v2
<category>
q-bio.TO
<abstract>
We make comments on the paper by Miller [{\it J. Theor. Biol.} {\bf 234}
(2005) 511].

<id>
q-bio/0503033v1
<category>
q-bio.TO
<abstract>
Colour parameters of European beech were measured using CIELab system. 103
logs from 87 trees in 9 sites were cut into boards to study the variations of
wood colour parameters. Both site and tree effect on colour were observed.
Patterns of red heartwood occurrence were defined. When excepting red heartwood
there was still a highly significant effect of site and tree; differences
remained after veneer processing. Axial variations were small, except very near
the pith or in red heartwood, suggesting possible early selection at periphery
under colour criteria. Red heartwood is darker, redder and more yellow than
normal peripheral wood.

<id>
q-bio/0506023v2
<category>
q-bio.TO
<abstract>
Remodelling is defined as an evolution of microstructure or variations in the
configuration of the underlying manifold. The manner in which a biological
tissue and its subsystems remodel their structure is treated in a continuum
mechanical setting. While some examples of remodelling are conveniently
modelled as evolution of the reference configuration (Case I), others are more
suited to an internal variable description (Case II). In this paper we explore
the applicability of stationary energy states to remodelled systems. A
variational treatment is introduced by assuming that stationary energy states
are attained by changes in microstructure via one of the two mechanisms--Cases
I and II. An example is presented to illustrate each case. The example
illustrating Case II is further studied in the context of the thermodynamic
dissipation inequality.

<id>
q-bio/0507042v1
<category>
q-bio.TO
<abstract>
The impulse response function (IRF) of a localized bolus in cerebral blood
flow codes important information on the tissue type. It is indirectly
accessible both from MR- and CT-imaging methods, at least in principle. In
practice, however, noise and limited signal resolution render standard
deconvolution techniques almost useless. Parametric signal descriptions look
more promising, and it is the aim of this contribution to develop some
improvements along this line.

<id>
q-bio/0507043v1
<category>
q-bio.TO
<abstract>
A theoretical model based on the molecular interactions between a growing
tumor and a dynamically evolving blood vessel network describes the
transformation of the regular vasculature in normal tissues into a highly
inhomogeneous tumor specific capillary network. The emerging morphology,
characterized by the compartmentalization of the tumor into several regions
differing in vessel density, diameter and necrosis, is in accordance with
experimental data for human melanoma. Vessel collapse due to a combination of
severely reduced blood flow and solid stress exerted by the tumor, leads to a
correlated percolation process that is driven towards criticality by the
mechanism of hydrodynamic vessel stabilization.

<id>
q-bio/0510044v1
<category>
q-bio.TO
<abstract>
The RR series extracted from human electrocardiogram signal (ECG) is
considered as a fractal stochastic process. The manifestation of long-range
dependencies is the presence of power laws in scale dependent process
characteristics. Exponents of these laws: $\beta$ - describing power spectrum
decay, $\alpha$ - responsible for decay of detrended fluctuations or $H$
related to, so-called, roughness of a signal, are known to differentiate hearts
of healthy people from hearts with congestive heart failure. There is a strong
expectation that resolution spectrum of exponents, so-called, local exponents
in place of global exponents allows to study differences between hearts in
details. The arguments are given that local exponents obtained in multifractal
analysis by the two methods: wavelet transform modulus maxima (WTMM) and
multifractal detrended fluctuation analysis (MDFA), allow to recognize the
following four stages of the heart: healthy and young, healthy and advance in
years, subjects with left ventricle systolic dysfunction (NYHA I--III class)
and characterized by severe congestive heart failure (NYHA III-IV class).

<id>
q-bio/0601021v1
<category>
q-bio.TO
<abstract>
We argue that volumetric growth dynamics of a solid cancer depend on the
tumor system's overall surface extension. While this at first may seem evident,
to our knowledge, so far no theoretical argument has been presented explaining
this relationship explicitly. In here, we therefore develop a conceptual
framework based on the universal scaling law and then support our conjecture
through evaluation with experimental data.

<id>
q-bio/0603016v1
<category>
q-bio.TO
<abstract>
Experiments that discuss influence of noise to H. Seidel and H. Herzel
dynamics model of human cardiovascular system are presented. Noise is
introduced by considering stochastic delays in response to the sympathetic
system. It appears that in the presence of the noise 10 s heart rate
oscillations connected with Mayer waves are preserved. Moreover the heart rate
becomes approximately normally distributed (even in unstable phase of original
Seidel Herzel model), similarly like the real RR intervals data.

<id>
q-bio/0604008v1
<category>
q-bio.TO
<abstract>
We model the interaction between the immune system and tumor cells including
a time delay to simulate the time needed by the latter to develop a chemical
and cell mediated response to the presence of the tumor. The results are
compared with those of a previous paper, concluding that the delay introduces
new instabilities in the system leading to an uncontrolable growth of the
tumour. Then a cytokine based periodic immunotherapy treatment is included in
the model and the effects of its dossage are studied for the case of a weak
immune system and a growing tumour. We find the existence of metastable states
(that may last for tens of years) induced by the treatment, and also of
potentially adverse effects of the dossage frequency on the stabilization of
the tumour. These two effects depend on the delay, the cytokine dose burden and
other parameters considered in the model.

<id>
q-bio/0605035v1
<category>
q-bio.TO
<abstract>
The differential Adhesion Hypothesis (DAH) is a theory of the organization of
cells within a tissue. In this study we introduce a stochastic model supporting
the DAH, that can be seen as a continuous version of a discrete model of Graner
and Glazier. Our approach is based on the mathematical framework of Gibbsian
marked point processes. We provide a Markov chain Monte Carlo algorithm that
can reproduce classical biological patterns, and we propose an estimation
procedure for a parameter that quantifies the strength of adhesion between
cells. This procedure is tested through simulations.

<id>
q-bio/0607019v1
<category>
q-bio.TO
<abstract>
We investigate, both experimentally and theoretically, the bifurcation to
alternans in heart tissue. Previously, this phenomenon has been modeled either
as a smooth or as border-collision period-doubling bifurcation. Using a new
experimental technique, we find a hybrid behavior: very close to the
bifurcation point the dynamics are smooth-like, whereas further away they are
border-collision-like. This behavior is captured by a new type of model, called
an unfolded border-collision bifurcation.

<id>
q-bio/0607047v2
<category>
q-bio.TO
<abstract>
We describe an asymptotic approach to gated ionic models of single-cell
cardiac excitability. It has a form essentially different from the Tikhonov
fast-slow form assumed in standard asymptotic reductions of excitable systems.
This is of interest since the standard approaches have been previously found
inadequate to describe phenomena such as the dissipation of cardiac wave fronts
and the shape of action potential at repolarization. The proposed asymptotic
description overcomes these deficiencies by allowing, among other non-Tikhonov
features, that a dynamical variable may change its character from fast to slow
within a single solution. The general asymptotic approach is best demonstrated
on an example which should be both simple and generic. The classical model of
Purkinje fibers (Noble, 1962) has the simplest functional form of all cardiac
models but according to the current understanding it assigns a physiologically
incorrect role to the Na current. This leads us to suggest an ``Archetypal
Model'' with the simplicity of the Noble model but with a structure more
typical to contemporary cardiac models. We demonstrate that the Archetypal
Model admits a complete asymptotic solution in quadratures. To validate our
asymptotic approach, we proceed to consider an exactly solvable ``caricature''
of the Archetypal Model and demonstrate that the asymptotic of its exact
solution coincides with the solutions obtained by substituting the
``caricature'' right-hand sides into the asymptotic solution of the generic
Archetypal Model. This is necessary, because, unlike in standard asymptotic
descriptions, no general results exist which can guarantee the proximity of the
non-Tikhonov asymptotic solutions to the solutions of the corresponding
detailed ionic model.

<id>
q-bio/0610027v2
<category>
q-bio.TO
<abstract>
Assuming that there is feedback between an expanding cancer system and its
organ-typical microenvironment, we argue here that such local tumor growth is
guided by co-existence rather than competition with the surrounding tissue. We
then present a novel concept that understands cancer dissemination as a
biological mechanism to evade the specific carrying capacity limit of its host
organ. This conceptual framework allows us to relate the tumor system's
volumetric growth rate to the host organ's functionality-conveying composite
infrastructure, and, intriguingly, already provides useful insights into
several clinical findings.

<id>
q-bio/0611048v1
<category>
q-bio.TO
<abstract>
Muscles crossing a joint usually outnumber its degrees of freedom, which
renders the motor system underdetermined. Typically, optimization laws are
postulated to cope with this redundancy. A natural question then arises whether
all muscular load sharing patterns can be regarded as results of optimization.
To answer it, we propose a method of constructing an objective function whose
minimization yields a given load sharing pattern. We give necessary conditions
for this construction to be feasible and investigate its uniqueness. For linear
load sharing patterns the Crowninshield and Brand objective function is
reproduced, nonlinear ones require a more general formula.

<id>
q-bio/0701003v4
<category>
q-bio.TO
<abstract>
In this paper we address some modelling issues related to biological growth.
Our treatment is based on a recently-proposed, general formulation for growth
within the context of Mixture Theory (Journal of the Mechanics and Physics of
Solids, 52, 2004, 1595--1625). We aim to enhance this treatment by making it
more appropriate for the biophysics of growth in porous soft tissue,
specifically tendon. This involves several modifications to the mathematical
formulation to represent the reactions, transport and mechanics, and their
interactions. We also reformulate the governing differential equations for
reaction-transport to represent the incompressibility constraint on the fluid
phase of the tissue. This revision enables a straightforward implementation of
numerical stabilisation for the hyperbolic, or advection-dominated, limit. A
finite element implementation employing an operator splitting scheme is used to
solve the coupled, non-linear partial differential equations that arise from
the theory. Motivated by our experimental model, an in vitro scaffold-free
engineered tendon formed by self-assembly of tendon fibroblasts (Tissue
Engineering, 10, 2004, 755--761), we solve several numerical examples
demonstrating biophysical aspects of tissue growth, and the improved numerical
performance of the models.

<id>
q-bio/0703015v1
<category>
q-bio.TO
<abstract>
Employing a novel two-dimensional computational model we have simulated the
feedback between angiogenesis and tumor growth dynamics. Analyzing vessel
formation and elongation towards the concentration gradient of the
tumor-derived angiogenetic basic fibroblast growth factor, bFGF, we assumed
that prior to the blood vessels reaching the tumor surface, the resulting
pattern of tumor growth is symmetric, circular with a common center point.
However, after the vessels reach the tumor surface, we assumed that the growth
rate of that particular cancer region is accelerated compared to the tumor
surface section that lacks neo-vascularization. Therefore, the resulting
asymmetric tumor growth pattern is biased towards the site of the nourishing
vessels. The simulation results show over time an increase in vessel density, a
decrease in vessel branching length, and an increase in fracticality of the
vascular branching architecture. Interestingly, over time the fractal dimension
displayed a sigmoidal pattern with a reduced rate increase at earlier and later
tumor growth stages due to distinct characteristics in vessel length and
density. The finding that, at later stages, higher vascular fracticality
resulted in a marked increase of tumor slice volume provides further in silico
evidence for a functional impact of vascular patterns on cancer growth.

<id>
0706.3067v1
<category>
q-bio.TO
<abstract>
It is now evident that the commonly accepted strategy for treatment of
HIV/AIDS by highly active antiretroviral therapy (HAART) will not lead to
eradication of HIV in a reasonable time. This is straightforward from the
typical exponential viral load decay upon treatment revealing initial
considerable but incomplete reduction of plasma HIV RNA with subsequent low
level HIV persistence even in patients on effective antiretroviral therapy.
Here we show that the viral load follows a simple zero trend linear regression
line under different treatment approach recently proposed by us. This
unambiguously indicates a whole body HIV eradication in reasonable time.

<id>
0708.0186v1
<category>
q-bio.TO
<abstract>
Prostaglandin E2 (PGE2), is a major prostanoid produced by the activity of
cyclooxygenases (COX) in response to various physiological and pathological
stimuli. PGE2 exerts its effects by activating four specific E-type prostanoid
receptors (EP1, EP2, EP3, and EP4). In the present study, we analyzed the
expression of the PGE2 receptor EP1 (mRNA and protein) in different regions of
the adult rat brain (hippocampus, hypothalamus, striatum, prefrontal cerebral
cortex, parietal cortex, brain stem, and cerebellum) using reverse
transcription- polymerase chain reaction, Western blotting, and
immunohistochemical methods. On a regional basis, levels of EP1 mRNA were the
highest in parietal cortex and cerebellum. At the protein level, we found very
strong expression of EP1 in cerebellum, as revealed by Western blotting
experiments. Furthermore, the present study provides for the first time
evidence that the EP1 receptor is highly expressed in the cerebellum, where the
Purkinje cells displayed very high immunolabeling of their perikaryon and
dendrites, as observed in the immunohistochemical analysis. Results from the
present study indicate that the EP1 prostanoid receptor is expressed in
specific neuronal populations, which possibly determine the region-specific
response to PGE2.

<id>
0708.0559v1
<category>
q-bio.TO
<abstract>
Metabolism of arachidonic acid by cyclooxygenase is one of the primary
sources of reactive oxygen species in the ischemic brain. Neuronal
overexpression of cyclooxygenase-2 has recently been shown to contribute to
neurodegeneration following ischemic injury. In the present study, we examined
the possibility that the neuroprotective effects of the cyclooxygenase-2
inhibitor nimesulide would depend upon reduction of oxidative stress following
cerebral ischemia. Gerbils were subjected to 5 min of transient global cerebral
ischemia followed by 48 h of reperfusion and markers of oxidative stress were
measured in hippocampus of gerbils receiving vehicle or nimesulide treatment at
three different clinically relevant doses (3, 6 or 12 mg/kg). Compared with
vehicle, nimesulide significantly (P<0.05) reduced hippocampal glutathione
depletion and lipid peroxidation, as assessed by the levels of malondialdehyde
(MDA), 4-hydroxy-alkenals (4-HDA) and lipid hydroperoxides levels, even when
the treatment was delayed until 6 h after ischemia. Biochemical evidences of
nimesulide neuroprotection were supported by histofluorescence findings using
the novel marker of neuronal degeneration Fluoro-Jade B. Few Fluoro-Jade B
positive cells were seen in CA1 region of hippocampus in ischemic animals
treated with nimesulide compared with vehicle. These results suggest that
nimesulide may protect neurons by attenuating oxidative stress and reperfusion
injury following the ischemic insult with a wide therapeutic window of
protection.

<id>
cs/9810011v1
<category>
cs.AR
<abstract>
As the one-chip integration of HW-modules designed by different companies
becomes more and more popular reliability of a HW-design and evaluation of the
timing behavior during the prototype stage are absolutely necessary. One way to
guarantee reliability is the use of robust design styles, e.g.,
delay-insensitivity. For early timing evaluation two aspects must be
considered: a) The timing needs to be proportional to technology variations and
b) the implemented architecture should be identical for prototype and target.
The first can be met also by delay-insensitive implementation. The latter one
is the key point. A unified architecture is needed for prototyping as well as
implementation. Our new approach to rapid prototyping of signal processing
tasks is based on a configurable, delay-insensitive implemented processor
called Flysig. In essence, the Flysig processor can be understood as a complex
FPGA where the CLBs are substituted by bit-serial operators. In this paper the
general concept is detailed and first experimental results are given for
demonstration of the main advantages: delay-insensitive design style, direct
correspondence between prototyping and target architecture, high performance
and reasonable shortening of the design cycle.

<id>
cs/0111029v1
<category>
cs.AR
<abstract>
Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson
National Accelerator Facility (Jefferson Lab) with versatile VME-based data
acquisition and control interfaces with minimal development times. FPGA designs
have been used to interface to VME and provide control logic for numerous
systems. The building blocks of these logic designs can be tailored to the
individual needs of each system and provide system operators with read-backs
and controls via a VME interface to an EPICS based computer. This versatility
allows the system developer to choose components and define operating
parameters and options that are not readily available commercially. Jefferson
Lab has begun developing standard FPGA libraries that result in quick turn
around times and inexpensive designs.

<id>
cs/0111030v1
<category>
cs.AR
<abstract>
A Dual Digital Signal Processing VME Board was developed for the Continuous
Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at
Jefferson Lab. It is a versatile general-purpose digital signal processing
board using an open architecture, which allows for adaptation to various
applications. The base design uses two independent Texas Instrument (TI)
TMS320C6711, which are 900 MFLOPS floating-point digital signal processors
(DSP). Applications that require a fixed point DSP can be implemented by
replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The
design can be manufactured with a reduced chip set without redesigning the
printed circuit board. For example it can be implemented as a single-channel
DSP with no analog I/O.

<id>
cs/0111032v1
<category>
cs.AR
<abstract>
This poster describes the timing system being designed for Spallation Neutron
Source being built at Oak Ridge National lab.

<id>
cs/0207012v1
<category>
cs.AR
<abstract>
This paper introduces a novel method for synthesizing digital circuits
derived from Binary Decision Diagrams (BDDs) that can yield to reduction in
power dissipation. The power reduction is achieved by decreasing the switching
activity in a circuit while paying close attention to information measures as
an optimization criterion. We first present the technique of efficient
BDD-based computation of information measures which are used to guide the power
optimization procedures. Using this technique, we have developed an algorithm
of BDD reordering which leads to reducing the power consumption of the circuits
derived from BDDs. Results produced by the synthesis on the ISCAS benchmark
circuits are very encouraging.

<id>
cs/0207014v1
<category>
cs.AR
<abstract>
This paper addresses a new approach to find a spectrum of information
measures for the process of digital circuit synthesis. We consider the problem
from the information engine point of view. The circuit synthesis as a whole and
different steps of the design process (an example of decision diagram is given)
are presented via such measurements as entropy, logical work and information
vitality. We also introduce new information measures to provide better
estimates of synthesis criteria. We show that the basic properties of
information engine, such as the conservation law of information flow and the
equilibrium law of information can be formulated.

<id>
cs/0405015v1
<category>
cs.AR
<abstract>
Reconfigurable computing refers to the use of processors, such as Field
Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to
take on different processing tasks. A reconfigurable computing platform
describes the hardware and software base on top of which modular extensions can
be created, depending on the desired application. Such reconfigurable computing
platforms can take on varied designs and implementations, according to the
constraints imposed and features desired by the scope of applications. This
paper introduces a PC-based reconfigurable computing platform software
frameworks that is flexible and extensible enough to abstract the different
hardware types and functionality that different PCs may have. The requirements
of the software platform, architectural issues addressed, rationale behind the
decisions made, and frameworks design implemented are discussed.

<id>
cs/0407019v2
<category>
cs.AR
<abstract>
A standard approach to building a fuzzy controller based on stochastic logic
uses binary random signals with an average (expected value of a random
variable) in the range [0, 1]. A different approach is presented, founded on a
representation of the membership functions with the probability density
functions.

<id>
cs/0407032v1
<category>
cs.AR
<abstract>
Many reconfigurable platforms require that applications be written
specifically to take advantage of the reconfigurable hardware. In a PC-based
environment, this presents an undesirable constraint in that the many already
available applications cannot leverage on such hardware. Greatest benefit can
only be derived from reconfigurable devices if even native OS applications can
transparently utilize reconfigurable devices as they would normal full-fledged
hardware devices. This paper presents how Proteus Virtual Devices are used to
expose reconfigurable hardware in a transparent manner for use by typical
native OS applications.

<id>
cs/0409025v1
<category>
cs.AR
<abstract>
In the paper we define and characterize the asynchronous systems from the
point of view of their autonomy, determinism, order, non-anticipation, time
invariance, symmetry, stability and other important properties. The study is
inspired by the models of the asynchronous circuits.

<id>
cs/0412040v1
<category>
cs.AR
<abstract>
This paper presents a data stationary architecture in which each word has an
attached address field. Address fields massively update in parallel to record
data interchanges. Words do not move until memory is read for post processing.
A sea of such cells can test large-scale quantum algorithms, although other
programming is possible.

<id>
cs/0503066v1
<category>
cs.AR
<abstract>
Management of communication by on-line routing in new FPGAs with a large
amount of logic resources and partial reconfigurability is a new challenging
problem. A Network-on-Chip
  (NoC) typically uses packet routing mechanism, which has often unsafe data
transfers, and network interface overhead. In this paper, circuit routing for
such dynamic NoCs is investigated, and a practical 1-dimensional network with
an efficient routing algorithm is proposed and implemented. Also, this concept
has been extended to the 2-dimensional case. The implementation results show
the low area overhead and high performance of this network.

<id>
cs/0508038v1
<category>
cs.AR
<abstract>
Wiring diagrams are given for a quantum algorithm processor in CMOS to
compute, in parallel, all divisors of an n-bit integer. Lines required in a
wiring diagram are proportional to n. Execution time is proportional to the
square of n.

<id>
cs/0510039v1
<category>
cs.AR
<abstract>
A new paradigm to support the communication among modules dynamically placed
on a reconfigurable device at run-time is presented. Based on the network on
chip (NoC) infrastructure, we developed a dynamic communication infrastructure
as well as routing methodologies capable to handle routing in a NoC with
obstacles created by dynamically placed components. We prove the unrestricted
reachability of components and pins, the deadlock-freeness and we finally show
the feasibility of our approach by means on real life example applications.

<id>
cs/0602096v1
<category>
cs.AR
<abstract>
This paper reviews various engineering hurdles facing the field of quantum
computing. Specifically, problems related to decoherence, state preparation,
error correction, and implementability of gates are considered.

<id>
cs/0603088v1
<category>
cs.AR
<abstract>
IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. This
paper proposes two novel BCD adders called carry skip and carry look-ahead BCD
adders respectively. Furthermore, in the recent years, reversible logic has
emerged as a promising technology having its applications in low power CMOS,
quantum computing, nanotechnology, and optical computing. It is not possible to
realize quantum computing without reversible logic. Thus, this paper also paper
provides the reversible logic implementation of the conventional BCD adder as
the well as the proposed Carry Skip BCD adder using a recently proposed TSG
gate. Furthermore, a new reversible gate called TS-3 is also being proposed and
it has been shown that the proposed reversible logic implementation of the BCD
Adders is much better compared to recently proposed one, in terms of number of
reversible gates used and garbage outputs produced. The reversible BCD circuits
designed and proposed here form the basis of the decimal ALU of a primitive
quantum CPU.

<id>
cs/0603091v1
<category>
cs.AR
<abstract>
In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. This paper proposes a new 4 * 4 reversible gate called TSG
gate. The proposed gate is used to design efficient adder units. The most
significant aspect of the proposed gate is that it can work singly as a
reversible full adder i.e reversible full adder can now be implemented with a
single gate only. The proposed gate is then used to design reversible ripple
carry and carry skip adders. It is demonstrated that the adder architectures
designed using the proposed gate are much better and optimized, compared to
their existing counterparts in literature; in terms of number of reversible
gates and garbage outputs. Thus, this paper provides the initial threshold to
building of more complex system which can execute more complicated operations
using reversible logic.

<id>
cs/0603092v1
<category>
cs.AR
<abstract>
In recent years, reversible logic has emerged as a promising computing
paradigm having its applications in low power computing, quantum computing,
nanotechnology, optical computing and DNA computing. The classical set of gates
such as AND, OR, and EXOR are not reversible. Recently, it has been shown how
to encode information in DNA and use DNA amplification to implement Fredkin
gates. Furthermore, in the past Fredkin gates have been constructed using DNA,
whose outputs are used as inputs for other Fredkin gates. Thus, it can be
concluded that arbitrary circuits of Fredkin gates can be constructed using
DNA. This paper provides the initial threshold to building of more complex
system having reversible sequential circuits and which can execute more
complicated operations. The novelty of the paper is the reversible designs of
sequential circuits using Fredkin gate. Since, Fredkin gate has already been
realized using DNA, it is expected that this work will initiate the building of
complex systems using DNA. The reversible circuits designed here are highly
optimized in terms of number of gates and garbage outputs. The modularization
approach that is synthesizing small circuits and thereafter using them to
construct bigger circuits is used for designing the optimal reversible
sequential circuits.

<id>
cs/0605004v1
<category>
cs.AR
<abstract>
In the recent years, reversible logic has emerged as a promising technology
having its applications in low power CMOS, quantum computing, nanotechnology,
and optical computing. The classical set of gates such as AND, OR, and EXOR are
not reversible. Recently a 4 * 4 reversible gate called TSG is proposed. The
most significant aspect of the proposed gate is that it can work singly as a
reversible full adder, that is reversible full adder can now be implemented
with a single gate only. This paper proposes a NXN reversible multiplier using
TSG gate. It is based on two concepts. The partial products can be generated in
parallel with a delay of d using Fredkin gates and thereafter the addition can
be reduced to log2N steps by using reversible parallel adder designed from TSG
gates. Similar multiplier architecture in conventional arithmetic (using
conventional logic) has been reported in existing literature, but the proposed
one in this paper is totally based on reversible logic and reversible cells as
its building block. A 4x4 architecture of the proposed reversible multiplier is
also designed. It is demonstrated that the proposed multiplier architecture
using the TSG gate is much better and optimized, compared to its existing
counterparts in literature; in terms of number of reversible gates and garbage
outputs. Thus, this paper provides the initial threshold to building of more
complex system which can execute more complicated operations using reversible
logic.

<id>
cs/0605125v1
<category>
cs.AR
<abstract>
We detail a procedure for the computation of the polynomial form of an
electronic combinational circuit from the design equations in a truth table.
The method uses the Buchberger algorithm rather than current traditional
methods based on search algorithms. We restrict the analysis to a single
output, but the procedure can be generalized to multiple outputs. The procedure
is illustrated with the design of a simple arithmetic and logic unit with two
3-bit operands and two control bits.

<id>
cs/0605142v1
<category>
cs.AR
<abstract>
The systems supporting signal and image applications process large amount of
data. That involves an intensive use of the memory which becomes the bottleneck
of systems. Memory limits performances and represents a significant proportion
of total consumption. In the development high level synthesis tool called GAUT
Low Power, we are interested in the synthesis of the memory unit. In this work,
we integrate the data storage and data transfert to constraint the high level
synthesis of the datapath's execution unit.

<id>
cs/0605143v1
<category>
cs.AR
<abstract>
The design of complex Systems-on-Chips implies to take into account
communication and memory access constraints for the integration of dedicated
hardware accelerator. In this paper, we present a methodology and a tool that
allow the High-Level Synthesis of DSP algorithm, under both I/O timing and
memory constraints. Based on formal models and a generic architecture, this
tool helps the designer to find a reasonable trade-off between both the
required I/O timing behavior and the internal memory access parallelism of the
circuit. The interest of our approach is demonstrated on the case study of a
FFT algorithm.

<id>
cs/0605144v1
<category>
cs.AR
<abstract>
We introduce a new approach to take into account the memory architecture and
the memory mapping in High- Level Synthesis for data intensive applications. We
formalize the memory mapping as a set of constraints for the synthesis, and
defined a Memory Constraint Graph and an accessibility criterion to be used in
the scheduling step. We use a memory mapping file to include those memory
constraints in our HLS tool GAUT. It is possible, with the help of GAUT, to
explore a wide range of solutions, and to reach a good tradeoff between time,
power-consumption, and area.

<id>
cs/0605145v1
<category>
cs.AR
<abstract>
We introduce a new approach to take into account the memory architecture and
the memory mapping in the High- Level Synthesis of Real-Time embedded systems.
We formalize the memory mapping as a set of constraints used in the scheduling
step. We use a memory mapping file to include those memory constraints in our
HLS tool GAUT. Our scheduling algorithm exhibits a relatively low complexity
that permits to tackle complex designs in a reasonable time. Finally, we show
how to explore, with the help of GAUT, a wide range of solutions, and to reach
a good tradeoff between time, power-consumption, and area.

<id>
cs/0605146v1
<category>
cs.AR
<abstract>
The design of complex Digital Signal Processing systems implies to minimize
architectural cost and to maximize timing performances while taking into
account communication and memory accesses constraints for the integration of
dedicated hardware accelerator. Unfortunately, the traditional Matlab/ Simulink
design flows gather not very flexible hardware blocs. In this paper, we present
a methodology and a tool that permit the High-Level Synthesis of DSP
applications, under both I/O timing and memory constraints. Based on formal
models and a generic architecture, our tool GAUT helps the designer in finding
a reasonable trade-off between the circuit's performance and its architectural
complexity. The efficiency of our approach is demonstrated on the case study of
a FFT algorithm.

<id>
cs/0608075v1
<category>
cs.AR
<abstract>
Media-processing applications, such as signal processing, 2D and 3D graphics
rendering, and image compression, are the dominant workloads in many embedded
systems today. The real-time constraints of those media applications have
taxing demands on today's processor performances with low cost, low power and
reduced design delay. To satisfy those challenges, a fast and efficient
strategy consists in upgrading a low cost general purpose processor core. This
approach is based on the personalization of a general RISC processor core
according the target multimedia application requirements. Thus, if the extra
cost is justified, the general purpose processor GPP core can be enforced with
instruction level coprocessors, coarse grain dedicated hardware, ad hoc
memories or new GPP cores. In this way the final design solution is tailored to
the application requirements. The proposed approach is based on three main
steps: the first one is the analysis of the targeted application using
efficient metrics. The second step is the selection of the appropriate
architecture template according to the first step results and recommendations.
The third step is the architecture generation. This approach is experimented
using various image and video algorithms showing its feasibility.

<id>
cs/0609023v1
<category>
cs.AR
<abstract>
In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. This paper utilizes a new 4 * 4 reversible
gate called TSG gate to build the components of a primitive reversible/quantum
ALU. The most significant aspect of the TSG gate is that it can work singly as
a reversible full adder, that is reversible full adder can now be implemented
with a single gate only. A Novel reversible 4:2 compressor is also designed
from the TSG gate which is later used to design a novel 8x8 reversible Wallace
tree multiplier. It is proved that the adder, 4:2 compressor and multiplier
architectures designed using the TSG gate are better than their counterparts
available in literature, in terms of number of reversible gates and garbage
outputs. This is perhaps, the first attempt to design a reversible 4:2
compressor and a reversible Wallace tree multiplier as far as existing
literature and our knowledge is concerned. Thus, this paper provides an initial
threshold to build more complex systems which can execute complicated
operations using reversible logic.

<id>
cs/0609028v1
<category>
cs.AR
<abstract>
This paper proposes the hardware implementation of RSA encryption/decryption
algorithm using the algorithms of Ancient Indian Vedic Mathematics that have
been modified to improve performance. The recently proposed hierarchical
overlay multiplier architecture is used in the RSA circuitry for multiplication
operation. The most significant aspect of the paper is the development of a
division architecture based on Straight Division algorithm of Ancient Indian
Vedic Mathematics and embedding it in RSA encryption/decryption circuitry for
improved efficiency. The coding is done in Verilog HDL and the FPGA synthesis
is done using Xilinx Spartan library. The results show that RSA circuitry
implemented using Vedic division and multiplication is efficient in terms of
area/speed compared to its implementation using conventional multiplication and
division architectures

<id>
cs/0609029v1
<category>
cs.AR
<abstract>
In recent years, reversible logic has emerged as a promising computing
paradigm having application in low power CMOS, quantum computing,
nanotechnology, and optical computing. The classical set of gates such as AND,
OR, and EXOR are not reversible. In this paper, the contributors have proposed
reversible programmable logic array (RPLA) architecture using reversible
Fredkin and Feynman gates. The proposed RPLA has n inputs and m outputs and can
realize m functions of n variables. In order to demonstrate the design of RPLA,
a 3 input RPLA is designed which can perform any 28 functions using the
combination of 8 min terms (23). Furthermore, the application of the designed 3
input RPLA is shown by implementing the full adder and full subtractor
functions through it.

<id>
cs/0609036v1
<category>
cs.AR
<abstract>
IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and
a major enhancement to the standard is the addition of decimal format. Firstly,
this paper proposes novel two transistor AND and OR gates. The proposed AND
gate has no power supply, thus it can be referred as the Powerless AND gate.
Similarly, the proposed two transistor OR gate has no ground and can be
referred as Groundless OR. Secondly for IEEE 754r format, two novel BCD adders
called carry skip and carry look-ahead BCD adders are also proposed in this
paper. In order to design the carry look-ahead BCD adder, a novel 4 bit carry
look-ahead adder called NCLA is proposed which forms the basic building block
of the proposed carry look-ahead BCD adder. Finally, the proposed two
transistors AND and OR gates are used to provide the optimized small area low
power high throughput circuitries of the proposed BCD adders.

<id>
cs/9308101v1
<category>
cs.AI
<abstract>
Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches.

<id>
cs/9308102v1
<category>
cs.AI
<abstract>
Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms.

<id>
cs/9309101v1
<category>
cs.AI
<abstract>
We describe an extensive study of search in GSAT, an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search: rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems,
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase, the
average gradient of this phase, and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms.

<id>
cs/9311101v1
<category>
cs.AI
<abstract>
As real logic programmers normally use cut (!), an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning, clauses containing cut cannot be
learned using an extensional evaluation method, as is done in most learning
systems. On the other hand, searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples, and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused, in general, by the need for intensional evaluation. As a conclusion,
the analysis of this paper suggests, on precise and technical grounds, that
learning cut is difficult, and current induction techniques should probably be
restricted to purely declarative logic languages.

<id>
cs/9311102v1
<category>
cs.AI
<abstract>
To support the goal of allowing users to record and retrieve information,
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First, it actively predicts what the user is
going to write. Second, it automatically constructs a custom, button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix: People like to record information. Doing this
on paper is initially efficient, but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre, the user records information directly on a computer. Behind the
interface, an agent acts for the user. To help, it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime.

<id>
cs/9312101v1
<category>
cs.AI
<abstract>
Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied, often required in practical applications, can be summarized
in three main points. First, we consider a highly expressive terminological
language, called ALCNR, including general complements of concepts, number
restrictions and role conjunction. Second, we allow to express inclusion
statements between general concepts, and terminological cycles as a particular
case. Third, we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability, subsumption and instance checking) through a
sound, complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof, we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles, if descriptive semantics is
adopted.

<id>
cs/9401101v1
<category>
cs.AI
<abstract>
A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback, T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition, T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots.

<id>
cs/9402101v1
<category>
cs.AI
<abstract>
Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms.

<id>
cs/9402102v1
<category>
cs.AI
<abstract>
The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data, multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar, but not identical, instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix: This is a compressed tar file containing the SUBDUE discovery
system, written in C. The program accepts as input databases represented in
graph form, and will output discovered substructures with their corresponding
value.

<id>
cs/9402103v1
<category>
cs.AI
<abstract>
The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here, called
PTR, uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.

<id>
cs/9403101v1
<category>
cs.AI
<abstract>
We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees.

<id>
cs/9406101v1
<category>
cs.AI
<abstract>
This paper analyzes the correctness of the subsumption algorithm used in
CLASSIC, a description logic-based knowledge representation system that is
being used in practical applications. In order to deal efficiently with
individuals in CLASSIC descriptions, the developers have had to use an
algorithm that is incomplete with respect to the standard, model-theoretic
semantics for description logics. We provide a variant semantics for
descriptions with respect to which the current implementation is complete, and
which can be independently motivated. The soundness and completeness of the
polynomial-time subsumption algorithm is established using description graphs,
which are an abstracted version of the implementation structures used in
CLASSIC, and are of independent interest.

<id>
cs/9406102v1
<category>
cs.AI
<abstract>
In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time, without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far.

<id>
cs/9408101v1
<category>
cs.AI
<abstract>
Given a knowledge base KB containing first-order and statistical facts, we
consider a principled method, called the random-worlds method, for computing a
degree of belief that some formula Phi holds given KB. If we are reasoning
about a world or system consisting of N individuals, then we can consider all
possible worlds, or first-order models, with domain {1,...,N} that satisfy KB,
and compute the fraction of them in which Phi is true. We define the degree of
belief to be the asymptotic value of this fraction as N grows large. We show
that when the vocabulary underlying Phi and KB uses constants and unary
predicates only, we can naturally associate an entropy with each world. As N
grows larger, there are many more worlds with higher entropy. Therefore, we can
use a maximum-entropy computation to compute the degree of belief. This result
is in a similar spirit to previous work in physics and artificial intelligence,
but is far more general. Of equal interest to the result itself are the
limitations on its scope. Most importantly, the restriction to unary predicates
seems necessary. Although the random-worlds method makes sense in general, the
connection to maximum entropy seems to disappear in the non-unary case. These
observations suggest unexpected limitations to the applicability of
maximum-entropy methods.

<id>
cs/9408102v1
<category>
cs.AI
<abstract>
Information extraction is the task of automatically picking up information of
interest from an unconstrained text. Information of interest is usually
extracted in two steps. First, sentence level processing locates relevant
pieces of information scattered throughout the text; second, discourse
processing merges coreferential information to generate the output. In the
first step, pieces of information are locally identified without recognizing
any relationships among them. A key word search or simple pattern search can
achieve this purpose. The second step requires deeper knowledge in order to
understand relationships among separately identified pieces of information.
Previous information extraction systems focused on the first step, partly
because they were not required to link up each piece of information with other
pieces. To link the extracted pieces of information and map them onto a
structured output format, complex discourse processing is essential. This paper
reports on a Japanese information extraction system that merges information
using a pattern matcher and discourse processor. Evaluation results show a high
level of system performance which approaches human performance.

<id>
cs/9408103v1
<category>
cs.AI
<abstract>
This article describes a new system for induction of oblique decision trees.
This system, OC1, combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric, although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies, using both real and artificial data, that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees.

<id>
cs/9409101v1
<category>
cs.AI
<abstract>
This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems, a plan can be presented and verified in a reasonable time. However,
coming up algorithmically with a plan, even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes, and show that, in most natural cases, the verification (projection)
part can be carried out in an efficient algorithmic manner.

<id>
cs/9412101v1
<category>
cs.AI
<abstract>
The vast amounts of on-line text now available have led to renewed interest
in information extraction (IE) systems that analyze unrestricted text,
producing a structured representation of selected information from the text.
This paper presents a novel approach that uses machine learning to acquire
knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE
discourse component that makes intersentential inferences and identifies
logical relations among information extracted from the text. Previous
corpus-based approaches were limited to lower level processing such as
part-of-speech tagging, lexical disambiguation, and dictionary construction.
Wrap-Up is fully trainable, and not only automatically decides what classifiers
are needed, but even derives the feature set for each classifier automatically.
Performance equals that of a partially trainable discourse module requiring
manual customization for each domain.

<id>
cs/9412102v1
<category>
cs.AI
<abstract>
This paper is a multidisciplinary review of empirical, statistical learning
from a graphical model perspective. Well-known examples of graphical models
include Bayesian networks, directed graphs representing a Markov chain, and
undirected networks representing a Markov field. These graphical models are
extended to model data analysis and empirical learning using the notation of
plates. Graphical operations for simplifying and manipulating a problem are
provided including decomposition, differentiation, and the manipulation of
probability models from the exponential family. Two standard algorithm schemas
for learning are reviewed in a graphical framework: Gibbs sampling and the
expectation maximization algorithm. Using these operations and schemas, some
popular algorithms can be synthesized from their graphical specification. This
includes versions of linear regression, techniques for feed-forward networks,
and learning Gaussian and discrete Bayesian networks from data. The paper
concludes by sketching some implications for data analysis and summarizing how
some popular algorithms fall within the framework presented. The main original
contributions here are the decomposition techniques and the demonstration that
graphical models provide a framework for understanding and developing complex
learning algorithms.

<id>
cs/9412103v1
<category>
cs.AI
<abstract>
For many years, the intuitions underlying partial-order planning were largely
taken for granted. Only in the past few years has there been renewed interest
in the fundamental principles underlying this paradigm. In this paper, we
present a rigorous comparative analysis of partial-order and total-order
planning by focusing on two specific planners that can be directly compared. We
show that there are some subtle assumptions that underly the wide-spread
intuitions regarding the supposed efficiency of partial-order planning. For
instance, the superiority of partial-order planning can depend critically upon
the search strategy and the structure of the search space. Understanding the
underlying assumptions is crucial for constructing efficient planners.

<id>
cs/9501101v1
<category>
cs.AI
<abstract>
Multiclass learning problems involve finding a definition for an unknown
function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k
``classes''). The definition is acquired by studying collections of training
examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning
problems include direct application of multiclass algorithms such as the
decision-tree algorithms C4.5 and CART, application of binary concept learning
algorithms to learn individual binary functions for each of the k classes, and
application of binary concept learning algorithms with distributed output
representations. This paper compares these three approaches to a new technique
in which error-correcting codes are employed as a distributed output
representation. We show that these output representations improve the
generalization performance of both C4.5 and backpropagation on a wide range of
multiclass learning tasks. We also demonstrate that this approach is robust
with respect to changes in the size of the training sample, the assignment of
distributed representations to particular classes, and the application of
overfitting avoidance techniques such as decision-tree pruning. Finally, we
show that---like the other methods---the error-correcting code technique can
provide reliable class probability estimates. Taken together, these results
demonstrate that error-correcting output codes provide a general-purpose method
for improving the performance of inductive learning programs on multiclass
problems.

<id>
cs/9501102v1
<category>
cs.AI
<abstract>
The paradigms of transformational planning, case-based planning, and plan
debugging all involve a process known as plan adaptation - modifying or
repairing an old plan so it solves a new problem. In this paper we provide a
domain-independent algorithm for plan adaptation, demonstrate that it is sound,
complete, and systematic, and compare it to other adaptation algorithms in the
literature. Our approach is based on a view of planning as searching a graph of
partial plans. Generative planning starts at the graph's root and moves from
node to node using plan-refinement operators. In planning by adaptation, a
library plan - an arbitrary node in the plan graph - is the starting point for
the search, and the plan-adaptation algorithm can apply both the same
refinement operators available to a generative planner and can also retract
constraints and steps from the plan. Our algorithm's completeness ensures that
the adaptation algorithm will eventually search the entire graph and its
systematicity ensures that it will do so without redundantly searching any
parts of the graph.

<id>
cs/9501103v1
<category>
cs.AI
<abstract>
Temporal difference (TD) methods constitute a class of methods for learning
predictions in multi-step prediction problems, parameterized by a recency
factor lambda. Currently the most important application of these methods is to
temporal credit assignment in reinforcement learning. Well known reinforcement
learning algorithms, such as AHC or Q-learning, may be viewed as instances of
TD learning. This paper examines the issues of the efficient and general
implementation of TD(lambda) for arbitrary lambda, for use with reinforcement
learning algorithms optimizing the discounted sum of rewards. The traditional
approach, based on eligibility traces, is argued to suffer from both
inefficiency and lack of generality. The TTD (Truncated Temporal Differences)
procedure is proposed as an alternative, that indeed only approximates
TD(lambda), but requires very little computation per action and can be used
with arbitrary function representation methods. The idea from which it is
derived is fairly simple and not new, but probably unexplored so far.
Encouraging experimental results are presented, suggesting that using lambda
&gt 0 with the TTD procedure allows one to obtain a significant learning
speedup at essentially the same cost as usual TD(0) learning.

<id>
cs/9503102v1
<category>
cs.AI
<abstract>
This paper introduces ICET, a new algorithm for cost-sensitive
classification. ICET uses a genetic algorithm to evolve a population of biases
for a decision tree induction algorithm. The fitness function of the genetic
algorithm is the average cost of classification when using the decision tree,
including both the costs of tests (features, measurements) and the costs of
classification errors. ICET is compared here with three other algorithms for
cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,
which classifies without regard to cost. The five algorithms are evaluated
empirically on five real-world medical datasets. Three sets of experiments are
performed. The first set examines the baseline performance of the five
algorithms on the five datasets and establishes that ICET performs
significantly better than its competitors. The second set tests the robustness
of ICET under a variety of conditions and shows that ICET maintains its
advantage. The third set looks at ICET's search in bias space and discovers a
way to improve the search.

<id>
cs/9504101v1
<category>
cs.AI
<abstract>
Theory revision integrates inductive learning and background knowledge by
combining training examples with a coarse domain theory to produce a more
accurate theory. There are two challenges that theory revision and other
theory-guided systems face. First, a representation language appropriate for
the initial theory may be inappropriate for an improved theory. While the
original representation may concisely express the initial theory, a more
accurate theory forced to use that same representation may be bulky,
cumbersome, and difficult to reach. Second, a theory structure suitable for a
coarse domain theory may be insufficient for a fine-tuned theory. Systems that
produce only small, local changes to a theory have limited value for
accomplishing complex structural alterations that may be required.
Consequently, advanced theory-guided learning systems require flexible
representation and flexible structure. An analysis of various theory revision
systems and theory-guided learning systems reveals specific strengths and
weaknesses in terms of these two desired properties. Designed to capture the
underlying qualities of each system, a new system uses theory-guided
constructive induction. Experiments in three domains show improvement over
previous theory-guided systems. This leads to a study of the behavior,
limitations, and potential of theory-guided constructive induction.

<id>
cs/9505101v1
<category>
cs.AI
<abstract>
Many studies have been carried out in order to increase the search efficiency
of constraint satisfaction problems; among them, some make use of structural
properties of the constraint network; others take into account semantic
properties of the constraints, generally assuming that all the constraints
possess the given property. In this paper, we propose a new decomposition
method benefiting from both semantic properties of functional constraints (not
bijective constraints) and structural properties of the network; furthermore,
not all the constraints need to be functional. We show that under some
conditions, the existence of solutions can be guaranteed. We first characterize
a particular subset of the variables, which we name a root set. We then
introduce pivot consistency, a new local consistency which is a weak form of
path consistency and can be achieved in O(n^2d^2) complexity (instead of
O(n^3d^3) for path consistency), and we present associated properties; in
particular, we show that any consistent instantiation of the root set can be
linearly extended to a solution, which leads to the presentation of the
aforementioned new method for solving by decomposing functional CSPs.

<id>
cs/9505102v1
<category>
cs.AI
<abstract>
We study the process of multi-agent reinforcement learning in the context of
load balancing in a distributed system, without use of either central
coordination or explicit communication. We first define a precise framework in
which to study adaptive load balancing, important features of which are its
stochastic nature and the purely local information available to individual
agents. Given this framework, we show illuminating results on the interplay
between basic adaptive behavior parameters and their effect on system
efficiency. We then investigate the properties of adaptive load balancing in
heterogeneous populations, and address the issue of exploration vs.
exploitation in that context. Finally, we show that naive use of communication
may not improve, and might even harm system efficiency.

<id>
cs/9505103v1
<category>
cs.AI
<abstract>
Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory.

<id>
cs/9505104v1
<category>
cs.AI
<abstract>
We present algorithms that learn certain classes of function-free recursive
logic programs in polynomial time from equivalence queries. In particular, we
show that a single k-ary recursive constant-depth determinate clause is
learnable. Two-clause programs consisting of one learnable recursive clause and
one constant-depth determinate non-recursive clause are also learnable, if an
additional ``basecase'' oracle is assumed. These results immediately imply the
pac-learnability of these classes. Although these classes of learnable
recursive programs are very constrained, it is shown in a companion paper that
they are maximally general, in that generalizing either class in any natural
way leads to a computationally difficult learning problem. Thus, taken together
with its companion paper, this paper establishes a boundary of efficient
learnability for recursive logic programs.

<id>
cs/9505105v1
<category>
cs.AI
<abstract>
In a companion paper it was shown that the class of constant-depth
determinate k-ary recursive clauses is efficiently learnable. In this paper we
present negative results showing that any natural generalization of this class
is hard to learn in Valiant's model of pac-learnability. In particular, we show
that the following program classes are cryptographically hard to learn:
programs with an unbounded number of constant-depth linear recursive clauses;
programs with one constant-depth determinate clause containing an unbounded
number of recursive calls; and programs with one linear recursive clause of
constant locality. These results immediately imply the non-learnability of any
more general class of programs. We also show that learning a constant-depth
determinate program with either two linear recursive clauses or one linear
recursive clause and one non-recursive clause is as hard as learning boolean
DNF. Together with positive results from the companion paper, these negative
results establish a boundary of efficient learnability for recursive
function-free clauses.

<id>
cs/9809020v1
<category>
cs.CL
<abstract>
We present a new method for discovering a segmental discourse structure of a
document while categorizing segment function. We demonstrate how retrieval of
noun phrases and pronominal forms, along with a zero-sum weighting scheme,
determines topicalized segmentation. Futhermore, we use term distribution to
aid in identifying the role that the segment performs in the document. Finally,
we present results of evaluation in terms of precision and recall which surpass
earlier approaches.

<id>
cs/9809022v1
<category>
cs.CL
<abstract>
We outline how utterances in dialogs can be interpreted using a partial first
order logic. We exploit the capability of this logic to talk about the truth
status of formulae to define a notion of coherence between utterances and
explain how this coherence relation can serve for the construction of AND/OR
trees that represent the segmentation of the dialog. In a BDI model we
formalize basic assumptions about dialog and cooperative behaviour of
participants. These assumptions provide a basis for inferring speech acts from
coherence relations between utterances and attitudes of dialog participants.
Speech acts prove to be useful for determining dialog segments defined on the
notion of completing expectations of dialog participants. Finally, we sketch
how explicit segmentation signalled by cue phrases and performatives is covered
by our dialog model.

<id>
cs/9809024v2
<category>
cs.CL
<abstract>
This document describes a sizable grammar of English written in the TAG
formalism and implemented for use with the XTAG system. This report and the
grammar described herein supersedes the TAG grammar described in an earlier
1995 XTAG technical report. The English grammar described in this report is
based on the TAG formalism which has been extended to include lexicalization,
and unification-based feature structures. The range of syntactic phenomena that
can be handled is large and includes auxiliaries (including inversion), copula,
raising and small clause constructions, topicalization, relative clauses,
infinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO
constructions, noun-noun modifications, extraposition, determiner sequences,
genitives, negation, noun-verb contractions, sentential adjuncts and
imperatives. This technical report corresponds to the XTAG Release 8/31/98. The
XTAG grammar is continuously updated with the addition of new analyses and
modification of old ones, and an online version of this report can be found at
the XTAG web page at http://www.cis.upenn.edu/~xtag/

<id>
cs/9809026v1
<category>
cs.CL
<abstract>
Language models for speech recognition typically use a probability model of
the form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other
hand, are typically used to assign structure to utterances. A language model of
the above form is constructed from such grammars by computing the prefix
probability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all
possible terminations of the prefix a_1 ... a_n. The main result in this paper
is an algorithm to compute such prefix probabilities given a stochastic Tree
Adjoining Grammar (TAG). The algorithm achieves the required computation in
O(n^6) time. The probability of subderivations that do not derive any words in
the prefix, but contribute structurally to its derivation, are precomputed to
achieve termination. This algorithm enables existing corpus-based estimation
techniques for stochastic TAGs to be used for language modelling.

<id>
cs/9809027v1
<category>
cs.CL
<abstract>
Much of the power of probabilistic methods in modelling language comes from
their ability to compare several derivations for the same string in the
language. An important starting point for the study of such cross-derivational
properties is the notion of _consistency_. The probability model defined by a
probabilistic grammar is said to be _consistent_ if the probabilities assigned
to all the strings in the language sum to one. From the literature on
probabilistic context-free grammars (CFGs), we know precisely the conditions
which ensure that consistency is true for a given CFG. This paper derives the
conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can
be shown to be consistent. It gives a simple algorithm for checking consistency
and gives the formal justification for its correctness. The conditions derived
here can be used to ensure that probability models that use TAGs can be checked
for _deficiency_ (i.e. whether any probability mass is assigned to strings that
cannot be generated).

<id>
cs/9809028v1
<category>
cs.CL
<abstract>
In this paper we present a new tree-rewriting formalism called Link-Sharing
Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using
LSTAG we define an approach towards coordination where linguistic dependency is
distinguished from the notion of constituency. Such an approach towards
coordination that explicitly distinguishes dependencies from constituency gives
a better formal understanding of its representation when compared to previous
approaches that use tree-rewriting systems which conflate the two issues.

<id>
cs/9809029v1
<category>
cs.CL
<abstract>
This paper describes the incremental generation of parse tables for the
LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented
handles modifications to the input grammar by updating the parser generated so
far. In this paper, a lazy generation of LR-type parsers for TALs is defined in
which parse tables are created by need while parsing. We then describe an
incremental parser generator for TALs which responds to modification of the
input grammar by updating parse tables built so far.

<id>
cs/9809050v1
<category>
cs.CL
<abstract>
In this paper we present Morphy, an integrated tool for German morphology,
part-of-speech tagging and context-sensitive lemmatization. Its large lexicon
of more than 320,000 word forms plus its ability to process German compound
nouns guarantee a wide morphological coverage. Syntactic ambiguities can be
resolved with a standard statistical part-of-speech tagger. By using the output
of the tagger, the lemmatizer can determine the correct root even for ambiguous
word forms. The complete package is freely available and can be downloaded from
the World Wide Web.

<id>
cs/9809106v1
<category>
cs.CL
<abstract>
The lexical acquisition system presented in this paper incrementally updates
linguistic properties of unknown words inferred from their surrounding context
by parsing sentences with an HPSG grammar for German. We employ a gradual,
information-based concept of ``unknownness'' providing a uniform treatment for
the range of completely known to maximally unknown lexical entries. ``Unknown''
information is viewed as revisable information, which is either generalizable
or specializable. Updating takes place after parsing, which only requires a
modified lexical lookup. Revisable pieces of information are identified by
grammar-specified declarations which provide access paths into the parse
feature structure. The updating mechanism revises the corresponding places in
the lexical feature structures iff the context actually provides new
information. For revising generalizable information, type union is required. A
worked-out example demonstrates the inferential capacity of our implemented
system.

<id>
cs/9809107v1
<category>
cs.CL
<abstract>
This paper describes a computational, declarative approach to prosodic
morphology that uses inviolable constraints to denote small finite candidate
sets which are filtered by a restrictive incremental optimization mechanism.
The new approach is illustrated with an implemented fragment of Modern Hebrew
verbs couched in MicroCUF, an expressive constraint logic formalism. For
generation and parsing of word forms, I propose a novel off-line technique to
eliminate run-time optimization. It produces a finite-state oracle that
efficiently restricts the constraint interpreter's search space. As a
byproduct, unknown words can be analyzed without special mechanisms. Unlike
pure finite-state transducer approaches, this hybrid setup allows for more
expressivity in constraints to specify e.g. token identity for reduplication or
arithmetic constraints for phonetics.

<id>
cs/9809112v1
<category>
cs.CL
<abstract>
This paper addresses the issue of {\sc pos} tagger evaluation. Such
evaluation is usually performed by comparing the tagger output with a reference
test corpus, which is assumed to be error-free. Currently used corpora contain
noise which causes the obtained performance to be a distortion of the real
value. We analyze to what extent this distortion may invalidate the comparison
between taggers or the measure of the improvement given by a new system. The
main conclusion is that a more rigorous testing experimentation
setting/designing is needed to reliably evaluate and compare tagger accuracies.

<id>
cs/9809113v1
<category>
cs.CL
<abstract>
We present a bootstrapping method to develop an annotated corpus, which is
specially useful for languages with few available resources. The method is
being applied to develop a corpus of Spanish of over 5Mw. The method consists
on taking advantage of the collaboration of two different POS taggers. The
cases in which both taggers agree present a higher accuracy and are used to
retrain the taggers.

<id>
cs/9810014v1
<category>
cs.CL
<abstract>
We report on two corpora to be used in the evaluation of component systems
for the tasks of (1) linear segmentation of text and (2) summary-directed
sentence extraction. We present characteristics of the corpora, methods used in
the collection of user judgments, and an overview of the application of the
corpora to evaluating the component system. Finally, we discuss the problems
and issues with construction of the test set which apply broadly to the
construction of evaluation resources for language technologies.

<id>
cs/9810015v1
<category>
cs.CL
<abstract>
Several methods are known for parsing languages generated by Tree Adjoining
Grammars (TAGs) in O(n^6) worst case running time. In this paper we investigate
which restrictions on TAGs and TAG derivations are needed in order to lower
this O(n^6) time complexity, without introducing large runtime constants, and
without losing any of the generative power needed to capture the syntactic
constructions in natural language that can be handled by unrestricted TAGs. In
particular, we describe an algorithm for parsing a strict subclass of TAG in
O(n^5), and attempt to show that this subclass retains enough generative power
to make it useful in the general case.

<id>
cs/9811008v1
<category>
cs.CL
<abstract>
This paper argues that an interlingual representation must explicitly
represent some parts of the meaning of a situation as possibilities (or
preferences), not as necessary or definite components of meaning (or
constraints). Possibilities enable the analysis and generation of nuance,
something required for faithful translation. Furthermore, the representation of
the meaning of words, especially of near-synonyms, is crucial, because it
specifies which nuances words can convey in which contexts.

<id>
cs/9811009v1
<category>
cs.CL
<abstract>
This paper presents a partial solution to a component of the problem of
lexical choice: choosing the synonym most typical, or expected, in context. We
apply a new statistical approach to representing the context of a word through
lexical co-occurrence networks. The implementation was trained and evaluated on
a large corpus, and results show that the inclusion of second-order
co-occurrence relations improves the performance of our implemented lexical
choice program.

<id>
cs/9811016v1
<category>
cs.CL
<abstract>
In this paper we present the results of comparing a statistical tagger for
German based on decision trees and a rule-based Brill-Tagger for German. We
used the same training corpus (and therefore the same tag-set) to train both
taggers. We then applied the taggers to the same test corpus and compared their
respective behavior and in particular their error rates. Both taggers perform
similarly with an error rate of around 5%. From the detailed error analysis it
can be seen that the rule-based tagger has more problems with unknown words
than the statistical tagger. But the results are opposite for tokens that are
many-ways ambiguous. If the unknown words are fed into the taggers with the
help of an external lexicon (such as the Gertwol system) the error rate of the
rule-based tagger drops to 4.7%, and the respective rate of the statistical
taggers drops to around 3.7%. Combining the taggers by using the output of one
tagger to help the other did not lead to any further improvement.

<id>
cs/9811022v2
<category>
cs.CL
<abstract>
The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words--binary-parse-structure with headword annotation and
operates in a left-to-right manner --- therefore usable for automatic speech
recognition. The model, its probabilistic parameterization, and a set of
experiments meant to evaluate its predictive power are presented; an
improvement over standard trigram modeling is achieved.

<id>
cs/9811025v2
<category>
cs.CL
<abstract>
The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words - binary-parse-structure with headword annotation. The
model, its probabilistic parametrization, and a set of experiments meant to
evaluate its predictive power are presented.

<id>
cs/9812001v3
<category>
cs.CL
<abstract>
In this thesis, I address the problem of automatically acquiring lexical
semantic knowledge, especially that of case frame patterns, from large corpus
data and using the acquired knowledge in structural disambiguation. The
approach I adopt has the following characteristics: (1) dividing the problem
into three subproblems: case slot generalization, case dependency learning, and
word clustering (thesaurus construction). (2) viewing each subproblem as that
of statistical estimation and defining probability models for each subproblem,
(3) adopting the Minimum Description Length (MDL) principle as learning
strategy, (4) employing efficient learning algorithms, and (5) viewing the
disambiguation problem as that of statistical prediction. Major contributions
of this thesis include: (1) formalization of the lexical knowledge acquisition
problem, (2) development of a number of learning methods for lexical knowledge
acquisition, and (3) development of a high-performance disambiguation method.

<id>
cs/9812005v1
<category>
cs.CL
<abstract>
There exist several methods of calculating a similarity curve, or a sequence
of similarity values, representing the lexical cohesion of successive text
constituents, e.g., paragraphs. Methods for deciding the locations of fragment
boundaries are, however, scarce. We propose a fragmentation method based on
dynamic programming. The method is theoretically sound and guaranteed to
provide an optimal splitting on the basis of a similarity curve, a preferred
fragment length, and a cost function defined. The method is especially useful
when control on fragment size is of importance.

<id>
cs/9812018v1
<category>
cs.CL
<abstract>
In order to support the efficient development of NL generation systems, two
orthogonal methods are currently pursued with emphasis: (1) reusable, general,
and linguistically motivated surface realization components, and (2) simple,
task-oriented template-based techniques. In this paper we argue that, from an
application-oriented perspective, the benefits of both are still limited. In
order to improve this situation, we suggest and evaluate shallow generation
methods associated with increased flexibility. We advise a close connection
between domain-motivated and linguistic ontologies that supports the quick
adaptation to new tasks and domains, rather than the reuse of general
resources. Our method is especially designed for generating reports with
limited linguistic variations.

<id>
cs/9901005v1
<category>
cs.CL
<abstract>
Scheduling dialogs, during which people negotiate the times of appointments,
are common in everyday life. This paper reports the results of an in-depth
empirical investigation of resolving explicit temporal references in scheduling
dialogs. There are four phases of this work: data annotation and evaluation,
model development, system implementation and evaluation, and model evaluation
and analysis. The system and model were developed primarily on one set of data,
and then applied later to a much more complex data set, to assess the
generalizability of the model for the task being performed. Many different
types of empirical methods are applied to pinpoint the strengths and weaknesses
of the approach. Detailed annotation instructions were developed and an
intercoder reliability study was performed, showing that naive annotators can
reliably perform the targeted annotations. A fully automatic system has been
developed and evaluated on unseen test data, with good results on both data
sets. We adopt a pure realization of a recency-based focus model to identify
precisely when it is and is not adequate for the task being addressed. In
addition to system results, an in-depth evaluation of the model itself is
presented, based on detailed manual annotations. The results are that few
errors occur specifically due to the model of focus being used, and the set of
anaphoric relations defined in the model are low in ambiguity for both data
sets.

<id>
cs/9902001v1
<category>
cs.CL
<abstract>
Treebanks, such as the Penn Treebank (PTB), offer a simple approach to
obtaining a broad coverage grammar: one can simply read the grammar off the
parse trees in the treebank. While such a grammar is easy to obtain, a
square-root rate of growth of the rule set with corpus size suggests that the
derived grammar is far from complete and that much more treebanked text would
be required to obtain a complete grammar, if one exists at some limit. However,
we offer an alternative explanation in terms of the underspecification of
structures within the treebank. This hypothesis is explored by applying an
algorithm to compact the derived grammar by eliminating redundant rules --
rules whose right hand sides can be parsed by other rules. The size of the
resulting compacted grammar, which is significantly less than that of the full
treebank grammar, is shown to approach a limit. However, such a compacted
grammar does not yield very good performance figures. A version of the
compaction algorithm taking rule probabilities into account is proposed, which
is argued to be more linguistically motivated. Combined with simple
thresholding, this method can be used to give a 58% reduction in grammar size
without significant change in parsing performance, and can produce a 69%
reduction with some gain in recall, but a loss in precision.

<id>
cs/9902029v1
<category>
cs.CL
<abstract>
The paper argues that Fodor and Lepore are misguided in their attack on
Pustejovsky's Generative Lexicon, largely because their argument rests on a
traditional, but implausible and discredited, view of the lexicon on which it
is effectively empty of content, a view that stands in the long line of
explaining word meaning (a) by ostension and then (b) explaining it by means of
a vacuous symbol in a lexicon, often the word itself after typographic
transmogrification. (a) and (b) both share the wrong belief that to a word must
correspond a simple entity that is its meaning. I then turn to the semantic
rules that Pustejovsky uses and argue first that, although they have novel
features, they are in a well-established Artificial Intelligence tradition of
explaining meaning by reference to structures that mention other structures
assigned to words that may occur in close proximity to the first. It is argued
that Fodor and Lepore's view that there cannot be such rules is without
foundation, and indeed systems using such rules have proved their practical
worth in computational systems. Their justification descends from line of
argument, whose high points were probably Wittgenstein and Quine that meaning
is not to be understood by simple links to the world, ostensive or otherwise,
but by the relationship of whole cultural representational structures to each
other and to the world as a whole.

<id>
cs/9902030v1
<category>
cs.CL
<abstract>
This paper compares the tasks of part-of-speech (POS) tagging and
word-sense-tagging or disambiguation (WSD), and argues that the tasks are not
related by fineness of grain or anything like that, but are quite different
kinds of task, particularly becuase there is nothing in POS corresponding to
sense novelty. The paper also argues for the reintegration of sub-tasks that
are being separated for evaluation

<id>
cs/9903003v1
<category>
cs.CL
<abstract>
`Linguistic annotation' covers any descriptive or analytic notations applied
to raw language data. The basic data may be in the form of time functions --
audio, video and/or physiological recordings -- or it may be textual. The added
notations may include transcriptions of all sorts (from phonetic features to
discourse structures), part-of-speech and sense tagging, syntactic analysis,
`named entity' identification, co-reference annotation, and so on. While there
are several ongoing efforts to provide formats and tools for such annotations
and to publish annotated linguistic databases, the lack of widely accepted
standards is becoming a critical problem. Proposed standards, to the extent
they exist, have focussed on file formats. This paper focuses instead on the
logical structure of linguistic annotations. We survey a wide variety of
existing annotation formats and demonstrate a common conceptual core, the
annotation graph. This provides a formal framework for constructing,
maintaining and searching linguistic annotations, while remaining consistent
with many alternative data structures and file formats.

<id>
cs/9903008v1
<category>
cs.CL
<abstract>
Recent technological advances have made it possible to build real-time,
interactive spoken dialogue systems for a wide variety of applications.
However, when users do not respect the limitations of such systems, performance
typically degrades. Although users differ with respect to their knowledge of
system limitations, and although different dialogue strategies make system
limitations more apparent to users, most current systems do not try to improve
performance by adapting dialogue behavior to individual users. This paper
presents an empirical evaluation of TOOT, an adaptable spoken dialogue system
for retrieving train schedules on the web. We conduct an experiment in which 20
users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,
resulting in a corpus of 80 dialogues. The values for a wide range of
evaluation measures are then extracted from this corpus. Our results show that
adaptable TOOT generally outperforms non-adaptable TOOT, and that the utility
of adaptation depends on TOOT's initial dialogue strategies.

<id>
cs/9904008v1
<category>
cs.CL
<abstract>
Context sensitive rewrite rules have been widely used in several areas of
natural language processing, including syntax, morphology, phonology and speech
processing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various
algorithms to compile such rewrite rules into finite-state transducers. The
present paper extends this work by allowing a limited form of backreferencing
in such rules. The explicit use of backreferencing leads to more elegant and
general solutions.

<id>
cs/9904009v1
<category>
cs.CL
<abstract>
The two principal areas of natural language processing research in pragmatics
are belief modelling and speech act processing. Belief modelling is the
development of techniques to represent the mental attitudes of a dialogue
participant. The latter approach, speech act processing, based on speech act
theory, involves viewing dialogue in planning terms. Utterances in a dialogue
are modelled as steps in a plan where understanding an utterance involves
deriving the complete plan a speaker is attempting to achieve. However,
previous speech act based approaches have been limited by a reliance upon
relatively simplistic belief modelling techniques and their relationship to
planning and plan recognition. In particular, such techniques assume
precomputed nested belief structures. In this paper, we will present an
approach to speech act processing based on novel belief modelling techniques
where nested beliefs are propagated on demand.

